

## Papers for 2025-02-14

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU (Read more on [arXiv](https://arxiv.org/abs/2502.08910) or [HuggingFace](https://huggingface.co/papers/2502.08910))| Sung Ju Hwang, Losif63, geonp, gmlwns5176 | InfiniteHiP is a novel and practical large language model (LLM) inference framework designed for efficient and practical long-context utilization, extending context up to 3 million tokens on a single GPU. The research aims to address the challenges of slow inference speeds, increased memory costs, and generalization beyond training sequence lengths in LLMs. The key methodology involves dynamically eliminating irrelevant context tokens via a modular hierarchical token pruning algorithm, selectively applying RoPE adjustment methods, and offloading the key-value cache to host memory.  The framework achieves an 18.95x speedup in attention decoding for a 1 million token context without additional training, and enables processing of up to 3 million tokens on a single 48GB GPU. The main implication is enabling significantly longer context inference with pre-trained LLMs with improved inference speed, without requiring additional training or permanently sacrificing context. |
| Multi-Modal | Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation (Read more on [arXiv](https://arxiv.org/abs/2502.08690) or [HuggingFace](https://huggingface.co/papers/2502.08690))| Se Young Chun, Jae-sun Seo, Wongi Jeong, Agorium | This paper introduces Skrr, a novel method for compressing text encoders in text-to-image (T2I) diffusion models to improve memory efficiency. The main research objective is to reduce the memory consumption of text encoders in T2I models without significantly compromising performance. Skrr employs a two-phase approach: 'Skip,' which identifies and prunes redundant layers using a T2I diffusion-tailored discrepancy metric, and 'Re-use,' which recycles remaining layers to mitigate performance loss, supported by theoretical analysis. Experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, achieving state-of-the-art memory efficiency and improving GenEval scores by up to 20.4% at high sparsity over 40%. AI practitioners can utilize Skrr to deploy memory-efficient T2I models, making them more scalable and accessible without significant performance degradation. |
| Natural Language Processing | SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.09604) or [HuggingFace](https://huggingface.co/papers/2502.09604))| Hu Xu, Shannon Zejiang Shen, ZhaofengWu, bencw, voidism | SelfCite is a novel self-supervised framework designed to align large language models (LLMs) to generate high-quality, fine-grained, sentence-level citations for statements in their generated responses. The main research objective is to improve the accuracy and reliability of citations generated by LLMs without relying on costly and labor-intensive human annotations. SelfCite leverages context ablation to create a reward signal that evaluates the necessity and sufficiency of citations, using probability drop and probability hold metrics. The approach was demonstrated to increase citation F1 by up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks. This method enables AI practitioners to develop more verifiable and trustworthy LLM applications by improving citation quality with a self-supervised approach. |
| Multi-Modal | Exploring the Potential of Encoder-free Architectures in 3D LMMs (Read more on [arXiv](https://arxiv.org/abs/2502.09620) or [HuggingFace](https://huggingface.co/papers/2502.09620))| Ray Zhang, Zhuhao Wang, Zoey Guo, delinqu, IvanTang | This paper presents the first comprehensive investigation into encoder-free architectures for 3D Large Multimodal Models (LMMs). The main research question is whether encoder-free architectures can be effectively applied to 3D understanding scenarios, overcoming limitations of encoder-based 3D LMMs related to point cloud resolution and semantic discrepancies. The key methodology involves proposing LLM-embedded Semantic Encoding and Hierarchical Geometry Aggregation strategies to enable the LLM to assume the role of the 3D encoder. The proposed Encoder-free 3D LMM, ENEL, achieves 50.92% on the 3D object captioning task using GPT-4 score, on par with existing encoder-based models. This research suggests that encoder-free architectures are highly promising in 3D LMMs and can offer a scalable approach to 3D understanding. |
| Natural Language Processing | An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging (Read more on [arXiv](https://arxiv.org/abs/2502.09056) or [HuggingFace](https://huggingface.co/papers/2502.09056))| Kasima Tharnpipitchai, Potsawee Manakul, Kunat Pipatanakul, pittawat | This paper investigates data selection and model merging methods to enhance the reasoning capabilities of language-specific Large Language Models (LLMs), focusing on Thai. The main research objective is to improve the reasoning abilities of a Thai LLM to match that of a general reasoning model (DeepSeek R1) without sacrificing its Thai language proficiency. The methodology involves supervised fine-tuning using a translated reasoning dataset and ability-aware model merging, combining a language-specific model with a reasoning-focused model.  The proposed method, Typhoon2-R1-70B, achieved 76.5% average score across all tasks which boosts average across all tasks performance by 41.6% over Typhoon2 70B Instruct and by 12.8% over DeepSeek R1 70B Distill.  This approach offers a resource-efficient way for AI practitioners to enhance reasoning in low-resource language models. |
| Machine Learning | Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights (Read more on [arXiv](https://arxiv.org/abs/2502.09619) or [HuggingFace](https://huggingface.co/papers/2502.09619))| Yedid Hoshen, Or Nathan, Jonathan Kahana, Eliahu | This paper introduces ProbeLog, a novel method for retrieving pre-trained classification models capable of recognizing specific concepts without access to model metadata or training data. The main research objective is to enable efficient zero-shot model search based on model weights, addressing the limitations of current text-based model search methods. ProbeLog computes logit-level descriptors by observing model responses to a fixed set of input probes and enables text-based search using a text alignment model. The method achieves a top-1 retrieval accuracy of over 40% on ImageNet concepts, significantly outperforming random chance (0.1%). AI practitioners can utilize ProbeLog to search for suitable pre-trained models based on functional capability, potentially saving significant computational resources and improving model selection accuracy. |
| Natural Language Processing | CoSER: Coordinating LLM-Based Persona Simulation of Established Roles (Read more on [arXiv](https://arxiv.org/abs/2502.09082) or [HuggingFace](https://huggingface.co/papers/2502.09082))| Rui Xu, Xinfeng Yuan, Yifei Zhang, Heng Wang, Xintao Wang | This paper introduces CoSER, a framework for simulating established characters using large language models (LLMs). The main research objective is to address the challenges of simulating established characters due to the lack of authentic character datasets and nuanced evaluation methods. The methodology involves creating a high-quality dataset from 771 renowned books, developing open models (CoSER 8B and CoSER 70B) based on LLaMA-3.1, and introducing given-circumstance acting for training and evaluation. CoSER 70B achieves state-of-the-art performance, matching or surpassing GPT-40 on several benchmarks, with 75.80% and 93.47% accuracy on the InCharacter and LifeChoice benchmarks, respectively. AI practitioners can leverage CoSER's dataset, models, and evaluation protocols to develop and assess role-playing language agents that more accurately and authentically simulate established characters. |
| Computer Vision | TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models (Read more on [arXiv](https://arxiv.org/abs/2502.06608) or [HuggingFace](https://huggingface.co/papers/2502.06608))| Yuan Liang, Dehu Wang, Zexiang Liu, Zi-Xin Zou, Yangguang Li | TripoSG is a new 3D shape generation model that uses a large-scale rectified flow transformer to generate high-fidelity 3D meshes from input images. The research aims to establish an optimal paradigm for generating high-quality 3D models that accurately align with input image conditions. The key methodology includes a large-scale rectified flow transformer, a hybrid supervised training strategy for the 3D VAE, and a data processing pipeline that can produce 2 million high quality 3D samples. Experiments show TripoSG achieves state-of-the-art 3D shape generation, reflected by a Normal-FID of 3.36 using the largest model and training set. The implication for AI practitioners is a new, streamlined method for generating high-quality, production-ready 3D assets and that data quantity and quality are both critical for training 3D generative models. |
| Multi-Modal | EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents (Read more on [arXiv](https://arxiv.org/abs/2502.09560) or [HuggingFace](https://huggingface.co/papers/2502.09560))| Cheng Qian, Mark Zhao, Junyu Zhang, Rui Yang, Hanyang81 | This paper introduces EmbodiedBench, a comprehensive benchmark for evaluating multi-modal large language models (MLLMs) as vision-driven embodied agents. The main research objective is to assess the capabilities of MLLMs in performing tasks across various action levels and capabilities, including household tasks, navigation, and manipulation. The key methodology involves designing a unified agent framework and evaluating 13 proprietary and open-source MLLMs across four environments with hierarchical action levels and six capability-oriented subsets. Results show that while MLLMs excel at high-level tasks, they struggle with low-level manipulation; the best model, GPT-4o, scores only 28.9% on average, indicating the limitations of existing approaches. The study highlights the challenges and offers insights to advance MLLM-based embodied agent development, particularly in long-horizon planning and the effective integration of visual inputs. |
| Natural Language Processing | Logical Reasoning in Large Language Models: A Survey (Read more on [arXiv](https://arxiv.org/abs/2502.09100) or [HuggingFace](https://huggingface.co/papers/2502.09100))| Chaoli Zhang, Mengru Ding, Hanmeng Liu, ruoxining, HarryFu | This survey synthesizes the advancements and persistent challenges in logical reasoning for large language models (LLMs). The main objective is to comprehensively review the scope of logical reasoning in LLMs, including its theoretical foundations and the benchmarks used to evaluate reasoning proficiency. The key methodology involves analyzing existing capabilities across different reasoning paradigms (deductive, inductive, abductive, and analogical), examining data-centric, model-centric and neuro-symbolic approaches, and assessing strategies to enhance reasoning performance. The results highlight that while LLMs demonstrate impressive reasoning abilities, they struggle with robustness, generalization, and interpretability, with specific benchmarks like LogiQA and ReClor revealing limitations in performance compared to human levels (quantitative metrics unclear in provided text). For AI practitioners, this implies a need for continued research into hybrid architectures, improved evaluation frameworks, and scalable training techniques to enhance LLMs' logical reasoning capabilities for real-world applications. |
| Multi-Modal | MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency (Read more on [arXiv](https://arxiv.org/abs/2502.09621) or [HuggingFace](https://huggingface.co/papers/2502.09621))| Yu Qi, Yanwei Li, Ziyu Guo, Renrui Zhang, CaraJ | This paper introduces MME-CoT, a benchmark for evaluating Chain-of-Thought (CoT) reasoning in Large Multimodal Models (LMMs). The main research objective is to systematically assess the quality, robustness, and efficiency of CoT reasoning in LMMs across diverse visual domains. The methodology involves curating a dataset spanning six domains and proposing novel metrics to evaluate reasoning quality (recall and precision), robustness (stability and efficacy), and efficiency (relevance rate and reflection quality). Primary results indicate that while reflection mechanisms improve CoT quality (Kimi k1.5 achieving the highest F1-score of 64.2), CoT prompting often degrades performance on perception-heavy tasks, and many models demonstrate inefficiency in long reasoning processes. AI practitioners should be aware of these limitations and consider trade-offs when applying CoT reasoning to multimodal tasks, focusing on enhancing robustness and efficiency in future developments. |
| Natural Language Processing | Typhoon T1: An Open Thai Reasoning Model (Read more on [arXiv](https://arxiv.org/abs/2502.09042) or [HuggingFace](https://huggingface.co/papers/2502.09042))| Kunat Pipatanakul, Kasima Tharnpipitchai, Potsawee Manakul, pittawat | This paper introduces Typhoon T1, an open Thai reasoning model built on top of large language models (LLMs). The main objective is to develop a reasoning model capable of generating reasoning traces in a low-resource language, Thai, and to investigate effective approaches for training such models. The key methodology involves supervised fine-tuning using synthetic data generated through a transformation-and-refinement pipeline with few-shot prompting, and explores different reasoning formats including a novel 'structured thinking' approach. Primary results, measured across benchmarks like GSM8K and HumanEval+, show that structured thinking improved performance (e.g., 62.02 on GSM8K), and training with translated data enabled Thai reasoning. The main implication is that effective reasoning models can be developed in a cost-effective way without reinforcement learning, and small models can generate reasoning traces in low-resource languages through specific training strategies. |
| Natural Language Processing | CoT-Valve: Length-Compressible Chain-of-Thought Tuning (Read more on [arXiv](https://arxiv.org/abs/2502.09601) or [HuggingFace](https://huggingface.co/papers/2502.09601))| Xinchao Wang, Gongfan Fang, Runpeng Yu, Guangnian Wan, Xinyin Ma | This paper introduces CoT-Valve, a method for tuning language models to generate reasoning chains (Chain-of-Thought) of varying lengths. The research objective is to enable elastic control over the length of reasoning paths in models, reducing inference overhead dynamically based on task difficulty. The key methodology involves identifying a direction in the parameter space using LoRA that, when manipulated, controls CoT length, along with enhanced strategies like precise length-compressible tuning and progressive chain length compression. Experiments show CoT-Valve successfully controls and compresses chain length; for example, on GSM8K, it reduced reasoning chains from 741 to 225 tokens with a minor performance drop (95.07% to 94.92%). This allows AI practitioners to optimize reasoning models for efficiency by adjusting the verbosity of generated explanations based on the complexity of the task, potentially reducing computational cost without significant performance loss. |
| Natural Language Processing | SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.09390) or [HuggingFace](https://huggingface.co/papers/2502.09390))| Moshe Wasserblat, Gad Markovits, Moshe Berchansky, danf | This paper introduces SQuARE, a novel prompting technique designed to improve reasoning in Large Language Models (LLMs) through a self-interrogation paradigm. The main objective is to enhance chain-of-thought prompting by having the LLM generate and answer a series of sub-questions before addressing the main query. The key methodology involves systematically decomposing queries into multiple auxiliary questions, promoting a more in-depth exploration of the topic.  Evaluations on question-answering datasets using Llama 3 and GPT-4o show that SQuARE significantly surpasses traditional CoT prompts, with improvements like a 6.5% increase over Retrieval-Augmented Generation (RAG) on TriviaQA using Llama-3.2 3B. This approach suggests that AI practitioners can improve LLM reasoning capabilities in tasks by implementing structured, self-interrogative prompt strategies. |
| Multi-Modal | mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data (Read more on [arXiv](https://arxiv.org/abs/2502.08468) or [HuggingFace](https://huggingface.co/papers/2502.08468))| Ziliang Zhao, Yutao Zhu, Nan Yang, Liang Wang, Haon-Chen | This paper introduces mmE5, a multimodal multilingual embedding model designed to map data from different modalities into a unified representation space. The main objective is to improve embedding performance by addressing the scarcity of labeled multimodal data through high-quality synthetic data generation. The methodology involves synthesizing datasets that cover a wide range of tasks, modality combinations, and languages, using a deep thinking process within a single pass of a multimodal large language model, and incorporating real-world images with refined texts. mmE5 achieves state-of-the-art performance on the MMEB Benchmark with an overall score of 58.6 in a zero-shot setting, and 69.8 in supervised finetuning, using 45 times less training data than previous SOTA. The implication for AI practioners is that high-quality, diverse synthetic data can be created and used to address the scarcity of labelled multi-modal data, to improve model training. |
| Natural Language Processing | The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding (Read more on [arXiv](https://arxiv.org/abs/2502.08946) or [HuggingFace](https://huggingface.co/papers/2502.08946))| Shunchi Zhang, Tsz Ting Chung, Junjie Wu, Lemao Liu, Mo Yu | This paper investigates whether Large Language Models (LLMs) truly understand physical concepts, or if they are merely 'stochastic parrots' mimicking learned patterns. The main research objective is to quantitatively assess LLMs' understanding of physical concepts beyond simple memorization.  The authors introduce PHYSICO, a summative assessment task with grid-format inputs representing physical phenomena, and evaluate various LLMs using it.  The primary result shows a significant performance gap between LLMs and humans, with LLMs achieving around 40% lower accuracy than humans on high-level understanding subtasks, although doing very well on lower-level definitional tasks (over 95%). The main implication is that, despite strong performance on surface-level tasks, current LLMs lack deep understanding of physical concepts, which limits their ability to reason abstractly. |
| Reinforcement Learning | DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References (Read more on [arXiv](https://arxiv.org/abs/2502.09614) or [HuggingFace](https://huggingface.co/papers/2502.09614))| Li Yi, Yuzhe Qin, Qianwei Han, Jianibieke Adalibieke, Xueyi Liu | DexTrack is a novel neural tracking controller that enables a robotic hand to perform dexterous manipulation by following kinematic human-object interaction references. The research aims to develop a generalizable and robust controller capable of mimicking diverse manipulation skills from human demonstrations. DexTrack leverages a data flywheel approach, iteratively combining reinforcement learning, imitation learning, and homotopy optimization to mine high-quality tracking demonstrations and train the controller. The method achieves over a 10% improvement in success rates compared to leading baselines in both simulation and real-world evaluations. AI practitioners can adapt this approach to develop versatile controllers for complex robotic manipulation tasks, benefiting from the combination of human demonstrations and iterative learning. |
| Computer Vision | 3CAD: A Large-Scale Real-World 3C Product Dataset for Unsupervised Anomaly (Read more on [arXiv](https://arxiv.org/abs/2502.05761) or [HuggingFace](https://huggingface.co/papers/2502.05761))| Yuanwei Ma, Wenbo Guo, Hanyang Sun, Peng Xing, enquan2022 | This paper introduces 3CAD, a large-scale real-world dataset for unsupervised anomaly detection in 3C product manufacturing, and proposes a new detection framework. The research addresses the limitations of existing industrial anomaly detection datasets by providing a more realistic and challenging benchmark derived from actual production lines. The proposed Coarse-to-Fine detection paradigm with Recovery Guidance (CFRG) utilizes heterogeneous distillation for coarse localization and segmentation for fine localization, guided by recovery features. The CFRG framework achieves a P-AUROC of 93.4% and an AP of 17.6% on the 3CAD dataset, outperforming existing methods. The dataset and method can enable AI practioners to develop better models that address real-world application scenarios in industrial inspection, improving both accuracy and robustness. |
