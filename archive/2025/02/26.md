

## Papers for 2025-02-26

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference (Read more on [arXiv](https://arxiv.org/abs/2502.18411) or [HuggingFace](https://huggingface.co/papers/2502.18411))| Jiaqiwang, Weiyun1025, UniverseCA, ChrisDing1105, PhoenixZ | This paper introduces OmniAlign-V, a new dataset and benchmark for improving the alignment of Multi-Modal Large Language Models (MLLMs) with human preferences. The research aims to address the gap in human preference alignment in existing open-source MLLMs, which primarily focus on enhancing foundational capabilities. The methodology involves curating a dataset of 200K high-quality training samples with diverse images, complex questions, and varied response formats, and constructing a human-annotated benchmark, MM-AlignBench. Finetuning MLLMs with OmniAlign-V using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO) significantly enhances human preference alignment; for instance LLaVA-Next baseline with Qwen2.5-32B surpasses the state-of-the-art model Qwen2VL-72B. AI practitioners can leverage OmniAlign-V and MM-AlignBench to develop and evaluate MLLMs that better align with human values and conversational preferences, improving user experience. |
| Machine Learning | SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference (Read more on [arXiv](https://arxiv.org/abs/2502.18137) or [HuggingFace](https://huggingface.co/papers/2502.18137))| Haofeng Huang, surfingtomchen, hxi0408, Xiang-cd, jt-zhang | SpargeAttn is a universal sparse and quantized attention mechanism designed to accelerate inference across various models and tasks without loss of performance. The research aims to develop a training-free sparse attention operator that maintains accuracy while improving efficiency for any input. It achieves this through a two-stage online filter: an initial stage for rapid, accurate attention map prediction to skip matrix multiplications, and a subsequent softmax-aware filter to further reduce computations. Experimental results demonstrate that SpargeAttn achieves a 1.83x speedup on the Mochi model on an L40 GPU with no quality loss, and can be 2.5x to 5x faster than existing dense and sparse attention models. For AI practitioners, SpargeAttn offers a generalizable method to improve the inference speed of large models, like those for language, image, and video, without sacrificing accuracy by exploiting attention map sparsity. |
| Computer Vision | KV-Edit: Training-Free Image Editing for Precise Background Preservation (Read more on [arXiv](https://arxiv.org/abs/2502.17363) or [HuggingFace](https://huggingface.co/papers/2502.17363))| Yansong Tang, jewelshaw, shiyi0408, xilluill | KV-Edit is a training-free image editing method that achieves precise background preservation by leveraging KV cache in Diffusion Transformers (DiTs). The research addresses the challenge of maintaining background consistency during semantic image editing operations such as object addition, removal, and change. The core methodology involves preserving the key-value pairs of background tokens during the inversion and denoising process of a DiT, effectively reconstructing only the edited regions. Experiments on the PIE-Bench dataset show that KV-Edit achieves a Masked Region Preservation MSE of 4.69, significantly outperforming existing methods, while maintaining comparable image quality and superior text alignment. This approach offers AI practitioners a practical and efficient way to perform high-quality image editing with guaranteed background consistency without additional training. |
| Computer Vision | ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation (Read more on [arXiv](https://arxiv.org/abs/2502.18364) or [HuggingFace](https://huggingface.co/papers/2502.18364))| JianminBao, DongChen06, 131131yhx, 2JZ, yifanpu001 | This paper introduces the Anonymous Region Transformer (ART), a novel approach for generating variable multi-layer transparent images from a global text prompt and an anonymous region layout. The main research objective is to enable efficient and scalable generation of multi-layer images, allowing users to isolate, select, and edit specific image layers.  The key methodology utilizes an anonymous region layout, a layer-wise region crop mechanism, and a multi-layer transparent image autoencoder to directly encode and decode transparent layers.  The results show that ART is over 12 times faster than the full attention approach and achieves superior performance compared to existing methods like LayerDiffuse and COLE in user studies. AI practitioners can leverage ART for interactive content creation, graphic design, and digital art applications requiring precise control and scalable layer generation. |
| Reinforcement Learning | SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution (Read more on [arXiv](https://arxiv.org/abs/2502.18449) or [HuggingFace](https://huggingface.co/papers/2502.18449))| RishabhSingh021, gsynnaeve, lingming, JadeCopet, yuxiang630 | This paper introduces SWE-RL, a novel approach that leverages reinforcement learning (RL) to enhance the reasoning capabilities of large language models (LLMs) for real-world software engineering tasks. The main research objective is to improve LLMs' performance on software engineering tasks by training them on extensive open-source software evolution data. SWE-RL uses a lightweight rule-based reward (similarity score between generated and ground-truth solutions) and trains LLMs to recover developers' reasoning processes and solutions, leveraging code snapshots, code changes, issues, and pull requests. The resulting model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified, which is the best reported performance for medium-sized LLMs and comparable to proprietary LLMs like GPT-4o. This research demonstrates that RL, applied solely to software evolution data, can improve generalized reasoning skills, offering a new direction for enhancing LLM reasoning via software engineering data and it's beneficial to AI practioners focusing on software development. |
| Natural Language Processing | Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective (Read more on [arXiv](https://arxiv.org/abs/2502.17262) or [HuggingFace](https://huggingface.co/papers/2502.17262))| Chenggang Li, Xiao Li, shenke18, Lucky2022, JerryXu98 | This paper introduces a novel framework, Clustering-On-Difficulty (COD), for predicting the downstream performance of Large Language Models (LLMs). The main research objective is to accurately predict LLM performance on downstream tasks prior to model training, addressing the challenges of emergent abilities and uneven task difficulty distributions. The COD framework clusters tasks based on difficulty features, fits performance-compute curves within clusters, and maps aggregated predictions to the complete task set. Notably, COD achieves a mean absolute deviation of 1.36% across eight LLM evaluation benchmarks when predicting the performance of a 70B parameter model, significantly outperforming existing methods. The primary implication is that this approach can guide training resource allocation and assist in monitoring the training process of LLMs more effectively. |
| Natural Language Processing | Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.15499) or [HuggingFace](https://huggingface.co/papers/2502.15499))| Ya Wang, LLIXQ, xunzhou, Taoer, BryceZhuo | This paper introduces Scale-Distribution Decoupling (SDD), a novel approach for stabilizing and improving the training of Large Language Models (LLMs). The main research objective is to address training instabilities, such as gradient explosion and vanishing gradients, particularly in Post-Norm Transformer architectures. SDD explicitly decouples the scale and distribution of the weight matrix in fully-connected layers, applying normalization to regulate activations and a learnable scaling vector to maintain well-conditioned gradients. Experimental results show that SDD improves convergence; for instance, a 1B dense model achieves a training loss of 2.65 with SDD, compared to 2.70 for the baseline model. For AI practitioners, SDD offers a lightweight and effective solution to stabilize LLM training, applicable across various architectures, and enhances both convergence and generalization. |
| Computer Vision | K-LoRA: Unlocking Training-Free Fusion of Any Subject and Style LoRAs (Read more on [arXiv](https://arxiv.org/abs/2502.18461) or [HuggingFace](https://huggingface.co/papers/2502.18461))| Qibin Hou, Zhen Li, oyzh2005 | This paper introduces K-LoRA, a training-free method for fusing subject and style LoRAs (Low-Rank Adaptations) to generate images with combined characteristics. The research objective is to effectively merge learned subject and style information from different LoRAs without requiring additional training or manual parameter tuning, overcoming limitations of existing LoRA fusion methods. The proposed K-LoRA methodology uses a Top-K selection mechanism in each attention layer to determine which LoRA (subject or style) to prioritize for optimal fusion, balancing their contributions based on feature representativeness.  Experimentally, K-LoRA achieves a DINO score of 46.9% and CLIP score of 69.4% outperforming state-of-the-art training-based approaches in both qualitative and quantitative evaluations. AI practitioners can leverage K-LoRA for efficient and flexible image stylization by combining pre-trained LoRAs without extensive retraining. |
| Multi-Modal | WebGames: Challenging General-Purpose Web-Browsing AI Agents (Read more on [arXiv](https://arxiv.org/abs/2502.18356) or [HuggingFace](https://huggingface.co/papers/2502.18356))| Fraser, semitable, BiggieW, XanderJC, georgethomas | The paper introduces WebGames, a comprehensive benchmark suite for evaluating general-purpose web-browsing AI agents. The main research objective is to systematically assess the capabilities and limitations of current AI systems in interacting with web environments, compared to human performance. The methodology involves over 50 interactive challenges designed to test fundamental browser interactions, advanced input processing, cognitive tasks, workflow automation, and interactive entertainment, with a hermetic testing environment and verifiable ground-truth solutions. Results show that the best AI system (GPT-4o) achieved a 41.2% success rate, significantly lower than the human performance of 95.7%. The main implication is that there are significant remaining limitations in current AI systems' ability to interact with websites in the ways that humans easily can, highlighting the need for further research and providing a new robust benchmark. |
| Multi-Modal | Introducing Visual Perception Token into Multimodal Large Language Model (Read more on [arXiv](https://arxiv.org/abs/2502.17425) or [HuggingFace](https://huggingface.co/papers/2502.17425))| wxcTest, horseee, rp-yu | This paper introduces Visual Perception Tokens (VPT) to enhance Multimodal Large Language Models (MLLMs) by enabling them to autonomously control their visual perception processes. The research aims to overcome the limitation of MLLMs lacking control over visual perception, such as selective region review or object-specific focus.  The proposed method involves designing two types of VPTs: Region Selection Tokens, for cropping and re-encoding image regions, and Vision Re-Encoding Tokens, for incorporating additional vision features controlled by hidden states. Experiments demonstrate that VPTs significantly improve performance; on average, a 2B model with VPTs improved by 30.9% (from 0.572 to 0.749) and outperformed a 7B model by 20.0%. This approach allows AI practitioners to build MLLMs with improved spatial reasoning, fine-grained understanding, and overall visual perception capabilities. |
| Machine Learning | The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve? (Read more on [arXiv](https://arxiv.org/abs/2502.17535) or [HuggingFace](https://huggingface.co/papers/2502.17535))| Peijie Dong, Qian Wang, Xiang Liu, wenxinsiju, coolzhtang | This paper explores the compression of Large Language Models (LLMs) and proposes the "Lottery LLM Hypothesis," suggesting that smaller, compressed models can achieve comparable performance to larger models with the aid of external tools and reasoning. The main research question is what essential capabilities should be preserved in LLM compression to maintain performance when assisted by techniques like retrieval-augmented generation and multi-step reasoning. The authors review recent LLM advancements and propose a recursive multi-step reasoning algorithm (Algorithm 1) demonstrating how a smaller model can leverage external knowledge. Results show that smaller LLMs with techniques like PAL(Program-aided Language models) approach improves performance (eg. 72.0 on GSM8K vs 58.8. for a large model) compared to larger, uncompressed model. The main implication is that LLM compression methods should focus on preserving retrieval, reasoning, planning capabilities, and approximation of fundamental operations, rather than solely on perplexity or basic task accuracy. |
| Multi-Modal | AAD-LLM: Neural Attention-Driven Auditory Scene Understanding (Read more on [arXiv](https://arxiv.org/abs/2502.16794) or [HuggingFace](https://huggingface.co/papers/2502.16794))| Ashesh Mehta, Stephan Bickel, vchoudhari, susameddin, xi-j | This paper introduces AAD-LLM, a prototype system that integrates brain signals with an auditory large language model (LLM) for intention-informed auditory scene understanding. The main objective is to align machine listening with human perception by incorporating listener intent, specifically selective auditory attention, into auditory AI systems. The methodology involves decoding the attended speaker from intracranial electroencephalography (iEEG) recordings and conditioning the LLM's response generation on this inferred attentional state. The results show improved alignment with listener intention across multiple auditory tasks, with AAD-LLM achieving a word error rate (WER) of 14.4 and a ROUGE-L score of 58.3 for summarization in a multi-talker scenario. The main implication is that AI practitioners can develop listener-centered auditory systems that process sound according to human cognitive and perceptual priorities, offering potential benefits for assistive hearing technologies and human-computer interaction. |
| Multi-Modal | Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI (Read more on [arXiv](https://arxiv.org/abs/2502.17092) or [HuggingFace](https://huggingface.co/papers/2502.17092))| KartikAngadi, kruthika, SyedAbdul | Shakti-VLM is a family of vision-language models designed for enterprise AI, focusing on efficiency and scalability in multimodal learning. The research aims to achieve competitive performance with fewer training tokens by leveraging architectural innovations, rather than relying on sheer data volume. Key advancements include QK-Normalization, hybrid normalization strategies, enhanced positional encoding, and a three-stage training process. Evaluations show that Shakti-VLM-4B achieves an MMMU score of 59.78%, surpassing other models like Qwen2VL-7B. AI practitioners can achieve high performance in vision-language tasks through efficient model design, reducing the need for extensive training data. |
