

## Papers for 2025-02-10

| Title | Authors | Summary |
|-------|---------|---------|
| VideoRoPE: What Makes for Good Video Rotary Position Embedding? (Read more on [arXiv](https://arxiv.org/abs/2502.05173) or [HuggingFace](https://huggingface.co/papers/2502.05173))| Pan Zhang, Xiaoyi Dong, Xilin Wei, yuhangzang, LiuXR | VideoRoPE introduces a novel rotary position embedding method for video that outperforms existing methods by addressing key spatio-temporal characteristics. The main research objective is to identify and address the limitations of existing Rotary Position Embedding (RoPE) methods when applied to video data with complex spatio-temporal structures. The key methodology involves analyzing four essential characteristics for effective RoPE adaptation to video (2D/3D structure, frequency allocation, spatial symmetry, temporal index scaling), and proposing VideoRoPE, which incorporates a 3D structure, low-frequency temporal allocation, diagonal layout, and adjustable temporal spacing. The primary result is that VideoRoPE outperforms previous RoPE variants on benchmarks such as long video retrieval, where VideoROPE showed a +12.4 improvement on V-NIAH and on V-NIAH-D. The principal implication is that AI practitioners should consider using VideoRoPE to improve long-range dependency modeling and robustness in Video Large Language Models.  |
| Fast Video Generation with Sliding Tile Attention (Read more on [arXiv](https://arxiv.org/abs/2502.04507) or [HuggingFace](https://huggingface.co/papers/2502.04507))| Ion Stoica, Hangliang Ding, Runlong Su, Peiyuan Zhang, BrianChen1129 | Sliding Tile Attention (STA) accelerates video diffusion models by efficiently computing attention within local spatiotemporal windows. The main research objective is to reduce the computational cost of 3D attention in diffusion transformers (DiTs) for video generation without significant quality degradation. The key methodology is introducing tile-based sliding window attention (STA), which operates on contiguous token groups (tiles) with a hardware-aware design, and optimizing attention at the kernel level. STA reduces end-to-end latency in a 720p 5-second video generation from 945 seconds (using FlashAttention-3) to 685 seconds with no quality reduction without training, and 268 seconds after finetuning with 0.09% VBench score drop. AI practitioners can use STA to significantly speed up video generation in diffusion models, achieving over 10x faster attention computation and up to 3.53x end-to-end speedup.  |
| AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting (Read more on [arXiv](https://arxiv.org/abs/2502.05176) or [HuggingFace](https://huggingface.co/papers/2502.05176))| Jie-Ying Lee, Ying-Huan Chen, Yang-Jung Chen, Chung-Ho Wu, cmhungsteve | AuraFusion360 is a reference-based method for 360° unbounded scene inpainting that enables high-quality object removal and hole-filling in 3D scenes represented by Gaussian Splatting. The main research objective is to address the challenges of view consistency and geometric accuracy in 360° unbounded scene inpainting, particularly for object removal. The key methodology involves depth-aware unseen mask generation, Adaptive Guided Depth Diffusion (AGDD) for initial point placement, and SDEdit-based detail enhancement for multi-view coherence. The method achieves a PSNR of 17.661 and LPIPS of 0.388 on the 360-USID dataset, outperforming existing methods. AI practitioners can utilize this framework for improved 3D scene editing, enabling higher-quality object removal and inpainting in 360° unbounded environments with better multi-view consistency.  |
| Goku: Flow Based Video Generative Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2502.04896) or [HuggingFace](https://huggingface.co/papers/2502.04896))| Fengda Zhu, Yida Zhang, Yuqi Zhang, Chongjian Ge, ShoufaChen | Goku is a state-of-the-art family of joint image-and-video generation models based on rectified flow Transformers. The main research objective is to develop a unified framework for high-quality image and video generation with industry-leading performance. The key methodology involves a 3D joint image-video variational autoencoder (VAE) for latent space representation, a Transformer architecture with full attention, rectified flow formulation, and a multi-stage training strategy with data curation and infrastructure optimizations. Goku achieves 0.76 on GenEval for text-to-image generation and 84.85 on VBench for text-to-video tasks, demonstrating superior performance. The principal implication for AI practitioners is that Goku provides an effective architecture and training approach that can advance state-of-the-art performance in developing and deploying high-performance joint image-and-video generation models.  |
| QuEST: Stable Training of LLMs with 1-Bit Weights and Activations (Read more on [arXiv](https://arxiv.org/abs/2502.05003) or [HuggingFace](https://huggingface.co/papers/2502.05003))| Jiale Chen, d-alistarh, mnikdan97, soroushtabesh, BlackSamorez | QuEST enables stable training of large language models (LLMs) with weights and activations quantized to as low as 1-bit. The main research objective is to determine whether quantization-aware training (QAT) can produce accurate LLMs with low-bitwidth weights and activations, specifically pushing beyond the previously identified 8-bit limit. The key methodology involves a new QAT method called QuEST, which uses Hadamard normalization and MSE-optimal fitting for quantization, and a trust gradient estimator that minimizes the difference between the full-precision gradient and the quantized gradient. Primary results show that QuEST allows stable training of Llama-family models with 1-bit weights and activations and that QuEST with 4-bit weights and activations (W4A4) is Pareto-dominant relative to BF16, achieving lower loss at lower model size on the C4 dataset. The principal implication is that AI practitioners can train accurate, compressed LLMs at significantly lower precision (4-bit or even 1-bit) than previously thought possible, improving inference efficiency.  |
| DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails (Read more on [arXiv](https://arxiv.org/abs/2502.05163) or [HuggingFace](https://huggingface.co/papers/2502.05163))| Bo Li, Wei Wang, Junkai Zhang, Yu Yang, ydeng9 | i) The paper introduces DuoGuard, a two-player reinforcement learning framework for training multilingual LLM guardrails. ii) The research aims to develop a self-improving system to generate high-quality synthetic data for multilingual guardrail training, addressing the scarcity of non-English safety data. iii) The methodology uses a two-player RL framework with a generator and a guardrail model that co-evolve adversarially, formalized as a minimax game with convergence to a Nash equilibrium. iv) Empirical evaluations show that DuoGuard outperforms state-of-the-art models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English benchmarks while being 4.5x faster at inference with a significantly smaller model (0.5B). v) The key implication is that this scalable and efficient synthetic data generation approach can improve multilingual guardrail models, enhancing LLM safety, especially in resource-scarce languages; code, model, and data are expected to be open-sourced.  |
| Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach (Read more on [arXiv](https://arxiv.org/abs/2502.05171) or [HuggingFace](https://huggingface.co/papers/2502.05171))| Siddharth Singh, John Kirchenbauer, Neel Jain, Sean McLeish, Jonas Geiping | This paper introduces a language model architecture that scales test-time computation by using a recurrent block to enable latent reasoning. The main research objective is to develop a language model architecture capable of improving its performance by scaling computation at test time without relying on chain-of-thought prompting. The key methodology involves training a transformer model with a recurrent block that iterates a variable number of times during training and inference, modifying a latent state. Primary results include the demonstration of 3.5 billion parameter model with 800 billion tokens, showing consistent accuracy improvement on the GSM8k benchmark, reaching 34.80% strict accuracy with 32 recurrent steps. The principal implication for AI practitioners is that recurrent-depth models offer a viable path to increasing model performance at test time, and providing a more parameter-efficient alternative to traditional scaling methods.  |
| Agency Is Frame-Dependent (Read more on [arXiv](https://arxiv.org/abs/2502.04403) or [HuggingFace](https://huggingface.co/papers/2502.04403))| Shi Dong, Will Dabney, Michael Bowling, André Barreto, David Abel | Agency, a system's capacity to steer outcomes toward a goal, is fundamentally dependent on a chosen reference frame. The main research question is whether agency is an invariant, measurable property of an input-output system or if it varies depending on other independent commitments. The key methodology is a philosophical argument, supported by examples from reinforcement learning (RL), demonstrating that the four essential properties of agency (individuality, source of action, normativity, and adaptivity) are relative to a chosen reference frame. A primary result is that Proposition 10 by Jiang (2019) illustrates that many critical quantities of an RL agent, including the optimal policy or Bellman error, are dependent on the agent's boundary, a component of the reference frame. The principle implication for AI practitioners is that any basic science of agency requires acknowledgement of frame-dependence, meaning determinations of agency must be made in reference to commitments, such as an arbitrarily selected boundary, of a given frame.  |
| FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation (Read more on [arXiv](https://arxiv.org/abs/2502.05179) or [HuggingFace](https://huggingface.co/papers/2502.05179))| Peize Sun, Chongjian Ge, Wenbo Li, Shilong Zhang, ShoufaChen | FlashVideo is a two-stage diffusion model framework for efficient high-resolution text-to-video generation that prioritizes prompt fidelity and visual quality. The main research objective is to address the high computational demands of generating high-resolution videos with diffusion models while maintaining high prompt fidelity and visual detail. The key methodology involves a two-stage cascade: a low-resolution stage (270p) using a 5-billion parameter DiT for content and motion generation, followed by a high-resolution stage (1080p) using a 2-billion parameter DiT and flow matching for detail enhancement with minimal function evaluations. The primary result is that FlashVideo achieves a VBench-Long score of 82.99 and can generate 1080p videos in 102.3 seconds of function evaluation time, which is significantly faster than the baseline model and the vanilla cascade. For AI practitioners, the two stage design enables users to first preview the output before full resolution generation, significantly curtailing computational costs and waiting times.  |
| Linear Correlation in LM's Compositional Generalization and Hallucination (Read more on [arXiv](https://arxiv.org/abs/2502.04520) or [HuggingFace](https://huggingface.co/papers/2502.04520))| Chengyu Dong, Shibo Hao, Chenyang An, Letian Peng, shangjingbo | Language models (LMs) exhibit linear correlations between logits of related knowledge, which is resilient to training and affects compositional generalization/hallucination. The main research question is how linear correlations manifest in LMs during knowledge composition and affect their generalization ability. The key methodology involves fitting linear transformations between logits of related knowledge pairs (e.g., City-Country) and evaluating their resilience, precision, and impact on generalization after fine-tuning. A primary result is that City-Country knowledge pairs show a high logit correlation of 0.89 and gradient correlation of 0.79, with the fitted linear transformation (W) having a precision of 0.42 for top-1 influenced target predictions. The principal implication is that AI practitioners can potentially use such linear correlations as indicators of LM generalization and diagnose potential compositional hallucinations before they occur.  |
| Generating Symbolic World Models via Test-time Scaling of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.04728) or [HuggingFace](https://huggingface.co/papers/2502.04728))| Fuxiang Frank Xia, Tim Z. Xiao, Yuhuan Yuan, Zhouliang Yu, zhangysk | This paper introduces a method for generating symbolic world models using large language models (LLMs) by scaling computation at test time. The main research question is how to effectively generate PDDL-based world models without requiring model finetuning. The key methodology combines Best-of-N (BoN) sampling for initialization with instance Verbalized Machine Learning (iVML) for iterative refinement of Planning Domain Definition Language (PDDL) domains. The method achieved an 86.2% success rate on the NL2Domain task using Qwen2.5-Instruct (32B), significantly outperforming a baseline BoN-8 approach. The principal implication for AI practitioners is that test-time computation scaling can enhance LLMs' formal reasoning and planning capabilities, enabling more robust PDDL domain generation without additional model training.  |
| CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference (Read more on [arXiv](https://arxiv.org/abs/2502.04416) or [HuggingFace](https://huggingface.co/papers/2502.04416))| Wulong Liu, Xianzhi Yu, Hui-Ling Zhen, Lancheng Zou, Eleven-P | CMoE is a framework that efficiently converts dense Large Language Models (LLMs) into sparse Mixture-of-Experts (MoE) models for improved inference efficiency. The main research objective is to develop a method to transform dense LLMs into MoE architectures without extensive retraining, thereby reducing computational overhead. The key methodology involves strategically grouping neurons in Feed-Forward Networks (FFNs) into shared and routed experts based on activation rates and constructing a differentiable routing mechanism analytically from activation statistics, followed by optional lightweight adaptation. Primary results show that CMoE, with a 25% activation ratio, can maintain reasonable perplexity without fine-tuning, and achieves 76.59% of the accuracy of a dense model on the SciQ dataset after lightweight fine-tuning. AI practitioners can use CMoE to deploy LLMs in resource-constrained environments by efficiently creating sparse MoE models with minimal performance loss and reduced computational costs compared to traditional MoE training approaches.  |
| On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices (Read more on [arXiv](https://arxiv.org/abs/2502.04363) or [HuggingFace](https://huggingface.co/papers/2502.04363))| Yeojin Lee, Jungmin Cheon, Isu Jeong, Kyuhwan Lee, Bosung Kim | On-device Sora enables efficient diffusion-based text-to-video generation on mobile devices. The research aims to address the computational and memory challenges of running diffusion-based video generation models on resource-constrained mobile devices. The key methodologies are Linear Proportional Leap (LPL) to reduce denoising steps, Temporal Dimension Token Merging (TDTM) to minimize token processing, and Concurrent Inference with Dynamic Loading (CI-DL) to optimize memory usage. On-device Sora achieves video quality comparable to Open-Sora on high-end GPUs, while LPL accelerates video generation up to 1.94x on an iPhone 15 Pro. AI practitioners can deploy high-quality video generation models on mobile devices, expanding accessibility and enabling new applications without relying on cloud infrastructure.  |
| Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.04404) or [HuggingFace](https://huggingface.co/papers/2502.04404))| Jie-Jing Shao, Ding-Chu Zhang, Wen-Da Wei, Xuan-Yi Zhu, yangxw | This paper introduces a self-backtracking mechanism that enables language models to improve reasoning by learning to autonomously backtrack during training and inference. The main research objective is to address the limitations of current slow-thinking language models, specifically inefficient overthinking and over-reliance on auxiliary reward models. The key methodology involves training the model to recognize suboptimal reasoning paths and backtrack to earlier states, utilizing this learned capability for dynamic search during inference, and using expert iteration for self-improvement. The proposed method achieved over 40% performance gain in accuracy on the Countdown task compared to optimal-path supervised fine-tuning. For AI practitioners, internalizing the backtracking process within LLMs allows to mitigate overthinking and reduce dependencies on external reward models.  |
| CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance (Read more on [arXiv](https://arxiv.org/abs/2502.04350) or [HuggingFace](https://huggingface.co/papers/2502.04350))| Chuchu Fan, Yang Zhang, Yueying Liu, Yilun Hao, Yongchao Chen | CodeSteer guides large language models (LLMs) to effectively switch between textual reasoning and code generation for symbolic computing tasks. The main research objective is to develop a method that effectively steers LLMs between textual reasoning and code generation to improve symbolic computing capabilities. The key methodology involves fine-tuning a Llama-3-8B model, named CodeSteerLLM, with multi-round supervised fine-tuning (SFT) and direct preference optimization (DPO), along with symbolic and self-answer checkers. The primary result is that augmenting GPT-4o with CodeSteer raised its average performance score on the SymBench benchmark from 53.3 to 86.4. AI practitioners can leverage CodeSteer to enhance the performance of LLMs on tasks requiring symbolic computation, benefiting from improved accuracy and robustness compared to text-only or code-only approaches.  |
| ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning (Read more on [arXiv](https://arxiv.org/abs/2502.04689) or [HuggingFace](https://huggingface.co/papers/2502.04689))| Giuseppe Carenini, yuweiyin | ARR prompting improves large language model (LLM) performance on question-answering (QA) tasks by guiding the models to analyze question intent, retrieve information, and reason step-by-step. The main research objective is to evaluate a refined zero-shot prompting method, ARR, that explicitly incorporates analyzing, retrieving, and reasoning steps to enhance LLM performance in QA. The key methodology is zero-shot prompting of open-weight LLMs, comparing the proposed ARR method (using the trigger sentence "Let's analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning.") against a Baseline (no specific trigger) and zero-shot Chain-of-Thought (CoT) prompting. Primary results show ARR consistently outperforms Baseline and CoT methods across 10 diverse QA datasets; specifically, ARR achieved an average accuracy of 69.58% using LLaMA3-8B-Chat, compared to 65.48% for Baseline and 68.14% for CoT. For AI practitioners, ARR provides an intuitive, general, and immediately applicable prompting strategy to improve accuracy in a variety of QA tasks.  |
| Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More (Read more on [arXiv](https://arxiv.org/abs/2502.03738) or [HuggingFace](https://huggingface.co/papers/2502.03738))| Yuyin Zhou, Wei Shao, Guoyizhe Wei, Yaodong Yu, Feng Wang | This paper investigates the impact of patchification, an image tokenization method, on the performance of vision models. The main research objective is to examine how compressive encoding through patchification affects visual representations and determine if patch size can be a new scaling dimension. The key methodology involves conducting scaling experiments by systematically reducing the patch size in Vision Transformer (ViT) and Mamba-based architectures across various vision tasks and input scales. Primary results show that model performance consistently improves with decreasing patch sizes, achieving 84.6% test accuracy on ImageNet-1k with a base-sized model using a 1x1 patch size (50,176 tokens). The principal implication for AI practitioners is that reducing the patch size in visual models can unlock performance improvements.  |
| YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing Multi-Objective Optimization based DPO for Text-to-Image Alignment (Read more on [arXiv](https://arxiv.org/abs/2502.03512) or [HuggingFace](https://huggingface.co/papers/2502.03512))| Vinija Jain, Gurpreet Singh, Yaswanth Narsupalli, Amitava Das, amanchadha | YinYangAlign introduces a benchmark and framework for evaluating and optimizing text-to-image (T2I) models across six contradictory alignment objectives using multi-objective optimization.  The main research question is how to effectively balance multiple, often conflicting, alignment objectives (e.g., faithfulness to prompt vs. artistic freedom) in T2I models.  The key methodology used is Contradictory Alignment Optimization (CAO), a novel extension of Direct Preference Optimization (DPO), which incorporates per-axiom loss design, multi-objective optimization techniques (including synergy-driven global preferences and axiom-specific regularization), and the Sinkhorn-regularized Wasserstein Distance.  Primary results indicate training DPO on a single axiom, like Artistic Freedom improved it by 40%, but decreased Cultural Sensitivity (-30%) and Verifiability (-35%). CAO provided a more balanced alignment across conflicting objectives.  Principal implication: AI practitioners can utilize the YinYangAlign benchmark and CAO framework to develop T2I models that better navigate inherent trade-offs between competing objectives, leading to more robust and ethically aligned image generation.  |
| QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation (Read more on [arXiv](https://arxiv.org/abs/2502.05178) or [HuggingFace](https://huggingface.co/papers/2502.05178))| Yuke Zhu, Linxi Fan, Scott Reed, Fuzhao Xue, zhaoyue-zephyrus | QLIP is a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding. The main research objective is to develop a visual tokenizer that excels at both image reconstruction and semantic understanding for unified multimodal learning. The key methodology involves training a Binary Spherical Quantization (BSQ)-based autoencoder with a combined reconstruction and contrastive language-image alignment objective, using a two-stage training process and automated loss weighting. QLIP achieves a zero-shot classification accuracy of 74.3% on ImageNet, comparable to CLIP, while maintaining a reconstruction FID of 3.21. For AI practitioners, QLIP offers a visual tokenizer that can serve as a drop-in replacement in vision-language models and text-to-image generation models, achieving comparable or better performance.  |
| MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf (Read more on [arXiv](https://arxiv.org/abs/2502.04376) or [HuggingFace](https://huggingface.co/papers/2502.04376))| Qingwei Lin, Jue Zhang, Xiaoting Qin, Shurun Yuan, Lingxiang Hu | This paper introduces a prototype LLM-powered meeting delegate system and a benchmark to evaluate LLMs' ability to participate in meetings as a user representative. The main research question is whether LLMs can effectively delegate participants in meetings. The methodology involves developing a prototype system and a benchmark dataset from real meeting transcripts, then evaluating various LLMs using metrics like response rate, silence rate, recall, and attribution. A key finding is that approximately 60% of LLM-generated responses address at least one key point from the ground-truth, indicating promise, yet improvements are required. The principal implication is that AI practitioners can utilize LLMs to build meeting assistants, but need to address challenges related to transcription errors, irrelevant or repetitive responses, in real time.  |
