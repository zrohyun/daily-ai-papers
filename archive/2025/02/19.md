

## Papers for 2025-02-19

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Soundwave: Less is More for Speech-Text Alignment in LLMs (Read more on [arXiv](https://arxiv.org/abs/2502.12900) or [HuggingFace](https://huggingface.co/papers/2502.12900))| Benyou, PhoenixAxis, FanBuCUHK, puccho, Yoohao | Soundwave is a new speech large language model (LLM) that achieves state-of-the-art performance with significantly less training data by addressing the representation space gap and sequence length inconsistency between speech and text. The main objective is to develop a data-efficient training strategy for speech LLMs. The key methodology is a two-stage training framework: the first stage resolves the representation space gap using an alignment adapter and CTC loss, and the second stage reduces the sequence length of speech using a shrinking adapter. Soundwave outperforms Qwen2-Audio in speech translation and AIR-Bench speech tasks, using only one-fiftieth of the training data, and achieves an average score of 75.5 on AIR-Bench speech foundation tasks. AI practitioners can leverage Soundwave's efficient training framework to build high-performing speech LLMs with reduced data requirements. |
| Multi-Modal | Phantom: Subject-consistent video generation via cross-modal alignment (Read more on [arXiv](https://arxiv.org/abs/2502.11079) or [HuggingFace](https://huggingface.co/papers/2502.11079))| Jiawei Liu, ZhuoweiChen, lbc402, Grayson111, liulj13 | This paper introduces Phantom, a framework for subject-consistent video generation that aligns both textual and visual content from reference images. The main research objective is to achieve balanced dual-modal prompting of text and image, leading to deeply and simultaneously aligned text and visual content in generated videos. The methodology is built upon existing text-to-video and image-to-video architectures, redesigning the joint text-image injection model and using text-image-video triplet data for cross-modal alignment learning, with specific handling of in-paired and cross-paired data. Phantom achieves a 67.6% preference rate in user studies for multi-subject consistency, outperforming other evaluated methods, with particular strengths shown also in identity consistency. AI practitioners can utilize Phantom for improved subject-consistent video generation, especially in scenarios requiring specific visual element preservation combined with textual instructions. |
| Natural Language Processing | Continuous Diffusion Model for Language Modeling (Read more on [arXiv](https://arxiv.org/abs/2502.11564) or [HuggingFace](https://huggingface.co/papers/2502.11564))| Sung Ju Hwang, harryjo97 | This paper introduces the Riemannian Diffusion Language Model (RDLM), a continuous diffusion framework for language modeling that incorporates the geometry of the statistical manifold. The main research objective is to develop a coherent diffusion framework for discrete data by establishing a link between discrete and continuous diffusion, and thereby improve generative performance. The key methodology involves parameterizing discrete data on a hypersphere, designing diffusion processes that generalize previous discrete diffusion models, and applying a simulation-free training method. Primary results show that RDLM outperforms existing discrete diffusion models and approaches the performance of autoregressive models on language modeling benchmarks, achieving a Bits Per Character (BPC) of 1.32 on the Text8 dataset, and a Perplexity (PPL) of 29.72 on the LM1B dataset. The main implication for AI practitioners is that leveraging continuous diffusion models with geometric considerations can improve the generative modeling of discrete data, especially in language modeling. |
| Natural Language Processing | Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity (Read more on [arXiv](https://arxiv.org/abs/2502.13063) or [HuggingFace](https://huggingface.co/papers/2502.13063))| Aydar Bulatov, Mikhail Arkhipov, mbur, yurakuratov | This paper explores the limits of compressing text sequences into a small set of dense vectors within large language models (LLMs). The main research objective is to quantify the maximum information capacity of input embeddings and how much text can be encoded and decoded from a single vector. The key methodology involves replacing the encoder with a per-sample optimization procedure, using trainable memory vectors prepended to the input text and optimizing them via next-token prediction loss. The primary result shows that a single input vector can enable the reconstruction of up to 1568 tokens with the Llama-3.1-8B model, achieving lossless compression far beyond previous efforts (previous lossless methods were limited to a factor of 10). This research suggests that input embeddings in current LLMs are underutilized, highlighting a substantial gap between theoretical capacity and practical utilization and implying that there's significant room for optimization in model design. |
| Natural Language Processing | SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.12464) or [HuggingFace](https://huggingface.co/papers/2502.12464))| Minki Kang, Dong Bok Lee, hbseong, dwgnr, Seanie-lee | This paper introduces SafeRoute, an adaptive model selection method for deploying safety guardrails in large language models (LLMs) that improves the trade-off between computational cost and safety performance. The research objective is to reduce the computational overhead of safety guard models while maintaining high accuracy in detecting harmful prompts. SafeRoute trains a binary router to distinguish between "hard" and "easy" examples, selectively applying a larger, more accurate safety guard model only to hard examples and a smaller, more efficient model to easy ones. Experimental results show that SafeRoute improves the F1 score by 13% and 10% compared to using only smaller or larger models alone, while only using the large model 5.09% of the time on the WildGuardMix dataset. This approach allows AI practitioners to deploy safer LLMs more efficiently by dynamically selecting the appropriate safety guard model based on input complexity. |
| Machine Learning | Rethinking Diverse Human Preference Learning through Principal Component Analysis (Read more on [arXiv](https://arxiv.org/abs/2502.13131) or [HuggingFace](https://huggingface.co/papers/2502.13131))| Hao Sun, Feng Luo, huanzhang12, CharlesDDDD, Ray2333 | This paper introduces Decomposed Reward Models (DRMs), a novel framework for extracting diverse human preferences from binary comparisons to improve foundation models and build personalized AI systems. The research question asks if multidimensional human preferences can be inferred directly from large-scale binary comparisons. DRMs represent human preferences as vectors and analyze them using Principal Component Analysis (PCA), identifying orthogonal basis vectors that capture distinct preference aspects. DRMs outperform baseline methods, improving the single-head baseline accuracy from 0.733 to 0.814 on RewardBench when using Gemma-2B-RM. DRMs offer a scalable and interpretable alternative to traditional reward models for personalized and interpretable LLM alignment. |
| Multi-Modal | SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation (Read more on [arXiv](https://arxiv.org/abs/2502.13143) or [HuggingFace](https://huggingface.co/papers/2502.13143))| codered010, RunpeiDong, YufeiD, WenyaoZhang, qizekun | This paper introduces SOFAR, a system that integrates semantic orientation into spatial reasoning and robotic manipulation to enhance the understanding and interaction of embodied AI with their environments. The research aims to address the limitation of current vision-language models (VLMs) in understanding object orientations, which is crucial for fine-grained manipulation tasks. SOFAR employs a novel concept of semantic orientation and constructs a large-scale dataset, OrienText300K, to support this; it integrates semantic orientation into a VLM system to generate manipulation actions with both positional and orientational constraints. Experiments in simulation and the real world demonstrate that SOFAR significantly enhances robotic manipulation capabilities, achieving 48.7% accuracy on Open6DOR and 74.9% accuracy on SIMPLER. The main implication is that AI practitioners can leverage semantic orientation for more flexible and accurate instruction-following robotic systems, improving performance in tasks requiring precise object alignment and rearrangement. |
| Multi-Modal | Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation (Read more on [arXiv](https://arxiv.org/abs/2502.13145) or [HuggingFace](https://huggingface.co/papers/2502.13145))| Qian Zhang, wenyuliu, wondervictor, HongyuanTao, LegendBC | This paper introduces mmMamba, a framework for developing linear-complexity, native multimodal state space models by distilling knowledge from existing Multimodal Large Language Models (MLLMs). The main research objective is to create efficient decoder-only multimodal models that overcome the quadratic computational complexity of Transformer-based MLLMs. The key methodology involves a three-stage distillation process, transferring knowledge from a trained Transformer-based decoder-only VLM (HoVLE) to Mamba-2 layers, and supports flexible hybrid architectures combining Transformer and Mamba layers. At 103K tokens, mmMamba-linear demonstrates a 20.6x speedup compared to HoVLE and saves 75.8% GPU memory. This framework allows AI practitioners to deploy efficient and scalable multimodal models with customizable performance-efficiency trade-offs, particularly beneficial for long-context processing. |
| Reinforcement Learning | FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading (Read more on [arXiv](https://arxiv.org/abs/2502.11433) or [HuggingFace](https://huggingface.co/papers/2502.11433))| ShirleyY, Acatsama, YupengCao, zdeng10, xionggj001 | This paper introduces FLAG-TRADER, a novel framework that integrates Large Language Models (LLMs) with reinforcement learning (RL) for financial trading. The research question explores whether an architecture that seamlessly integrates LLMs' reasoning capabilities with RL's reward-driven optimization can tackle the challenges of sequential decision-making in finance.  FLAG-TRADER uses a partially fine-tuned LLM as a policy network within a gradient-driven RL framework, encoding market data and textual information to drive trading decisions.  Empirical results demonstrate that FLAG-TRADER, even with a small-scale (135M parameter) LLM, outperforms larger proprietary models and baseline strategies like buy-and-hold in terms of cumulative return and Sharpe Ratio (e.g, achieved a CR of 20.106 and SR of 1.373 on MSFT). The results suggest that combining LLMs with RL provides a promising direction for developing adaptive and high-performing financial trading agents. |
| Machine Learning | You Do Not Fully Utilize Transformer's Representation Capacity (Read more on [arXiv](https://arxiv.org/abs/2502.09245) or [HuggingFace](https://huggingface.co/papers/2502.09245))| kefirski, ummagumm-a, elephantmipt, yaraksen, gudleifrr | This paper introduces Layer-Integrated Memory (LIMe), an extension to the Transformer architecture designed to mitigate representation collapse in deep learning models. The main research objective is to address the issue where standard Transformers compress all learned features into a single residual stream, leading to suboptimal performance and loss of information in deeper layers. LIMe allows the model to retrieve and integrate representations from all earlier layers using a learned routing mechanism, preserving the overall memory footprint.  Experiments demonstrate that LIMe consistently outperforms standard Transformer baselines; for example, LIMe achieves an average accuracy of 58.4% on the LM Evaluation Harness benchmarks, compared to 57.7% for LLaMA. This suggests that AI practitioners can utilize LIMe to build deeper and more robust Transformers by decoupling the context storage from a single residual stream, thereby enhancing feature utilization. |
| Multi-Modal | Magma: A Foundation Model for Multimodal AI Agents (Read more on [arXiv](https://arxiv.org/abs/2502.13130) or [HuggingFace](https://huggingface.co/papers/2502.13130))| cheryyunl, Baolin, rzheng12, qianhuiwu, tanreuben | This paper introduces Magma, a foundation model for multimodal AI agents capable of understanding and acting in both digital and physical environments. The main objective is to develop a unified model that integrates vision-language understanding with spatial-temporal reasoning for agentic tasks, bridging the gap between verbal and spatial intelligence.  The key methodology involves pre-training on heterogeneous datasets (images, videos, robotics data) using Set-of-Mark (SoM) and Trace-of-Mark (ToM) techniques for action grounding and planning, facilitating the acquisition of spatial-temporal intelligence. Magma achieves state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models; for example achieving a 96.3 success rate on UI Action Grounding using the SS-Overall metric. AI practitioners can leverage Magma as a versatile foundation model for building multimodal agents with strong capabilities in understanding, planning, and action across various domains. |
| Multi-Modal | RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm (Read more on [arXiv](https://arxiv.org/abs/2502.12513) or [HuggingFace](https://huggingface.co/papers/2502.12513))| Kaicheng Yang, JiankangDeng, SeriousBro, Nina0607, GaryGuuu | This paper introduces RealSyn, a new dataset and paradigm for vision-language representation learning using multimodal interleaved documents. The research objective is to effectively utilize unpaired image and text data from interleaved documents and to leverage both realistic and synthetic texts for enhanced representation. The key methodology involves extracting image-text pairs from real-world data using hierarchical retrieval and generating synthetic text via an image semantic augmented generation module, employing semantic balance sampling for improved dataset diversity. Pre-training models on RealSyn achieves state-of-the-art performance on multiple downstream tasks, with a 6.9% average improvement on linear probe evaluations compared to YFCC15M. The main implication is that AI practitioners can effectively utilize large-scale multimodal interleaved document data, a previously underutilized resource, to improve vision-language representation learning and achieve robust model performance. |
| Natural Language Processing | PAFT: Prompt-Agnostic Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2502.12859) or [HuggingFace](https://huggingface.co/papers/2502.12859))| Fei Richard Yu, Ying Tiffany He, Mingwen Ou, Yao Shu, kittttttt | PAFT: Prompt-Agnostic Fine-Tuning is a novel approach to improve the robustness of large language models (LLMs) to variations in prompt phrasing during fine-tuning. The research objective is to mitigate the performance degradation of fine-tuned LLMs caused by minor prompt variations. PAFT operates by first constructing a diverse set of candidate prompts and then dynamically sampling from this set during fine-tuning, encouraging the model to learn underlying task principles.  Experiments across diverse datasets and LLMs show that PAFT improves both model performance and inference speed while maintaining training efficiency, and significantly reduces the impact of different prompts (e.g. Figure 4 variance is consistently lower).  AI practitioners can use PAFT to create more reliable and user-friendly LLMs, reducing the need for extensive prompt engineering. |
| Machine Learning | MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections (Read more on [arXiv](https://arxiv.org/abs/2502.12170) or [HuggingFace](https://huggingface.co/papers/2502.12170))| Xingyuan Yuan, Da Xiao, lishengping, Hilbertmeng | This paper introduces MUDDFormer, a novel Transformer architecture designed to improve cross-layer information flow by addressing limitations of residual connections. The research aims to overcome the diminishing returns of increasing Transformer depth and the issue of representation collapse. The key methodology involves Multiway Dynamic Dense (MUDD) connections, which dynamically generate connection weights based on hidden states and decoupled input streams (query, key, value, residual). Experiments demonstrate that MUDDFormer-834M matches the loss of a Transformer++ trained with 1.89x compute, while improving downstream tasks; MUDDPythia-2.8B matches Pythia-6.9B's performance with significantly less compute. These results imply that MUDD connections can improve the efficiency, effectiveness, and scalability of Transformer-based models, particularly useful when scaling up deep learning models. |
| Natural Language Processing | Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities? (Read more on [arXiv](https://arxiv.org/abs/2502.12215) or [HuggingFace](https://huggingface.co/papers/2502.12215))| Yunhua Zhou, Qinyuan Cheng, Zhiyuan Zeng, xpqiu, yinzhangyue | This paper investigates the test-time scaling capabilities of o1-like large language models, specifically examining whether increasing Chain-of-Thought (CoT) length improves performance. The primary research question is whether models like QwQ, Deepseek-R1, and LIMO truly possess test-time scaling capabilities, contrary to the assumption that longer CoTs lead to better accuracy. The methodology involves analyzing the relationship between CoT length and reasoning performance, comparing sequential and parallel scaling strategies, and proposing a new method called "Shortest Majority Vote". The results show that longer CoTs do not consistently improve accuracy, and correct solutions are often shorter; parallel scaling often outperforms sequential scaling, and the "Shortest Majority Vote" method outperforms conventional majority voting, with accuracy improvements demonstrated on benchmarks like AIME (e.g., improving from 56.66 to 60.88 on LIMO when utilizing two solutions). AI practitioners should consider that longer CoTs do not always improve the accuracy of o1-like models, therefore, the performance may be more nuanced and reliant on factors like self-revision capability and using approaches like the Shortest Majority Vote. |
| Natural Language Processing | Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge (Read more on [arXiv](https://arxiv.org/abs/2502.12501) or [HuggingFace](https://huggingface.co/papers/2502.12501))| zhangsan5421, lifengshang, horiz94, YuxinJiang, DonJoey | This paper introduces Crowd-based Comparative Evaluation (CCE) to improve the reliability and comprehensiveness of LLM-as-a-Judge. The main research question is how to guide LLMs to engage in deeper, more detail-rich Chain-of-Thought (CoT) reasoning during judgment. CCE gathers additional 'crowd' responses and compares them with candidate responses to expose more nuanced details, guiding the LLM-as-a-Judge towards a more detailed CoT judgment. The method achieves an average accuracy gain of 6.7% across five benchmarks. The main implication is that CCE provides a more reliable and effective method for auto-evaluation, enhancing judge distillation and improving SFT through crowd rejection sampling. |
| Machine Learning | OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning (Read more on [arXiv](https://arxiv.org/abs/2502.11271) or [HuggingFace](https://huggingface.co/papers/2502.11271))| Joseph Boen, Rahul Thapa, Sheng Liu, Bowen Chen, lupantech | This paper introduces OctoTools, an agentic framework designed for complex reasoning tasks across diverse domains. The main research objective is to develop a training-free, extensible framework that enables large language models (LLMs) to effectively utilize external tools for multi-step problem-solving. The key methodology involves standardized tool cards to encapsulate tool functionality, a planner for high-level and low-level task planning, and an executor for tool usage, and evaluated on diverse reasoning benchmarks. Primary results show that OctoTools achieves an average accuracy gain of 9.3% over GPT-4o without function plugins and out performs other agent frameworks. The main implication for AI practitioners is that OctoTools provides a modular and extensible approach for building AI agents that leverage LLMs for more complex multi-step problem solving than using only LLMs, enabling easier integration and maintenance of tools. |
| Multi-Modal | HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation (Read more on [arXiv](https://arxiv.org/abs/2502.09838) or [HuggingFace](https://huggingface.co/papers/2502.09838))| Binhe Yu, Yuqian Yuan, Sijing Li, Wenqiao Zhang, Tianwei Lin | HealthGPT is a Medical Large Vision-Language Model (Med-LVLM) that unifies medical visual comprehension and generation within an autoregressive framework. The research objective is to adapt heterogeneous comprehension and generation knowledge to pre-trained large language models in the data-constrained medical domain, addressing conflicts between these two tasks. The key methodology involves a novel heterogeneous low-rank adaptation (H-LoRA) technique, along with a hierarchical visual perception approach and a three-stage learning strategy.  HealthGPT achieved a score of 66.4 on medical visual comprehension, which indicates superior performance. For AI practitioners, HealthGPT demonstrates the feasibility of creating unified, high-performing medical AI models capable of both understanding and generating diverse medical visual data, even with limited data. |
| Machine Learning | HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading (Read more on [arXiv](https://arxiv.org/abs/2502.12574) or [HuggingFace](https://huggingface.co/papers/2502.12574))| beidic, junjiehu, jinqixiao, ZefanCai, wdlctc | HeadInfer is a memory-efficient inference framework for large language models (LLMs) that reduces GPU memory usage by offloading the key-value (KV) cache to CPU RAM. The main research objective is to enable long-context inference on consumer-grade GPUs with limited memory, which is typically a major bottleneck. The key methodology involves a fine-grained, head-wise offloading strategy, maintaining only selective attention heads' KV cache on the GPU and dynamically computing attention output.  Primary results show that HeadInfer reduces the GPU memory footprint of the KV cache for a 1-million-token sequence on the Llama-3-8B model from 128 GB to 1 GB, enabling 4-million-token inference on a single NVIDIA RTX 4090.  AI practitioners can utilize HeadInfer to run significantly longer context LLM inference tasks on resource-constrained hardware. |
| Natural Language Processing | Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey (Read more on [arXiv](https://arxiv.org/abs/2502.10708) or [HuggingFace](https://huggingface.co/papers/2502.10708))| Mingzhe Li, Miao Fang, Yuhan Liu, Bin Yan, Ziruibest | This paper provides a comprehensive survey of methods for injecting domain-specific knowledge into Large Language Models (LLMs) to enhance their performance in specialized tasks. The main objective is to examine how domain-specific knowledge can be effectively integrated into LLMs to improve accuracy and reliability. The key methodologies are categorized into four approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization. Results show that domain-specific LLMs often outperform general-domain models; for example, PMC LLaMA-13B surpasses LLaMA2-70B by over 10 points on the MedQA dataset. The main implication is that AI practitioners should consider and choose appropriate knowledge injection methods to adapt LLMs based on different domain requirements like training costs, inference speed, limitations, and data. |
| Machine Learning | Eager Updates For Overlapped Communication and Computation in DiLoCo (Read more on [arXiv](https://arxiv.org/abs/2502.12996) or [HuggingFace](https://huggingface.co/papers/2502.12996))| Yanislav Donchev, Arthur Douillard, Satyen Kale | This paper introduces "eager updates," a technique to improve the efficiency of Distributed Low Communication (DiLoCo) training by overlapping communication and computation. The research aims to mitigate performance slowdowns in DiLoCo caused by blocking during outer optimization steps, especially in low-bandwidth settings. The key methodology involves eagerly applying local outer gradients before global aggregation and delaying the application of non-local outer gradients to overlap communication with computation. Experiments using Chinchilla architecture and the C4 dataset, simulating various bandwidth constraints, showed that eager updates maintain competitive performance with standard DiLoCo; Specifically, the eager method with H=30 inner steps reaches the same performance as Data-Parallel at 1 billion scale. For AI practitioners, this approach presents a method for more efficient distributed training, especially beneficial in cross-datacenter scenarios with limited bandwidth. |
| Natural Language Processing | Atom of Thoughts for Markov LLM Test-Time Scaling (Read more on [arXiv](https://arxiv.org/abs/2502.12018) or [HuggingFace](https://huggingface.co/papers/2502.12018))| Chenglin Wu, Jiayi Zhang, Quan Shi, Zhaoyang Yu, leavendough | This paper introduces Atom of Thoughts (AOT), a novel reasoning framework for Large Language Models (LLMs) that transforms complex reasoning processes into a Markov process of atomic questions. The main objective is to address the issue of accumulated historical information in existing test-time scaling methods, which wastes computational resources and interferes with effective reasoning. AOT achieves this by iteratively decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming new atomic question states. Experiments across six benchmarks show that AOT achieves an 80.6% F1 score on HotpotQA, surpassing 03-mini by 3.4% and DeepSeek-R1 by 10.6%. AOT enables more efficient use of computational resources during test-time scaling and can serve as both a standalone framework and a plug-in enhancement, offering AI practitioners a method to improve LLM reasoning capabilities. |
| Natural Language Processing | FinMTEB: Finance Massive Text Embedding Benchmark (Read more on [arXiv](https://arxiv.org/abs/2502.10990) or [HuggingFace](https://huggingface.co/papers/2502.10990))| Yi Yang, yixuantt | This paper introduces FinMTEB, a new benchmark for evaluating text embedding models in the financial domain. The main research objective is to assess how well existing embedding models capture domain-specific financial information and whether domain adaptation improves performance. The methodology involves creating a benchmark with 64 datasets across 7 tasks and developing a finance-adapted model, Fin-E5, using persona-based data synthesis. Results show that domain-adapted models consistently outperform general-purpose models, with Fin-E5 achieving a state-of-the-art average score of 0.6767 on FinMTEB. The main implication is the demonstration of a strong need for domain-specific evaluation and adaptation of embedding models for financial applications. |
| Natural Language Processing | Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research (Read more on [arXiv](https://arxiv.org/abs/2502.12669) or [HuggingFace](https://huggingface.co/papers/2502.12669))| Shuyan Chen, wenxinsiju, yongqi2023, sunpenglei, Dominic789654 | This paper presents a comprehensive knowledge-enhanced system for perovskite solar cell (PSC) research, integrating a knowledge graph, specialized datasets, and large language models (LLMs). The main objective is to develop a system that can efficiently manage and reason with the rapidly growing body of PSC research literature. The key methodology involves constructing a domain-specific knowledge graph (Perovskite-KG), creating two datasets (Perovskite-Chat and Perovskite-Reasoning) using a multi-agent framework, and developing two specialized LLMs (Perovskite-Chat-LLM and Perovskite-Reasoning-LLM).  Perovskite-Chat-LLM achieves a Rouge-L score of 41.25 and an LLM-Judge score of 2.97 on the Perovskite QA dataset, significantly outperforming baseline models. The main implication is that specialized LLMs, enhanced with domain-specific knowledge, can significantly improve performance on tasks like literature review, experimental design, and complex problem-solving in materials science. |
| Machine Learning | Pre-training Auto-regressive Robotic Models with 4D Representations (Read more on [arXiv](https://arxiv.org/abs/2502.13142) or [HuggingFace](https://huggingface.co/papers/2502.13142))| trevordarrell, zitengj0618, gbiamby, yuvansharma, NdtSoCool | This paper introduces ARM4R, an auto-regressive robotic model that leverages low-level 4D representations learned from human video data for improved pre-training. The research objective is to overcome the limitations of robotic foundation models by pretraining the model on unlabeled human video data, creating a robust representation of the physical world applicable to robot tasks.  The key methodology involves learning 4D representations (3D point tracks across time) from videos by lifting 2D representations into 3D space via monocular depth estimation, and then fine-tuning on robotic scenes and proprioceptive data.  ARM4R achieves an average success rate of 59.47% on 12 RLBench simulation tasks, surpassing baselines like PerAct and LLARVA. The main implication is that pre-training robotic models on readily available human videos leveraging 4D representations can substantially improve performance on downstream robotic control tasks, even surpassing models pre-trained on large robotic datasets. |
| Natural Language Processing | Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages (Read more on [arXiv](https://arxiv.org/abs/2502.10852) or [HuggingFace](https://huggingface.co/papers/2502.10852))| XU Han, Jianing Liu, Guixian Xu, Ziyin Zhang, Zeli Su | This paper introduces a novel framework, XLM-SWCM, for adapting multilingual encoders to text generation in extremely low-resource languages, specifically focusing on Chinese minority languages. The research aims to address the poor performance of existing multilingual language models in extremely low-resource settings and the scarcity of text generation models for many languages. The proposed methodology involves a weight-sharing mechanism between the encoder and decoder of a transformer-based model, interleaving transferred weights with randomly initialized ones to enable efficient learning. The model outperforms mBART-CM by 198.8% in F1-score for text summarization and also surpasses a much larger MC2-LLaMA 13B in cross-lingual transfer settings. This framework offers a practical approach for developing robust text generation models for under-resourced languages, facilitating their inclusion in NLP applications. |
