

## Papers for 2025-02-11

| Title | Authors | Summary |
|-------|---------|---------|
| SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators (Read more on [arXiv](https://arxiv.org/abs/2502.06394) or [HuggingFace](https://huggingface.co/papers/2502.06394))| Alexander Panchenko, tlenusik, memyprokotow, chameleon-lizard, etomoscow | This paper introduces SynthDetoxM, a multilingual parallel text detoxification dataset generated using large language models (LLMs) and evaluates its effectiveness for training detoxification models. The main research objective is to address the scarcity of parallel multilingual detoxification datasets by proposing a framework for synthetic data generation. The key methodology involves few-shot prompting of open-source LLMs to rewrite toxic sentences across German, French, Spanish, and Russian, followed by filtering and selection based on toxicity and similarity metrics. Results show that models trained on the generated SynthDetoxM dataset achieve a higher J score of 0.521 for Russian compared to those trained on human-annotated MultiParaDetox (0.434), showing superior J score for German and Spanish as well. The primary implication is that AI practitioners can leverage the proposed LLM-based framework and released SynthDetoxM dataset to train more effective and robust multilingual text detoxification models, even in low-resource scenarios.  |
| Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning (Read more on [arXiv](https://arxiv.org/abs/2502.06781) or [HuggingFace](https://huggingface.co/papers/2502.06781))| Yuzhe Gu, Songyang Gao, Chengqi Lyu, zsytony, ZwwWayne | The paper introduces OREAL, a new reinforcement learning framework for improving mathematical reasoning in large language models using only binary outcome rewards. The main research objective is to push the performance limit achievable through Outcome REwArd-based reinforcement Learning (OREAL) for mathematical reasoning tasks. The key methodology involves behavior cloning on positive trajectories, reward reshaping for negative samples, and a token-level reward model for credit assignment within long reasoning chains. The primary result is that OREAL achieves 95.0 pass@1 accuracy on the MATH-500 benchmark with a 32B model, surpassing prior state-of-the-art methods. The principle implication is that AI practitioners can achieve high performance in mathematical reasoning tasks by using OREAL, a theoretically grounded RL framework, and focusing on optimizing policy with outcome rewards.  |
| Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling (Read more on [arXiv](https://arxiv.org/abs/2502.06703) or [HuggingFace](https://huggingface.co/papers/2502.06703))| Xiu Li, Jian Zhao, Junqi Gao, iseesaw, RyanLiu112 | This paper investigates compute-optimal test-time scaling (TTS) strategies for large language models (LLMs) by examining the influence of policy models, process reward models (PRMs), and problem difficulty. The main research question is what is the optimal approach to scale test-time computation across different policy models, PRMs, and problem difficulty levels, and to what extent can this improve LLM performance. The key methodology involves comprehensive experiments on MATH-500 and AIME24 tasks using various LLMs (0.5B to 72B parameters) and PRMs (1.5B to 72B parameters), evaluating different TTS methods (Best-of-N, beam search, DVTS). Primary results show that a 3B LLM with compute-optimal TTS can outperform a 405B LLM (75.6% vs 71.4% on MATH-500), and that optimal TTS strategy varies with different model configurations. For AI practitioners, the principal implication is that smaller LLMs, when combined with appropriately chosen compute-optimal TTS strategies that specifically account for the selected reward function, can surpass the performance of much larger LLMs, optimizing efficiency and performance.  |
| Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding (Read more on [arXiv](https://arxiv.org/abs/2502.05609) or [HuggingFace](https://huggingface.co/papers/2502.05609))| Soyeong Jeong, Jeongyeon Seo, Sangjin Choi, doubleyyh, zomss | Here's a 4-5 sentence summary of the provided paper, strictly adhering to the specified guidelines:  The paper proposes Hierarchy Drafting (HD), a lossless acceleration method for large language model (LLM) inference that leverages temporal locality in speculative decoding. The main research objective is to develop a drafting strategy that improves inference speed without requiring model fine-tuning or incurring significant overhead. HD organizes diverse token sources into multiple databases in a hierarchical framework based on temporal locality and sequentially accesses these databases during drafting. Experiments on Spec-Bench using 7B and 13B LLMs demonstrate that HD outperforms existing lossless drafting methods, achieving a 1.51x speedup compared to autoregressive decoding on Vicuna-7B with T=0.0. For AI practitioners, HD offers a method to accelerate LLM inference without model modification by efficiently organizing and retrieving tokens from varied sources based on their temporal locality.  |
| Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation (Read more on [arXiv](https://arxiv.org/abs/2502.05415) or [HuggingFace](https://huggingface.co/papers/2502.05415))| Yishun Li, Zhenyi Liao, zhijie3, asunalove, UnhurriedDawn | Show-o Turbo accelerates inference for the unified multimodal model Show-o by extending consistency distillation to multimodal denoising trajectories. The main research question is whether a unified approach can enhance the inference efficiency of Show-o, which progressively denoises image tokens and autoregressively decodes text tokens. The key methodology involves unifying image and text generation as a denoising process via parallel text decoding, extending consistency distillation (CD) to multimodal trajectories, and using trajectory segmentation and curriculum learning. In text-to-image generation, Show-o Turbo achieves a GenEval score of 0.625 at 4 sampling steps without classifier-free guidance, surpassing the original Show-o with 8 steps and CFG. For AI practitioners, Show-o Turbo offers a method to significantly speed up both text-to-image and image-to-text generation in unified multimodal models, directly relevant to development involving this type of architecture.  |
| Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2502.06060) or [HuggingFace](https://huggingface.co/papers/2502.06060))| Dorsa Sadigh, C. Karen Liu, Warren Xia, bidiptas | Language models are trained to play the social deduction game Among Us, achieving communication and reasoning without human demonstrations. The main research objective is to train language models to engage in productive natural language discussions about their environment without reliance on human demonstrations. The key methodology decomposes communication into "listening" (predicting environment information from discussions) and "speaking" (rewarding messages based on their influence, using multi-agent reinforcement learning). The trained models doubled win rates compared to standard reinforcement learning, demonstrating stronger discussions through emergent behaviors like accusing suspects. AI practitioners can leverage this self-improvement framework to develop multi-agent communication strategies in partially observable settings without needing task-specific human data.  |
| ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates (Read more on [arXiv](https://arxiv.org/abs/2502.06772) or [HuggingFace](https://huggingface.co/papers/2502.06772))| Mengdi Wang, Bin Cui, Zhaochen Yu, Ling Yang | ReasonFlux is a hierarchical LLM reasoning framework that enhances mathematical problem-solving by scaling thought templates. The main research objective is to improve LLM reasoning capabilities on complex mathematical problems by using a structured and scalable approach with thought templates. The key methodology involves constructing a structured template library, performing hierarchical reinforcement learning on sequences of thought templates, and adaptively scaling templates during inference. Primary results show that ReasonFlux-32B achieves 91.2% accuracy on the MATH benchmark, surpassing o1-preview by 6.7%. AI practitioners can leverage ReasonFlux's hierarchical reasoning and template scaling to enhance LLMs' performance on complex reasoning tasks, particularly in mathematical problem-solving domains requiring structured and multi-step solutions.  |
| The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering (Read more on [arXiv](https://arxiv.org/abs/2502.03628) or [HuggingFace](https://huggingface.co/papers/2502.03628))| Zhenting Wang, Di Liu, Yunhe Gao, Haizhou Shi, Zhuowei Li | This paper proposes VISTA, a training-free framework to reduce hallucination in Large Vision-Language Models (LVLMs) by improving visual information utilization during text generation. The main research objective is to investigate and mitigate the phenomenon of gradual visual information loss and early excitation of semantically meaningful tokens during LVLM text generation. The key methodology involves token logits ranking analysis across generation stages and layers, and a Visual Information Steering approach using a Visual Steering Vector (VSV) and Self-Logits Augmentation (SLA). Primary results show that VISTA reduces hallucination by about 40% on average on evaluated open-ended generation tasks and consistently outperforms existing methods across four architectures and three decoding strategies. Principal implication for AI practitioners is that VISTA offers an effective, inference-time intervention to improve the factual grounding of LVLMs without requiring additional training or model modifications, thus directly enhancing their reliability in real-world applications.  |
| Matryoshka Quantization (Read more on [arXiv](https://arxiv.org/abs/2502.06786) or [HuggingFace](https://huggingface.co/papers/2502.06786))| Aditya Kusupati, Prateek Jain, Jeff Dean, Puranjay Datta, Pranav Nair | Matryoshka Quantization is a multi-scale quantization technique that trains a single model capable of operating at multiple integer precision levels. The main research question is whether a single model can be trained to extract multiple accurate lower-precision models. The key methodology is Matryoshka Quantization (MatQuant), which jointly optimizes model weights across multiple precision levels (int8, int4, int2) using shared most significant bits. Primary results show that MatQuant-derived int2 models achieve up to 10% higher accuracy than standard int2 quantization methods, with an int2 FFN-quantized Gemma-2 9B model surpassing an int8 FFN-quantized Gemma-2 2B model in accuracy. The principal implication is that AI practitioners can maintain and serve a single quantized model at different precision levels, offering improved accuracy-vs-cost trade-offs, and reducing maintain multiple models, particularly benefiting low-precision (int2) deployments.  |
| EVEv2: Improved Baselines for Encoder-Free Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.06788) or [HuggingFace](https://huggingface.co/papers/2502.06788))| Yueze Wang, Yufeng Cui, Xiaotong Li, Haiwen Diao, PhyscalX | EVEv2.0 is an improved encoder-free vision-language model (VLM) that demonstrates superior data efficiency and strong vision-reasoning capabilities compared to previous encoder-free models. The main research objective is to develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based models, and to clarify the performance gap between VLMs and encoder-based approaches. The key methodology involves decomposing and hierarchically associating vision and language within a unified, decoder-only model using a divide-and-conquer architecture, and employing an optimized training strategy. EVEv2.0 achieves an accuracy of 71.4 on the ScienceQA-Img (SQA-IMG) benchmark with only 92M parameters and 7.3M training data. The principal implication is that AI practitioners can build more efficient and scalable VLMs by using a decoder-only architecture with modality-wise sparsity, and enhanced captioning, thus facilitating improved multi-modal understanding without relying on pre-trained vision encoders.  |
| LM2: Large Memory Models (Read more on [arXiv](https://arxiv.org/abs/2502.06049) or [HuggingFace](https://huggingface.co/papers/2502.06049))| Fraser Greenlee, Alex J. Chan, Filippos Christianos, Wenqi Wu, Jikun Kang | LM2 is a decoder-only Transformer architecture enhanced with an auxiliary memory module to address limitations in long-context reasoning. The main research objective is to improve the performance of Transformers on tasks requiring multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The key methodology involves incorporating a memory module that acts as a contextual representation repository, interacting with input tokens via cross-attention and updating through gating mechanisms, while preserving the original information flow. Primary results show that on the BABILong benchmark, LM2 outperforms the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. The principal implication for AI practitioners is that incorporating explicit memory mechanisms within Transformer architectures can enable more robust handling of extended contexts, offering significant performance improvements in memory-intensive tasks.  |
| Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT (Read more on [arXiv](https://arxiv.org/abs/2502.06782) or [HuggingFace](https://huggingface.co/papers/2502.06782))| Kai Wang, Zhen Li, Yutong Liu, Shicheng Li, Dongyang Liu | Lumina-Video is a text-to-video generation framework built upon the Next-DiT architecture, featuring enhanced efficiency and controllability. The main research objective is to address the spatiotemporal complexity and high computational cost of video generation using Diffusion Transformers. The key methodology involves a Multi-scale Next-DiT architecture with multiple patch sizes, motion score conditioning, progressive training, and a multi-source training scheme. Primary results show that Lumina-Video achieves a total score of 82.94 on the VBench benchmark, with a time cost of 0.34 when combining all path sizes. The principal implication for AI practitioners is that Lumina-Video offers a more efficient and flexible framework for high-quality video generation, with explicit control over motion dynamics and adaptable computational cost.  |
| History-Guided Video Diffusion (Read more on [arXiv](https://arxiv.org/abs/2502.06764) or [HuggingFace](https://huggingface.co/papers/2502.06764))| Russ Tedrake, Yilun Du, Max Simchowitz, Boyuan Chen, Kiwhan Song | The paper introduces a video diffusion model, DFoT, and a family of guidance methods, History Guidance, to improve video generation quality and enable exceptionally long video rollouts. The main research question is: Can different portions of video history (variable lengths, subsets of frames, and different frequencies) be used as a form of guidance for improved video generation? The key methodology is Diffusion Forcing Transformer (DFoT), a video diffusion architecture enabling conditioning on a flexible number of history frames, and History Guidance (HG), a family of methods that leverage different history conditionings. A primary result is that DFoT with vanilla history guidance achieves a FVD score of 181.6 on Kinetics-600, outperforming baselines including standard diffusion (247.5). AI practitioners can leverage DFoT and History Guidance to create higher-quality and temporally consistent videos, particularly for long video generation, and for tasks requiring conditioning on variable-length historical context.  |
| CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2502.06527) or [HuggingFace](https://huggingface.co/papers/2502.06527))| Zhen Yang, Jin Wang, Jingxuan Pang, Mushui Liu, D. She | CustomVideoX is a zero-shot personalized video generation framework based on the Video Diffusion Transformer, enabling consistent video generation from a reference image and text prompt. The main research objective is to develop a method for generating high-quality, temporally consistent customized videos that adhere to both a reference image and a textual description without additional training. The key methodology involves integrating a 3D Reference Attention mechanism, Time-Aware Attention Bias, and Entity Region-Aware Enhancement (ERAE) within a video diffusion transformer, utilizing LoRA parameters for efficient adaptation. The primary results show that CustomVideoX achieves a CLIP-T score of 34.28 and CLIP-I of 85.47, outperforming other methods on DreamBench. The paper implies for AI practitioners that they can generate high-fidelity customized videos by efficiently adapting pre-trained video diffusion models, leveraging spatial and temporal coherence enhancements.  |
| APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding (Read more on [arXiv](https://arxiv.org/abs/2502.05431) or [HuggingFace](https://huggingface.co/papers/2502.05431))| Beidi Chen, Tianqi Chen, Hanyuezhuohua | APE improves context-augmented generation in LLMs by enabling faster and longer context processing through adaptive parallel encoding. The main research objective is to address the computational burden and performance degradation in context-augmented generation (CAG) caused by sequential encoding of multiple contexts. The key methodology, Adaptive Parallel Encoding (APE), uses shared prefixes, attention temperature adjustment, and scaling factors to align the attention distribution of parallel encoding with that of sequential encoding. APE achieves a 4.5x end-to-end speedup by reducing prefilling time by 28x for a 128K-length context, while preserving 98% of sequential encoding performance on RAG tasks. AI practitioners can use APE to achieve faster inference and integrate more, larger external documents for context-augmented applications using LLMs, and process inputs in parrallel.  |
| Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile (Read more on [arXiv](https://arxiv.org/abs/2502.06155) or [HuggingFace](https://huggingface.co/papers/2502.06155))| Peiyuan Zhang, Runlong Su, Dacheng Li, zhijie3, foreverpiano | EFFICIENT-VDIT accelerates video diffusion transformers (DiTs) by sparsifying 3D attention and reducing sampling steps. The main research objective is to address the inefficiency of 3D DiTs with full attention due to high computational complexity and numerous sampling steps. The key methodology involves identifying a "tile-style" repetitive pattern in 3D attention maps to create a sparse attention mask with linear complexity, and applying multi-step consistency distillation. The primary result is that EFFICIENT-VDIT achieves up to 7.8x speedup on Open-Sora-Plan-1.2 models for video generation with minimal performance degradation on VBench. The principal implication for AI practitioners is that they can use EFFICIENT-VDIT to significantly accelerate the inference speed of pre-trained 3D DiT video generation models while preserving most of the video quality.  |
| MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2502.05957) or [HuggingFace](https://huggingface.co/papers/2502.05957))| Chao Huang, Tianyu Fan, Jiabin Tang | MetaChain is a fully-automated, zero-code framework for developing and deploying LLM agents using natural language. The main research question is: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? The key methodology involves a self-developing framework with four components: Agentic System Utilities, an LLM-powered Actionable Engine, a Self-Managing File System, and a Self-Play Agent Customization module. Primary results include ranking #1 among open-source solutions on the GAIA benchmark and achieving 73.51% accuracy on a MultiHop-RAG task, outperforming other baselines. The principal implication is that AI practitioners can democratize AI agent development, allowing users without coding expertise to create, customize, and deploy LLM agents for diverse applications.  |
| Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM (Read more on [arXiv](https://arxiv.org/abs/2502.06635) or [HuggingFace](https://huggingface.co/papers/2502.06635))| Zhaoxiang Zhang, Shu Li, Qingshui Gu, aaabiao | Steel-LLM is a fully open-source, 1-billion-parameter Chinese-centric language model developed with limited computational resources and full transparency. The main objective was to create a high-quality, open-source Chinese-centric LLM despite limited computational resources, emphasizing transparency and practical insights. The methodology involved training a 1-billion-parameter model on a primarily Chinese dataset using a modified TinyLlama framework, incorporating Soft Mixture of Experts (Soft MOE) and an enhanced Feed-Forward Network. The best model, Steel-LLM-Chat, achieved a CEVAL accuracy of 41.90% and a CMMLU accuracy of 36.08%. AI practitioners can replicate or extend a similar transparent approach, such as utilizing the open-source model, training scripts, and checkpoints, for resource-efficient LLM development, particularly in non-English languages.  |
| The Curse of Depth in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.05795) or [HuggingFace](https://huggingface.co/papers/2502.05795))| Yefeng Zheng, Lu Yin, Xinyuan Song, Wenfang Sun, pengxiang | The paper introduces "Curse of Depth" in Large Language Models (LLMs), where deeper layers contribute less to learning than earlier layers due to Pre-Layer Normalization (Pre-LN). The main research objective is to identify and rectify the phenomenon where deeper layers in LLMs are less effective, specifically examining the impact of Pre-LN. The key methodology involves theoretical analysis of variance propagation in Pre-LN Transformers and empirical evaluation via layer pruning and performance measurement on benchmarks like MMLU and SQuAD. Primary results show that LayerNorm Scaling reduces perplexity by 1.31 on LLaMA-1B compared to standard Pre-LN, and that LayerNorm Scaling makes deep layers contribute significatively more. The principle implication is that AI practitioners should consider using LayerNorm Scaling to improve LLM performance and efficiency by ensuring that all layers contribute effectively to training.  |
| DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2502.04370) or [HuggingFace](https://huggingface.co/papers/2502.04370))| Yi Yang, Hehe Fan, Fan Ma, Xiaobo Xia, Zhenglin Zhou | DreamDPO is an optimization-based framework for text-to-3D generation that aligns 3D content with human preferences through direct preference optimization. The main research objective is to improve the alignment of text-to-3D generated content with human preferences and increase controllability. The methodology involves constructing pairwise examples, ranking them based on human preference alignment using reward or large multimodal models, and optimizing the 3D representation with a preference-driven loss function. DreamDPO achieved a GPTEval3D overall score of 1203.1, outperforming existing methods like MVDream (1097.7) and ProlificDreamer (1012.5). AI practitioners can use DreamDPO to generate higher-quality and more controllable 3D content that better aligns with human preferences, without a need to rely on pointwise scoring during optimization.  |
| Dual Caption Preference Optimization for Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2502.06023) or [HuggingFace](https://huggingface.co/papers/2502.06023))| Bimsara Pathiraja, Shamanthak Hegde, Agneet Chatterjee, Yiran Luo, sahsaeedi | Dual Caption Preference Optimization (DCPO) improves text-to-image diffusion models by using distinct captions for preferred and less-preferred images during training. The main research objective is to address the issues of conflict distribution and irrelevant prompts in existing preference optimization methods for diffusion models. The key methodology involves generating distinct captions for preferred and less-preferred images using captioning and perturbation techniques, and a modified objective function, leveraging a new dataset, Pick-Double Caption. Primary results show that DCPO-h outperforms other methods, achieving a +0.21 increase in Pickscore compared to baselines. The principal implication for AI practitioners is that using dual captions during preference optimization can significantly enhance the alignment of diffusion models with human preferences, improving image quality and prompt relevance.  |
