

## Papers for 2025-02-17

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | Region-Adaptive Sampling for Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2502.10389) or [HuggingFace](https://huggingface.co/papers/2502.10389))| Lili Qiu, Yiqi Zhang, Chengruidong Zhang, Yifan Yang, Ziming Liu | This paper introduces Region-Adaptive Sampling (RAS), a novel training-free sampling strategy for Diffusion Transformers (DiTs) that dynamically adjusts sampling ratios across different image regions. The research objective is to accelerate the inference speed of diffusion models without significant quality degradation, by addressing the limitation of existing methods that uniformly process all image regions. RAS leverages the flexibility of DiTs to assign different sampling ratios based on the model's focus, updating only semantically meaningful regions in each step and reusing cached noise for others, guided by output noise from the previous step to determine focus regions. Evaluation on Stable Diffusion 3 and Lumina-Next-T2I shows speedups up to 2.36x and 2.51x respectively, with minimal quality degradation and achieving a 1.6x speedup in a user study with comparable quality. AI practitioners can use RAS to achieve substantial acceleration in diffusion transformer inference, enhancing the practicality of DiTs for real-time applications. |
| Multi-Modal | Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model (Read more on [arXiv](https://arxiv.org/abs/2502.10248) or [HuggingFace](https://huggingface.co/papers/2502.10248))| Nan Duan, Liangyu Chen, Kun Yan, Haoyang Huang, Guoqing Ma | This technical report introduces Step-Video-T2V, a state-of-the-art text-to-video pre-trained model capable of generating high-quality videos up to 204 frames in length. The research aims to develop and evaluate a video foundation model that can translate textual prompts into corresponding video outputs, while also exploring the challenges and future directions of such models. The methodology involves a deep compression Variational Autoencoder (Video-VAE) for video representation, bilingual text encoders, a diffusion Transformer (DiT) with 3D full attention trained using Flow Matching, and a video-based Direct Preference Optimization (DPO) approach.  The Video-VAE achieves 16x16 spatial and 8x temporal compression ratios, and Step-Video-T2V outperforms other models in generating videos, although specific comparative metrics beyond qualitative assessments are spread through the results section (e.g. Table 7 show Win-Tie-Loss of 59-22-47, 46-47-35, 54-41-33 against HunyuanVideo). For AI practitioners, the implication is that combining advanced model architectures, high-compression techniques, and human feedback can lead to significant progress in video generation quality, although challenges remain in areas like physics simulation and long-form video generation. |
| Multi-Modal | ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models (Read more on [arXiv](https://arxiv.org/abs/2502.09696) or [HuggingFace](https://huggingface.co/papers/2502.09696))| Samuel Roberts, Akash Gupta, Ansh Sharma, Mohammad Reza Taesiri, Jonathan Roberts | This paper introduces ZeroBench, a new visual reasoning benchmark designed to be impossible for current state-of-the-art Large Multimodal Models (LMMs). The research objective is to create a lightweight and challenging benchmark to assess and push the limits of LMMs' visual understanding and reasoning capabilities. The methodology involves manually curating 100 difficult questions with 334 subquestions, rigorously reviewed and filtered based on the performance of current frontier models.  The primary result is that all 20 evaluated LMMs scored 0.0% on the main questions, demonstrating the benchmark's difficulty. The main implication is that there's significant headroom for improvement in LMMs' visual reasoning, and ZeroBench provides a robust tool for tracking progress in this area. |
| Natural Language Processing | Large Language Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2502.09992) or [HuggingFace](https://huggingface.co/papers/2502.09992))| Jingyang Ou, Xiaolu Zhang, Zebin You, Fengqi Zhu, Shen Nie | This paper introduces LLaDA, a large language diffusion model, as an alternative to autoregressive models for language generation. The research challenges the dominance of autoregressive models (ARMs) and investigates whether diffusion models can achieve comparable performance in natural language tasks. LLaDA employs a forward data masking process and a reverse process parameterized by a Transformer to predict masked tokens, optimizing a likelihood bound for probabilistic inference.  Results show LLaDA 8B achieves competitive performance with LLaMA3 8B base in zero/few-shot benchmarks, with LLaDA showing an MMLU score of 65.9; and it outperforms GPT-4o in a reversal poem completion task. This suggests diffusion models are a viable alternative to ARMs, offering potential benefits like bidirectional context and addressing issues such as the reversal curse. |
| Multi-Modal | MM-RLHF: The Next Step Forward in Multimodal LLM Alignment (Read more on [arXiv](https://arxiv.org/abs/2502.10391) or [HuggingFace](https://huggingface.co/papers/2502.10391))| Peiyan Li, Chaoyou Fu, Haochen Tian, Tao Yu, Yi-Fan Zhang | This paper introduces MM-RLHF, a new dataset and approach for aligning Multimodal Large Language Models (MLLMs) with human preferences. The main research question is whether aligning MLLMs with human preferences can systematically enhance their capabilities beyond specific tasks. The key methodology involves creating a large, fine-grained, human-annotated preference dataset (MM-RLHF), proposing a Critique-Based Reward Model, and introducing Dynamic Reward Scaling within the Direct Preference Optimization (DPO) framework.  Fine-tuning LLaVA-ov-7B with MM-RLHF and the proposed alignment algorithm leads to a 19.5% increase in conversational abilities and a 60% improvement in safety. The main implication is that a well-designed alignment pipeline using high-quality human preference data can comprehensively enhance MLLMs across multiple dimensions, improving performance, safety, and reliability. |
| Computer Vision | Precise Parameter Localization for Textual Generation in Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2502.09935) or [HuggingFace](https://huggingface.co/papers/2502.09935))| Adam Dziedzic, Kamil Deja, Franziska Boenisch, Bartosz Cywiński, Łukasz Staniszewski | This paper introduces a method for localizing and manipulating the parameters in diffusion models responsible for textual generation within images. The research objective is to identify the specific attention layers in diffusion models that influence the generation of textual content and use this knowledge to improve text generation, enable text editing, and prevent harmful text output. The key methodology involves activation patching of cross and joint attention layers, combined with LoRA-based fine-tuning of the localized layers. The authors found that less than 1% of diffusion models' parameters (0.61% of Stable Diffusion XL, 0.21% of DeepFloyd IF, and 0.23% of Stable Diffusion 3 parameters) significantly influence text generation. This localization allows for improved text generation, precise image editing of text, and cost-free prevention of toxic text generation, providing AI practitioners with more efficient and controllable text-to-image diffusion models. |
| Natural Language Processing | We Can't Understand AI Using our Existing Vocabulary (Read more on [arXiv](https://arxiv.org/abs/2502.07586) or [HuggingFace](https://huggingface.co/papers/2502.07586))| Been Kim, Robert Geirhos, John Hewitt | This position paper argues that understanding and controlling AI requires developing new vocabulary (neologisms) to represent the differing concepts between humans and machines, framing interpretability as a communication problem. The main research question is how to bridge the communication gap caused by differing conceptualizations of the world between humans and machines. The proposed methodology centers on creating and learning new words, specifically through 'neologism embedding learning', to represent human and machine concepts within prompts.  Experiments demonstrate that 'length neologisms' enable controlling language model (LLM) response length, and 'diversity neologisms' increase response variability (e.g., increasing the chance of finding a correct number in a guessing game from ~20% to above 60% with 100 guesses). The main implication is that AI practitioners should consider developing new vocabulary to facilitate better communication and control of AI systems, rather than relying solely on existing human vocabulary. |
| Natural Language Processing | FoNE: Precise Single-Token Number Embeddings via Fourier Features (Read more on [arXiv](https://arxiv.org/abs/2502.09741) or [HuggingFace](https://huggingface.co/papers/2502.09741))| Vatsal Sharan, Robin Jia, Mahdi Soltanolkotabi, Deqing Fu, Tianyi Zhou | This paper introduces Fourier Number Embedding (FoNE), a novel method for representing numbers as single tokens in Large Language Models (LLMs) using Fourier features. The research objective is to address the limitations of traditional subword and digit-wise tokenization methods, which fragment numerical representations and hinder performance on number-related tasks. FoNE maps numbers directly into the embedding space with their Fourier features, encoding each digit using two dimensions, allowing it to represent numbers as single tokens. On a 6-digit decimal addition task, FoNE achieved 99% accuracy with 64 times less data than traditional methods and attained perfect accuracy across various arithmetic tasks. AI practitioners can leverage FoNE for representing numbers more efficiently and to achieve greater accuracy in LLMs, overcoming limitations of conventional methods and allowing for better performance in numerical reasoning tasks. |
| Machine Learning | AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting (Read more on [arXiv](https://arxiv.org/abs/2502.10235) or [HuggingFace](https://huggingface.co/papers/2502.10235))| Maurizio Filippone, Albert Thomas, Giuseppe Paolo, Vasilii Feofanov, abenechehab | This paper introduces AdaPTS, a framework for adapting pre-trained univariate time series foundation models to probabilistic multivariate time series forecasting. The main research objective is to address the challenge of leveraging univariate models for multivariate tasks while managing inter-feature dependencies and quantifying prediction uncertainty. The methodology involves using "adapters"—stochastic feature-space transformations—that project multivariate inputs into a latent space where a frozen, pre-trained univariate foundation model (FM) can be applied independently to each dimension, then inverting the forecast back.  The AdaPTS framework improves the performance of Moment in 5 of the 8 tested forecasting scenarios, in some cases significantly; for example, a 15% Mean Squared Error (MSE) improvement was shown using VAE adapters on the Illness dataset with Horizon=24.  AI practitioners can use AdaPTS to effectively adapt large, pre-trained univariate time series models for multivariate forecasting tasks, enabling cost savings and improved uncertainty quantification. |
| Natural Language Processing | Jailbreaking to Jailbreak (Read more on [arXiv](https://arxiv.org/abs/2502.09638) or [HuggingFace](https://huggingface.co/papers/2502.09638))| Bijan Varjavand, Robert Vacareanu, Vaughn Robinson, Jeremy Kritz, ZifanScale | This paper introduces a novel jailbreaking-to-jailbreak (J2) approach where a jailbroken LLM is used to red-team and jailbreak other LLMs. The research aims to develop a scalable red-teaming strategy by leveraging LLMs to evaluate target models and improve performance through in-context learning. The methodology involves first jailbreaking a capable LLM to act as a J2 attacker, which then systematically attempts to jailbreak target models using various red teaming strategies. Experiments show that Sonnet-3.5 and Gemini-1.5-pro achieve 93.0% and 91.0% attack success rates (ASRs) against GPT-40 on Harmbench, respectively. The main implication is a new failure mode in LLM safeguards, where a jailbroken version of an LLM can assist in further jailbreaking efforts, highlighting the need for stronger safeguards against exploiting an LLM's willingness to assist in harmful behaviors. |
| Reinforcement Learning | STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning (Read more on [arXiv](https://arxiv.org/abs/2502.10177) or [HuggingFace](https://huggingface.co/papers/2502.10177))| Shuguang Cui, Zhixin Mai, Ge Wang, Yiming Zhao, Mingcong Lei | This paper introduces the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning and execution in embodied agents by integrating spatio-temporal memory. The main objective is to enable agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. STMA utilizes a spatio-temporal memory module, a dynamic knowledge graph, and a planner-critic mechanism to iteratively refine task strategies.  Experimental results in the TextWorld environment demonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7% increase in average score compared to state-of-the-art models. The results suggest that incorporating spatio-temporal memory offers significant performance benefits for embodied agents operating in complex, dynamic environments. |
| Machine Learning | MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers (Read more on [arXiv](https://arxiv.org/abs/2502.07856) or [HuggingFace](https://huggingface.co/papers/2502.07856))| Ge Yang, Le Lu, Hongbo Zhao, Wei Fang, Ao Li | This paper introduces MRS, a fast sampler for Mean Reverting (MR) Diffusion models, accelerating controllable image generation. The research objective is to reduce the sampling time of MR Diffusion models, which currently require hundreds of iterative steps. The key methodology involves deriving semi-analytical solutions to the reverse-time stochastic differential equation (SDE) and probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion.  The experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks, achieving convergence in 5-10 NFEs. This acceleration makes MR Diffusion more practical for controllable generation, particularly in applications like image restoration. |
| Multi-Modal | V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.09980) or [HuggingFace](https://huggingface.co/papers/2502.09980))| Yu-Chiang Frank Wang, Stephen F. Smith, Chien-Yi Wang, Ryo Hachiuma, Hsu-kuang Chiu | This paper introduces a novel problem setting and dataset for cooperative autonomous driving using multi-modal large language models (LLMs). The research objective is to investigate how LLMs can integrate perception information from multiple connected autonomous vehicles (CAVs) to improve driving safety, specifically in situations with sensor occlusions. The proposed methodology, V2V-LLM, utilizes an LLM to fuse scene-level feature maps and object-level feature vectors, enabling it to answer driving-related questions based on the combined perception data. Experimental results show that V2V-LLM achieves an F1 score of 59.7% for notable object identification and a 4.99m average L2 distance error for planning, outperforming other baseline fusion methods on these critical tasks. The main implication is that LLMs can be used to create a unified model for perception and planning tasks, and can contribute to safety improvements, in cooperative autonomous driving scenarios. |
| Machine Learning | Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a Language Diffusion Model (Read more on [arXiv](https://arxiv.org/abs/2502.10173) or [HuggingFace](https://huggingface.co/papers/2502.10173))| Markus J. Buehler, Bo Ni | This paper introduces VibeGen, a generative AI framework for de novo protein design conditioned on normal mode vibrations, enabling the creation of proteins with targeted dynamic properties. The central research objective is to design proteins with specific dynamic behaviors, moving beyond static structural design. The methodology employs an agentic dual-model architecture comprising a protein designer (sequence generation) and a protein predictor (dynamic accuracy evaluation), trained using protein language diffusion models. Results demonstrate that designed proteins accurately reproduce prescribed normal mode amplitudes; the Pearson correlation coefficient between measured and target mode shapes has a median value of 0.53 and reaches 0.72 for smoothed data, showing strong correlation. This framework can potentially expand the design space, and enable AI practitioners to engineer biomolecules with enhanced dynamic and functional properties. |
