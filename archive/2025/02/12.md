

## Papers for 2025-02-12

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Reinforcement Learning | Competitive Programming with Large Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2502.06807) or [HuggingFace](https://huggingface.co/papers/2502.06807))| Borys Minaev, Andre Saraiva, Alexander Wei, Ahmed El-Kishky, OpenAI | This paper demonstrates that reinforcement learning (RL) significantly improves the performance of large language models (LLMs) on complex coding and reasoning tasks, specifically competitive programming. The main research objective is to compare domain-specific, hand-engineered inference strategies with learned approaches in large reasoning models. The key methodology involves fine-tuning LLMs with RL, and comparing performance with and without hand-crafted test-time strategies (like clustering and reranking). A primary result is that the 03 model achieved a CodeForces rating of 2724 (99.8th percentile), outperforming models that utilize human-defined strategies. The main implication is that scaling general-purpose reinforcement learning, rather than relying on domain-specific techniques, can lead to state-of-the-art AI performance in reasoning domains. |
| Natural Language Processing | CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction (Read more on [arXiv](https://arxiv.org/abs/2502.07316) or [HuggingFace](https://huggingface.co/papers/2502.07316))| Yu Wu, Runxin Xu, Dejian Yang, Daya Guo, Junlong Li | This paper introduces CODEI/O, a novel approach to improve the reasoning abilities of Large Language Models (LLMs) by training them on code input-output prediction tasks. The main objective is to enhance LLM performance on diverse reasoning tasks by leveraging the structured and readily available nature of code. CODEI/O transforms raw code into an input-output prediction format and trains models to predict inputs or outputs given code and test cases in natural language, formulated as Chain-of-Thought (CoT) rationales. Experimental results show that CODEI/O leads to consistent improvements across various reasoning benchmarks, achieving an average score increase of 2.4% to 8.2% across four different base models. This suggests that training LLMs on code input-output prediction can provide a scalable and effective means of condensing diverse reasoning patterns, benefiting a wide array of downstream tasks. |
| Computer Vision | Magic 1-For-1: Generating One Minute Video Clips within One Minute (Read more on [arXiv](https://arxiv.org/abs/2502.07701) or [HuggingFace](https://huggingface.co/papers/2502.07701))| Qingyu Yin, Jiantong Zhao, Shitong Shao, Hongwei Yi, Owen777 | This paper introduces Magic 1-For-1 (Magic141), an efficient video generation model that optimizes memory consumption and inference latency. The main objective is to generate high-quality, minute-long video clips within one minute, addressing the computational costs associated with diffusion models. The key methodology involves factorizing the text-to-video task into text-to-image and image-to-video generation, alongside optimizations like multi-modal prior condition injection, adversarial step distillation, and parameter sparsification. The model achieves an average score of 0.8134 on a customized VBench, outperforming other open-source models in both performance and efficiency, generating 5-second video clips in 3 seconds. The primary implication is that AI practitioners can leverage this approach to create high-quality videos rapidly and efficiently, significantly reducing computational demands. |
| Reinforcement Learning | Teaching Language Models to Critique via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2502.03492) or [HuggingFace](https://huggingface.co/papers/2502.03492))| Jingjing Xu, Weichao Mao, Liyu Chen, Jie chen, Zhihui | This paper introduces CTRL, a framework for training large language models (LLMs) to critique and refine their outputs, specifically for code generation, via reinforcement learning. The main research objective is to develop a critic model that can generate feedback to maximize the correction performance of a fixed generator model without human supervision. The key methodology involves a two-stage training pipeline: synthesizing high-quality critiques using execution feedback for supervised finetuning, and then optimizing the critic through Group Relative Policy Optimization (GRPO). The results demonstrate that critics trained with CTRL significantly enhance pass rates, achieving up to 106.1% relative improvements on challenging code generation benchmarks, and also act as effective generative reward models. The main implication is that specialized critic models can be trained to effectively guide task-performing models, enabling iterative self-improvement in LLMs without requiring extensive human feedback. |
| Natural Language Processing | Expect the Unexpected: FailSafe Long Context QA for Finance (Read more on [arXiv](https://arxiv.org/abs/2502.06329) or [HuggingFace](https://huggingface.co/papers/2502.06329))| Mateusz Russak, Dmytro Mozolevskyi, Melisa Russak, muayad, kiranr | This paper introduces FailSafeQA, a new long-context financial benchmark designed to test the robustness and context-awareness of LLMs against variations in human-interface interactions. The research evaluates LLM resilience against input perturbations in query and context, focusing on scenarios like query failure (misspellings, incompleteness, out-of-domain queries) and context failure (missing, OCR-degraded, or irrelevant documents). The methodology employs LLM-as-a-Judge with Qwen2.5-72B-Instruct and fine-grained rating criteria to calculate Robustness, Context Grounding, and Compliance scores for 24 models. Results show that while some models excel at mitigating input perturbations, they struggle to balance robust answering with avoiding hallucinations; for example, the most robust model (OpenAI 03-mini) fabricated information in 41% of tested cases, and a new metric is introduced, LLM Compliance. AI practitioners should prioritize developing LLMs optimized for dependability in critical applications by balancing factual accuracy with resilience to diverse user inputs. |
| Multi-Modal | Scaling Pre-training to One Hundred Billion Data for Vision Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.07617) or [HuggingFace](https://huggingface.co/papers/2502.07617))| Keran Rong, Zhe Li, Daniel Salz, Ibrahim Alabdulmohsin, Xiao Wang | This paper investigates the impact of pre-training vision-language models (VLMs) on an unprecedented scale of 100 billion image-text pairs. The main research objective is to determine the benefits and limitations of scaling pre-training data to this magnitude, particularly regarding cultural diversity and multilinguality. The key methodology involves training SigLIP models on datasets of varying sizes (1 billion, 10 billion, and 100 billion examples) and evaluating their performance on a wide range of benchmarks, including Western-centric tasks, cultural diversity metrics, and multilingual tasks. A key result is that scaling to 100 billion examples substantially improves cultural diversity metrics; for instance, ViT-L/16 achieves 41.7% accuracy on Dollar Street geo-localization with 100B data, versus only 35.9% with 10B. The main implication is that while traditional benchmarks may saturate, scaling to 100 billion examples remains crucial for building inclusive multimodal systems with improved performance on long-tail concepts and for diverse subgroups, although existing data filtering approaches can hinder those benefits. |
| Natural Language Processing | LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters! (Read more on [arXiv](https://arxiv.org/abs/2502.07374) or [HuggingFace](https://huggingface.co/papers/2502.07374))| Xiangxi Mo, Shu Liu, Tyler Griggs, Shiyi Cao, Dacheng Li | This paper investigates how Large Language Models (LLMs) can learn long chain-of-thought (Long CoT) reasoning through efficient fine-tuning. The main research question is to understand the data and training requirements for eliciting Long CoT reasoning, specifically the relative importance of reasoning structure versus content. The authors use supervised fine-tuning (SFT) and low-rank adaptation (LoRA) on a curated dataset of Long CoT samples, and then systematically perturb the samples to assess the impact of structural and content modifications. The results show that a Qwen2.5-32B-Instruct model achieves 56.7% on AIME 2024, competitive with proprietary models, and that the structure of Long CoT is significantly more crucial than the content of individual reasoning steps. The main implication is that AI practitioners can efficiently elicit strong reasoning abilities in LLMs by focusing on the logical structure of demonstrations, rather than solely on the accuracy of every detail. |
| Multi-Modal | Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents (Read more on [arXiv](https://arxiv.org/abs/2502.04223) or [HuggingFace](https://huggingface.co/papers/2502.04223))| Lukas Voegtle, Ilia Karmanov, jseppanen, katerynaCh, amalad | ÉCLAIR is a general-purpose text-extraction tool designed to process a wide range of document types, extracting formatted text, bounding boxes, and semantic classes. The main research objective is to address the limitations of existing OCR systems in handling complex documents, which require understanding structure, reading order, and semantic information beyond simple text extraction. ÉCLAIR uses a multi-modal LLM (MLLM) with a ViT-like encoder and an auto-regressive decoder, trained on a novel dataset (arXiv-5M) and a diverse human-annotated benchmark (DROBS). ÉCLAIR achieves state-of-the-art accuracy on DROBS, with a word-level F1 score of 0.937, outperforming methods like Kosmos-2.5 and GOT. AI practitioners can use ÉCLAIR for improved document digitization, data retrieval, and training data generation for LLMs and VLMs, especially with complex, visually diverse documents. |
| Machine Learning | CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing (Read more on [arXiv](https://arxiv.org/abs/2502.03997) or [HuggingFace](https://huggingface.co/papers/2502.03997))| Jiang Bian, Qi Liu, Yu Yuan, ShizhaoSun | This paper introduces CAD-Editor, the first framework for text-based editing of Computer-Aided Design (CAD) models. The main research objective is to enable automatic modification of CAD models based on textual instructions, a previously underexplored area. The proposed methodology includes an automated data synthesis pipeline using design variation models and Large Vision-Language Models (LVLMs) to create training data, and a locate-then-infill framework that leverages Large Language Models (LLMs) to identify and modify specific regions within the CAD model's sequence representation.  CAD-Editor achieves a 95.6% Valid Ratio for generated CAD sequences and a D-CLIP score of 0.27, indicating effectiveness and text alignment. This work provides a novel approach that can significantly accelerate CAD model development and empower broader users including non-experts, by simplifying modification via language. |
| Computer Vision | Enhance-A-Video: Better Generated Video for Free (Read more on [arXiv](https://arxiv.org/abs/2502.07508) or [HuggingFace](https://huggingface.co/papers/2502.07508))| Wenqi Shao, Kaipeng Zhang, Mengzhao Chen, Xuanlei Zhao, Yang Luo | Enhance-A-Video is a training-free method designed to improve the temporal consistency and visual quality of videos generated by diffusion transformer (DiT) models. The central research question is whether cross-frame information can be efficiently utilized to improve consistency and detail in DiT-based video generation without retraining. The method enhances cross-frame correlations using a novel Enhance Block, which calculates a Cross-Frame Intensity (CFI) from temporal attention maps and adjusts it with an enhance temperature parameter. Experiments across several DiT models like HunyuanVideo and CogVideoX show qualitative improvements and quantitative gains such as consistent VBench score improvements (e.g. CogVideoX going from 77.27 to 77.34); user studies indicate preference for the Enhanced-A-Video output. This offers AI practitioners a plug-and-play approach to boost generated video quality without any additional training cost. |
| Natural Language Processing | NatureLM: Deciphering the Language of Nature for Scientific Discovery (Read more on [arXiv](https://arxiv.org/abs/2502.07527) or [HuggingFace](https://huggingface.co/papers/2502.07527))| Chuan Cao, Liang He, Shufang Xie, Peiran Jin, Yingce Xia | NatureLM is a sequence-based science foundation model designed for scientific discovery across multiple domains, including chemistry, biology, and material science. The main research objective is to develop a unified model capable of generating and optimizing scientific entities, such as molecules, proteins, RNA, and materials, using text-based instructions. The model is pre-trained on a large corpus of sequence data from various scientific domains and then post-trained with instruction-response pairs, with scale testing at 1 billion, 8 billion, and 46.7 billion parameters. NatureLM achieves state-of-the-art performance in tasks like SMILES-to-IUPAC translation and retrosynthesis on USPTO-50k, showing clear improvement with increasing model size; for example, retrosynthesis top-1 accuracy improves from 68.6% (1B) to 71.9% (8x7B). This model offers a generalist approach for tasks such as drug discovery and material design and suggests a path toward more unified scientific AI models, though its general language and few-shot learning skills still lag behind leading large language models. |
| Natural Language Processing | Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training (Read more on [arXiv](https://arxiv.org/abs/2502.06589) or [HuggingFace](https://huggingface.co/papers/2502.06589))| Kewei Cheng, Xin Liu, Haoming Jiang, Jingfeng Yang, yczhuang | This paper introduces Hephaestus, a large language model (LLM) agent designed to improve fundamental agent capabilities through continual pre-training. The main objective is to enhance LLM agents' abilities in API function calling, intrinsic reasoning and planning, and adaptation to environmental feedback. The key methodology involves creating Hephaestus-Forge, a large-scale pre-training corpus comprising 103B agent-specific data, and using it for continual pre-training. Primary results show Hephaestus-8B outperforming open-source LLMs by 9.6% over LLAMA-3-8B and 17.6% over Mixtral-8x22B, and rivaling commercial LLMs on agent benchmarks. The main implication is that continual pre-training on agent-specific data significantly enhances LLM agents' fundamental capabilities and generalization to new tasks. |
| Natural Language Processing | Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon (Read more on [arXiv](https://arxiv.org/abs/2502.07445) or [HuggingFace](https://huggingface.co/papers/2502.07445))| Seffi Cohen, Lior Rokach, Bracha Shapira, Yehonatan Elisha, Nurit Cohen-Inger | This paper introduces a meta-evaluation framework called Chameleon Benchmark Overfit Detector (C-BOD) to assess the overfitting of large language models (LLMs) to public benchmarks. The main research question is whether LLMs rely excessively on benchmark-specific cues and if this behavior can be systematically detected. C-BOD systematically distorts benchmark prompts via parametric transformations, preserving semantic meaning while altering surface features, to test model performance. The evaluation on the MMLU benchmark using 26 LLMs revealed an average performance degradation of 2.15% under modest perturbations, with 20 models showing statistically significant differences. The research suggests that high benchmark scores may mask an overreliance on superficial patterns, and AI practitioners should prioritize resilience and generalization in LLM evaluation. |
| Computer Vision | VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation (Read more on [arXiv](https://arxiv.org/abs/2502.07531) or [HuggingFace](https://huggingface.co/papers/2502.07531))| Hang Xu, Yi Zhu, Yanpeng Zhou, Zimian Peng, Sixiao Zheng | VidCRAFT3 is a novel image-to-video generation framework offering precise control over camera motion, object motion, and lighting direction. The paper introduces a model that can generate high-quality videos from a single image based on specified controls. The approach employs a Spatial Triple-Attention Transformer and three core technical advancements, including 3D point cloud rendering for camera control, sparse trajectory encoding for object motion and a lighting-aware attention mechanism. Experiments show that on the RealEstate10K dataset, VidCRAFT3 achieves a Camera Motion Control (CamMC) score of 4.07, outperforming existing models like CameraCtrl (4.19), and CamI2V (4.24). For AI practitioners, the model enables more nuanced and realistic video content creation, albeit with certain limitations concerning complex dynamic motions and interactions. |
| Multi-Modal | FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks (Read more on [arXiv](https://arxiv.org/abs/2502.04465) or [HuggingFace](https://huggingface.co/papers/2502.04465))| Mirco Ravanelli, Cem Subakan, Francesco Paissan, lucadellalib | FocalCodec is a novel, low-bitrate speech codec that leverages focal modulation networks for efficient speech compression and representation. The research introduces and evaluates a new hybrid codec architecture to address the limitations of existing speech codecs in preserving both acoustic and semantic information at ultra-low bitrates. The proposed FocalCodec utilizes a single binary codebook and a compressor-quantizer-decompressor architecture based on focal modulation to achieve competitive performance in speech resynthesis and voice conversion. Evaluation on LibriSpeech test-clean dataset shows FocalCodec@50 achieves a dWER of 2.18 and a Sim of 97.4, surpassing many existing codecs while operating at 0.65 kbps. For AI practitioners, FocalCodec provides an efficient and effective method for speech tokenization suitable for downstream tasks that benefits from preserving both acoustic and semantic speech components even at ultra-low bitrates. |
| Natural Language Processing | Auditing Prompt Caching in Language Model APIs (Read more on [arXiv](https://arxiv.org/abs/2502.07776) or [HuggingFace](https://huggingface.co/papers/2502.07776))| Percy Liang, Rohith Kuditipudi, Xiang Lisa Li, Chenchen Gu, thashim | This paper investigates the security and privacy implications of prompt caching in large language model (LLM) APIs. The main research objective is to determine if LLM API providers are caching prompts and, if so, to assess the level of cache sharing and potential privacy leakage. The authors develop a statistical audit methodology using hypothesis testing to detect prompt caching and differentiate between per-user, per-organization, and global cache sharing. Audits of 17 API providers revealed that 8 cached prompts, with 7 exhibiting global cache sharing; average precision for classifying cache hits was approximately 0.8 in several tested APIs. These findings suggest that AI practitioners should be aware of potential privacy risks associated with prompt caching in LLM APIs and consider per-user caching as a mitigation strategy, especially when dealing with user data. |
| Natural Language Processing | Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More (Read more on [arXiv](https://arxiv.org/abs/2502.07490) or [HuggingFace](https://huggingface.co/papers/2502.07490))| Li Shen, Zhenyu Zhang, Jianjin Li, Zhikai Jia, Xialie Zhuang | This paper introduces Mask-Enhanced Autoregressive Prediction (MEAP), a training paradigm for large language models that integrates masked language modeling into next-token prediction to improve in-context retrieval capabilities. The main research objective is to address the limitation of next-token prediction-based LLMs in accurately retrieving key information from context. MEAP randomly masks a small fraction of input tokens and performs standard next-token prediction autoregressively using a decoder-only Transformer, eliminating the need for bidirectional attention.  Experiments demonstrate that MEAP substantially outperforms next-token prediction on key information retrieval tasks, with, for example, MEAP improving Multi-Document Question Answering performance by up to 27.2 percentage points. AI practitioners can use MEAP as a drop-in training technique to improve LLM performance on information retrieval and reasoning tasks, without increasing computational overhead. |
| Reinforcement Learning | Skill Expansion and Composition in Parameter Space (Read more on [arXiv](https://arxiv.org/abs/2502.05932) or [HuggingFace](https://huggingface.co/papers/2502.05932))| Yixing Lan, Haoyi Niu, Yinan Zheng, Jianxiong Li, LTL07 | This paper introduces Parametric Skill Expansion and Composition (PSEC), a new framework for reinforcement learning that allows agents to iteratively expand their capabilities and efficiently address new challenges. The primary objective is to develop a system that can progressively integrate skill primitives and leverage prior knowledge for efficient skill expansion and composition, addressing limitations like catastrophic forgetting and limited efficiency in learning new tasks. PSEC uses Low-Rank Adaptation (LoRA) modules in parameter-efficient finetuning to manage a skill library, enabling flexible skill expansion and parameter-space skill compositions via merged LoRA modules. Evaluations on D4RL, DSRL benchmarks, and the DeepMind Control Suite show that PSEC achieves superior capacity to leverage prior knowledge (e.g. a normalized score of 0.62 vs 0.41 of a next-best method, NSEC, on the BulletGym average), and efficiently tackle new challenges. AI practitioners can benefit by implementing systems that continuously build, evolve, and refine skills, and enable agents that improve their long-term performance by building upon earlier work. |
