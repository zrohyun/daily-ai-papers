

## Papers for 2025-02-13

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation (Read more on [arXiv](https://arxiv.org/abs/2502.07870) or [HuggingFace](https://huggingface.co/papers/2502.07870))| Zhuobai Dong, Weiming Han, Jiawei Zhang, Dongxing Mao, Alex Jinpeng Wang | This paper introduces TextAtlas5M, a large-scale dataset for dense text image generation, designed to address the limitations of existing datasets that focus on short and simple text. The main research objective is to develop a dataset and benchmark to evaluate and improve the performance of text-to-image generation models in handling long-form, complex, and dense text within images. The methodology involves collecting and generating 5 million long-text images across diverse data types, including synthetic and real-world scenarios, and curating a 3000-image human-improved test set, TextAtlasEval, for comprehensive evaluation. Evaluations using TextAtlasEval show that even advanced proprietary models like GPT-4o with DALL-E 3 face significant challenges, with an F1-score of 0.73 for OCR on the TextScenesHQ subset, while open-source counterparts demonstrate a larger performance gap. The implication for AI practitioners is the provision of a valuable resource for training and evaluating future text-conditioned image generation models, especially in applications that require rendering dense and intricate text. |
| Computer Vision | Light-A-Video: Training-free Video Relighting via Progressive Light Fusion (Read more on [arXiv](https://arxiv.org/abs/2502.08590) or [HuggingFace](https://huggingface.co/papers/2502.08590))| Pan Zhang, Pengyang Ling, Jiazi Bu, Yujie Zhou, yuhangzang | The paper introduces Light-A-Video, a training-free approach for temporally consistent video relighting using a pre-trained image relighting model and video diffusion model. It addresses the challenge of inconsistent lighting and appearance in frame-by-frame relighting by incorporating Consistent Light Attention (CLA) and Progressive Light Fusion (PLF). The CLA module enhances cross-frame interactions to stabilize background lighting, while PLF ensures smooth illumination transitions by linearly blending source and relighted appearances. Quantitative results demonstrate improved temporal consistency and image quality, achieving a FID score of 29.63 when compared to frame-by-frame relighting. This method offers AI practitioners a flexible and efficient solution for video relighting without the need for task-specific training. |
| Natural Language Processing | BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.07346) or [HuggingFace](https://huggingface.co/papers/2502.07346))| Lei Li, Conghui He, Hanxu Hu, Wenhao Zhu, ggdcr | This paper introduces BenchMAX, a comprehensive multilingual evaluation suite for large language models (LLMs) across 17 languages. The main objective is to assess LLMs' proficiency in instruction following, reasoning, long context understanding, and code generation in a multilingual setting, addressing the lack of comprehensive benchmarks for these capabilities. BenchMAX utilizes a multi-way parallel design with human-annotated and machine-translated data, covering diverse tasks and language families. Results on BenchMAX reveal varying effectiveness of core capabilities across languages and models, with performance gaps not always bridged by scaling; for example, DeepSeek-V3 671B achieved 83.9% accuracy in rule-based instruction following, significantly exceeding the capabilities of other models.. This benchmark provides a more thorough and fair method for practitioners to evaluate and improve multilingual language model performance, especially in non-English contexts. |
| Computer Vision | CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation (Read more on [arXiv](https://arxiv.org/abs/2502.08639) or [HuggingFace](https://huggingface.co/papers/2502.08639))| Huchuan Lu, Xu Jia, Xiaoyu Shi, Yawen Luo, Qinghe Wang | CineMaster is a novel framework for 3D-aware and controllable text-to-video generation, aiming to provide users with director-level control over scene composition and camera movement. The research objective is to empower users with precise placement of objects, flexible manipulation of objects and cameras in 3D space, and intuitive layout control. The methodology operates in two stages: an interactive 3D scene setup workflow for defining control signals, followed by a text-to-video diffusion model conditioned on rendered depth maps, camera trajectories, and object class labels. CineMaster achieves a mean Intersection over Union (mIoU) of 0.551 and a Trajectory Deviation (Traj-D) of 66.29, outperforming previous state-of-the-art methods. CineMaster allows AI practitioners to generate videos with fine-grained control that more faithfully adhere to user specifications in 3D space. |
| Multi-Modal | WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation (Read more on [arXiv](https://arxiv.org/abs/2502.08047) or [HuggingFace](https://huggingface.co/papers/2502.08047))| Mike Zheng Shou, Difei Gao, Henry Hengyuan Zhao | This paper introduces WorldGUI, a new benchmark for evaluating Graphical User Interface (GUI) agents in dynamic desktop environments, and proposes a novel agent framework called GUI-Thinker. The main research objective is to assess and improve the performance of GUI agents in realistic scenarios with varying initial states, addressing limitations of existing static benchmarks. The methodology involves creating diverse GUI tasks with different starting conditions across 10 popular software applications and designing a critical-thinking-based agent, GUI-Thinker, comprising planning, critiquing, and execution components.  Experimental results show that GUI-Thinker significantly outperforms the Claude-3.5 (Computer Use) baseline by 14.9% in success rate on WorldGUI tasks. The main implication is that incorporating critical thinking and dynamic testing improves GUI agent robustness, which can lead to more practical advancements in software testing and intelligent assistants. |
| Machine Learning | LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid (Read more on [arXiv](https://arxiv.org/abs/2502.07563) or [HuggingFace](https://huggingface.co/papers/2502.07563))| Yu Cheng, Xiaoye Qu, Yiran Zhong, landisen, weigao266 | This paper introduces LASP-2, a new sequence parallelism (SP) method designed to enhance training efficiency of linear attention Transformer models with long input sequences. The research objective is to improve both communication and computation parallelism in distributed training of linear attention models, addressing limitations of existing SP methods. The key methodology involves reorganizing the communication-computation workflow and using a single AllGather collective communication on intermediate memory states, independent of sequence length. LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The main implication is that AI practitioners can leverage LASP-2 for more efficient and scalable training of large language models, especially when processing extra-long sequences. |
| Natural Language Processing | TransMLA: Multi-head Latent Attention Is All You Need (Read more on [arXiv](https://arxiv.org/abs/2502.07864) or [HuggingFace](https://huggingface.co/papers/2502.07864))| Muhan Zhang, Zengwei Yao, fxmeng | This paper introduces TransMLA, a method for converting Group Query Attention (GQA) based large language models (LLMs) to Multi-head Latent Attention (MLA) based models to improve inference efficiency and expressiveness. The research objective is to demonstrate that MLA offers superior expressive power compared to GQA for the same Key-Value (KV) cache overhead, and to provide a practical conversion method. The methodology involves transforming GQA models into equivalent MLA representations, followed by further training to enhance model expressiveness without increasing KV cache size. The transformed TransMLA model shows lower training loss and higher accuracy on downstream tasks like GSM8K (87% vs 86%) and HumanEval (64% vs 59%) when fine-tuned on instruction dataset, compared to the original Qwen-2.5 GQA-based model. This implies that AI practitioners can migrate existing GQA models to MLA to achieve better performance with resource efficiency. |
| Natural Language Processing | Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance (Read more on [arXiv](https://arxiv.org/abs/2502.08127) or [HuggingFace](https://huggingface.co/papers/2502.08127))| Yan Wang, Weipeng Zhou, Lingfei Qian, QianqianXie1994, jiminHuang | This paper comprehensively evaluates the performance of reasoning-enhanced and general large language models (LLMs) on financial reasoning tasks. The primary objective is to assess the transferability of general-domain reasoning enhancements to the financial domain and identify limitations. The authors evaluate 16 LLMs on three financial datasets involving text, tables, and equations, and introduce a new model, Fino1, fine-tuned with domain-specific reasoning paths. Results show that Fino1-8B achieves a consistent 10% performance improvement across tasks, outperforming all 8B models, and even larger models like Llama3-70B-Instruct on average. The main implication is that domain-specific adaptations, rather than general reasoning enhancements, are crucial for improving LLM performance in financial analysis. |
| Machine Learning | Distillation Scaling Laws (Read more on [arXiv](https://arxiv.org/abs/2502.08606) or [HuggingFace](https://huggingface.co/papers/2502.08606))| Etai Littwin, Jason Ramapuram, Floris Weers, Amitis Shidani, Dan Busbridge | This paper provides a distillation scaling law that estimates distilled model performance as a function of compute budget and allocation between teacher and student language models. The main research objective is to determine how to correctly allocate resources during distillation pretraining to produce the most capable student models, and to compare the effectiveness of distillation against supervised pretraining. The methodology involves a large-scale controlled study of distillation with models ranging from 143M to 12.6B parameters, trained on up to 512B tokens, resulting in a distillation scaling law. The results show that distillation outperforms supervised pretraining only when the total compute or tokens used for the student are below a student size-dependent threshold, and when a teacher is reused or already available; otherwise, supervised learning is more efficient. The main implication for AI practioners is that distillation is resource-efficient for making smaller, powerful models, and the presented distillation scaling laws help mitigate risks and understand trade-offs. |
| Multi-Modal | SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation (Read more on [arXiv](https://arxiv.org/abs/2502.08168) or [HuggingFace](https://huggingface.co/papers/2502.08168))| HaiPeng Wang, Peidong Wang, Sihao Dong, Xiayang Xiao, JimmyMa99 | This paper introduces SARChat-Bench-2M, the first large-scale multimodal dialogue dataset and benchmark for SAR image interpretation, addressing the limitations of existing vision-language models (VLMs) in this specialized domain. The main research objective is to develop a comprehensive dataset and benchmark to evaluate and improve VLMs' capabilities in understanding and interpreting SAR images across various tasks. The methodology involves constructing a dataset of approximately 2 million high-quality SAR image-text pairs and defining six core tasks (classification, description, counting, localization, recognition, and referring) for evaluation, and Supervised Fine-Tuning(SFT) is used for VLMs training. Primary results show that while some models achieve high accuracy on specific tasks (e.g., mPLUG-Owl3-7B achieves 99.51% and 99.27% accuracy rates in multi and single-target cross-modal identification respectively), there's significant room for improvement in tasks like multi-target referring and spatial understanding. The main implication is that AI practitioners now have a robust resource for developing and evaluating VLMs specialized for SAR image interpretation, facilitating advancements in remote sensing applications. |
| Reinforcement Learning | Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2502.06533) or [HuggingFace](https://huggingface.co/papers/2502.06533))| lecraquito, Nbeau, supertardigrade | This paper investigates the exploration dynamics of a small language model fine-tuned with reinforcement learning (RL) on an arithmetic task, focusing on how pre-training influences exploration efficiency. The research objective is to understand how varying levels of pre-training affect language model performance in tasks requiring exploration and to propose a solution to enhance RL fine-tuning. The key methodology involves modifying the Kullback-Leibler (KL) penalty to prioritize exploration on 'critical tokens'â€”those significantly impacting the final outcome and where the pre-trained model exhibits high uncertainty. The authors find that pre-training on larger numbers improve generalization initially, and a prioritized KL penalty, weighting state-action terms by the pre-trained models confidence, increases exploration efficiency and improves accuracy on out-of-distribution operations, demonstrated, for example, on models finetuned on N=7 digits, with a figure (Fig 4) provided showing clear improvement over standard KL approaches. The main implication is that adapting the KL penalty based on pre-trained model confidence can substantially improve the efficiency of RL fine-tuning in language models, particularly in tasks with a distribution shift. |
| Natural Language Processing | LLM Pretraining with Continuous Concepts (Read more on [arXiv](https://arxiv.org/abs/2502.08524) or [HuggingFace](https://huggingface.co/papers/2502.08524))| Andrew Cohen, Jane Yu, Jack Lanchantin, Jihoon Tack, xlxxl | This paper introduces Continuous Concept Mixing (CoCoMix), a novel pretraining framework for large language models that combines discrete next-token prediction with continuous concept learning. The main research question is whether augmenting the next-token prediction objective with explicit concept modeling in a latent space can improve language model pretraining. CoCoMix extracts concepts from a pretrained sparse autoencoder, predicts them, and mixes them into the model's hidden state by interleaving with token representations. The results show that CoCoMix achieves comparable performance to next-token prediction with 21.5% fewer training tokens on a 1.38B-sized model, and consistently outperforms standard next token prediction and knowledge distillation methods on various benchmarks. This framework enhances model interpretability and steerability, allowing AI practitioners to guide the model's internal reasoning process and potentially improve performance, especially in settings with limited training data. |
| Computer Vision | Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance (Read more on [arXiv](https://arxiv.org/abs/2502.06145) or [HuggingFace](https://huggingface.co/papers/2502.06145))| Dechao Meng, Xin Gao, Zhen Shen, Guangyuan Wang, Hookszdp | Animate Anyone 2 is a framework for high-fidelity character image animation that incorporates environmental affordance. The research objective is to animate characters while realistically integrating them with their surrounding environments and preserving object interactions, moving beyond methods that only utilize motion signals. The methodology involves extracting environmental representations as conditional inputs, utilizing a shape-agnostic mask strategy, employing an object guider with spatial blending for feature injection, and introducing pose modulation for diverse motion handling. The method achieves a Frechet Video Distance (FVD) of 144.65 on the TikTok benchmark, outperforming previous state-of-the-art approaches. AI practitioners can leverage this framework to create more realistic and context-aware character animations for applications such as filmmaking and advertising. |
| Natural Language Processing | NoLiMa: Long-Context Evaluation Beyond Literal Matching (Read more on [arXiv](https://arxiv.org/abs/2502.05167) or [HuggingFace](https://huggingface.co/papers/2502.05167))| Ryan A. Rossi, Trung Bui, Hanieh Deilamsalehy, Franck-Dernoncourt, amodaresi | The paper introduces NoLiMa, a new benchmark for evaluating large language models' (LLMs) long-context understanding capabilities beyond simple literal matching. The main research objective is to assess how well LLMs can perform associative reasoning and information retrieval when explicit textual overlaps between questions and answers are minimized. The methodology involves creating a 'needle-in-a-haystack' task where the 'needle' (relevant information) and question have minimal lexical overlap, requiring the model to infer latent associations.  Results show that while many LLMs perform well in short contexts (e.g., <1K tokens), performance degrades significantly as context length increases; for example, at 32K tokens, 10 out of 12 evaluated models achieved only half of their short context performance. The main implication is that existing LLMs struggle with long-context reasoning when surface-level cues are absent, highlighting a need for improved attention mechanisms and reasoning capabilities. |
| Machine Learning | Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing (Read more on [arXiv](https://arxiv.org/abs/2502.04411) or [HuggingFace](https://huggingface.co/papers/2502.04411))| Peijie Dong, Xinglin Pan, Zhenheng Tang, Kunfeng Lai, Dominic789654 | This paper introduces Mediator, a memory-efficient framework for merging Large Language Models (LLMs) that addresses parameter conflicts and enhances performance. The research question revolves around how to merge common and unique knowledge from various fine-tuned models while minimizing system costs and avoiding parameter conflicts. The key methodology involves adaptive layer-wise model merging, task-level expert routing, sparse expert decomposition, and uncertainty-based routing for out-of-distribution samples. Experiments on LLaMA and Qwen with various reasoning tasks demonstrate that Mediator achieves significant performance improvements (up to 71.80% average accuracy on LLaMA-3.2-8B) while requiring less system cost compared to existing methods. AI practitioners can leverage Mediator to create more powerful and efficient LLMs by combining the strengths of multiple fine-tuned models without substantial resource overhead. |
| Computer Vision | Next Block Prediction: Video Generation via Semi-Autoregressive Modeling (Read more on [arXiv](https://arxiv.org/abs/2502.07737) or [HuggingFace](https://huggingface.co/papers/2502.07737))| Furu Wei, Xu Sun, Shuming Ma, Shuhuai Ren | This paper introduces a semi-autoregressive framework called Next-Block Prediction (NBP) for video generation, improving upon traditional next-token prediction methods. The research objective is to enhance the efficiency and quality of autoregressive video generation by shifting the generation unit from individual tokens to blocks. The key methodology involves decomposing video content into equal-sized blocks and using bidirectional attention within each block to predict multiple tokens in parallel. The model achieves FVD scores of 103.3 on UCF101 and 25.5 on K600, outperforming vanilla NTP models and achieving an 11x inference speedup. This implies that AI practitioners can leverage NBP for faster and more scalable video generation, maintaining or improving the quality of generated content. |
| Reinforcement Learning | DPO-Shift: Shifting the Distribution of Direct Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2502.07599) or [HuggingFace](https://huggingface.co/papers/2502.07599))| Xiao Li, Lei Zhao, Qianen Zhang, Feng Jiang, Xiliang Yang | This paper introduces DPO-Shift, an extension of Direct Preference Optimization (DPO), to mitigate the issue of likelihood displacement in aligning language models with human preferences. The research objective is to address the problem where the probability of chosen responses decreases during DPO training. DPO-Shift adds a parameter function f(x) to the rejected reward in the Bradley-Terry model to controllably shift the distribution of chosen probability. Experiments show DPO-Shift can achieve a reward accuracy of 0.743, which is comparable to or better than the baseline DPO model in some configurations. The main implication is that AI practitioners can use DPO-Shift to improve chosen response probability in language models while maintaining or improving reward margins by carefully selecting the f(x) parameter. |
| Natural Language Processing | LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention (Read more on [arXiv](https://arxiv.org/abs/2502.08213) or [HuggingFace](https://huggingface.co/papers/2502.08213))| kkolomeitsev | This paper introduces LLM Modules, an architecture for transferring knowledge from a large, pre-trained language model to a smaller one using an Enhanced Cross-Attention mechanism. The main research objective is to develop a method that allows smaller models to achieve performance comparable to larger models, thus reducing computational costs. The methodology involves freezing a large model (Qwen2-1.5B) as a knowledge source and training a smaller model (GPT-Neo-125M) to generate responses by incorporating the larger model's representations through modified Cross-Attention layers.  Experimental results on the Bespoke-Stratos-17k dataset show that the combined model's training and validation loss decreased significantly to 1.1 after 15 epochs, demonstrating a substantial improvement in quality and coherence compared to the original small model. This approach offers AI practitioners a way to leverage large pre-trained models' knowledge in resource-constrained environments, achieving efficient model adaptation for specific tasks. |
| Natural Language Processing | MetaSC: Test-Time Safety Specification Optimization for Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.07985) or [HuggingFace](https://huggingface.co/papers/2502.07985))| vicgalle | This paper introduces MetaSC, a dynamic safety framework that optimizes language model safety reasoning at inference time without modifying model weights. The main objective is to improve safety performance against adversarial jailbreak requests and in diverse general safety-related tasks. The key methodology involves a meta-critique mechanism that iteratively updates safety prompts, termed specifications, to adaptively drive the critique and revision process. Empirical evaluations across several language models demonstrate that dynamically optimized safety prompts yield significantly higher safety scores compared to fixed system prompts and static self-critique defenses (e.g., achieving a safety score of 1.00 on the Hermes-3-Llama-3.1-405B model against jailbreak attacks). The main implication is that AI practitioners can enhance model safety at inference time by dynamically optimizing safety prompts, rather than relying solely on fixed prompts or model weight modifications. |
