

## Papers for 2025-02-27

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Kanana: Compute-efficient Bilingual Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.18934) or [HuggingFace](https://huggingface.co/papers/2502.18934))| seopbo, Doohae, daniel-rl2, jiyeonham, bzantium | The paper introduces Kanana, a series of bilingual language models that demonstrate strong performance in Korean and competitive performance in English with significantly lower computational cost than state-of-the-art models of similar size. The main objective is to develop compute-efficient bilingual language models. Techniques employed include high-quality data filtering, staged pre-training, depth up-scaling, pruning, and distillation. Kanana Flag 32.5B achieves an average score of 69.15 across multiple benchmarks, outperforming Llama 3.1 70B on knowledge-intensive tasks while using substantially less computational resources. The strategies used in Kanana can be applied to develop high-performing, cost-effective language models, especially for under-resourced languages. |
| Computer Vision | GHOST 2.0: generative high-fidelity one shot transfer of heads (Read more on [arXiv](https://arxiv.org/abs/2502.18417) or [HuggingFace](https://huggingface.co/papers/2502.18417))| Andrey Kuznetsov, Denis Dimitrov, Pavel Paramonov, Alexander Groshev, nastasia-y | This paper introduces GHOST 2.0, a two-stage framework for high-fidelity one-shot head swapping in images and videos. The main research objective is to address the challenges of head swapping, which include preserving structural information, inpainting gaps, and transferring skin color, especially in scenarios with large differences between source and target heads. The proposed method consists of an Aligner module for head reenactment using multiple encoders and a StyleGAN-based generator, and a Blender module for seamless integration of the reenacted head with the target background using reference-based color transfer and inpainting. GHOST 2.0 achieves state-of-the-art results in head swapping, outperforming baselines in self-reenactment on metrics such as LPIPS (0.116) and SSIM (0.815) at 256x256 resolution. AI practitioners can utilize GHOST 2.0 for improved head swapping applications, such as in virtual try-on, deepfake detection, and movie production, with enhanced robustness to variations in head pose and hair styles. |
| Multi-Modal | TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding (Read more on [arXiv](https://arxiv.org/abs/2502.19400) or [HuggingFace](https://huggingface.co/papers/2502.19400))| Jonathan Leung, AlvinYuVotee, KrishKrosh, chongcht, vinesmsuic | This paper introduces TheoremExplainAgent, an agentic system for generating multimodal theorem explanation videos, and TheoremExplainBench, a benchmark for evaluating these explanations. The main research objective is to assess the ability of large language models (LLMs) to generate coherent and pedagogically meaningful visual explanations of theorems, going beyond text-based reasoning. The methodology involves an agentic approach where a planner agent creates video plans and a coding agent generates Python animation scripts using Manim. The 03-mini agent achieved a success rate of 93.8% and an overall score of 0.77, indicating good performance but with room for improvement, especially in visual element layout. AI practitioners can use this approach and benchmark to develop and evaluate systems capable of producing structured, multimodal explanations for complex concepts. |
| Natural Language Processing | Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning? (Read more on [arXiv](https://arxiv.org/abs/2502.19361) or [HuggingFace](https://huggingface.co/papers/2502.19361))| Weixun Wang, Jiaheng Liu, Shilong Li, Yancheng He, zhangysk | This paper introduces DeltaBench, a new dataset designed to evaluate the ability of large language models (LLMs) to detect errors in long chain-of-thought (CoT) reasoning. The main research objective is to assess the quality of long CoTs generated by o1-like models and the critique abilities of existing LLMs on these reasoning chains. The methodology involves collecting diverse long CoTs, annotating them with fine-grained error labels, and evaluating process reward models (PRMs) and critic models on their ability to identify these errors. A primary result is that the top-performing model, GPT-4-turbo-128k, achieves an F1-score of only 40.8% in error detection. The main implication is that current LLMs have limited capabilities in identifying errors in long CoT reasoning, highlighting the need for improved critique abilities and more robust reasoning processes. |
| Natural Language Processing | Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems (Read more on [arXiv](https://arxiv.org/abs/2502.19328) or [HuggingFace](https://huggingface.co/papers/2502.19328))| Bin Xu, Zijun Yao, Xiaozhi Wang, Yunjia Qi, Hao Peng | This paper proposes agentic reward modeling, a reward system that combines reward models with verifiable correctness signals to provide more reliable rewards. The main objective is to address the limitations of existing reward models that primarily focus on human preferences and neglect verifiable correctness signals. The key methodology is the implementation of a reward agent, named REWARDAGENT, which integrates human preference rewards with factuality and instruction-following signals. Primary results show that REWARDAGENT significantly outperforms vanilla reward models on benchmarks and in best-of-n searches, achieving an overall score of 72.5% on reward modeling benchmarks. The main implication for AI practitioners is that integrating verifiable correctness signals with human preferences can lead to more reliable and effective reward systems for training large language models. |
| Natural Language Processing | Language Models' Factuality Depends on the Language of Inquiry (Read more on [arXiv](https://arxiv.org/abs/2502.17955) or [HuggingFace](https://huggingface.co/papers/2502.17955))| Hamid Palangi, Kumar Ayush, Kumar Tanmay, ayush1801, AggarwalTushar | This paper investigates the cross-lingual factuality of multilingual language models (LMs), revealing inconsistencies in factual recall across different languages. The main research objective is to determine whether multilingual LMs truly internalize and transfer factual knowledge across languages or encode isolated linguistic silos. The methodology involves introducing a benchmark of 10,000 country-related facts across 13 languages, and proposing metrics including Factual Recall Score (FRS), Knowledge Transferability Score (KTS), and Cross-Lingual Factual Knowledge Transferability (X-FaKT) Score.  The results reveal that Llama-3-70B achieves the highest X-FaKT score of 0.848, yet significant performance asymmetry exists, with models often failing to transfer knowledge from one language to others.  The main implication is that AI practitioners should recognize language-specific factual reliability and leverage the most trustworthy information across languages when working with multilingual LMs, rather than assuming consistent knowledge transfer. |
| Machine Learning | Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation (Read more on [arXiv](https://arxiv.org/abs/2502.19414) or [HuggingFace](https://huggingface.co/papers/2502.19414))| Matthias Bethge, Jonas Geiping, Ponnurangam Kumaraguru, Shashwat Goel, Shiven Sinha | This paper introduces REFUTE, a benchmark for evaluating language models' (LMs) ability to falsify incorrect solutions to algorithmic problems by creating counterexamples. The main research question is whether LMs can effectively generate counterexamples for subtly incorrect code submissions from programming competitions. The methodology involves curating a dynamically updating dataset of problems and incorrect solutions, then prompting LMs to generate inputs that cause the incorrect code to fail. The primary result is that even the best reasoning agents, like OpenAI 03-mini (high) with code execution feedback, can only create counterexamples for less than 9% of incorrect solutions in REFUTE. The main implication is that falsification, specifically counterexample creation, remains a significant challenge for current LMs, highlighting a crucial gap in their reasoning abilities and potential for self-improvement. |
| Other | Towards an AI co-scientist (Read more on [arXiv](https://arxiv.org/abs/2502.18864) or [HuggingFace](https://huggingface.co/papers/2502.18864))| Anil Palepu, Tao Tu, Alexander Daryin, Wei-Hung Weng, Juraj Gottweis | This paper introduces an AI co-scientist, a multi-agent system built on Gemini 2.0, designed to assist scientists in generating novel hypotheses and experimental designs. The main research objective is to develop an AI system capable of formulating demonstrably novel research hypotheses and proposals, building upon prior evidence and aligned with scientist-provided guidance. The system's design incorporates a generate, debate, and evolve approach to hypothesis generation, utilizing a multi-agent architecture with asynchronous task execution and a tournament evolution process for self-improvement. Automated evaluations show continued benefits of test-time compute, with the AI co-scientist achieving a top-1 accuracy of 78.4% on the GPQA diamond set, and experimental validation was performed in drug repurposing, novel target discovery, and explaining mechanisms of bacterial evolution. The main implication for AI practitioners is the demonstration of a scalable, collaborative AI system that can augment scientific discovery by generating and validating novel hypotheses across diverse biomedical domains, though further development and broader evaluations are needed. |
| Reinforcement Learning | VEM: Environment-Free Exploration for Training GUI Agent with Value Environment Model (Read more on [arXiv](https://arxiv.org/abs/2502.18906) or [HuggingFace](https://huggingface.co/papers/2502.18906))| Lingrui Mei, Lu Wang, Jiani Zheng, vyokky, keanudicap | This paper introduces VEM, an environment-free reinforcement learning framework for training graphical user interface (GUI) agents. The main objective is to address the challenges of costly interactions in environment-based RL and distribution shift/reward generalization in environment-free methods for GUI agent training.  The key methodology decouples value estimation from policy optimization by using a pretrained Value Environment Model (VEM) to predict state-action values directly from offline data, distilling human-like priors.  Evaluated on the Android-in-the-Wild benchmark, VEM achieves a 28.0% task success rate on the General domain in the offline setting, outperforming environment-free baselines. AI practitioners can leverage VEM for more efficient and robust GUI agent training, particularly when environment interactions are expensive or unavailable. |
| Natural Language Processing | Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance (Read more on [arXiv](https://arxiv.org/abs/2502.18772) or [HuggingFace](https://huggingface.co/papers/2502.18772))| Polydoros Giannouris, Efstathia Soufleri, Triantafillos Papadopoulos, Xueqing Peng, jiminHuang | This paper introduces Plutus-ben, the first Greek financial evaluation benchmark, and Plutus-8B, a Greek financial Large Language Model (LLM), to address the gap in financial NLP for low-resource languages. The primary objective is to evaluate how well current LLMs perform on core Greek financial tasks and to determine the impact of fine-tuning on Greek financial data. The methodology includes creating a new benchmark with five financial NLP tasks in Greek, developing three new datasets annotated by experts, and fine-tuning an LLM with Greek domain-specific data. Plutus-8B achieves the best performance, surpassing GPT-4 by 15.38%, GPT-40 by 46.34%, and Deepseek-V3 by 93.55% on mean performance. The findings demonstrate that domain-specific fine-tuning significantly enhances model performance, providing practitioners with a benchmark and methods to advance Greek financial NLP and fostering multilingual inclusivity. |
| Computer Vision | Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator (Read more on [arXiv](https://arxiv.org/abs/2502.19204) or [HuggingFace](https://huggingface.co/papers/2502.19204))| Ying Cui, Ruibo Li, Hongji Li, Dongyan Guo, Xiankang He | This paper introduces a new distillation framework for training zero-shot monocular depth estimation (MDE) models using unlabeled images. The main research objective is to improve the generalization and accuracy of MDE models by addressing the limitations of existing depth normalization strategies and leveraging multiple teacher models. The proposed method, Cross-Context Distillation, combines local and global depth cues, and a multi-teacher distillation framework utilizes diverse depth estimation models.  The approach achieves state-of-the-art results on several benchmark datasets; for example on the NYUv2 dataset an AbsRel of 0.043 is achieved. AI practitioners can use this framework to improve the robustness and detail of monocular depth estimators, particularly for applications requiring generalization to diverse and unseen environments. |
| Natural Language Processing | AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement (Read more on [arXiv](https://arxiv.org/abs/2502.16776) or [HuggingFace](https://huggingface.co/papers/2502.16776))| Xijie Huang, Junxiao Yang, Leqi Lei, Zhexin Zhang, LLLeo612 | AISafetyLab is a comprehensive framework and toolkit for evaluating and improving AI safety, particularly for large language models (LLMs). The research introduces AISafetyLab to address the lack of standardized frameworks for AI safety, integrating attack, defense, and evaluation methodologies. It features a modular design with 13 attack methods, 16 defense mechanisms, and 7 evaluation scorers, supporting both local and API-based models.  Experiments on Vicuna-7B-v1.5 showed that AutoDAN achieved a 56.4% average attack success rate across various defenses, while some defenses like Prompt Guard completely neutralized attacks at the cost of higher overrefusal rates. AISafetyLab provides a valuable resource for researchers and developers to systematically assess and enhance the safety of AI systems, promoting standardization and comparison of safety techniques. |
| Natural Language Processing | Project Alexandria: Towards Freeing Scientific Knowledge from Copyright Burdens via LLMs (Read more on [arXiv](https://arxiv.org/abs/2502.19413) or [HuggingFace](https://huggingface.co/papers/2502.19413))| Andreas Hochlehnert, Tawsif Ahmed, Ameya Prabhu, Gollam Rabby, Christoph Schuhmann | This paper proposes a method for extracting and openly sharing scientific knowledge from copyrighted research papers using large language models (LLMs), while respecting copyright restrictions. The main research objective is to determine if it's both legally and technically feasible to separate factual scientific knowledge from copyrighted creative expression in scholarly texts. The proposed methodology involves converting scholarly documents into structured "Knowledge Units" using LLMs, which capture entities, attributes, and relationships, but omit stylistic content. Evaluation via multiple-choice question answering shows that Knowledge Units retain approximately 95% of factual information across several research domains, suggesting a high degree of information preservation. The main implication is that this approach could democratize access to scientific knowledge for AI models and practitioners while navigating copyright law. |
| Natural Language Processing | BIG-Bench Extra Hard (Read more on [arXiv](https://arxiv.org/abs/2502.19187) or [HuggingFace](https://huggingface.co/papers/2502.19187))| Chrysovalantis Anastasiou, John Palowitch, Hritik Bansal, Mehran Kazemi, baharefatemi | This paper introduces BIG-Bench Extra Hard (BBEH), a new benchmark designed to evaluate the general reasoning capabilities of large language models (LLMs). The main research objective is to address the saturation of existing LLM reasoning benchmarks, particularly BIG-Bench Hard (BBH), by creating a more challenging and diverse set of tasks. BBEH replaces each task in BBH with a novel, more difficult task probing similar reasoning skills, requiring capabilities such as many-hop reasoning, handling long-range dependencies, and dealing with distractors.  Evaluation of various LLMs on BBEH revealed a harmonic mean accuracy of 9.8% for the best general-purpose model and 44.8% for the best reasoning-specialized model, indicating substantial room for improvement.  The findings highlight the ongoing challenge of achieving robust general reasoning in LLMs and provide a more discriminating benchmark for future development. |
| Natural Language Processing | CritiQ: Mining Data Quality Criteria from Human Preferences (Read more on [arXiv](https://arxiv.org/abs/2502.19279) or [HuggingFace](https://huggingface.co/papers/2502.19279))| Zhiheng Xi, Tianyi Liang, Qipeng Guo, Kai Lv, KYLN24 | CritiQ is a novel data selection method that automatically mines criteria from human preferences for data quality and performs efficient data selection. The main research objective is to address the limitations of existing data selection approaches that rely on manually designed heuristics, perplexity, or classifiers, which require significant effort and introduce biases. CritiQ employs an agent workflow, CritiQ Flow, using a manager agent to evolve quality criteria and worker agents to make pairwise judgments, boosted by a knowledge base extracted from previous work. CritiQ achieves high accuracy on human-annotated test sets across code, math, and logic domains, with accuracies of 89.89%, 90.00%, and 90.22% respectively and shows improved performance on downstream tasks when used for continual pretraining compared to uniform sampling. This method provides AI practitioners with a way to automatically capture human preferences and improve the training of language models with high-quality data at a lower cost of human annotation. |
| Machine Learning | MolSpectra: Pre-training 3D Molecular Representation with Multi-modal Energy Spectra (Read more on [arXiv](https://arxiv.org/abs/2502.16284) or [HuggingFace](https://huggingface.co/papers/2502.16284))| Qiang Liu, Deli Zhao, Yu Rong, Shaozhen Liu, AzureLeon1 | This paper introduces MolSpectra, a novel framework for pre-training 3D molecular representations by incorporating multi-modal energy spectra. The research objective is to enhance 3D molecular representation learning by integrating quantized energy level structures, represented by molecular spectra, beyond the limitations of classical mechanics.  The methodology involves a SpecFormer multi-spectrum encoder using masked patch reconstruction and a contrastive objective to align 3D and spectral representations.  Experiments on the QM9 dataset show MolSpectra achieved state-of-the-art or competitive Mean Absolute Error (MAE) results on 8 out of 12 properties compared to existing baselines. AI practitioners can leverage MolSpectra for improved molecular property prediction and dynamics modeling by utilizing the often-overlooked information of molecular spectra. |
| Multi-Modal | PosterSum: A Multimodal Benchmark for Scientific Poster Summarization (Read more on [arXiv](https://arxiv.org/abs/2502.17540) or [HuggingFace](https://huggingface.co/papers/2502.17540))| Frank Keller, Pasquale Minervini, rohitsaxena | This paper introduces POSTERSUM, a new multimodal benchmark for summarizing scientific posters into research paper abstracts. The main research objective is to develop vision-language models that can effectively understand and summarize the complex visual and textual information presented in scientific posters. The key methodology involves collecting a large dataset of 16,305 poster-abstract pairs and proposing a hierarchical approach called SEGMENT & SUMMARIZE, which segments the poster, generates localized summaries, and combines them into a final abstract. The proposed SEGMENT & SUMMARIZE method achieves a ROUGE-L score of 24.18, outperforming existing state-of-the-art Multimodal Large Language Models (MLLMs). AI practitioners can use POSTERSUM to advance the development of MLLMs capable of processing and understanding information-dense scientific content, particularly in visually complex formats like posters. |
