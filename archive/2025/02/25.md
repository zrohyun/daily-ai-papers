

## Papers for 2025-02-25

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks (Read more on [arXiv](https://arxiv.org/abs/2502.17157) or [HuggingFace](https://huggingface.co/papers/2502.17157))| Zhiyue Zhao, Mingyu Liu, Z-MU-Z, zhyya, Canyu | DICEPTION is a generalist diffusion model designed for various visual perceptual tasks, achieving performance on par with specialized state-of-the-art models. The primary goal is to create a single model capable of handling multiple perception tasks within limited computational resources and training data by leveraging pre-trained text-to-image diffusion models.  The methodology formulates different perceptual task outputs using color encoding and leverages conditional image generation within a unified diffusion framework (DiT). DICEPTION achieves results comparable to SAM-vit-h using only 0.06% of its training data (600K vs. 1B pixel-level annotated images). AI practitioners can efficiently train a generalist visual perception model with significantly reduced data and computational costs compared to traditional, specialized models, and can quickly adapt the generalist model to a new task with minimal data. |
| Natural Language Processing | Thus Spake Long-Context Large Language Model (Read more on [arXiv](https://arxiv.org/abs/2502.17129) or [HuggingFace](https://huggingface.co/papers/2502.17129))| Yuerong Song, Zhigeng Liu, Mianqiu Huang, Ruixiao Li, LiuXR | This paper presents a survey of long-context large language models (LLMs), focusing on architectural, infrastructural, training, and evaluation technologies. The research aims to provide a comprehensive overview of the lifecycle of long-context LLMs. It analyzes existing literature, categorizing research into architecture, infrastructure, training, and evaluation methodologies. The study showcases advancements in long-context technology, including breakthroughs to millions of tokens and a global perspective of technologies. The survey concludes with ten unanswered questions for future research, aiming to serve as a systematic introduction to the research on long-context LLMs. |
| Natural Language Processing | Slamming: Training a Speech Language Model on One GPU in a Day (Read more on [arXiv](https://arxiv.org/abs/2502.15814) or [HuggingFace](https://huggingface.co/papers/2502.15814))| Yossi Adi, avishai-elmakies, gallilmaimon | The paper introduces "Slam," a recipe for efficiently training high-quality Speech Language Models (SLMs) on a single academic GPU within 24 hours. The main research objective is to determine if it's possible to train high-quality SLMs using a single GPU within a 24-hour timeframe. The methodology involves empirical analysis of model initialization, architecture, synthetic training data, preference optimization, and other training components. The primary results show that the Slam recipe achieves a Topic-StoryCloze score of 82.04 and a GenPPL of 62.8, rivaling state-of-the-art models trained with significantly more compute. This implies that AI practitioners can train and research effective SLMs with limited computational resources by following the presented methods. |
| Multi-Modal | Audio-FLAN: A Preliminary Release (Read more on [arXiv](https://arxiv.org/abs/2502.16584) or [HuggingFace](https://huggingface.co/papers/2502.16584))| Shuai Fan, Zixuan Li, Jiahao Pan, Ziya Zhou, Liumeng Xue | Audio-FLAN is a large-scale instruction-tuning dataset designed to unify audio understanding and generation tasks across speech, music, and sound domains. The main objective is to create a comprehensive dataset that enables zero-shot learning and generalization in audio-language models, similar to successes seen in text and vision. The methodology involves collecting and standardizing nearly all publicly available academic audio datasets into a common instruction-based format, covering 80 diverse tasks with over 100 million instances.  The initial release provides the dataset's structure and examples, but performance is not fully quantitatively benchmarked, only citing related works which achieve ~52.97% on a multi-modal benchmark. The dataset's implication is that it will form the foundational resource needed for AI practitioners to develop unified audio-language models that can seamlessly handle both understanding and generation tasks in a zero-shot manner. |
| Computer Vision | GCC: Generative Color Constancy via Diffusing a Color Checker (Read more on [arXiv](https://arxiv.org/abs/2502.17435) or [HuggingFace](https://huggingface.co/papers/2502.17435))| Yu-Chee Tseng, Yi-Chen Lo, Chia-Che Chang, Cheng-De Fan, Chen-Wei Chang | This paper introduces GCC, a novel method for color constancy that leverages diffusion models to estimate scene illumination by inpainting a color checker into images. The research objective is to improve the generalization of color constancy methods across different camera sensors with varying spectral sensitivities. GCC employs a single-step deterministic inference approach, Laplacian composition for structural preservation, and mask-based data augmentation. The method achieves state-of-the-art worst-25% error rates of 5.15° and 4.32° in bi-directional evaluations across different cameras. This offers AI practitioners a versatile solution for real-world color constancy applications without requiring sensor-specific training. |
| Natural Language Processing | CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.16614) or [HuggingFace](https://huggingface.co/papers/2502.16614))| Yejie Wang, Wei Zhang, Jiaheng Liu, Marcus Dong, Alexander Zhang | This paper introduces CodeCriticBench, a new benchmark for evaluating the code critique capabilities of Large Language Models (LLMs). The research objective is to assess LLMs' ability to provide detailed analysis and constructive feedback on code, addressing limitations of existing critique benchmarks that focus on general domains and lack comprehensive evaluation. CodeCriticBench includes code generation and code QA tasks with varying difficulty, using basic and advanced critique evaluation protocols, including fine-grained checklists. Results from evaluating 38 LLMs show that performance generally improves with model size, with the best models achieving over 70% accuracy on basic critique evaluation and DeepSeek-R1 achieving the best MSE of 3.92 on the advanced code generation critique evaluation. This benchmark provides a more robust and nuanced evaluation of LLMs' code critique capacity, enabling more effective development and deployment of LLMs in coding assistance tools. |
| Natural Language Processing | Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning (Read more on [arXiv](https://arxiv.org/abs/2502.17407) or [HuggingFace](https://huggingface.co/papers/2502.17407))| James Thorne, Jiwoo Hong, Guijin Son, Cartinoe5930 | This paper investigates the linguistic generalizability of test-time scaling methods in mathematical reasoning, focusing on whether performance gains observed in English extend to other languages. The primary research question is whether test-time scaling confers cross-lingual benefits similar to those observed with pre-training scaling. The authors introduce MCLM, a multilingual math benchmark, and evaluate three test-time scaling methods: Outcome Reward Modeling (ORM), Process Reward Modeling (PRM), and Budget Forcing (BF). Key results show that while ORM on Qwen2.5-1.5B Math achieves a score of 35.8 on MCLM, test-time scaling gains are not consistent across languages, improving by an average of only 1.94 points, thus implying that test-time scaling may not effectively generalize to multilingual tasks, in comparison to scaling pre-training. |
| Machine Learning | Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment (Read more on [arXiv](https://arxiv.org/abs/2502.16894) or [HuggingFace](https://huggingface.co/papers/2502.16894))| Wei Wei, Xiaoye Qu, Sichen Liu, Zhenyi Lu, Facico | This paper introduces GOAT, a framework to improve Low-Rank Adaptation (LoRA) for fine-tuning large language models (LLMs) by leveraging adaptive singular value decomposition (SVD) and Mixture-of-Experts (MoE) optimization alignment. The main research questions are how to adaptively integrate pre-trained knowledge into LoRA MoE and how to mitigate the optimization gap in LoRA MoE initialized with prior information. The key methodology involves initializing LoRA MoE experts with distinct SVD segments of pre-trained weights and deriving a theoretical scaling factor to align optimization with full fine-tuned MoE. Experiments across 25 datasets demonstrate that GOAT achieves state-of-the-art performance, outperforming existing methods; for instance, on image classification tasks, GOAT achieves 99.07% of full fine-tuning performance, surpassing LoRA by 6.0%. For AI practitioners, GOAT offers a way to significantly improve LoRA's performance, closing the gap with full fine-tuning while maintaining scalability and computational efficiency. |
| Multi-Modal | Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2502.16033) or [HuggingFace](https://huggingface.co/papers/2502.16033))| Yang Zhao, Shan Jiang, Hongquan Li, Yue Fan, Qianqi Yan | This paper introduces the Multimodal Inconsistency Reasoning (MMIR) benchmark to evaluate the ability of Multimodal Large Language Models (MLLMs) to detect semantic mismatches in real-world artifacts. The main research objective is to assess whether MLLMs can identify and reason about inconsistencies, such as factual contradictions or identity misattributions, within complex, layout-rich content like webpages and slides. The methodology involves creating a dataset of 534 challenging samples with synthetically injected errors across five reasoning-heavy categories and evaluating six state-of-the-art MLLMs. Results show that the top-performing model (o1) achieved just over 50% accuracy, significantly outperforming open-source models, which scored below 25%, highlighting a substantial area for improvement. These findings indicate that even advanced MLLMs still struggle with robust cross-modal reasoning needed to effectively address multimodal inconsistencies, suggesting a need for more advanced multimodal reasoning techniques. |
| Multi-Modal | Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration (Read more on [arXiv](https://arxiv.org/abs/2502.17110) or [HuggingFace](https://huggingface.co/papers/2502.17110))| Ji Zhang, Ming Yan, Xi Zhang, Junyang Wang, xhyandwyy | Mobile-Agent-V is a framework that leverages video guidance to provide operational knowledge for mobile device automation. The main research objective is to address the challenge of insufficient operational knowledge in existing AI-driven frameworks for mobile device task management. The key methodology involves a sliding window strategy, a video agent, and a deep-reflection agent to process video inputs and ensure actions align with user instructions. Experimental results show that Mobile-Agent-V achieves a 30% performance improvement compared to existing frameworks. This implies that AI practitioners can improve mobile automation by incorporating video-guided learning, providing a scalable solution for task execution. |
| Natural Language Processing | Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties (Read more on [arXiv](https://arxiv.org/abs/2502.16922) or [HuggingFace](https://huggingface.co/papers/2502.16922))| Deyu Zhou, Yong Jiang, Pengfei LI, Jialong Wu, wzl0228 | This paper introduces the Chinese Time Reasoning (CTM) benchmark, a new dataset for evaluating large language models (LLMs) on temporal reasoning within the context of Chinese dynastic history. The main objective is to assess LLMs' ability to perform temporal reasoning and alignment across a wide range of entities and complex relationships, going beyond existing benchmarks that often lack contextual depth. The methodology involves creating a dataset with 8,750 QA pairs and 60 Timeline Ito Game instances, emphasizing cross-entity relationships, pairwise temporal alignment, and culturally grounded reasoning, using an LLM-based generation process. Evaluations show that the CTM benchmark poses a significant challenge to state-of-the-art LLMs like GPT-4o and Qwen models, with the best model, Deepseek-R1, achieving an average accuracy of 64.02 on QA tasks.  AI practitioners can use this benchmark to develop and refine LLMs with enhanced temporal understanding, particularly in culturally rich contexts. |
| Computer Vision | RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2502.15894) or [HuggingFace](https://huggingface.co/papers/2502.15894))| Chongxuan Li, Yixiao Chen, Guande He, Min Zhao, zhuhz22 | This paper introduces RIFLEx, a novel method for length extrapolation in video diffusion transformers that addresses the common issues of temporal repetition and motion deceleration. The central research question is how to enable video diffusion models to generate temporally coherent videos longer than those they were trained on, without extensive retraining. RIFLEx works by identifying and reducing an 'intrinsic frequency' within the positional embeddings that primarily governs extrapolation behavior, suppressing repetition while preserving motion consistency. Experiments on state-of-the-art video diffusion transformers show that RIFLEx achieves high-quality 2x extrapolation in a training-free manner and up to 3x with minimal fine-tuning, outperforming existing methods; for example, on the CogVideoX-5B model, RIFLEx achieved a NoRepeat Score of 54.2 compared to PE's 46.6 in a training free setting. AI practitioners can utilize RIFLEx as an efficient way to extend video generation length with diffusion transformers, achieving considerable quality improvements with minimal or no additional training. |
| Natural Language Processing | Can Community Notes Replace Professional Fact-Checkers? (Read more on [arXiv](https://arxiv.org/abs/2502.14132) or [HuggingFace](https://huggingface.co/papers/2502.14132))| Isabelle Augenstein, Desmond Elliott, gretawarren, Nadav | This paper investigates the relationship between community-generated notes and professional fact-checking on Twitter/X, aiming to determine the reliance of community notes on fact-checking sources. The main research questions are to what extent community notes rely on the work of professional fact-checkers and what traits of posts and notes are linked to the use of fact-checking sources. The authors use language models to annotate a large corpus of Twitter/X community notes with attributes such as topic, cited sources, and narrative refutation, and conduct manual and LLM-based annotation.  A key finding is that at least 5% of all English community notes contain an external link to professional fact-checkers, growing to 7% for notes rated 'helpful'. The results indicate that community moderation heavily relies on fact-checking, suggesting that the success of community-based moderation systems is contingent on the availability and integration of professional fact-checking resources. |
| Multi-Modal | Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation (Read more on [arXiv](https://arxiv.org/abs/2502.16707) or [HuggingFace](https://huggingface.co/papers/2502.16707))| Sergey Levine, Xiangyu Yue, Zhuoran Yang, csuhan, yunhaif | This paper introduces Reflective Planning, a novel framework that enhances vision-language models (VLMs) for multi-stage, long-horizon robotic manipulation tasks. The main research objective is to improve VLMs' physical reasoning and long-horizon planning capabilities in complex robotic manipulation problems involving sequential interactions with interlocking objects. The key methodology combines a pretrained VLM with a "reflection" mechanism, using a generative diffusion model to imagine future states, guide action selection, and critically refine reasoning through iterative feedback. Experimental results demonstrate that the proposed method significantly outperforms several state-of-the-art commercial VLMs and other post-training approaches, achieving a success rate of 85.4% compared to 47.8 of a pretrained VLM baseline across 100 tasks with various unseen object configurations. The main implication is that integrating structured reasoning mechanisms like visual imagination and reflection at test time can significantly enhance VLMs' performance on physically-grounded tasks, opening up new avenues for applying these models in complex real-world robotic applications. |
| Machine Learning | Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam (Read more on [arXiv](https://arxiv.org/abs/2502.17055) or [HuggingFace](https://huggingface.co/papers/2502.17055))| Xiang Li, Gaojie Jin, Zhenyu Zhang, Haotian Hu, Tianjin Huang | This paper introduces Stable-SPAM, a new optimizer designed to improve the stability of 4-bit training of large language models (LLMs). The research question is how to mitigate the increased sensitivity to learning rates and unstable gradient norms observed during low-bit precision training compared to 16-bit Adam. The proposed method, Stable-SPAM, incorporates adaptive gradient normalization, adaptive spike-aware gradient clipping, and momentum reset.  Experiments show that 4-bit LLaMA-1B model trained with Stable-SPAM outperforms the BF16 LLaMA-1B with Adam by up to 2 perplexity points.  AI practitioners can use Stable-SPAM to train LLMs with reduced memory and computational costs more reliably, achieving better performance than standard optimizers in low-bit precision settings. |
| Machine Learning | Forecasting Open-Weight AI Model Growth on Hugging Face (Read more on [arXiv](https://arxiv.org/abs/2502.15987) or [HuggingFace](https://huggingface.co/papers/2502.15987))| Jianxi Gao, Pin-Yu Chen, KBhandari11 | This paper proposes a framework to forecast the growth and influence of open-weight AI models on the Hugging Face platform. The main objective is to predict the trajectory of influence an open-weight model will have, analogous to how scientific paper citations are tracked.  The methodology adapts a citation dynamics model, using parameters like immediacy, longevity, and relative fitness to track the cumulative number of fine-tuned models. Results show the approach effectively captures diverse adoption patterns, with most model trajectories fitting well, though outliers reveal unusual usage; for example the immediacy (μ₁) is concentrated around 10^-4. AI practitioners can use this framework to predict the trajectory of fine-tuning models in order to make optimal strategic decisions, such as what models will be most influential in the future. |
| Other | Beyond Release: Access Considerations for Generative AI Systems (Read more on [arXiv](https://arxiv.org/abs/2502.16701) or [HuggingFace](https://huggingface.co/papers/2502.16701))| Yacine Jernite, Ariel Herbert-Voss, Dan Hendrycks, Rishi Bommasani, irenesolaiman | This paper explores the critical distinction between the release and accessibility of generative AI systems, focusing on how access to system components influences potential risks and benefits. The main research objective is to analyze access variables beyond mere release decisions to better inform system deployment, research, and policy. The methodology involves deconstructing access along three axes—resourcing, technical usability, and utility—and examining variables within each category for different system components, using case studies like Llama 3.1, DeepSeek v3, GPT-4, and Claude 3.5 Sonnet for comparative analysis, though specific quantitative comparisons between the models are lacking in the provided text. The main implication for AI practitioners is that a focus beyond component release and towards granular factors of accessibility provides valuable insights into risk-benefit management for generative AI systems. |
| Reinforcement Learning | TAG: A Decentralized Framework for Multi-Agent Hierarchical Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2502.15425) or [HuggingFace](https://huggingface.co/papers/2502.15425))| Balázs Kégl, Albert Thomas, Hamza Cherkaoui, Abdelhakim Benechehab, Giuseppe Paolo | This paper introduces TAME Agent Framework (TAG), a decentralized framework for constructing multi-agent hierarchical reinforcement learning systems of arbitrary depth. The research objective is to address the limitations of current hierarchical reinforcement learning approaches, which often restrict hierarchies to two levels or require centralized training. TAG achieves this through a novel LevelEnv concept, abstracting each hierarchy level as an environment for agents above it, standardizing information flow while enabling loose coupling and diverse agent integration. Empirical validation on standard benchmarks showed improved performance over classical multi-agent RL baselines (e.g., in the MPE-Spread environment, only the depth-three agents, 3PPO and 2MAPPO-PPO, matched a hand-designed heuristic performance). TAG offers AI practitioners a promising direction for scalable multi-agent systems by enhancing both learning speed and final performance through decentralized hierarchical organization. |
| Multi-Modal | X-Dancer: Expressive Music to Human Dance Video Generation (Read more on [arXiv](https://arxiv.org/abs/2502.17414) or [HuggingFace](https://huggingface.co/papers/2502.17414))| Chenxu Zhang, You Xie, Guoxian Song, Hongyi Xu, Zeyuan Chen | X-Dancer is a novel zero-shot music-driven image animation pipeline that generates diverse and long-range lifelike human dance videos from a single static image. The research objective is to generate smooth, diverse, full-body dance movements synchronized with input music and translate these movements into high-fidelity videos. The key methodology integrates an autoregressive transformer model for generating 2D pose sequences aligned with music, and a diffusion model for producing high-resolution video frames, using a multi-part tokenization scheme for 2D whole-body poses. The method outperforms baselines, achieving a FVD of 531.52 and 238.22 on AIST++ and in-house dataset, respectively, which demonstrates a great improvement compared to existing solutions. X-Dancer presents for AI practitioners a scalable and customizable approach combining transformer-based motion generation with diffusion-based video synthesis, enabling expressive and realistic human image animation. |
| Computer Vision | VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing (Read more on [arXiv](https://arxiv.org/abs/2502.17258) or [HuggingFace](https://huggingface.co/papers/2502.17258))| Yi Yang, Hehe Fan, Linchao Zhu, Xiangpeng Yang | VideoGrain is a zero-shot approach for multi-grained video editing that modulates space-time attention mechanisms within diffusion models. The main objective is to achieve fine-grained control over video content at the class, instance, and part levels, addressing challenges like semantic misalignment and feature coupling. The key methodology involves modulating cross-attention to enhance text-to-region control and self-attention to improve feature separation between regions. The method achieves state-of-the-art performance on real-world scenarios, demonstrating an Edit-Accuracy of 88.4, outperforming existing methods.  AI practitioners can leverage this approach for improved, fine-grained video editing without the need for parameter tuning. |
| Machine Learning | MONSTER: Monash Scalable Time Series Evaluation Repository (Read more on [arXiv](https://arxiv.org/abs/2502.15122) or [HuggingFace](https://huggingface.co/papers/2502.15122))| Amish Mishra, Lynn Miller, Chang Wei Tan, Navid Mohammadi Foumani, angus924 | This paper introduces MONSTER, a new benchmark repository of large datasets for time series classification. The research objective is to address the limitations of existing benchmarks, which predominantly feature small datasets and thus favor low-variance models, potentially hindering progress in real-world applications with larger datasets. The methodology involves curating a collection of 29 large univariate and multivariate datasets and evaluating the performance of several existing time series classification methods, including deep learning and specialized approaches. Initial baseline results across the MONSTER datasets shows that the QUANT model achieves the lowest overall mean 0-1 loss of 0.1880. AI practioners may find it a better representation of a benchmark to the broader task of time series classification and to improving the relavance for real-world time serires classification problems. |
| Other | The snake in the Brownian sphere (Read more on [arXiv](https://arxiv.org/abs/2502.13074) or [HuggingFace](https://huggingface.co/papers/2502.13074))| Grégory Miermont, Brett Kolesnik, Emmanuel Jacob, Omer Angel | This paper describes the inverse of the continuous Cori-Vauquelin-Schaeffer (CVS) bijection, relating the Brownian sphere to the Brownian snake. The main research objective is to construct the Brownian snake as a measurable function of the Brownian sphere, effectively inverting the continuous CVS mapping. The methodology involves using the geometric notion of a cut locus and its properties within the Brownian sphere to reconstruct the labeled tree (Brownian snake). The primary result, detailed in Theorem 1, demonstrates that there exists a measurable function that outputs an R-tree T and a label function Z such that T has the law of the CRT, and Z represents Brownian labels on T, and the application of the continuum CVS mapping to (T,Z) will recover (X,d,µ). There are several supporting theorems and not a single quantitative value making it impossible to include a specific metric. |
| Multi-Modal | M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment (Read more on [arXiv](https://arxiv.org/abs/2502.15167) or [HuggingFace](https://huggingface.co/papers/2502.15167))| Weiming Zhang, Wen Shen, Zhihua Wei, Kejiang Chen, Chuan Cui | This paper introduces M3-AGIQA, a comprehensive framework for assessing the quality of AI-generated images (AGIs) across multiple dimensions. The main research objective is to develop a method that evaluates AGIs considering perceptual quality, prompt correspondence, and authenticity, aligning with human perceptual judgments. The methodology leverages Multimodal Large Language Models (MLLMs) as joint text and image encoders, uses a multi-round evaluation mechanism with intermediate image descriptions, and incorporates an xLSTM-based predictor to align predictions with Mean Opinion Scores (MOSs). Experimental results demonstrate state-of-the-art performance, achieving a PLCC score of 0.9317 and SRCC of 0.9045 on the AGIQA-3k dataset for quality assessment, outperforming existing methods. AI practitioners can use this framework to better evaluate and improve the generative capabilities of AGI models, ensuring higher quality and better alignment with user intentions. |
