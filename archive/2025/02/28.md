

## Papers for 2025-02-28

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Machine Learning | Self-rewarding correction for mathematical reasoning (Read more on [arXiv](https://arxiv.org/abs/2502.19613) or [HuggingFace](https://huggingface.co/papers/2502.19613))| Nan Jiang, Lichang Chen, Chenlu Ye, Hanning Zhang, Wei Xiong | This paper introduces a self-rewarding reasoning framework for large language models (LLMs) that integrates generation and evaluation capabilities into a single model for mathematical reasoning tasks. The main research objective is to enable LLMs to autonomously generate step-by-step reasoning, evaluate the correctness of their outputs, and revise their responses without external feedback.  A two-staged algorithmic framework is proposed, using sequential rejection sampling to create training data and reinforcement learning with rule-based signals to enhance self-correction. Experiments with Llama-3 and Qwen-2.5 show that the approach surpasses intrinsic self-correction and achieves performance on par with systems using external reward models, achieving 80.2% final accuracy on the MATH500 benchmark after self-rewarding IFT + PPO training. This framework offers computational advantages for model deployment and suggests a path towards more efficient and autonomous reasoning in LLMs. |
| Multi-Modal | MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2502.19634) or [HuggingFace](https://huggingface.co/papers/2502.19634))| Jiayuan Zhu, Fenglin Liu, Junde Wu, Jiazhen Pan, che111 | This paper introduces MedVLM-R1, a medical vision-language model (VLM) that generates natural language reasoning alongside answers for radiological visual question answering (VQA). The research aims to enhance transparency and trustworthiness in medical image analysis by developing a VLM that explicitly reveals its underlying reasoning process, unlike most existing VLMs that provide only final answers. MedVLM-R1 employs a reinforcement learning framework, specifically Group Relative Policy Optimization (GRPO), to incentivize the model to discover human-interpretable reasoning paths without using explicit reasoning supervision during training.  The model achieves an accuracy boost from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming larger models, demonstrating superior generalization and efficiency on out-of-distribution tasks. This research suggests that reinforcement learning is effective for improving VLM reasoning, particularly in low data medical domains where interpretable results are crucial. |
| Multi-Modal | R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts (Read more on [arXiv](https://arxiv.org/abs/2502.20395) or [HuggingFace](https://huggingface.co/papers/2502.20395))| Tianyi Zhou, Ziyue Li, Zhongyang Li | This paper introduces R2-T2 (Re-Routing in Test-Time), a novel method for improving the performance of multimodal Mixture-of-Experts (MoE) models during inference without retraining. The main research question is how to optimize routing weights in test-time to improve the selection of experts in a multimodal MoE, addressing the sub-optimality of end-to-end trained routers. The key methodology involves locally optimizing routing weights by moving them towards the weights of correctly predicted samples in a neighborhood of the test sample, using strategies like neighborhood gradient descent, kernel regression, and mode finding.  Applying R2-T2 to MoAI-7B, the method achieved a +6.9% improvement on the MMBench benchmark, significantly surpassing the base model's performance, and performs comparably to much larger models. The main implication for AI practitioners is that test-time routing weight optimization can significantly enhance multimodal MoE model performance without expensive retraining or changes in the model architecture. |
| Natural Language Processing | LongRoPE2: Near-Lossless LLM Context Window Scaling (Read more on [arXiv](https://arxiv.org/abs/2502.20082) or [HuggingFace](https://huggingface.co/papers/2502.20082))| Gilsinia Lopez, Gaokai Zhang, Siyuan Wang, Li Lyna Zhang, Ning Shang | LongRoPE2 is a novel approach for extending the effective context window of pre-trained large language models (LLMs) while preserving performance on the original shorter context window. The main research objective is to address the out-of-distribution (OOD) issues in rotary positional embeddings (RoPE) and the performance degradation on short-context tasks during context window extension.  The key methodology involves an evolutionary search for optimal RoPE rescaling factors, guided by "needle-driven" perplexity, and a mixed context window training approach.  LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context length while retaining over 98.5% of short-context performance, using only 10B tokens. For AI practitioners, this implies the potential to build LLMs with significantly extended context windows while maintaining strong performance across various tasks, and with reduced training token requirements. |
| Natural Language Processing | FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving (Read more on [arXiv](https://arxiv.org/abs/2502.20238) or [HuggingFace](https://huggingface.co/papers/2502.20238))| Chaoqun Liu, Hou Pong Chan, Hao Zhang, Weiwen Xu, Guizhen Chen | This paper introduces FINEREASON, a benchmark for evaluating the deliberate reasoning capabilities of Large Language Models (LLMs) through reflective puzzle solving. The research aims to assess and improve LLMs' ability to reflect and rectify mistakes during multi-step reasoning, beyond final-answer accuracy. The methodology involves decomposing logic puzzles into atomic steps and evaluating LLMs on two tasks: state checking (assessing solvability) and state transition (planning the next move).  Results show that reasoning-oriented models like o1 outperform others, achieving 75.6% average accuracy on state checking and transition in Sudoku, while general-purpose models struggle. The main implication is that incorporating puzzle-based training data enhances LLMs' general mathematical reasoning performance, suggesting a path for improving deep reasoning abilities. |
| Machine Learning | CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale (Read more on [arXiv](https://arxiv.org/abs/2502.16645) or [HuggingFace](https://huggingface.co/papers/2502.16645))| Kaiyue Qiu, Zhaoyang Chu, Chenlong Wang, yxy0807, zx10086 | This paper introduces CODESYNC, a data engine and benchmark for evaluating large language models' (LLMs) ability to adapt to evolving code, specifically third-party library API updates. The main research objective is to assess how effectively and efficiently LLMs can be updated to handle real-time API modifications. CODESYNC systematically identifies API updates and collects real-world code invocations, using them to create a benchmark (CODESYNCBENCH) with three evaluation tasks and a dataset for instruction tuning. Experiments on 14 LLMs reveal that they struggle with dynamic code evolution, even with knowledge updating methods; for example, on the Code Completion Task, leading commercial models achieved BLEU scores below 20%. AI practitioners need to develop more effective methods for real-time code knowledge updating in LLMs to ensure the reliability and stability of software in the face of dynamic API changes. |
| Reinforcement Learning | Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance (Read more on [arXiv](https://arxiv.org/abs/2502.16944) or [HuggingFace](https://huggingface.co/papers/2502.16944))| Zhixu Li, Pu Zhao, Lu Wang, Chenghua Huang, keanudicap | This paper introduces Decoupled Value Policy Optimization (DVPO), a new framework for Reinforcement Learning from Human Feedback (RLHF) that improves efficiency and stability. The main objective is to address the computational complexity and instability issues of Proximal Policy Optimization (PPO)-based RLHF caused by joint actor-critic training and lack of access to true environment rewards. DVPO replaces traditional reward modeling with a pre-trained global value model (GVM), which is conditioned on policy trajectories and predicts token-level return-to-go estimates, decoupling value estimation from policy training. Experiments show DVPO outperforms efficient RLHF methods like DPO, matches state-of-the-art PPO performance, while reducing GPU memory usage by 40% and training time by 35%. DVPO offers a lean, scalable approach to fine-tuning large language models, simplifying the RLHF process and mitigating common instability issues for AI practitioners. |
| Multi-Modal | UniTok: A Unified Tokenizer for Visual Generation and Understanding (Read more on [arXiv](https://arxiv.org/abs/2502.20321) or [HuggingFace](https://huggingface.co/papers/2502.20321))| Xin Yu, Jihan Yang, Junfeng Wu, Yi Jiang, Chuofan Ma | This paper introduces UniTok, a unified visual tokenizer designed to bridge the gap between visual generation and understanding within a single framework. The central research objective is to determine if reconstruction and contrastive losses truly conflict in unified tokenizer training, or if there's an underlying bottleneck. To address this, the authors propose multi-codebook quantization and attention factorization to enhance the expressiveness of discrete visual tokens. UniTok achieves a remarkable rFID of 0.38 and a zero-shot accuracy of 78.6% on ImageNet. The implication is that unified, discrete visual tokenizers can achieve state-of-the-art performance in both generation and understanding tasks, offering a strong foundation for multimodal models. |
| Computer Vision | FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute (Read more on [arXiv](https://arxiv.org/abs/2502.20126) or [HuggingFace](https://huggingface.co/papers/2502.20126))| Markos Georgopoulos, Jonas Kohler, Yeongmin Kim, Gregor Bachmann, Sotiris Anagnostidis | FlexiDiT is a framework that enables pre-trained Diffusion Transformer (DiT) models to dynamically adjust their compute budget during inference, achieving significant computational savings without quality loss. The main research question is whether it is possible to deviate from the conventional static compute allocation in DiTs and instead propose a dynamic strategy that utilizes compute more efficiently. The key methodology is "flexifying" DiTs, allowing them to process images with different patch sizes at different diffusion steps, controlled by a scheduler that leverages larger patches for early denoising steps. Primary results show a reduction of over 40% in FLOPs for class-conditioned and text-conditioned ImageNet generation, and up to 75% for video generation, with minimal or no drop in FID score (e.g. maintaining FID-50k = 2.25 for ImageNet generation). AI practitioners can apply FlexiDiT to existing DiT models to significantly reduce inference costs without sacrificing output quality, or to optimize the trade-off between quality and compute budget. |
| Multi-Modal | Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think (Read more on [arXiv](https://arxiv.org/abs/2502.20172) or [HuggingFace](https://huggingface.co/papers/2502.20172))| Haozhe Zhao, Weichu Xie, Wenhao Chai, Shuai Bai, Liang Chen | This paper introduces DREAM ENGINE, a framework for image generation that supports arbitrary text-image interleaved control. The research objective is to develop a system capable of merging visual concepts from multiple images guided by complex textual and visual instructions, a task where existing models struggle. The approach replaces text encoders in diffusion models like SD3.5 with a Large Multimodal Model (LMM), specifically QwenVL, and utilizes a two-stage training paradigm for joint text-image alignment and multimodal interleaved instruction tuning. DREAM ENGINE achieves a 0.69 overall score on the GenEval benchmark, matching state-of-the-art models and demonstrating strong capabilities in compositional image generation. For AI practitioners, this framework offers a more efficient and unified approach to multimodal image generation, making it simpler to create complex visual compositions by combining different image sources and descriptive texts without needing special architectures or multiple training steps. |
| Natural Language Processing | NeoBERT: A Next-Generation BERT (Read more on [arXiv](https://arxiv.org/abs/2502.19587) or [HuggingFace](https://huggingface.co/papers/2502.19587))| Sarath Chandar, Mariam El Mezouar, Quentin Fournier, Lola Le Breton | NeoBERT is a new bidirectional encoder model designed to improve upon existing BERT-like models for downstream NLP tasks. The main objective is to create a more efficient and powerful pre-trained encoder by incorporating architectural advancements, modern data, and optimized training strategies. NeoBERT utilizes a deeper architecture, rotary positional embeddings (RoPE), RMSNorm, SwiGLU activations, and is pre-trained on the RefinedWeb dataset with a two-stage procedure to extend the context length to 4,096 tokens.  It achieves a state-of-the-art score of 89.0% on the GLUE benchmark, comparable to larger models, and outperforms all competing pre-trained models on the MTEB benchmark with an average score of 51.3.  AI practitioners can use NeoBERT as a plug-and-play replacement for existing base models to improve performance in a variety of downstream applications, particularly those requiring longer context windows. |
| Computer Vision | Mobius: Text to Seamless Looping Video Generation via Latent Shift (Read more on [arXiv](https://arxiv.org/abs/2502.20307) or [HuggingFace](https://huggingface.co/papers/2502.20307))| Xiaodong Cun, Yong Zhang, Bo Liu, Jianfei Yuan, Xiuli Bi | This paper introduces Mobius, a novel method for generating seamless looping videos directly from text descriptions without user annotations. The research aims to create a training-free approach to generate looping videos using pre-trained text-to-video diffusion models, addressing limitations of prior cinemagraph methods. The key methodology involves a latent shift strategy, which connects the start and end noise latents and shifts the denoising context in each step, along with frame-invariant decoding and rotary position embedding interpolation. The method achieves a Frame-Video Distance (FVD) of 40.78 and reports higher visual quality and text-video alignment scores as compared to previous baselines, and a user study showed mean video dynamic score of 4.1. AI practitioners can leverage this approach to generate high-quality, seamless looping videos of arbitrary lengths directly from text prompts, enhancing multimedia content creation without the need for manual video editing or specific input images. |
| Reinforcement Learning | SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2502.20127) or [HuggingFace](https://huggingface.co/papers/2502.20127))| Yanzhen Zou, Xiangxin Meng, Pengfei Gao, Chao Peng, mizersy | This paper introduces Subtask-oriented Reinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue-resolving capabilities of Large Language Models (LLMs). The main objective is to improve the performance and generalization of open-source LLMs in resolving software issues while addressing the limitations of existing training approaches. SoRFT decomposes issue resolving into structured subtasks and utilizes a two-stage training process: rejection-sampled supervised fine-tuning and rule-based reinforcement learning with ground-truth based rewards.  Experimental results show that SoRFT-Qwen-7B achieves state-of-the-art performance among open-source models, resolving 21.4% of issues on the SWE-Bench Verified dataset. The main implication is that SoRFT provides a cost-efficient and effective alternative to commercial models for automated issue resolution, leveraging open-source development resources and improving model generalization. |
| Computer Vision | Building Interactable Replicas of Complex Articulated Objects via Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2502.19459) or [HuggingFace](https://huggingface.co/papers/2502.19459))| Song-Chun Zhu, Junfeng Ni, Ruijie Lu, Baoxiong Jia, Yu Liu | This paper introduces ArtGS, a novel approach for building interactable replicas of complex articulated objects using 3D Gaussian Splatting. The main research objective is to accurately reconstruct part-meshes and model part dynamics of complex multi-part articulated objects, overcoming limitations of existing methods that struggle with information integration across different object states. ArtGS leverages 3D Gaussians with coarse-to-fine initialization, canonical Gaussian updates, and a skinning-inspired part dynamics modeling module for improved articulation learning. Experiments on synthetic and real-world datasets, including a new benchmark, demonstrate that ArtGS achieves state-of-the-art performance in joint parameter estimation and part mesh reconstruction, for example achieving average joint angle and position errors of 0.02/0.00 and 0.01/0.00 for a fridge and a storage, surpassing existing method, especially for multi-part objects. The implication is that ArtGS provides a more efficient and accurate approach for creating digital twins of articulated objects, facilitating applications in robotics and virtual environments. |
| Natural Language Processing | R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning (Read more on [arXiv](https://arxiv.org/abs/2502.19735) or [HuggingFace](https://huggingface.co/papers/2502.19735))| Hongyong Zeng, Yuanchang Luo, Shimin Tao, Yilun Liu, boommmmm | This paper introduces R1-Translator (R1-T1), a novel framework for enhancing machine translation (MT) in large language models (LLMs) through reasoning learning. The main research objective is to improve MT performance by incorporating human-aligned chain-of-thought (CoT) reasoning, moving beyond existing methods that focus on specific sub-tasks or use synthetic CoTs. The key methodology involves formalizing six expert-curated CoT templates, enabling self-evolving CoT discovery via reinforcement learning (RL) with KL-constrained rewards, and supervised fine-tuning on a curated dataset of hybrid translation CoT trajectories. Experimental results show steady translation performance improvement in 21 languages and 80 directions on the Flores-101 test set, especially on the 15 languages unseen from training (Table 3), demonstrating the model's ability to generalize and maintain general multilingual abilities. This approach allows AI practitioners to develop more adaptable and robust MT systems capable of handling diverse translation scenarios, even those without pre-existing CoT data. |
