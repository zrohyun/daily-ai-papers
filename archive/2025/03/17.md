

## Papers for 2025-03-17

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | ReCamMaster: Camera-Controlled Generative Rendering from A Single Video (Read more on [arXiv](https://arxiv.org/abs/2503.11647) or [HuggingFace](https://huggingface.co/papers/2503.11647))| Zuozhu, Mu437, Xintao, menghanxia, jianhongbai | ReCamMaster is a camera-controlled generative video re-rendering framework that re-renders input videos with novel camera trajectories. The main research objective is to address the under-explored problem of altering camera trajectories of a given video while maintaining multi-frame appearance and dynamic synchronization. The core innovation is a video conditioning mechanism that leverages the generative capabilities of pre-trained text-to-video models, combined with a multi-camera synchronized video dataset created using Unreal Engine 5. The method outperforms existing state-of-the-art approaches, achieving a FID score of 57.10 and FVD score of 122.74, and finds applications in video stabilization, super-resolution, and outpainting. AI practitioners can use this approach for enhanced video editing and creation with controlled camera movements, potentially improving film production and content creation workflows. |
| Robotics | Adversarial Data Collection: Human-Collaborative Perturbations for
  Efficient and Robust Robotic Imitation Learning (Read more on [arXiv](https://arxiv.org/abs/2503.11646) or [HuggingFace](https://huggingface.co/papers/2503.11646))| AutobotZero, hsli-cuhk, Eralien, morninghaze, SiyuanH | This paper introduces Adversarial Data Collection (ADC), a novel framework for improving data efficiency and robustness in robotic imitation learning. The research objective is to maximize the informational density of demonstrations and reduce reliance on large-scale datasets. ADC employs a Human-in-the-Loop approach where an adversarial operator dynamically perturbs visual and linguistic aspects during teleoperation, forcing the primary operator to adapt.  Models trained with only 20% of ADC-collected demonstrations significantly outperformed models using 100% of traditionally collected data achieving a 0.89 average success rate on a combined static and dynamic task, vs. 0.24 for traditional method. This work suggests that strategic data acquisition, via human-guided perturbations, is crucial for scalable and robust robotic learning. |
| Machine Learning | Technologies on Effectiveness and Efficiency: A Survey of State Spaces
  Models (Read more on [arXiv](https://arxiv.org/abs/2503.11224) or [HuggingFace](https://huggingface.co/papers/2503.11224))| yuchenFan, xuekai, iseesaw, Youbang, XingtaiHF | This paper provides a comprehensive survey of State Space Models (SSMs), a class of sequence models emerging as an alternative to the transformer architecture. The main research objective is to systematically overview SSMs, including their theoretical motivations, formulations, comparisons with other model classes, and various applications. The key methodology involves categorizing SSMs into three stages (original SSM, structured SSM exemplified by S4, and selective SSM represented by Mamba) and detailing core techniques like discretization, HiPPO, and DPLR.  While the paper surveys a wide range of results, specific model performance comparison metrics are dispersed; DSS achieves 81.88 average accuracy on the LRA benchmark, closely rivaling S4's 80.21.  The main implication is that SSMs, especially Mamba and its variants, offer efficient and effective solutions for sequence modeling tasks, excelling in long-sequence handling, and can serve as a promising new direction for sequence modeling research. |
| Multi-Modal | API Agents vs. GUI Agents: Divergence and Convergence (Read more on [arXiv](https://arxiv.org/abs/2503.11069) or [HuggingFace](https://huggingface.co/papers/2503.11069))| Eliblo1969, SiQin88, liqul, shilhe, vyokky | This paper provides a comprehensive comparative analysis of API-based and GUI-based Large Language Model (LLM) agents for software automation. The main objective is to systematically examine the divergence, convergence, and potential hybrid approaches of these two agent paradigms. The methodology involves analyzing key dimensions such as modality, reliability, efficiency, and presenting practical use cases and decision criteria. While quantitative results are implied (e.g. GUI agents require more steps), specific numbers are not presented in the provided abstract, thus, an appropriate quantitative metric can not be offered here. The main implication is that AI practitioners should carefully consider the strengths and weaknesses of each approach or a hybrid approach based on specific project needs, balancing factors like development workflows, user interaction models, and system requirements. |
| Multi-Modal | Large-scale Pre-training for Grounded Video Caption Generation (Read more on [arXiv](https://arxiv.org/abs/2503.10781) or [HuggingFace](https://huggingface.co/papers/2503.10781))| Josef Sivic, Cordelia Schmid, ekazakos | This paper introduces a novel approach for generating grounded video captions, where objects in the generated captions are localized in the video with temporally consistent bounding boxes. The main research objective is to develop a method for large-scale pre-training of a model capable of both generating natural language descriptions of videos and grounding key noun phrases within those descriptions to corresponding objects in the video. The key methodology involves a three-stage automatic annotation process to create a large-scale dataset (HowToGround1M), a new model called GROVE with spatio-temporal adapters, a bounding box decoder, and temporal objectness head.  The GROVE model, when pre-trained on HowToGround1M and fine-tuned on the manually annotated iGround dataset, achieved a CIDEr score of 85.4 and an AP50 of 40.8 on the iGround test set. The approach demonstrates the effectiveness of large-scale pre-training with automatically generated annotations, followed by fine-tuning on smaller, high-quality datasets, significantly improving the state-of-the-art in grounded video caption generation and potentially impacting areas like human-robot interaction. |
| Natural Language Processing | TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of
  Tools (Read more on [arXiv](https://arxiv.org/abs/2503.10970) or [HuggingFace](https://huggingface.co/papers/2503.10970))| Richard Zhu, marinkaz, Blair1213, ayushnoori, shgao | TxAgent is an AI agent designed for precision therapeutics, leveraging multi-step reasoning and real-time biomedical knowledge retrieval. The research introduces TxAgent to address the need for personalized treatment recommendations by integrating information across a large toolbox of 211 tools. The methodology involves a combination of multi-step reasoning, function calls to external databases, and a retrieval-augmented generation (RAG) system to dynamically select relevant tools and synthesize evidence.  TxAgent achieves 92.1% accuracy in open-ended drug reasoning tasks, outperforming GPT-4o by up to 25.8%. AI practitioners can adapt the presented architecture for complex reasoning tasks requiring verifiable, real-time information in domains beyond therapeutics. |
| Multi-Modal | FlowTok: Flowing Seamlessly Across Text and Image Tokens (Read more on [arXiv](https://arxiv.org/abs/2503.10772) or [HuggingFace](https://huggingface.co/papers/2503.10772))| Liang-Chieh Chen, QHL067, QihangYu, turkeyju | FlowTok is a novel framework for seamless cross-modal generation, enabling direct flow matching between text and image tokens. The research aims to unify multimodal understanding and generation by directly evolving between text and image modalities through flow matching. FlowTok projects both modalities into a shared, compact 1D latent space using enhancements to TA-TiTok for image tokenization and a lightweight text projector. Experiments show FlowTok achieves comparable or superior performance to state-of-the-art models with significantly reduced training resources, reaching a FID-30K score of 9.67 on COCO. FlowTok's streamlined architecture and efficient training make cross-modal generation more accessible and accelerate research in this domain. |
| Computer Vision | Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision
  Transformers? (Read more on [arXiv](https://arxiv.org/abs/2503.10632) or [HuggingFace](https://huggingface.co/papers/2503.10632))| Xin Li, Killian Hitsman, aritradutta, maitysubhajit | This paper introduces a novel learnable attention mechanism, Kolmogorov-Arnold Attention (KArAt), for Vision Transformers (ViTs). The primary research question is whether integrating learnable activation functions, specifically via Kolmogorov-Arnold Networks (KANs), into the attention mechanism of ViTs improves performance compared to standard softmax attention. The authors propose a general KArAt and a more modular Fourier-KArAt, analyzing their performance through loss landscapes, weight distributions, and spectral behavior. Results show that Fourier-KArAt variants either outperform or show comparable performance to vanilla ViTs on CIFAR-10, CIFAR-100, and ImageNet-1K, with ViT-Tiny+Fourier KArAt outperforming ViT-Tiny on CIFAR-10 by 5.40%. AI practitioners should consider exploring KANs in conjunction with advanced architectures, but need careful understanding of learnable activations to realize performance gains. |
| Multi-Modal | Cockatiel: Ensembling Synthetic and Human Preferenced Training for
  Detailed Video Caption (Read more on [arXiv](https://arxiv.org/abs/2503.09279) or [HuggingFace](https://huggingface.co/papers/2503.09279))| Hao Li, Zhiyu Tan, xiaomengyang, Kobeshegu, Fr0zencr4nE | This paper introduces Cockatiel, a novel three-stage training pipeline for video detailed captioning (VDC) that ensembles synthetic and human-aligned training to improve performance and alignment with human preferences. The research objective is to address the limitations of existing VDC models, specifically their biased capability towards specific captioning aspects and misalignment with human preferences. The methodology involves curating a dataset using a human-aligned caption quality scorer, training a 13B parameter model (Cockatiel-13B) on this curated dataset, and then distilling a smaller 8B parameter model (Cockatiel-8B). Cockatiel-13B achieves a new state-of-the-art average score of 43.80/2.26 on the VDCSCORE benchmark and is consistently preferred by humans in comparative evaluations. AI practitioners can leverage Cockatiel's approach to generate more detailed, comprehensive, and human-aligned video captions, improving performance in VDC tasks. |
| Computer Vision | Neighboring Autoregressive Modeling for Efficient Visual Generation (Read more on [arXiv](https://arxiv.org/abs/2503.10696) or [HuggingFace](https://huggingface.co/papers/2503.10696))| Hong Zhou, Feng Chen, Shaoxuan He, Yuanyu He, Yefei He | This paper introduces Neighboring Autoregressive Modeling (NAR), a novel paradigm for efficient visual generation that frames it as a progressive outpainting process. The main objective is to improve the efficiency and quality of autoregressive visual generation compared to traditional raster-order 'next-token prediction' models. The key methodology involves a 'next-neighbor prediction' mechanism and dimension-oriented decoding heads, enabling parallel prediction of adjacent tokens. Experiments show NAR achieves superior FID/FVD scores and significantly higher throughput (e.g., 2.4x on ImageNet 256x256) compared to existing methods, the NAR-L model achieving an FID score of 3.06 on ImageNet 256x256 generation. AI practitioners can leverage NAR for faster and more efficient image and video generation with improved fidelity using autoregressive models. |
| Multi-Modal | ProJudge: A Multi-Modal Multi-Discipline Benchmark and
  Instruction-Tuning Dataset for MLLM-based Process Judges (Read more on [arXiv](https://arxiv.org/abs/2503.06553) or [HuggingFace](https://huggingface.co/papers/2503.06553))| Fanrui Zhang, Ming Li, Zhaopan Xu, Pengfei Zhou, Jiaxin Ai | This paper introduces ProJudgeBench, a benchmark for evaluating multi-modal large language models (MLLMs) as process judges in scientific problem-solving, and ProJudge-173k, a dataset for fine-tuning these models. The research aims to assess and improve the reliability of MLLMs in evaluating the correctness of step-by-step reasoning processes across various scientific disciplines.  The methodology involves creating a benchmark with human-annotated error types and explanations, as well as a fine-tuning strategy (Dynamic Dual-Phase) to enhance model performance.  Evaluation on ProJudgeBench shows that GPT-4o achieves 85.10% step correctness accuracy, significantly outperforming open-source models, while fine-tuned models like InternVL2.5-8B+ show substantial improvements (up to 58.92% gain in accuracy).  AI practitioners can utilize this benchmark and dataset to develop more reliable MLLMs capable of fine-grained process evaluation, which is crucial for improving the transparency and trustworthiness of AI-assisted problem-solving. |
| Multi-Modal | ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model
  with Interleaved Multimodal Generation via Asymmetric Synergy (Read more on [arXiv](https://arxiv.org/abs/2503.06542) or [HuggingFace](https://huggingface.co/papers/2503.06542))| Zizhen Li, Fanrui Zhang, Chuanhao Li, Yukang Feng, Jianwen Sun | This paper introduces ARMOR, a resource-efficient and autoregressive framework for multimodal understanding and generation that fine-tunes existing Multimodal Large Language Models (MLLMs). The main research objective is to empower existing MLLMs with interleaved text-image generation capabilities while preserving their multimodal understanding capabilities. ARMOR employs an asymmetric encoder-decoder architecture with a forward-switching mechanism, a curated interleaved dataset, and a three-stage "What or How to Generate" training algorithm. Experimental results show ARMOR outperforms existing Unified Models (UniMs) on multimodal understanding, achieving a score of 78.8 on the MMB benchmark compared to Janus-pro's 62.6, while maintaining comparable generation performance and requiring only ~7% additional parameters. ARMOR offers a method to upgrade MLLMs to UniMs with reduced training resource demands, providing a more scalable approach to multimodal learning. |
| Computer Vision | Learning Few-Step Diffusion Models by Trajectory Distribution Matching (Read more on [arXiv](https://arxiv.org/abs/2503.06674) or [HuggingFace](https://huggingface.co/papers/2503.06674))| Yujun Cai, jingtang, JIACSUN96, whatlegequ, Luo-Yihong | This paper introduces a novel distillation paradigm, Trajectory Distribution Matching (TDM), for accelerating diffusion model sampling in text-to-image and text-to-video generation. The research objective is to develop a unified framework that combines distribution and trajectory matching to enable efficient and high-quality few-step diffusion model generation. TDM employs a data-free score distillation objective that aligns the student's trajectory with the teacher's at the distribution level, and a sampling-steps-aware objective enables flexible sampling. The method outperforms existing methods on various benchmarks, such as SDXL and PixArt-α; in particular distilling PixArt-α into a 4-step generator outperforming its teacher with only 0.01% of the teacher's training cost, and improves video generation total score from 80.91 to 81.65 on VBench using 4 NFE. The main implication is that TDM offers a highly efficient and effective approach for few-step diffusion model training, significantly reducing computational costs while maintaining or surpassing image/video quality. |
| Computer Vision | ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant
  Tightness (Read more on [arXiv](https://arxiv.org/abs/2503.10624) or [HuggingFace](https://huggingface.co/papers/2503.10624))| Yuliang Xiu, Michael J. Black, Zeyu Cai, Haiwen Feng, Boqian-Li | This paper introduces ETCH, a novel framework for generalizing body fitting to 3D clothed humans. The main research objective is to accurately estimate the underlying body shape and pose from a 3D point cloud of a clothed human, even with loose clothing and challenging poses. ETCH models cloth-to-body mapping using equivariant tightness vectors and leverages pose-invariant features for sparse marker regression, simplifying the problem into a marker-based fitting task. Experiments on CAPE and 4D-Dress datasets show significant improvements over state-of-the-art methods; for example, on 4D-Dress, ETCH reduces MPJPE by 32.6% compared to ArtEq. The implication is that ETCH provides a more robust and generalizable approach to clothed human body fitting, useful for applications like virtual try-on and motion capture, even with limited or out-of-distribution data. |
| Reinforcement Learning | Open-World Skill Discovery from Unsegmented Demonstrations (Read more on [arXiv](https://arxiv.org/abs/2503.10684) or [HuggingFace](https://huggingface.co/papers/2503.10684))| Yitao Liang, Anji Liu, Shaofei Cai, Zihao Wang, Jingwen Deng | This paper introduces a self-supervised learning-based approach called Skill Boundary Detection (SBD) for segmenting long, unsegmented demonstration videos into a series of semantic-aware and skill-consistent segments. The main research objective is to enable open-world skill learning from unsegmented demonstrations by automatically identifying skill boundaries without explicit labels.  SBD leverages prediction errors from a pretrained unconditional action-prediction model, assuming that a significant increase in prediction error indicates a shift in the skill being executed.  When evaluated in Minecraft, SBD-generated segments improved the average performance of conditioned policies by 63.7% and 52.1% on short-term atomic skill tasks. The method allows AI practitioners to leverage diverse, unsegmented video data (like YouTube videos) to train instruction-following agents, particularly in open-world environments. |
| Multi-Modal | GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories
  Generation in End-to-End Autonomous Driving (Read more on [arXiv](https://arxiv.org/abs/2503.05689) or [HuggingFace](https://huggingface.co/papers/2503.05689))| Bo Jiang, Yang Hu, Xingyu Zhang, WonderingWorld, XXXXing | GoalFlow is a novel end-to-end autonomous driving method that generates high-quality multimodal trajectories. The primary research objective is to address issues of trajectory selection complexity, high trajectory divergence, and inconsistencies between guidance and scene information found in previous methods. GoalFlow constrains the generative process by introducing a goal point and a novel scoring mechanism, uses Flow Matching for efficient trajectory generation, and incorporates a refined scoring mechanism for selecting the optimal trajectory. The method achieved a PDMS of 90.3 on the Navsim benchmark, significantly outperforming other methods, and requires only a single denoising step for good performance. This implies that AI practitioners can use GoalFlow as a robust approach for generating diverse and reliable driving behaviors, with potential for real-world deployment due to its computational efficiency. |
| Computer Vision | MaRI: Material Retrieval Integration across Domains (Read more on [arXiv](https://arxiv.org/abs/2503.08111) or [HuggingFace](https://huggingface.co/papers/2503.08111))| Yuxuan Chen, Huixiong Zhang, Yangfan He, Jianhui Wang, yangzhifei | MaRI is a novel framework for material retrieval that bridges the gap between synthetic and real-world material representations. The research objective is to accurately retrieve materials from images by aligning visual representations with material properties in a shared embedding space. MaRI employs a contrastive learning strategy with dual encoders, trained on a comprehensive dataset comprising high-quality synthetic materials and real-world materials processed using material transfer techniques. In evaluations against a gallery of known (Trained) materials, MaRI achieves a Top-1 instance accuracy of 26.0%, and for novel, previously unseen (Unseen) materials it achieved 54% Top-1 Instance accuracy significantly outperforming other methods. AI practitioners can use MaRI for applications requiring accurate and efficient material retrieval across diverse and complex material types, advancing 3D asset creation, especially in realistic rendering, AR, and VR. |
| Multi-Modal | From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM (Read more on [arXiv](https://arxiv.org/abs/2503.10620) or [HuggingFace](https://huggingface.co/papers/2503.10620))| Tsz Kin Lam, Anil Keshwani, Sonal Sannigrahi, Kshitij Ambilduke, bpop | This paper introduces SPIRE, a speech-augmented language model built upon the multilingual text-only model TOWER, enabling it to process and transcribe English speech. The research investigates how to effectively integrate the speech modality into a pre-existing text-only LLM while preserving its original text-based capabilities. The methodology involves a two-stage training process of continued pre-training on a mixture of ASR and text data, and instruction tuning on ASR, speech translation, and machine translation data, using discretized speech units from a HuBERT-based k-means clustering. SPIRE achieves competitive ASR performance (4.2 WER on LibriSpeech test-clean) and maintains the original TOWER model's performance on machine translation tasks. This demonstrates a viable approach for augmenting text-only LLMs with speech capabilities, offering a reproducible method for extending LLMs to the speech modality. |
| Machine Learning | Group-robust Machine Unlearning (Read more on [arXiv](https://arxiv.org/abs/2503.09330) or [HuggingFace](https://huggingface.co/papers/2503.09330))| Massimiliano Mancini, Elisa Ricci, Stéphane Lathuilière, Subhankar Roy, Thomas De Min | This paper introduces group-robust machine unlearning, a novel approach to remove data influence from machine learning models while preserving performance for specific demographic groups. The research addresses the problem of performance degradation in dominant groups within the forget set when unlearning non-uniformly distributed data, a scenario overlooked by existing methods. The authors propose MIU (Mutual Information-aware Machine Unlearning), using sample distribution reweighting and minimizing mutual information between model features and group information during training, combined with calibration to match the original model. Experiments on CelebA, Waterbirds, and FairFace datasets show that MIU outperforms standard unlearning methods and maintains the highest Group Accuracy (GA) both with (69.0%) and without (55.9%) sample reweighting strategy. MIU is the first approximate unlearning approach that minimizes the mutual information on the forget set while calibrating it to match the group robustness of the original model, with implications for improving fairness and regulatory compliance in AI systems. |
