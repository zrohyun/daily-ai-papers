

## Papers for 2025-03-19

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | RWKV-7 "Goose" with Expressive Dynamic State Evolution (Read more on [arXiv](https://arxiv.org/abs/2503.14456) or [HuggingFace](https://huggingface.co/papers/2503.14456))| saitejautpala, Guangyu, SmerkyG, ZhangRC, BlinkDL | The paper introduces RWKV-7 "Goose", a new sequence modeling architecture for language models, establishing new state-of-the-art performance. The main objective is to develop a model that achieves high downstream performance while maintaining efficiency in memory usage and inference time. RWKV-7 utilizes a generalized delta rule with vector-valued gating, in-context learning rates, and a relaxed value replacement rule. The model matches current SoTA English language performance and achieves state-of-the-art results at the 3 billion parameter scale on multilingual tasks, despite training on fewer tokens than comparable models. This architecture offers AI practitioners a highly efficient alternative to Transformers, particularly beneficial for long sequence processing. |
| Computer Vision | Impossible Videos (Read more on [arXiv](https://arxiv.org/abs/2503.14378) or [HuggingFace](https://huggingface.co/papers/2503.14378))| Hai Ci, mikeshou, ZechenBai | The paper introduces IPV-BENCH, a new benchmark for evaluating video understanding and generation models on impossible videos. It aims to assess whether current models can effectively follow prompts to create impossible video content and understand impossible videos. The benchmark consists of a comprehensive taxonomy, a prompt suite (IPV-TXT), and a video set (IPV-VID) to evaluate the prompt following and reasoning capabilities of models.  Evaluations on state-of-the-art models revealed limitations in both generating high-quality impossible videos (best model achieved 37.3% IPV-Score) and understanding such videos, with performance gaps on open-ended QA tasks. IPV-BENCH highlights the need for improved temporal reasoning and world knowledge in video models, providing a valuable resource for future research in video understanding and generation. |
| Multi-Modal | Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM (Read more on [arXiv](https://arxiv.org/abs/2503.14478) or [HuggingFace](https://huggingface.co/papers/2503.14478))| Yingji Liang, Shengyuan Ding, Kai Lan, Zhijian Chen, Xinyu Fang | Creation-MMBench is a new multimodal benchmark designed to assess the creative capabilities of Multimodal Large Language Models (MLLMs) in real-world, image-based tasks. The main objective is to evaluate how well MLLMs can generate novel and appropriate solutions in creative contexts, addressing a gap in existing benchmarks. The methodology involves a benchmark comprising 765 test cases across 51 fine-grained tasks, with instance-specific evaluation criteria for assessing response quality and factual consistency with visual inputs, using both unitary scoring and pairwise comparison methods. Experimental results show that current open-source MLLMs significantly underperform compared to proprietary models in creative tasks, and visual fine-tuning can negatively impact the base LLM's creative abilities. The main implication is that there's a need for improved multimodal generative intelligence, and the benchmark provides a foundation for future advancements in MLLM creativity. |
| Reinforcement Learning | DAPO: An Open-Source LLM Reinforcement Learning System at Scale (Read more on [arXiv](https://arxiv.org/abs/2503.14476) or [HuggingFace](https://huggingface.co/papers/2503.14476))| Xiaochen Zuo, Yufeng Yuan, Ruofei Zhu, Zheng Zhang, Qiying Yu | This paper introduces DAPO, a fully open-source, large-scale reinforcement learning (RL) system for training large language models (LLMs). The research objective is to democratize and improve upon existing, often concealed, techniques for training reasoning LLMs using RL, focusing on overcoming challenges such as entropy collapse and training instability. The key methodology is the Decoupled Clip and Dynamic sampling Policy Optimization (DAPO) algorithm, which incorporates techniques like Clip-Higher, Dynamic Sampling, Token-Level Policy Gradient Loss, and Overlong Reward Shaping. The system achieves 50 points on the AIME 2024 benchmark using the Qwen2.5-32B base model, outperforming prior state-of-the-art results with 50% fewer training steps. AI practitioners can leverage DAPO's open-sourced algorithm, training code, and dataset to reproduce industry-level RL results and advance research in large-scale LLM RL. |
| Multi-Modal | DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs
  for Knowledge-Intensive Visual Grounding (Read more on [arXiv](https://arxiv.org/abs/2503.12797) or [HuggingFace](https://huggingface.co/papers/2503.12797))| Zonghao Guo, Zhicong Luo, carboncoo, sdudzy, MaxyLee | This paper introduces DeepPerception, a Multi-Modal Large Language Model (MLLM) designed to enhance knowledge-intensive visual grounding (KVG) by integrating cognitive visual perception capabilities. The main objective is to bridge the gap between fine-grained visual perception and domain-specific knowledge integration, which current MLLMs struggle with. The methodology involves an automated data synthesis pipeline and a two-stage training framework, combining supervised fine-tuning for cognitive reasoning and reinforcement learning for perception-cognition synergy. DeepPerception achieves an +8.08% accuracy improvement on the proposed KVG-Bench compared to direct fine-tuning baselines, and shows +4.60% superior cross-domain generalization. The key implication is that integrating cognitive processes into MLLMs significantly improves their ability to perform human-like visual perception and reasoning, advancing the field of multimodal AI. |
| Multi-Modal | CapArena: Benchmarking and Analyzing Detailed Image Captioning in the
  LLM Era (Read more on [arXiv](https://arxiv.org/abs/2503.12329) or [HuggingFace](https://huggingface.co/papers/2503.12329))| Qiushi Sun, Zheng Ma, Jiaxin Fan, songwp, cckevinn | This paper introduces CapArena, a new benchmark for evaluating detailed image captioning capabilities of Vision-Language Models (VLMs) in the era of Large Language Models (LLMs). The main research questions are how well current VLMs perform in detailed image captioning compared to humans and how automated metrics can reliably assess detailed caption quality. The authors built CapArena, a platform with over 6000 pairwise caption battles and human preference votes, and used it for arena-style evaluation along with evaluating traditional and VLM-as-a-Judge metrics. The primary results reveal that leading models like GPT-4o achieve or surpass human-level performance (ELO rating of 1142 vs 1171 for Human Baseline), while most open-source models lag behind, and the VLM-as-a-judge (GPT-4o with ref) shows strong 0.943 spearman correlation with human. AI practitioners can use the insights and the released automated benchmark, CapArena-Auto, for efficient evaluation and development of detailed image captioning models. |
| Computer Vision | Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated
  Objects via Procedural Generation (Read more on [arXiv](https://arxiv.org/abs/2503.13424) or [HuggingFace](https://huggingface.co/papers/2503.13424))| Li Ray Luo, Yitong Wang, Ruiming Liang, Zichao Yu, Xinyu Lian | This paper introduces Infinite Mobility, a novel method for synthesizing high-fidelity 3D articulated objects using procedural generation. The main research objective is to create a scalable and high-quality dataset of articulated objects, addressing limitations of existing data-driven and simulation-based methods. The methodology employs a tree-growing strategy for articulation structure generation, combining procedural mesh generation and curated dataset retrieval, ensuring physically plausible results.  Evaluations show the method produces results comparable to human-annotated datasets and surpassing state-of-the-art generative models in physics property and mesh quality, with an average generation time of 0.46 seconds per object. AI practitioners can utilize this method to generate large-scale, high-quality articulated object datasets, useful for embodied AI training and sim-to-real transfer. |
| Machine Learning | Frac-Connections: Fractional Extension of Hyper-Connections (Read more on [arXiv](https://arxiv.org/abs/2503.14125) or [HuggingFace](https://huggingface.co/papers/2503.14125))| Jundong Zhou, Hongzhi Huang, Defa Zhu, Taoer, FetchFortune | This paper introduces Frac-Connections, a novel approach to improve deep learning model training by addressing gradient vanishing and representation collapse. The main research objective is to explore whether the benefits of Hyper-Connections can be achieved without increasing memory access costs. The key methodology involves dividing hidden states into multiple parts (fractional expansion) rather than expanding their width, retaining partial benefits of Hyper-Connections while reducing memory consumption. Experiments on language tasks, including a 7B MoE model trained on up to 3T tokens, show that Frac-Connections significantly outperform residual connections, with a training loss reduction of 0.014 on OLMoE-1.3B models. AI practitioners can leverage Frac-Connections as a more memory-efficient method to enhance training stability and improve performance in deep learning models, particularly in natural language processing. |
| Multi-Modal | Aligning Multimodal LLM with Human Preference: A Survey (Read more on [arXiv](https://arxiv.org/abs/2503.14504) or [HuggingFace](https://huggingface.co/papers/2503.14504))| Jinda Lu, Junkang Wu, Chaoyou Fu, Tao Yu, yifanzhang114 | This paper provides a comprehensive and systematic review of alignment algorithms for Multimodal Large Language Models (MLLMs). The research aims to organize current advancements and inspire better alignment methods by addressing critical issues such as truthfulness, safety, and alignment with human preferences. The authors explore four key aspects: application scenarios of alignment algorithms, core factors in constructing alignment datasets, benchmarks used for evaluation, and potential future directions. The study does not appear to present novel quantitative results, as it is a survey; it focuses on analyzing and categorizing existing methods and datasets. For AI practitioners, this survey offers a structured overview of the MLLM alignment landscape, helping them identify appropriate tools and methodologies, and highlighting areas needing further research. |
| Multi-Modal | Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal
  Control (Read more on [arXiv](https://arxiv.org/abs/2503.14492) or [HuggingFace](https://huggingface.co/papers/2503.14492))| Tiffany Cai, Maciej Bala, Jose Alvarez, Hassan Abu Alhaija, NVIDIA | Cosmos-Transfer1 is a conditional world generation model capable of producing world simulations from multiple spatial control inputs, such as segmentation, depth, and edge. The research introduces a model to address how to do multimodal controllable world generation that is spatially and temporally adaptive. It uses a diffusion-based model with a novel ControlNet design incorporating multiple control branches for different modalities, fused during inference with a spatiotemporal control map for adaptive weighting. Evaluations on the TransferBench dataset show the adaptive multimodal control model achieved the best depth reconstruction (Depth si-RMSE of 0.47) and highest Quality Score (8.54). AI practitioners can leverage this for applications requiring controllable video generation, such as robotics Sim2Real and autonomous vehicle data enrichment, and improved synthetic-to-real domain adaptation. |
| Multi-Modal | MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process
  Errors Identification (Read more on [arXiv](https://arxiv.org/abs/2503.12505) or [HuggingFace](https://huggingface.co/papers/2503.12505))| Kai Wang, Wangbo Zhao, Jiaxin Ai, Pengfei Zhou, Zhaopan Xu | This paper introduces MPBench, a comprehensive multimodal benchmark for evaluating process-level reward models (PRMs) in reasoning tasks. The main research objective is to assess the effectiveness of PRMs in diverse scenarios, including step correctness, answer aggregation, and reasoning process search. MPBench employs three evaluation paradigms and includes 9,745 fine-grained data instances across six sub-categories, with human verification ensuring high data quality. The best performing model, GPT-4o, achieved an overall score of 71.2, significantly outperforming other models but still showing there is much room to improve. The findings imply that while current multimodal PRMs show promise, there are challenges in generalizing across different reasoning tasks and a need for more robust models, especially when dealing with the complex domain of mathematical reasoning. |
| Computer Vision | Concat-ID: Towards Universal Identity-Preserving Video Synthesis (Read more on [arXiv](https://arxiv.org/abs/2503.14151) or [HuggingFace](https://huggingface.co/papers/2503.14151))| Chongxuan Li, Xiaotao Gu, Jiayan Teng, Zhuoyi Yang, Yong Zhong | Concat-ID is a unified framework for identity-preserving video synthesis, applicable to single-identity, multi-identity, and multi-subject scenarios. The research objective is to develop a method that balances identity consistency and facial editability in video generation, without needing extra modules or parameters. The methodology extracts image features using Variational Autoencoders (VAEs) and concatenates them with video latents, leveraging 3D self-attention mechanisms, combined with a cross-video pairing strategy and a multi-stage training regimen. For single-identity generation, Concat-ID achieved an ArcSim score of 0.442 and a CLIPDist score of 0.325, surpassing existing methods in identity consistency and facial editability. AI practitioners can use Concat-ID as a versatile and scalable solution for various video synthesis applications, eliminating the need for complex architectures or extensive fine-tuning. |
| Machine Learning | Measuring AI Ability to Complete Long Tasks (Read more on [arXiv](https://arxiv.org/abs/2503.14499) or [HuggingFace](https://huggingface.co/papers/2503.14499))| Katharyn Garcia, Amy Deng, Joel Becker, Ben West, Thomas Kwa | This paper introduces a new metric, the 50%-task-completion time horizon, to quantify AI capabilities in terms of human abilities. The main research objective is to measure and track the progress of AI systems in completing increasingly complex, real-world tasks over time. The methodology involves timing human experts on a diverse set of tasks, evaluating AI model performance on these tasks, and fitting a logistic model to determine the time horizon at which AI models achieve a 50% success rate. Primary results show that the 50% time horizon of frontier AI models has been doubling approximately every seven months since 2019, with current models like Claude 3.7 Sonnet having a time horizon around 50 minutes. The main implication is that AI capabilities, particularly in software-related tasks, are rapidly progressing, which has significant implications for AI governance, risk mitigation, and forecasting future capabilities. |
| Natural Language Processing | Temporal Consistency for LLM Reasoning Process Error Identification (Read more on [arXiv](https://arxiv.org/abs/2503.14495) or [HuggingFace](https://huggingface.co/papers/2503.14495))| Xinzhe Juan, Kaixuan Huang, Jiahao Qiu, Yue Wu, Jiacheng Guo | This paper introduces a temporal consistency method for improving error identification in large language models (LLMs) during mathematical reasoning. The main research objective is to enhance the accuracy of verification in mathematical reasoning by leveraging consistency in a sequence of self-reflection actions. The methodology involves iterative self-checking, where LLMs refine their judgments based on previous assessments, culminating in a majority vote. The results show significant improvements across several benchmarks; for instance, the method enabled distilled 7B/8B models to achieve 71.3%/67.2% on ProcessBench, outperforming larger models and GPT-4o. This approach suggests that AI practitioners can improve the reliability of LLMs in mathematical reasoning by incorporating temporal consistency checks, reducing the need for extensive training data or model modifications. |
| Multi-Modal | PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for
  Multimodal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.12545) or [HuggingFace](https://huggingface.co/papers/2503.12545))| Wangbo Zhao, Jiaxin Ai, Weidong Tang, Pengfei Zhou, Zhaopan Xu | This paper introduces PEBench, a new benchmark dataset and framework for evaluating machine unlearning (MU) in Multimodal Large Language Models (MLLMs). The main research objective is to assess the effectiveness of MU methods in removing specific knowledge (both personal entities and general events) from MLLMs while preserving performance on unrelated tasks. The methodology involves creating a synthetic dataset of fictional individuals and events, and then evaluating six MU methods based on metrics including efficacy, generality, retain accuracy, and scope.  Results show that while existing methods can effectively unlearn individual identities (efficacy near 100%), they struggle with unlearning events and often negatively impact related concepts (average ROUGE-L drops from 0.99 to 0.88 when unlearning people). The main implication is that current MU methods for MLLMs require improvements, particularly in balancing unlearning targeted concepts while preserving general knowledge and avoiding unintended consequences on related concepts. |
| Computer Vision | Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion
  Transformers via In-Context Reflection (Read more on [arXiv](https://arxiv.org/abs/2503.12271) or [HuggingFace](https://huggingface.co/papers/2503.12271))| Yusuke Kato, Arsh Koneru, Akash Gokul, Konstantinos Kallidromitis, Shufan Li | This paper introduces Reflect-DiT, a framework for improving text-to-image generation by enabling Diffusion Transformers (DiTs) to refine their outputs iteratively through in-context reflection. The research objective is to enhance the inference-time scaling of DiTs, moving beyond naive best-of-N sampling.  The methodology utilizes a vision-language model (VLM) to critique generated images and provide textual feedback, which the DiT then uses, along with past generations, to improve subsequent outputs. Reflect-DiT achieves a new state-of-the-art score of 0.81 on the GenEval benchmark using only 20 samples per prompt, surpassing the previous best score of 0.80 which used a much larger model and 2048 samples. AI practitioners can leverage Reflect-DiT for more efficient and precise text-to-image generation, significantly reducing the computational cost while improving image quality and text alignment. |
| Multi-Modal | Florenz: Scaling Laws for Systematic Generalization in Vision-Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2503.09443) or [HuggingFace](https://huggingface.co/papers/2503.09443))| Sven Behnke, Sebastian Houben, Spravil | This paper investigates the scaling laws of systematic generalization in vision-language models (VLMs) for multilingual tasks. The main research objective is to study how model size and training data volume affect a monolingual VLM's ability to perform tasks in unseen language-task combinations.  The authors propose Florenz, a monolingual encoder-decoder VLM, and train it on a synthetic dataset with intentionally incomplete language coverage, focusing on image captioning and translation.  Results show that generalization to unseen task-language pairs follows a scaling law, with a 30B parameter model projected to achieve a cross-entropy loss of 2.31 on unseen captioning. The findings suggest that larger models can improve cross-lingual transfer in VLMs, even with limited task-specific data in target languages, which informs building of efficient multilingual models. |
| Natural Language Processing | Pensez: Less Data, Better Reasoning -- Rethinking French LLM (Read more on [arXiv](https://arxiv.org/abs/2503.13661) or [HuggingFace](https://huggingface.co/papers/2503.13661))| HoangHa | This paper introduces Pensez 7B, a bilingual English-French large language model focused on enhancing reasoning capabilities through strategic fine-tuning on a small, high-quality dataset. The research investigates whether targeted data curation and optimized training can achieve competitive or superior performance compared to training on massive datasets. The methodology involves supervised fine-tuning on a bilingual dataset of 2,000 carefully selected samples, with an emphasis on detailed reasoning chains.  Pensez 7B exhibits up to a 20-point accuracy increase on the AIME25 benchmark and a 12-point increase on a French MATH level 5 benchmark, challenging the assumption that massive datasets are necessary for strong reasoning performance. The findings suggest that strategic data selection and fine-tuning can lead to efficient development of high-performing, multilingual LLMs, especially in resource-limited environments. |
| Multi-Modal | MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.13111) or [HuggingFace](https://huggingface.co/papers/2503.13111))| Justin Lazarow, Haiming Gang, David Griffiths, Nina Wenzel, Erik Daxberger | This paper introduces MM-Spatial, a generalist multimodal large language model (MLLM) excelling at 3D spatial understanding in indoor scenes. The research objective is to enhance MLLMs' ability to reason about 3D space, which is currently limited compared to their 2D visual understanding. The authors leverage large-scale 3D scene data with open-set annotations to create a new supervised fine-tuning dataset (CA-VQA) and benchmark, covering diverse spatial tasks and incorporating metric depth and multi-view inputs. MM-Spatial achieves state-of-the-art performance on 3D spatial understanding benchmarks, including CA-VQA, with a score of 64.5 on aggregated metrics across several benchmark categories, outperforming the MM1.5 baseline. The research demonstrates that large language models can be trained for improved 3D vision, offering strong implications for improvements to robotics, and AR/VR applications. |
| Multi-Modal | Hyperbolic Safety-Aware Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.12127) or [HuggingFace](https://huggingface.co/papers/2503.12127))| Rita Cucchiara, Lorenzo Baraldi, Pascal Mettes, Tejaswi Kasarla, tobi1modna | This paper introduces HySAC, a hyperbolic safety-aware vision-language model that improves the handling of unsafe content in vision-language models (VLMs). The main research objective is to develop a VLM that can distinguish between safe and unsafe content, rather than simply unlearning or removing unsafe concepts, allowing for controlled access and improved user agency. HySAC leverages the hierarchical properties of hyperbolic space to encode safe and unsafe content in an entailment hierarchy, using entailment loss functions to model their relationships. The experimental result demonstrates improvement with a Text-to-Image R@1 of 49.8 and an Image-to-Text R@1 of 48.2 on safe content retrieval. AI practitioners can use this approach to enhance safety in VLMs, enabling controlled management of unsafe content with better interpretability and safety awareness. |
| Multi-Modal | KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for
  Open-Vocabulary Robotic Manipulation (Read more on [arXiv](https://arxiv.org/abs/2503.10546) or [HuggingFace](https://huggingface.co/papers/2503.10546))| Yunzhu Li, Mingtong Zhang, Zixian Liu | KUDA is an open-vocabulary robotic manipulation system that integrates dynamics learning and visual prompting through keypoints, leveraging both Vision Language Models (VLMs) and learning-based neural dynamics models. The paper explores how to develop open-vocabulary manipulation systems that harness the flexibility of task specification via VLMs while preserving the benefits of model-based planning. KUDA uses keypoints as a unified intermediate representation to bridge dynamics learning, and visual prompting; a VLM generates code from keypoints that represents the target state, which are translated into cost functions for planning. The system achieved an 80.0% success rate on evaluation tasks across diverse object categories, including rigid, deformable, and granular objects, surpassing baselines significantly. AI practitioners can leverage KUDA's approach to create robotic systems capable of performing complex manipulation tasks with free-form language instructions and across various object types. |
| Computer Vision | RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground
  Simulation (Read more on [arXiv](https://arxiv.org/abs/2503.10410) or [HuggingFace](https://huggingface.co/papers/2503.10410))| Junhao Ge, Yifan Lu, Zichen Chao, Anning Hu, yuwendu | This paper introduces RoCo-Sim, a simulation framework for roadside collaborative perception, designed to generate diverse, multi-view-consistent simulated data. The research aims to address data challenges in roadside perception, such as calibration errors, sparse information, and multi-view inconsistency, which hinder model performance. RoCo-Sim employs camera extrinsic optimization, a multi-view occlusion-aware sampler, DepthSAM for foreground-background relationship modeling, and scalable post-processing for style transfer. The results show that RoCo-Sim significantly improves roadside 3D object detection, outperforming state-of-the-art methods by 83.74% on Rcooper-Intersection for AP70. This suggests that AI practitioners can leverage RoCo-Sim to enhance roadside perception model performance by generating high-quality, labeled, simulated training data without expensive real-world data collection. |
