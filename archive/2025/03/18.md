

## Papers for 2025-03-18

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal
  Consistent Video Generation (Read more on [arXiv](https://arxiv.org/abs/2503.06053) or [HuggingFace](https://huggingface.co/papers/2503.06053))| Runze Zhang, NeilXu, EllenAP, lixiaochuan, georgedu | This paper introduces DropletVideo, a new dataset and model for generating videos with integral spatio-temporal consistency, addressing the challenge of maintaining coherence in both object appearance and plot progression across dynamic camera movements. The main research objective is to explore integral spatio-temporal consistency, considering the synergy between plot progression, camera techniques, and the long-term impact of prior content on subsequent generation. The key methodology involves constructing a large-scale dataset, DropletVideo-10M, with detailed captions and developing a corresponding DropletVideo model that incorporates a variable frame rate sampling strategy. The DropletVideo model achieves 37.93% in Camera Motion, significantly exceeding existing image-to-video models evaluated on the VBench++ benchmark and showing strong ability to handle camera motions within videos. AI practitioners can leverage this dataset and model to advance video generation research, particularly in creating more complex, multi-plot narratives with realistic camera movements and consistent content. |
| Multi-Modal | Being-0: A Humanoid Robotic Agent with Vision-Language Models and
  Modular Skills (Read more on [arXiv](https://arxiv.org/abs/2503.12533) or [HuggingFace](https://huggingface.co/papers/2503.12533))| tellarin, SherryXu, takenpeanut, fuyh, Yaya041 | The paper introduces Being-0, a hierarchical agent framework for humanoid robots that integrates vision-language models and a modular skill library. The main objective is to develop an agent capable of controlling a full-sized humanoid robot to solve complex, long-horizon embodied tasks in real-world environments, overcoming limitations of directly combining Foundation Models (FMs) and low-level skills. The key methodology involves a novel Connector module, powered by a lightweight vision-language model (VLM), that translates FM plans into actionable skill commands and coordinates locomotion and manipulation. Primary results show that Being-0 achieves an average completion rate of 84.4% on challenging long-horizon tasks, significantly improving efficiency and robustness compared to fully FM-based agents. The main implication is that using a VLM-based intermediate layer between high-level planning and low level control provides an efficient way to combine the high-level reasoning of Foundation Models and the low-level skills to operate humanoid robots. |
| Computer Vision | DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale
  Text-to-Image Models (Read more on [arXiv](https://arxiv.org/abs/2503.12885) or [HuggingFace](https://huggingface.co/papers/2503.12885))| Yi Yang, z-x-yang, aiJojosh, limuloo1999 | DreamRenderer is a training-free, plug-and-play controller for image-conditioned generation models that allows for fine-grained control over multiple instances within an image. The research objective is to address the challenge of accurately controlling the content of multiple regions or instances in image generation, particularly attribute leakage between instances. The key methodology involves two innovations: Bridge Image Tokens for Hard Text Attribute Binding to ensure correct visual attribute binding for each instance, and selective application of Hard Image Attribute Binding in vital layers of the model. Evaluations on COCO-POS and COCO-MIG benchmarks show that DreamRenderer improves the Image Success Ratio by 17.7% over FLUX and enhances performance of layout-to-image models like GLIGEN up to 26.8%. AI practitioners can use DreamRenderer to achieve more precise and controllable image generation, especially in scenarios with multiple controlled instances or regions, without additional training. |
| Computer Vision | Edit Transfer: Learning Image Editing via Vision In-Context Relations (Read more on [arXiv](https://arxiv.org/abs/2503.13327) or [HuggingFace](https://huggingface.co/papers/2503.13327))| Qi Mao, AnalMom, guyuchao, Orannue | This paper introduces Edit Transfer, a new image editing framework that learns a transformation from a single source-target example pair and applies it to a new query image. The research aims to learn image editing transformations directly from visual examples, bypassing limitations of text-based and reference-based image editing methods, particularly for non-rigid transformations.  The key methodology involves a visual relation in-context learning paradigm built upon a DiT-based text-to-image model, using a four-panel composite image input and lightweight LoRA fine-tuning. The method substantially outperforms state-of-the-art TIE and RIE methods on diverse non-rigid scenarios, achieving a user preference rate exceeding 80% across all aspects, including text alignment, referencce alignement and overal performance.  Edit Transfer offers AI practitioners a new approach to image editing, especially for complex spatial transformations, using minimal training data and visual, rather than textual, guidance. |
| Computer Vision | Personalize Anything for Free with Diffusion Transformer (Read more on [arXiv](https://arxiv.org/abs/2503.12590) or [HuggingFace](https://huggingface.co/papers/2503.12590))| Lu Sheng, Lin Li, Haoran Feng, lvhairong, huanngzh | This paper introduces "Personalize Anything," a training-free framework for personalized image generation using Diffusion Transformers (DiTs). The research objective is to achieve high-fidelity, versatile, personalized image generation in DiTs without requiring training or fine-tuning, while maintaining or surpassing identity preservation compared to existing methods. The key methodology involves timestep-adaptive token replacement, using early-stage injection for subject consistency and later-stage regularization for flexibility, combined with patch perturbation strategies to boost structural diversity.  Evaluations show the method performs at the state-of-the-art level, outperforming other approaches, with the FLUX model in single-subject personalization reaching 0.307 on CLIP-T and 0.179 on DreamSim. This work offers a practical and efficient personalization paradigm for DiTs, enabling applications like layout-guided generation and mask-controlled editing without extensive computational cost. |
| Computer Vision | WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range
  Movements and Scenes (Read more on [arXiv](https://arxiv.org/abs/2503.13435) or [HuggingFace](https://huggingface.co/papers/2503.13435))| mingbao, zbhpku, Juanxi, czkk566, Lingaaaaaaa | This paper introduces WideRange4D, a new 4D reconstruction benchmark and method (Progress4D) for high-quality 4D scene reconstruction with wide-range spatial movements. The main research objective is to address the limitations of existing 4D reconstruction datasets and methods in handling scenes with significant object motion. The key methodology, Progress4D, utilizes a two-stage approach involving high-quality 3D scene initialization followed by progressive fitting of 4D dynamics using a timestep alignment loss. Experimental results show that Progress4D outperforms existing state-of-the-art 4D reconstruction methods, achieving a PSNR of 28.86 on the WideRange4D benchmark. The main implication is that AI practitioners now have a robust benchmark and method for generating high-quality 4D scenes that contain large-scale spatial changes, improving upon previous limitations in generating dynamic content. |
| Computer Vision | BlobCtrl: A Unified and Flexible Framework for Element-level Image
  Generation and Editing (Read more on [arXiv](https://arxiv.org/abs/2503.13434) or [HuggingFace](https://huggingface.co/papers/2503.13434))| HongxiangLi, daoyuan98, ZyZcuhk, l-li, Yw22 | BlobCtrl is a unified and flexible framework for element-level image generation and editing that uses a probabilistic blob-based representation. The paper introduces a framework to unify element-level generation and editing, addressing the lack of precision and flexibility in current diffusion-based methods. It employs blobs as visual primitives, using a dual-branch diffusion architecture with self-supervised training and controllable dropout strategies. BlobCtrl demonstrates superior performance in element-level manipulation tasks, achieving a FID score of 102.8094 in image generation quality.Â AI practitioners can leverage BlobCtrl for precise and flexible visual content creation, benefiting from its ability to decouple and represent spatial location, semantic content, and identity information. |
| Natural Language Processing | reWordBench: Benchmarking and Improving the Robustness of Reward Models
  with Transformed Inputs (Read more on [arXiv](https://arxiv.org/abs/2503.11751) or [HuggingFace](https://huggingface.co/papers/2503.11751))| Yoon Kim, Andrew Cohen, mghazvininejad, michiyasunaga, ZhaofengWu | This paper introduces reWordBench, a benchmark for evaluating and improving the robustness of reward models (RMs) in natural language processing against input transformations. The research scrutinizes the robustness of state-of-the-art reward models and quantifies the extent of their overfitting to standard benchmarks. The authors systematically transform reward model inputs in meaning- or ranking-preserving ways and train RMs to assign similar scores to paraphrases. They find that state-of-the-art reward models suffer substantial performance degradation, sometimes dropping to below-random accuracy (e.g. RM ranking accuracy can drop from >95% to 73% by changing answer formatting), but regularized RMs reduce degradation by roughly half in some settings and lead to higher-quality outputs, winning in up to 59% of instances against a standardly trained RM. This work implies that AI practitioners should consider input robustness when using and evaluating reward models, as standard benchmarks can overestimate their true capabilities and regularizing RMs can lead to more reliable alignment and better downstream performance. |
| Multi-Modal | MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based
  Scientific Research (Read more on [arXiv](https://arxiv.org/abs/2503.13399) or [HuggingFace](https://huggingface.co/papers/2503.13399))| lundbergemma, chadliu, shcohen, suyc21, jmhb | This paper introduces MicroVQA, a new benchmark for evaluating multimodal reasoning in microscopy-based scientific research. The main objective is to assess the capabilities of multimodal large language models (MLLMs) in expert image understanding, hypothesis generation, and experiment proposal within the context of biological microscopy. The benchmark consists of 1,042 multiple-choice questions curated by biology experts, and a two-stage pipeline involving an optimized LLM prompt and a 'RefineBot' agent is used to generate high-quality questions. The best-performing MLLM achieved a peak performance of 53%, and a significant performance gap exists between expert-level performance, indicating the current limit of MLLMs. This benchmark is valuable for advancing AI-driven biomedical research and highlighting areas for improvement in MLLMs, particularly in integrating visual and textual information for scientific reasoning. |
| Multi-Modal | Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey (Read more on [arXiv](https://arxiv.org/abs/2503.12605) or [HuggingFace](https://huggingface.co/papers/2503.12605))| Yuecheng Zhang, scofield7419, liuziwei7, ChocoWu, Gh0stAR | This paper provides the first comprehensive survey of Multimodal Chain-of-Thought (MCoT) reasoning, a technique that extends chain-of-thought reasoning to multimodal contexts. The main objective is to systematically review MCoT reasoning, elucidating foundational concepts, methodologies, applications, and future research directions. The key methodology involves a comprehensive taxonomy and in-depth analysis of current MCoT approaches across various modalities (image, video, speech, audio, 3D, and structured data) and application scenarios (robotics, healthcare, autonomous driving). The paper does not offer quantitative primary results, but discusses challenges and future directions, aiming to foster further innovation. The main implication for AI practitioners is providing a structured understanding and compilation of resources related to MCOT reasoning for achieving multimodal AGI. |
| Multi-Modal | Free-form language-based robotic reasoning and grasping (Read more on [arXiv](https://arxiv.org/abs/2503.13082) or [HuggingFace](https://huggingface.co/papers/2503.13082))| Matteo Bortolon, Alice Fasoli, Runyu Jiao, SPovoli, FGiuliari | This paper introduces FreeGrasp, a novel method for free-form language-based robotic grasping that leverages Vision-Language Models (VLMs) for reasoning about human instructions and object spatial arrangements. The main research question explores how pre-trained VLMs can be used for robotic grasping in a zero-shot setting, specifically addressing the interpretation of free-form language instructions and spatial reasoning. The methodology, FreeGrasp, uses keypoint detection of all objects and mark-based visual prompting to facilitate GPT-4o's spatial reasoning, determining if a target is directly graspable or obstructed. Experiments on the synthetic FreeGraspData dataset show FreeGrasp achieves a Reasoning Success Rate (RSR) of 0.83 and 0.85 without and with object ambiguity respectively, on the Easy difficulty, outperforming the ThinkGrasp baseline. The work implies that, while powerful, current VLMs have limitations in complex spatial reasoning tasks, and suggests that integrating visual and language information with structured prompting offers improvements in robotic manipulation. |
| Multi-Modal | R1-VL: Learning to Reason with Multimodal Large Language Models via
  Step-wise Group Relative Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2503.12937) or [HuggingFace](https://huggingface.co/papers/2503.12937))| Jingyi Zhang, Xikun, liushunyu, HuanjinYao, huangjiaxing | This paper introduces R1-VL, a series of Multimodal Large Language Models (MLLMs) with enhanced step-by-step reasoning capabilities, achieved through a novel online reinforcement learning framework called Step-wise Group Relative Policy Optimization (StepGRPO). The research aims to improve MLLMs' reasoning abilities beyond simply imitating successful reasoning paths by enabling self-improvement via dense, step-wise rewards. StepGRPO incorporates two new rule-based rewards, Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity Reward (StepRVR), to guide the model's learning process.  Experiments across eight benchmarks demonstrate the superiority of the method; for example, R1-VL-7B achieves 63.5% accuracy on MathVista. AI practitioners can leverage StepGRPO to develop MLLMs with improved and more reliable reasoning by providing more granular and structured training feedback. |
| Multi-Modal | V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning (Read more on [arXiv](https://arxiv.org/abs/2503.11495) or [HuggingFace](https://huggingface.co/papers/2503.11495))| Wei Li, Ziquan Liu, ChenyangSi, lwpyh, Cade921 | This paper introduces V-STaR, a new benchmark for evaluating the spatio-temporal reasoning capabilities of Video Large Language Models (Video-LLMs). The research aims to determine whether Video-LLMs can genuinely understand and reason about object interactions in videos, beyond relying on pre-trained co-occurrence biases. The methodology involves a Reverse Spatio-Temporal Reasoning (RSTR) task that decomposes video understanding into evaluating object presence, event timing, and location, along with a coarse-to-fine Chain-of-Thought (CoT) dataset generated via a GPT-4-powered pipeline. Experiments on 14 Video-LLMs reveal significant gaps in spatio-temporal reasoning; for example, models such as GPT-4o achieves an LGM score of 39.51 on the what-when-where test sequence, while others like TimeChat score much lower (14.47), demonstrating inconsistency. This work highlights the need for improved Video-LLMs which perform robust and consistent spatio-temporal reasoning, offering a tool for practitioners to rigorously assess and improve such systems. |
| Multi-Modal | VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning (Read more on [arXiv](https://arxiv.org/abs/2503.13444) or [HuggingFace](https://huggingface.co/papers/2503.13444))| Chang Wen Chen, Ye Liu, AnalMom, KevinQHLin | VideoMind is a novel video-language agent designed for temporal-grounded video understanding, specifically focusing on long video reasoning. The research introduces VideoMind to address the challenges of multi-modal reasoning, particularly in understanding the temporal dimension of videos, which requires precise grounded understanding linking answers to visual evidence. VideoMind employs a role-based agentic workflow with a novel Chain-of-LoRA strategy for efficient role-switching, using modules like Planner, Grounder, Verifier, and Answerer. Extensive experiments on 14 benchmarks show that VideoMind's 2B model achieves state-of-the-art performance, outperforming GPT-40 on the CG-Bench with a 7.10 R@IoU score, and demonstrating strong capabilities in grounded video question-answering, video temporal grounding, and general video question-answering. This offers a flexible and efficient solution for long-form temporal reasoning, advancing video agent capabilities and setting a new state-of-the-art. |
| Computer Vision | Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation (Read more on [arXiv](https://arxiv.org/abs/2503.13070) or [HuggingFace](https://huggingface.co/papers/2503.13070))| Jing Tang, Kenji Kawaguchi, Weijian Luo, whatlegequ, Luo-Yihong | This paper introduces R0, a novel approach for fast photo-realistic text-to-image generation that relies primarily on reward maximization. The central research question is whether complex conditional image generation can be achieved through reward optimization alone, bypassing the need for diffusion distillation losses. The key methodology involves parameterizing the generator as a multi-step transformation from noise to image and employing regularization techniques like weight regularization and random Î·-sampling, optimizing for multiple rewards. The results show that R0 achieves state-of-the-art performance, with an HPS of 33.70 and a CS of 32.13 using 4-step generation on the SD-v1.5 backbone. The implication for AI practitioners is that rewards can serve as the dominant force for fast, high-quality image generation, simplifying the overall process. |
| Computer Vision | MTV-Inpaint: Multi-Task Long Video Inpainting (Read more on [arXiv](https://arxiv.org/abs/2503.11412) or [HuggingFace](https://huggingface.co/papers/2503.11412))| CeciliaJL, XiaodongChen, magicwpf, lianghou, GuZheng | MTV-Inpaint is a unified video inpainting framework that supports multiple tasks, including text/image-guided object insertion, scene completion, and handling long videos. The research aims to develop a versatile video inpainting system capable of handling both traditional scene completion and object insertion with multimodal control, and also addresses long videos. The proposed method uses a dual-branch spatial attention mechanism in the T2V diffusion U-Net and a two-stage pipeline (keyframe and in-between frame propagation). The approach achieves state-of-the-art mIOU of 85.00 in object insertion tasks, demonstrably better results than existing solutions. AI practitioners can leverage this framework for various video editing tasks, offering enhanced flexibility and controllability, including applications like multimodal inpainting, object removal/editing, and long video processing. |
| Computer Vision | Error Analyses of Auto-Regressive Video Diffusion Models: A Unified
  Framework (Read more on [arXiv](https://arxiv.org/abs/2503.10704) or [HuggingFace](https://huggingface.co/papers/2503.10704))| duchao, TIanyupang, xiaolili, Fengzhuo, k-nick | This paper provides a theoretical analysis of Auto-Regressive Video Diffusion Models (ARVDMs), identifying key sources of error and proposing architectural improvements. The main research objective is to understand the error characteristics unique to ARVDMs, such as error accumulation and memory bottlenecks, and to mitigate these issues.  The authors develop a unified framework, Meta-ARVDM, to analyze the KL-divergence between generated and true videos, deriving an information-theoretic impossibility result showing the memory bottleneck is unavoidable. Experiments on DMLab and Minecraft demonstrate a Pareto-frontier between error accumulation and memory bottlenecks, and prepending achieved a successful retrieval rate of up to 0.52 on DMLab when using a memory length of 16. AI practitioners can use these insights to design more robust ARVDMs and understand the inherent trade-offs in long-form video generation. |
| Multi-Modal | Long-Video Audio Synthesis with Multi-Agent Collaboration (Read more on [arXiv](https://arxiv.org/abs/2503.10719) or [HuggingFace](https://huggingface.co/papers/2503.10719))| Li Liu, Xiaojie Xu, yingcongchen, Xxlbigbrother, Buzz-lightyear | This paper introduces LVAS-Agent, a multi-agent collaborative framework for end-to-end long video audio synthesis. The main objective is to address the challenges of long-video dubbing, such as semantic coherence, temporal alignment, and scalable synthesis. The methodology decomposes the synthesis process into specialized stages with collaborative agents: scene segmentation, script generation, sound design, and audio synthesis, using mechanisms like discussion-correction and generation-retrieval-optimization.  The primary results show that LVAS-Agent outperforms baseline methods on the LVAS-Bench dataset, achieving a DeSync score of 0.53 compared to baselines of 0.61 and 1.24.  AI practitioners can utilize this framework to improve the quality and coherence of audio in long-form video content without large-scale long-video training data. |
| Multi-Modal | Sightation Counts: Leveraging Sighted User Feedback in Building a
  BLV-aligned Dataset of Diagram Descriptions (Read more on [arXiv](https://arxiv.org/abs/2503.13369) or [HuggingFace](https://huggingface.co/papers/2503.13369))| Jaime-Choi, sangryul, namin0202, eunkey, soarhigh | This paper introduces SIGHTATION, a novel dataset for generating diagram descriptions aligned with the needs of blind and low-vision (BLV) users. The main research question is how to generate diagram descriptions that are more useful for BLV users than those generated by conventional methods or sighted annotators. The key methodology involves a two-pass guided generation process using vision-language models (VLMs) and incorporating feedback from sighted assessors to refine the descriptions, validated by BLV educators.  The results show that preference-tuning a 2B model on the dataset increases usefulness ratings by BLV users by an average of 1.670 standard deviation units, and fine-tuning leads to an outperformance over models trained on other datasets in various automatic metrics. AI practitioners can leverage this approach and dataset to develop more accessible diagram description systems that better meet the needs of BLV individuals. |
| Multi-Modal | Basic Category Usage in Vision Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.12530) or [HuggingFace](https://huggingface.co/papers/2503.12530))| KyleMoore, JesseTNRoberts, HTSawyer | This research investigates basic-level categorization, a psychological phenomenon, in Vision Language Models (VLMs). The main research objective is to determine if VLMs exhibit human-like basic-level categorization behaviors, including preferences for basic-level terms, distinctions between biological and non-biological objects, and expert-level shifts. The methodology involves prompting two VLMs (Llama 3.2 Vision Instruct and Molmo 7B-D) with images and comparing the generated descriptions to known basic-level categories, alongside analyzing the impact of expert-level prompting. Primary results indicate that Llama 3.2 and Molmo 7B-D categorized images at the basic level 60% and 52% of the time, respectively, and demonstrated statistically significant shifts mirroring human expert and biological/non-biological distinctions. The implication is that VLMs acquire cognitive categorization biases from human-generated training data, suggesting that considerations for human-like categorization behaviors may prove useful in certain VLM/LLM task applications. |
| Natural Language Processing | Investigating Human-Aligned Large Language Model Uncertainty (Read more on [arXiv](https://arxiv.org/abs/2503.12528) or [HuggingFace](https://huggingface.co/papers/2503.12528))| Pamela Wisniewski, Daryl Watson, Kyle Moore, JesseTNRoberts | This paper investigates the alignment between human uncertainty and uncertainty measures in large language models (LLMs). The primary objective is to identify LLM uncertainty measures that correlate with human group-level uncertainty on non-factual questions. The authors compare various LLM uncertainty measures, including novel ones like nucleus size and top-k entropy, against human survey response data using correlation and linear regression. Results show that Bayesian measures and top-k entropy tend to agree with human behavior and a combination of all measures results in r â 0.5, while also balancing the generalization in a 3 fold cross-validation setting where r > 0.6. The findings suggest that mixtures of uncertainty measures can provide human-aligned uncertainty quantification in LLMs, which has implications for human-computer interaction and trust calibration. |
