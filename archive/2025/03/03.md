

## Papers for 2025-03-03

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking (Read more on [arXiv](https://arxiv.org/abs/2502.20730) or [HuggingFace](https://huggingface.co/papers/2502.20730))| luyaojie, sanmusunrise, xuanang, yhycai, lzq2021 | This paper introduces a new benchmark, SolutionBench, and a novel system, SolutionRAG, for complex engineering solution design using retrieval-augmented generation (RAG). The main research objective is to evaluate and improve a system's ability to generate complete and feasible solutions for engineering problems with multiple constraints. SolutionRAG utilizes tree-based exploration and bi-point thinking (alternating between solution design and review) to generate reliable solutions. Experimental results demonstrate that SolutionRAG achieves state-of-the-art performance on SolutionBench, significantly outperforming existing RAG methods; for example it improved by 10.4 compared to Naive-RAG. The implication is that by focusing on flexible improvement processes and ensuring multi-constraint satisfaction, AI can better address complex, real-world problem-solving tasks in engineering design. |
| Natural Language Processing | Chain of Draft: Thinking Faster by Writing Less (Read more on [arXiv](https://arxiv.org/abs/2502.18600) or [HuggingFace](https://huggingface.co/papers/2502.18600))| Lingxiao Zhao, Wenhao Xie, DeBERTa, sileixu | This paper introduces Chain of Draft (CoD), a novel prompting strategy for large language models (LLMs) that improves efficiency and reduces latency in complex reasoning tasks. The research objective is to develop a method that mimics human concise thought processes, minimizing verbosity while maintaining or improving accuracy compared to Chain-of-Thought (CoT) prompting. CoD encourages LLMs to generate minimal, information-dense intermediate reasoning steps, focusing on essential calculations and transformations.  Experiments across arithmetic, commonsense, and symbolic reasoning benchmarks show CoD achieves comparable or superior accuracy to CoT, while using significantly fewer tokens (as little as 7.6% in some cases) and thus dramatically reducing the inference cost and time.  AI practitioners can utilize CoD to deploy LLMs more efficiently in real-world applications, especially where cost and latency are critical concerns. |
| Multi-Modal | ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents (Read more on [arXiv](https://arxiv.org/abs/2502.18017) or [HuggingFace](https://huggingface.co/papers/2502.18017))| xpjandy, shihang, vickywu, lovesnowbest, autumncc | ViDoRAG is a multi-agent Retrieval-Augmented Generation (RAG) framework designed for complex reasoning across visually rich documents. The main objective is to address limitations of existing RAG approaches when handling visual documents, specifically focusing on efficient retrieval, comprehension, and reasoning.  The methodology employs a Gaussian Mixture Model (GMM)-based hybrid retrieval strategy for multi-modal retrieval and an iterative agent workflow (seeker, inspector, answer agents) incorporating exploration, summarization, and reflection.  The proposed ViDoRAG outperforms existing methods by over 10% on the competitive ViDoSeek benchmark.  AI practitioners can leverage ViDoRAG's architecture for enhanced performance in RAG systems operating on visually dense and complex documents. |
| Machine Learning | SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers (Read more on [arXiv](https://arxiv.org/abs/2502.20545) or [HuggingFace](https://huggingface.co/papers/2502.20545))| Coralia Cartis, Wenqi Zhu, Kechen Li, Shiweiliuiiiiiii, jitianbo | This paper investigates the ability of Large Language Models (LLMs) to solve a computationally challenging mathematical problem: determining the non-negativity of multivariate polynomials, related to the Sum-of-Squares (SoS) problem. The main research objective is to assess if reasoning LLMs can tackle this NP-hard problem and how reasoning instructions impact performance. The authors introduce a new dataset, SoS-1K, and evaluate various LLMs using progressively challenging reasoning prompts.  Key results show that high-quality reasoning instructions significantly improve accuracy, with the best model achieving 81% accuracy, and a fine-tuned 7B model outperforming larger models with significantly less computation. This demonstrates the potential of LLMs for advanced mathematical reasoning and suggests that focused instruction and fine-tuning can enable LLMs to address complex, research-level problems. |
| Reinforcement Learning | Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids (Read more on [arXiv](https://arxiv.org/abs/2502.20396) or [HuggingFace](https://huggingface.co/papers/2502.20396))| Yuke Zhu, Linxi Fan, Kartik Sachdev, Toru Lin, jitendra1995 | This paper presents a sim-to-real reinforcement learning (RL) framework for vision-based dexterous manipulation on humanoid robots. The research investigates the key challenges of applying RL to solve contact-rich manipulation tasks with a humanoid embodiment and proposes solutions to bridge the sim-to-real gap. The methodology includes an automated real-to-sim tuning module, a generalized reward design using contact and object goals, divide-and-conquer distillation for improved sample efficiency, and a mixture of sparse and dense object representations for vision-based sim-to-real transfer. The system achieves a 62.3% success rate on the grasp-and-reach task, 80% on box lift, and 52.5% on bimanual handover, demonstrating the feasibility of using RL for learning complex manipulation skills that can generalize to unseen objects and remain robust. The developed recipe provides a strong baseline and insights for future work in vision-based dexterous manipulation using reinforcement learning, particularly on humanoids. |
| Machine Learning | LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation (Read more on [arXiv](https://arxiv.org/abs/2502.20583) or [HuggingFace](https://huggingface.co/papers/2502.20583))| kasikci, kojimano, jungok, kamahori | LiteASR is a novel compression scheme for Automatic Speech Recognition (ASR) encoders that leverages low-rank approximation to improve efficiency. The main objective is to reduce the computational intensity of ASR encoders, which are a significant bottleneck in real-world applications, without significantly sacrificing transcription accuracy. The method utilizes Principal Component Analysis (PCA) to approximate linear transformations with a chain of low-rank matrix multiplications and optimizes self-attention. When applied to Whisper large-v3, LiteASR reduces encoder size by over 50% matching Whisper medium's size, yet maintaining improved word accuracy. AI practitioners can use LiteASR to deploy significantly more efficient ASR systems, achieving a better balance between accuracy and computational cost. |
| Computer Vision | Tell me why: Visual foundation models as self-explainable classifiers (Read more on [arXiv](https://arxiv.org/abs/2502.19577) or [HuggingFace](https://huggingface.co/papers/2502.19577))| Christian Lovis, Gianmarco Mengaldo, Mina Bjelogrlic, hturbe | This paper introduces ProtoFM, a novel architecture that adapts visual foundation models (VFMs) into self-explainable classifiers. The research aims to improve the interpretability of visual classification models while maintaining competitive performance by leveraging frozen VFMs. ProtoFM combines VFMs with a prototypical architecture and specialized training objectives, training only a lightweight head on top of the frozen VFM. Evaluations show the model achieves state-of-the-art classification accuracy and a mean explainability score (mX) of 0.92 on the FunnyBirds benchmark, outperforming existing prototypical models. The main implication is that AI practitioners can utilize this architecture to build highly interpretable and accurate visual classifiers, enhancing transparency without substantial computational overhead. |
| Multi-Modal | HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2502.20811) or [HuggingFace](https://huggingface.co/papers/2502.20811))| Fuzheng Zhang, Yuanxing Zhang, Jingyun Hua, Xiao Wang, lwher1996 | This paper introduces a two-stage data annotation pipeline, HAIC, to improve human action understanding and generation in Multi-modal Large Language Models (MLLMs). The research objective is to address the lack of high-quality data for training MLLMs on videos involving human actions. The methodology involves accumulating videos featuring clear human actions, and annotating them with a standardized caption format that distinguishes individuals and details their actions chronologically; this yields two datasets, HAICTrain and HAICBench. Experimental results show that training with HAICTrain enhances human understanding abilities across 4 benchmarks, with improvements of 1.4%-2.1% in model performance. AI practitioners can use this pipeline and datasets to improve MLLM performance in tasks requiring nuanced human action understanding, such as video captioning and generation. |
