

## Papers for 2025-03-28

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Video-R1: Reinforcing Video Reasoning in MLLMs (Read more on [arXiv](https://arxiv.org/abs/2503.21776) or [HuggingFace](https://huggingface.co/papers/2503.21776))| Potentialts, guozonghao96, BreakLee, kxgong, KaituoFeng | Video-R1 introduces a rule-based reinforcement learning (RL) framework to enhance video reasoning capabilities within Multi-modal Large Language Models (MLLMs). The primary objective is to adapt the successful R1 reasoning paradigm from text LLMs to the video domain, addressing challenges like the lack of temporal modeling in existing RL algorithms (e.g., GRPO) and the scarcity of high-quality video reasoning data. Key methodologies include proposing the Temporal Group Relative Policy Optimization (T-GRPO) algorithm, which explicitly rewards temporal understanding by comparing performance on ordered versus shuffled video frames, and constructing hybrid datasets (Video-R1-COT-165k, Video-R1-260k) incorporating both image and video reasoning samples for training. Notably, Video-R1-7B achieves 35.8% accuracy on the VSI-Bench spatial reasoning benchmark, surpassing the proprietary GPT-40 model. The main implication for AI practitioners is that carefully designed RL pipelines, including temporal-aware algorithms and strategic data mixing, can effectively elicit complex reasoning abilities in MLLMs for video understanding. |
| Reinforcement Learning | UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2503.21620) or [HuggingFace](https://huggingface.co/papers/2503.21620))| Xi Yin, hsli-cuhk, guoyaxuan0106, Yuxiang007, LZXzju | This paper introduces UI-R1, a framework that enhances GUI action prediction capabilities of multimodal large language models (MLLMs) using rule-based reinforcement learning (RL). The primary objective is to explore how rule-based RL, specifically using policy gradient optimization like GRPO, can improve the reasoning and action prediction accuracy of MLLMs on GUI tasks with high data efficiency. The methodology involves curating a small, high-quality dataset (136 tasks), defining a unified rule-based reward function encompassing action type, coordinate accuracy, and format correctness, and applying reinforcement fine-tuning (RFT). Key results show significant improvements over the base model (Qwen2.5-VL-3B) on the in-domain ANDROIDCONTROL benchmark, increasing action type accuracy by 15% and grounding accuracy by 10.3%, while also achieving competitive performance on out-of-domain benchmarks like ScreenSpot-Pro (+6.0% improvement). The main implication for AI practitioners is that rule-based RFT offers a potent, data-efficient alternative to SFT for improving GUI agent performance and generalization, particularly in resource-constrained scenarios. |
| Machine Learning | Challenging the Boundaries of Reasoning: An Olympiad-Level Math
  Benchmark for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.21380) or [HuggingFace](https://huggingface.co/papers/2503.21380))| Wayne Xin Zhao, jrwen, TimothyCzp, EliverQ, CoderBak | This paper introduces OlymMATH, a novel Olympiad-level mathematical benchmark designed to rigorously evaluate the complex reasoning capabilities of Large Language Models (LLMs). The primary objective is to address the saturation of existing benchmarks by providing a more challenging, manually curated, and bilingual (English/Chinese) test set. The methodology involved creating 200 problems (100 easy AIME-level, 100 hard) across four math fields, verified by experts and requiring verifiable numerical answers. Empirical results show that even state-of-the-art models like DeepSeek-R1 and OpenAI's o3-mini achieve low accuracy (e.g., 21.2% and 30.3% respectively on OlymMATH-EN-HARD), indicating significant challenges remain. The key implication for AI practitioners is that current LLMs struggle significantly with high-level mathematical reasoning, underscoring the need for more robust models and evaluation frameworks like OlymMATH to push the boundaries of AI reasoning. |
| Multi-Modal | VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic
  Faithfulness (Read more on [arXiv](https://arxiv.org/abs/2503.21755) or [HuggingFace](https://huggingface.co/papers/2503.21755))| mimihe, yinanhe, jackyhate, HongboLiu, Ziqi | VBench-2.0 introduces an advanced benchmark suite to evaluate the *intrinsic faithfulness* of video generation models, assessing adherence to real-world principles beyond superficial visual quality. Its objective is to rigorously test capabilities like commonsense reasoning, physics adherence, human fidelity, controllability, and creativity using 18 fine-grained dimensions. The methodology employs VLMs/LLMs for text-video alignment and VQA, specialist detectors (e.g., for human anomalies), and heuristics, all validated through large-scale human preference annotations. Evaluations demonstrate high human alignment (e.g., Spearman's ρ > 0.9 for dimensions like Human Anatomy and Identity) while highlighting current model weaknesses in areas like dynamic attribute control (<25% accuracy) and complex plot generation. VBench-2.0 provides practitioners with a standardized framework to drive the development of fundamentally realistic video generation, essential for simulation and AI-assisted filmmaking. |
| Multi-Modal | LeX-Art: Rethinking Text Generation via Scalable High-Quality Data
  Synthesis (Read more on [arXiv](https://arxiv.org/abs/2503.21749) or [HuggingFace](https://huggingface.co/papers/2503.21749))| Dakerqi, afdsafas, Xxxy13, QJerry, stzhao | LeX-Art introduces a data-centric framework to improve visual text generation in text-to-image models via scalable high-quality data synthesis. The main objective is to enhance text rendering fidelity, aesthetics, and attribute controllability in generated images by addressing limitations in existing datasets and methods. Key methodology involves creating the LeX-10K dataset using LLM-based prompt enrichment (DeepSeek-R1) and multi-stage filtering, developing a prompt enhancer (LeX-Enhancer), and fine-tuning text-to-image models (LeX-FLUX, LeX-Lumina) evaluated with a new benchmark (LeX-Bench) and metric (PNED). Experiments show significant improvements, with LeX-Lumina achieving a 79.81% PNED gain on CreateBench compared to its baseline. The main implication is that curating high-quality synthetic data is a highly effective strategy for boosting the inherent text rendering capabilities of text-to-image models without architectural modifications. |
| Machine Learning | Large Language Model Agent: A Survey on Methodology, Applications and
  Challenges (Read more on [arXiv](https://arxiv.org/abs/2503.21460) or [HuggingFace](https://huggingface.co/papers/2503.21460))| qqlong, joeyleo, evan-gyy, yszhao, luojunyu | This survey systematically deconstructs Large Language Model (LLM) agent systems, providing a comprehensive overview of their methodology, applications, and challenges. The main objective is to offer a unified taxonomy connecting agent construction (profile, memory, planning, action), collaboration mechanisms (centralized, decentralized, hybrid), and evolution pathways (self-learning, co-evolution, external resources). The key methodology involves a structured literature review, synthesizing fragmented research threads into a methodology-centered taxonomy illustrated through frameworks like Figure 1 and Figure 2. As a survey, it consolidates existing knowledge rather than presenting novel quantitative results; its primary contribution is the proposed organizational framework for understanding the LLM agent ecosystem. For AI practitioners, this work provides a structured taxonomy to navigate agent design principles, collaboration patterns, evaluation techniques, and critical real-world challenges like security and ethics, guiding future development. |
| Multi-Modal | Lumina-Image 2.0: A Unified and Efficient Image Generative Framework (Read more on [arXiv](https://arxiv.org/abs/2503.21758) or [HuggingFace](https://huggingface.co/papers/2503.21758))| luyiting, Paper99, RuoyiDu, JackyZhuo, Dakerqi | Lumina-Image 2.0 introduces a unified and efficient framework for advanced text-to-image generation, building upon Lumina-Next. The primary objective is to enhance multimodal fusion and prompt adherence by addressing architectural limitations and improving training data quality through better captions. Key methodologies include the Unified Next-DiT architecture, which treats text and image tokens as a joint sequence via single-stream attention blocks, and the Unified Captioner (UniCap), designed specifically to generate detailed, multi-granularity captions for training. The 2.6B parameter model achieves strong performance, scoring 87.2 on the DPG benchmark, demonstrating improved efficiency and scalability. For AI practitioners, this work highlights the benefits of unified sequence modeling over cross-attention for multimodal tasks and the significant impact of tailored, high-quality captioning on generative model performance. |
| Natural Language Processing | ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large
  Reasoning Models with Iterative Retrieval Augmented Generation (Read more on [arXiv](https://arxiv.org/abs/2503.21729) or [HuggingFace](https://huggingface.co/papers/2503.21729))| chenyn66, liuweichuan, NeoZ123, caoshulin, ZhiCheng0326 | ReaRAG enhances factual accuracy in Large Reasoning Models (LRMs) for multi-hop QA using iterative, knowledge-guided Retrieval-Augmented Generation (RAG). The objective is to improve LRM factuality and reasoning robustness by effectively integrating external knowledge via iterative retrieval without overthinking. The methodology involves fine-tuning an LRM using a Thought-Action-Observation paradigm on a custom dataset and iteratively searching/reflecting using a RAG engine until deriving a final answer. ReaRAG-9B significantly outperforms baselines, achieving a 14.5% ACCL gain on MuSiQue over SearChain, while requiring fewer reasoning steps than RL-based approaches. For practitioners, ReaRAG provides a robust framework to improve smaller LRMs' factuality on knowledge-intensive tasks through structured, iterative RAG and self-correction. |
| Multi-Modal | Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for
  Embodied Interactive Tasks (Read more on [arXiv](https://arxiv.org/abs/2503.21696) or [HuggingFace](https://huggingface.co/papers/2503.21696))| Guiyang1001, tricktreat, yijiang, Gangao, zwq2018 | This paper introduces Embodied-Reasoner, a model designed to synergize visual search, reasoning, and action for interactive embodied tasks by extending large model reasoning capabilities. The main objective is to address the challenge of continuous interaction and reasoning in embodied domains, which require spatial understanding, temporal reasoning, and self-reflection based on image-action interleaved trajectories. The methodology involves synthesizing 9.3k Observation-Thought-Action trajectories using a data engine and employing a three-stage training pipeline comprising imitation learning, self-exploration via rejection sampling, and self-correction via reflection tuning. Evaluation shows Embodied-Reasoner significantly outperforms models like OpenAI o1 (+9% success rate, +12% search efficiency) and GPT-o3-mini (+24% success rate), particularly excelling in complex, long-horizon tasks (+39.9% over the second-best on composite tasks). For AI practitioners, this work demonstrates a framework and training strategy to build embodied agents with enhanced interactive reasoning and planning by incorporating diverse thinking patterns and learning from interaction history. |
| Natural Language Processing | ResearchBench: Benchmarking LLMs in Scientific Discovery via
  Inspiration-Based Task Decomposition (Read more on [arXiv](https://arxiv.org/abs/2503.21248) or [HuggingFace](https://huggingface.co/papers/2503.21248))| yuqiangli, bgao22182, jinjieni, ZonglinY, yujieliu | This paper introduces ResearchBench, a large-scale benchmark to evaluate Large Language Models (LLMs) on scientific discovery tasks decomposed via an inspiration-based framework. The primary objective is to assess LLM performance on inspiration retrieval, hypothesis composition, and hypothesis ranking, using recent, uncontaminated scientific literature across 12 disciplines. An automated LLM-based agentic framework was developed to extract necessary components (questions, surveys, inspirations, hypotheses) from 1386 papers published in 2024, employing carefully selected negative inspirations for evaluation. Results show LLMs excel at inspiration retrieval (an out-of-distribution task), with GPT-4o achieving a 45.65% hit ratio in identifying ground truth inspirations within the top 4% candidates, while hypothesis composition and ranking show moderate performance with potential for improvement. The key implication is that LLMs can function as 'research hypothesis mines' for automated scientific discovery, although inspiration retrieval presents a significant bottleneck possibly linked to pretraining limitations. |
| Computer Vision | Optimal Stepsize for Diffusion Sampling (Read more on [arXiv](https://arxiv.org/abs/2503.21774) or [HuggingFace](https://huggingface.co/papers/2503.21774))| Han Hu, Jianning Pei, cientgu | This paper introduces Optimal Stepsize Distillation (OSS), a principled framework for accelerating diffusion model sampling by finding theoretically optimal stepsize schedules. The main objective is to minimize the global discretization error when reducing sampling steps, compared to a reference (teacher) trajectory with many steps. The key methodology involves reformulating stepsize optimization as a recursive error minimization problem solved via dynamic programming, distilling knowledge from the teacher trajectory to derive optimal student schedules. Experiments demonstrate significant speedups, such as 10x acceleration in text-to-image generation while preserving 99.4% performance on the GenEval benchmark compared to the teacher model, and robustness across architectures, solvers, and noise schedules. For AI practitioners, this method offers a way to efficiently deploy diffusion models by optimizing sampling schedules with low computational overhead and minimal performance degradation, applicable without model retraining. |
| Computer Vision | Exploring the Evolution of Physics Cognition in Video Generation: A
  Survey (Read more on [arXiv](https://arxiv.org/abs/2503.21765) or [HuggingFace](https://huggingface.co/papers/2503.21765))| huangsiteng, wangcunxiang, huangsiteng, yishanwang, minnielin | This survey reviews the evolution and integration of physics cognition within video generation models. The main objective is to systematically categorize approaches for incorporating physical understanding into video synthesis, addressing the prevalent issue of "visual realism but physical absurdity". It employs a novel three-tier taxonomy inspired by human cognitive development (schematic perception, passive knowledge cognition, active world simulation) to analyze methods ranging from motion-guided generation to physics simulation integration and LLM empowerment. The survey concludes that despite advancements, current state-of-the-art models frequently generate physically implausible content, failing comprehensive evaluations on benchmarks like Physics-IQ [84] and PhyGenBench [86]. For AI practitioners, the key implication is the need to move beyond purely data-driven visual mimicry towards incorporating explicit physical principles, simulation, or interactive learning to build truly world-aware generative models for robust real-world applications. |
| Multi-Modal | ChatAnyone: Stylized Real-time Portrait Video Generation with
  Hierarchical Motion Diffusion Model (Read more on [arXiv](https://arxiv.org/abs/2503.21144) or [HuggingFace](https://huggingface.co/papers/2503.21144))| Peng Zhang, Chaonan Ji, Jinwei Qi, Liefeng, shengxu97 | ChatAnyone introduces a novel framework for generating stylized, real-time upper-body portrait videos with synchronized facial expressions and body movements from audio and a single image. The primary objective is to extend real-time portrait generation beyond talking heads to include expressive upper-body interactions with fine-grained stylistic control, addressing limitations in motion synchronization and real-time performance. The methodology involves a two-stage approach: first, a hierarchical motion diffusion model generates explicit and implicit motion representations (face, body, hands) from audio with style control; second, a hybrid control fusion GAN synthesizes the video using these representations, injected hand renderings, and a face refinement step. The system achieves real-time generation at up to 30fps for 512x768 resolution on a single 4090 GPU, outperforming previous methods in expressiveness and synchronization. For AI practitioners, this work offers a robust solution for creating highly expressive and controllable real-time digital humans for interactive applications like virtual avatars and live streaming. |
| Natural Language Processing | ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging (Read more on [arXiv](https://arxiv.org/abs/2503.21088) or [HuggingFace](https://huggingface.co/papers/2503.21088))| Ziyan Jiang, Yi Zhong, Yanqiu Zhao, Saberlve, HaomingXu | This paper details ZJUKLAB's approach to selectively unlearn sensitive content from large language models using model merging techniques for SemEval-2025 Task 4. The primary objective is to effectively erase targeted sensitive information while mitigating over-forgetting and under-forgetting issues inherent in current methods. The methodology involves training two specialized models via LoRA with different hyperparameters on a composite Negative Preference Optimization (NPO), Gradient Descent on Retain set (GDR), and KL divergence minimization (KLR) objective, followed by merging them using TIES-Merging. The system achieved competitive results, ranking second online with a Task Aggregate score of 0.944, while local tests showed a near-optimal MIA AUC of 0.501. This merging strategy offers practitioners a way to potentially achieve more balanced unlearning outcomes by combining complementary models, though the authors highlight limitations in current evaluation metrics. |
| Multi-Modal | FinAudio: A Benchmark for Audio Large Language Models in Financial
  Applications (Read more on [arXiv](https://arxiv.org/abs/2503.20990) or [HuggingFace](https://huggingface.co/papers/2503.20990))| Yueru1, Shashidhar, ShirleyY, Acatsama, YupengCao | This paper introduces FINAUDIO, the first benchmark specifically designed to evaluate Audio Large Language Models (AudioLLMs) on financial audio tasks. The primary objective is to assess the capabilities of current AudioLLMs in processing financial audio, addressing a gap in existing evaluation resources for this domain. The methodology involves defining three tasks (short ASR, long ASR, summarization), compiling five datasets (totaling 400+ hours), and evaluating seven AudioLLMs using metrics like Word Error Rate (WER), Rouge-L, and BertScore. Key results show Whisper-v3 achieves the best ASR performance (WER 2.14% on MDRM-test), while overall performance degrades on long audio, impacting summarization quality (highest ROUGE-L 0.072 achieved). The benchmark highlights current AudioLLM limitations in finance, guiding practitioners towards improving long-context handling and domain-specific terminology understanding. |
| Multi-Modal | Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile
  Gaussian Feature Fields (Read more on [arXiv](https://arxiv.org/abs/2503.20776) or [HuggingFace](https://huggingface.co/papers/2503.20776))| Hui Ren, Fanzhiwen, ir1d, ShuwangZhang00, shijiezhou | Feature4X introduces a universal framework to lift capabilities from any 2D vision foundation model into the 4D domain using only monocular video input. The primary objective is to overcome the limitations of scarce 4D datasets by distilling features from pretrained 2D models into a dynamic 4D representation. The key methodology involves enhancing dynamic 3D Gaussian Splatting with a unified, compact latent feature field, optimized via a scaffold-based approach and leveraging a parallel N-dimensional Gaussian rasterizer for efficient feature distillation. Key results demonstrate comparable reconstruction (e.g., PSNR 25.197) and segmentation performance (e.g., mIoU 0.503 on NVIDIA dataset with CLIP-LSeg) to task-specific baselines while being significantly more compact (around 6.2x smaller model size). For AI practitioners, this work provides a scalable foundation for building contextually and spatiotemporally aware 4D agentic AI systems capable of complex interactions and manipulations guided by language or direct user input. |
| Computer Vision | Synthetic Video Enhances Physical Fidelity in Video Synthesis (Read more on [arXiv](https://arxiv.org/abs/2503.20822) or [HuggingFace](https://huggingface.co/papers/2503.20822))| Ziyan Yang, Ziyu Wang, Qi Zhao, fengcheng1, Univstar | This paper investigates leveraging synthetic videos generated via computer graphics (CGI) pipelines to enhance the physical fidelity of AI-based video synthesis models. The primary objective is to determine if physically consistent synthetic data can improve aspects like 3D consistency and reduce artifacts in generated videos, especially for challenging tasks. The methodology involves creating a synthetic video generation pipeline, curating the data based on factors like visual distribution and rendering quality, employing specific captioning strategies, and using a novel training approach called SimDrop to mitigate synthetic artifacts while transferring physical realism to a diffusion transformer model. Experiments demonstrate improved physical fidelity, such as achieving significantly higher success rates in user studies for large human motion (e.g., 61% vs 9% baseline for gymnastics) and better 3D reconstruction metrics for wide-angle camera rotation (e.g., reprojection error reduced to 0.135 from ~0.4). The key implication is that carefully curated synthetic data offers a practical pathway to improve the physical plausibility of video generation models, even without explicit physics modeling. |
| Multi-Modal | Unified Multimodal Discrete Diffusion (Read more on [arXiv](https://arxiv.org/abs/2503.20853) or [HuggingFace](https://huggingface.co/papers/2503.20853))| Katerina Fragkiadaki, Deepak765, Sid1275, mihirpd, aswerdlow | This paper introduces UniDisc, a unified generative model for joint text and image understanding and generation based on discrete diffusion. The primary objective is to explore discrete diffusion as an alternative to dominant autoregressive (AR) approaches for multimodal tasks, aiming to overcome AR model limitations in control, efficiency, and editability. UniDisc employs a bidirectional transformer trained to denoise sequences of text and image tokens corrupted by masking, using an absorbing state discrete diffusion process and classifier-free guidance. Key results indicate that UniDisc outperforms AR counterparts in conditional generation quality (FID/CLIP scores, Fig. 4) and retrieval tasks (e.g., 0.64 vs 0.17 joint retrieval accuracy on DataComp1B, Fig. 7), offers inherent joint inpainting capabilities (Fig. 1), but requires significantly more training compute (approx. 13.2x) to achieve similar loss levels (Fig. 3). For AI practitioners, UniDisc presents a compelling alternative framework for unified multimodal generation, excelling in controllability and flexible inference trade-offs, despite higher training costs compared to AR models. |
| Multi-Modal | LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized
  Text-Guided Image Editing (Read more on [arXiv](https://arxiv.org/abs/2503.21541) or [HuggingFace](https://huggingface.co/papers/2503.21541))| Sirisha Rambhatla, Meet Soni, Achint Soni | LOCATEdit introduces a graph Laplacian optimization approach to enhance cross-attention maps for precise, localized text-guided image editing. The primary objective is to improve spatial consistency and reduce editing artifacts in diffusion model-based editing by refining attention masks derived from cross-attention. Key methodology involves constructing a Cross and Self-Attention (CASA) graph from attention maps and applying graph Laplacian regularization to enforce smoothness and coherence, integrated within a dual-branch diffusion editing framework with selective embedding interpolation. Evaluated on PIE-Bench, LOCATEdit achieves state-of-the-art background preservation, notably reaching a PSNR of 29.20 (using DDIM sampler), outperforming baselines like ViMAEdit (28.75). For AI practitioners, this provides a training-free technique to achieve more spatially consistent and localized edits while preserving background integrity in text-guided image manipulation. |
| Natural Language Processing | LLPut: Investigating Large Language Models for Bug Report-Based Input
  Generation (Read more on [arXiv](https://arxiv.org/abs/2503.20578) or [HuggingFace](https://huggingface.co/papers/2503.20578))| Tarannum Shaila Zaman, imranraad, Subarna10, alifalhasan | This paper investigates the effectiveness of Large Language Models (LLMs) in extracting failure-inducing inputs from bug reports. The research aims to determine how well generative LLMs can automate this information extraction process, which is crucial for software debugging. The study employs three open-source generative LLMs (LLaMA, Qwen, and Qwen-Coder) and evaluates their performance on a dataset of 206 bug reports using the BLEU score metric. The results show that Qwen achieved the highest accuracy, with 62.62% of outputs having a BLEU-2 score ≥ 0.5, suggesting LLMs have potential for automating input extraction and aiding developers, but further refinements are required. |
