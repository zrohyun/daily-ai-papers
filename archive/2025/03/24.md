

## Papers for 2025-03-24

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | MAPS: A Multi-Agent Framework Based on Big Seven Personality and
  Socratic Guidance for Multimodal Scientific Problem Solving (Read more on [arXiv](https://arxiv.org/abs/2503.16905) or [HuggingFace](https://huggingface.co/papers/2503.16905))| Xinyu Zhang, Zhangqi Wang, Zhiyuan Wang, Qika, VentureZJ | This paper introduces MAPS, a multi-agent framework based on the Big Seven Personality theory and Socratic guidance for multimodal scientific problem-solving. The main objective is to address the challenges of multi-modal comprehensive reasoning and the lack of reflective capabilities in solving complex multimodal scientific problems. The methodology employs seven distinct agents, each representing a personality trait, leveraging feedback mechanisms and Socratic questioning to guide the problem-solving process. Experiments on the EMMA, Olympiad, and MathVista datasets show that MAPS outperforms the current state-of-the-art model by 15.84% across all tasks. The main implication is that leveraging a multi-agent framework with diverse personalities and Socratic feedback can significantly enhance performance in complex, multimodal reasoning tasks. |
| Natural Language Processing | MARS: A Multi-Agent Framework Incorporating Socratic Guidance for
  Automated Prompt Optimization (Read more on [arXiv](https://arxiv.org/abs/2503.16874) or [HuggingFace](https://huggingface.co/papers/2503.16874))| Jun Liu, Haiping Zhu, Zhangqi Wang, Qika, VentureZJ | This paper introduces MARS, a multi-agent framework for automated prompt optimization (APO) that incorporates Socratic guidance. The main research objective is to address the limitations of existing APO methods, specifically the limited flexibility of fixed templates and inefficient search in prompt spaces. MARS utilizes a multi-agent architecture with seven agents, including a Planner for autonomous optimization path design and a Teacher-Critic-Student Socratic dialogue pattern for iterative prompt refinement.  Experiments on 17 datasets show that MARS outperforms previous state-of-the-art methods by 6.04% on general tasks and 6.42% on domain-specific tasks. AI practitioners can leverage MARS to improve the performance of large language models across various tasks by generating more effective and interpretable prompts. |
| Multi-Modal | RoboFactory: Exploring Embodied Agent Collaboration with Compositional
  Constraints (Read more on [arXiv](https://arxiv.org/abs/2503.16408) or [HuggingFace](https://huggingface.co/papers/2503.16408))| Xiaohong Liu, Zhenfei Yin, Xiufeng Song, FACEONG, IranQin | This paper introduces RoboFactory, a framework and benchmark for exploring embodied multi-agent collaboration with compositional constraints. The main research objective is to address the challenges of generating safe and efficient collaborative data for multi-agent robotic systems. The key methodology involves leveraging large language models to generate subgoals and compositional constraints (logical, spatial, and temporal), and then using specifically designed interfaces (RoboChecker) to ensure that agent trajectories adhere to these constraints. Primary results show that, in single-agent tasks, performance reaches 49% success rate and, through architecture comparisons, reveal the Local View and Separate Policy improves performance in food place task to 20%. The main implication is that incorporating compositional constraints into multi-agent systems can significantly improve the safety, efficiency, and collaboration of embodied agents, providing a valuable tool for real-world robotic applications. |
| Multi-Modal | When Less is Enough: Adaptive Token Reduction for Efficient Image
  Representation (Read more on [arXiv](https://arxiv.org/abs/2503.16660) or [HuggingFace](https://huggingface.co/papers/2503.16660))| Andrey Kuznetsov, Elizaveta Goncharova, Eduard Allakhverdov | This paper introduces a novel method for adaptive token reduction in vision encoders to improve the efficiency of image representation in multimodal models. The research investigates whether all visual tokens generated by vision encoders are equally valuable or if some can be discarded without compromising performance. The key methodology involves integrating an autoencoder with a Gumbel-Softmax selection mechanism to identify and retain only the most informative visual tokens, based on the idea that less valuable features can be reconstructed from more valuable ones.  Experiments using the LLaVA-NeXT model show that over 50% of the visual context can be removed on OCR-based tasks with minimal performance loss, significantly outperforming random token selection. This approach offers a promising direction for scalable and low-overhead multimodal pruning, allowing AI practitioners to reduce computational costs without significantly degrading performance in vision-language tasks. |
| Computer Vision | Bridging Continuous and Discrete Tokens for Autoregressive Visual
  Generation (Read more on [arXiv](https://arxiv.org/abs/2503.16430) or [HuggingFace](https://huggingface.co/papers/2503.16430))| Yuanzhi Zhu, Yao Teng, Zhijie Lin, ShuhuaiRen, Epiphqny | This paper introduces TokenBridge, a novel approach for autoregressive visual generation that bridges continuous and discrete token representations. The main objective is to maintain the strong representation capacity of continuous tokens while preserving the modeling simplicity of discrete tokens for image generation. The key methodology involves post-training quantization of pretrained continuous VAE features and a dimension-wise autoregressive prediction mechanism.  The proposed model achieves state-of-the-art results on ImageNet 256x256, with an FID of 1.55 and IS of 313.3 for the H model. AI practitioners can achieve high-quality visual generation with simplified autoregressive modeling by leveraging this approach. |
| Multi-Modal | OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning
  via Iterative Self-Improvement (Read more on [arXiv](https://arxiv.org/abs/2503.17352) or [HuggingFace](https://huggingface.co/papers/2503.17352))| Wei Wang, Nanyun Peng, Fan Yin, Hritik Bansal, Yihe Deng | This paper introduces OpenVLThinker-7B, a large vision-language model (LVLM) designed to enhance complex reasoning capabilities through iterative self-improvement. The main objective is to investigate whether sophisticated reasoning abilities, similar to those in large language models, can be integrated into LVLMs and improve performance on multimodal reasoning tasks. The methodology involves iterative supervised fine-tuning (SFT) and reinforcement learning (RL), with each iteration's RL-improved model generating refined SFT datasets for the next round, along with distillation from a pure-text model.  OpenVLThinker-7B achieves an accuracy of 70.2% on MathVista, surpassing the baseline Qwen2.5-VL-7B's 68.5%. The results suggest that this iterative approach can significantly enhance the performance of LVLMs on challenging multimodal reasoning benchmarks, providing a promising strategy for robust vision-language reasoning. |
| Natural Language Processing | Modifying Large Language Model Post-Training for Diverse Creative
  Writing (Read more on [arXiv](https://arxiv.org/abs/2503.17126) or [HuggingFace](https://huggingface.co/papers/2503.17126))| Max Kreminski, Yuqian Sun, Melissa Roemmele, Vishakh Padmakumar, John Joon Young Chung | This paper introduces a novel post-training approach for large language models (LLMs) to enhance output diversity in creative writing tasks while maintaining quality. The research question focuses on how to modify LLM post-training objectives to promote diverse and high-quality creative writing generation. The key methodology involves incorporating 'deviation'—a measure of how much a training instance differs from others with the same prompt—into the training objectives of Direct Preference Optimization (DPO) and Odds Ratio Preference Optimization (ORPO). The primary result shows that the best model (8B parameters) achieved on-par diversity with human-created data while maintaining output quality comparable to state-of-the-art instruction-tuned models like GPT-4. The implication is that AI practitioners should consider balancing learning from both frequent and rare high-quality training instances to improve the diversity of LLM-generated creative text. |
| Computer Vision | Single Image Iterative Subject-driven Generation and Editing (Read more on [arXiv](https://arxiv.org/abs/2503.16025) or [HuggingFace](https://huggingface.co/papers/2503.16025))| Idan Schwartz, Gal Chechik, yairshp | This paper introduces SISO, a novel training-free method for subject-driven image generation and editing using only a single reference image. The main research objective is to enable high-quality image personalization without requiring extensive training data or computational resources, particularly addressing the challenge of single-image personalization. SISO iteratively optimizes a similarity score between the generated image and the input subject image using pre-trained identity metrics like DINO and IR, updating model parameters at each step.  In image generation, SISO achieved a FID score of 149.2 and a CLIP-T score of 0.31, outperforming baselines in prompt adherence while maintaining fidelity. The main implication is that AI practitioners can achieve effective image personalization through an iterative, inference-time optimization, even with limited data, bypassing the need for subject-specific model training or fine-tuning. |
| Multi-Modal | MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical
  Problems (Read more on [arXiv](https://arxiv.org/abs/2503.16549) or [HuggingFace](https://huggingface.co/papers/2503.16549))| Jun Cen, Tao Feng, Yunqiu Xu, Felix Chen, JacobYuan | This paper introduces MathFlow, a modular problem-solving pipeline designed to enhance the performance of Multimodal Large Language Models (MLLMs) on visual mathematical problems. The main research objective is to address the limitations of existing MLLMs in accurately perceiving and interpreting diagrams, which impacts their overall problem-solving capabilities. The methodology decouples the problem-solving process into distinct perception and inference stages, introducing a new benchmark, FlowVerse, to evaluate these capabilities and training a specialized perception model, MathFlow-P-7B. Experimental results indicate that MathFlow-P-7B significantly improves performance when integrated with various inference models, achieving a CoT-E score of 59.3% when combined with GPT-4V on FlowVerse. The implication for AI practitioners is that specializing MLLMs for enhanced visual perception can substantially improve visual mathematical reasoning without any additional training of state-of-the-art LLMS, extending their application in handling complex multi-modal data. |
| Multi-Modal | ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question
  Generation and Answering (Read more on [arXiv](https://arxiv.org/abs/2503.16867) or [HuggingFace](https://huggingface.co/papers/2503.16867))| Wei Liu, Peng Zhang, Yuchong Sun, Zhengfeng Lai, Guan123 | This paper introduces ETVA, a novel evaluation method for assessing text-to-video alignment using fine-grained question generation and answering. The main research objective is to develop a metric that better aligns with human preference than existing coarse-grained methods like CLIPScore. ETVA employs a multi-agent system for question generation and a knowledge-augmented multi-stage reasoning framework for question answering, leveraging Large Language Models (LLMs). Experiments show that ETVA achieves a Spearman's correlation coefficient of 58.47 with human judgment, significantly outperforming existing metrics.  For AI practitioners, ETVA offers a more human-aligned, reliable automatic evaluation metric for text-to-video generation models and helps identify their limitations. |
| Computer Vision | Enabling Versatile Controls for Video Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2503.16983) or [HuggingFace](https://huggingface.co/papers/2503.16983))| Jiaxing Yan, Xiaobin Lu, Haoming Qin, Hao Zhou, Xu Zhang | This paper introduces VCtrl, a novel framework for fine-grained control over pre-trained video diffusion models. The main research objective is to address the challenge of achieving precise and flexible spatiotemporal control in text-to-video generation. VCtrl integrates diverse user-specified control signals (Canny edges, segmentation masks, human keypoints) via a generalizable conditional module and a unified control signal encoding pipeline with sparse residual connections.  In Canny-to-Video generation, VCtrl achieves a Canny Matching score of 0.28 and an FVD of 345.00, improving control and video quality.  AI practitioners can leverage VCtrl for enhanced controllability and generation quality in various video generation tasks without modifying the underlying video diffusion model. |
| Computer Vision | When Preferences Diverge: Aligning Diffusion Models with Minority-Aware
  Adaptive DPO (Read more on [arXiv](https://arxiv.org/abs/2503.16921) or [HuggingFace](https://huggingface.co/papers/2503.16921))| Donghao Luo, Kai Hu, Chengming Xu, Chen Liu, Lingfan Zhang | This paper introduces Adaptive-DPO, a novel approach for aligning diffusion models with human preferences in image generation, while accounting for minority preference samples. The research explores the challenges posed by preference data heterogeneity, specifically erroneous annotations and subjective divergences, and how they impact the performance of Diffusion-DPO. The key methodology involves a minority-instance-aware metric, incorporating intra-annotator confidence and inter-annotator stability, to distinguish between majority and minority preferences, then adapts the DPO loss accordingly. Experiments show Adaptive-DPO improves performance; for example, on SD1.5, flipping 20% of preference data degrades ImageReward to 0.00 with standard DPO, while Adaptive-DPO maintains a score of 0.34. This suggests Adaptive-DPO offers a more robust training methodology for image generation, enabling better alignment with nuanced human preferences, even with noisy or subjectively divergent data. |
| Reinforcement Learning | FastCuRL: Curriculum Reinforcement Learning with Progressive Context
  Extension for Efficient Training R1-like Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2503.17287) or [HuggingFace](https://huggingface.co/papers/2503.17287))| Xuan Luo, Wenjie Yang, Zheng Li, Mao Zheng, Mingyang Song | This paper introduces FastCuRL, a curriculum reinforcement learning approach designed to improve the training efficiency and performance of R1-like reasoning models. The primary objective is to accelerate reinforcement learning training and enhance the performance in complex reasoning tasks with long chain-of-thought, specifically with a 1.5B parameter language model. FastCuRL uses length-aware training data segmentation and progressive context window extension training. Experimental results show that FastCuRL-1.5B-Preview surpasses DeepScaleR-1.5B-Preview across five benchmarks, achieving a 57.5 average Pass@1 accuracy while using only 50% of the training steps. This method offers AI practitioners a way to significantly reduce computational costs and improve model performance when training large language models for complex reasoning. |
| Multi-Modal | PVChat: Personalized Video Chat with One-Shot Learning (Read more on [arXiv](https://arxiv.org/abs/2503.17069) or [HuggingFace](https://huggingface.co/papers/2503.17069))| Yuchen Li, Yumeng Li, Gang Xu, Weilong Yan, Master-Shi | This paper introduces PVChat, a personalized video large language model (ViLLM) capable of subject-aware question answering from a single reference video. The main research objective is to enable ViLLMs to understand and reason about identity-specific information in videos, overcoming the limitation of existing models that focus on general video understanding. The methodology involves a novel data augmentation pipeline to synthesize identity-preserving training data, a ReLU Routing Mixture-of-Heads (ReMoH) attention mechanism for enhanced subject-specific learning, and a two-stage training strategy combining image pre-training and video fine-tuning. PVChat achieves a significant improvement over state-of-the-art ViLLMs, with an accuracy of 0.901 compared to 0.470 for VideoLLaMA2 and 0.342 for InternVideo2. AI practitioners can leverage PVChat's approach for developing personalized video understanding applications, particularly in contexts requiring identity-aware comprehension, such as smart healthcare and smart home environments. |
| Multi-Modal | From Head to Tail: Towards Balanced Representation in Large
  Vision-Language Models through Adaptive Data Calibration (Read more on [arXiv](https://arxiv.org/abs/2503.12821) or [HuggingFace](https://huggingface.co/papers/2503.12821))| Yu Cheng, Jiawei Zhou, Xiaoye Qu, hitsmy | This paper addresses the long-tail (LT) data distribution problem in Large Vision-Language Models (LVLMs) through an Adaptive Data Refinement (ADR) framework. The research objective is to mitigate the impact of imbalanced training data on LVLM performance, specifically the overrepresentation of head concepts and underrepresentation of tail concepts. The proposed ADR framework consists of two stages: Data Rebalancing (DR), which adaptively filters redundant data, and Data Synthesis (DS), which leverages diffusion models to generate synthetic data for underrepresented portions.  ADR improved the average performance of LLaVA 1.5 by 4.36% across eleven benchmarks, without increasing training data volume. AI practitioners can integrate ADR into the training data of open-source LVLMs to improve performance, particularly on tasks involving tail concepts. |
| Natural Language Processing | Implicit Bias-Like Patterns in Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2503.11572) or [HuggingFace](https://huggingface.co/papers/2503.11572))| Calvin K. Lai, l048596 | This paper investigates implicit bias-like patterns in reasoning language models, analogous to human implicit bias. The research objective is to determine if reasoning models exhibit differences in processing effort when handling association-compatible versus association-incompatible information. The authors adapt the Implicit Association Test (IAT) for reasoning models, termed RM-IAT, measuring computational effort via reasoning token counts from API calls. Results showed that the 03-mini model used significantly more reasoning tokens (average of 53.33% more) for association-incompatible pairings than compatible ones in 9 out of 10 RM-IATs, with effect sizes ranging from d = 0.53 to 1.26. AI practitioners should be aware that reasoning models may exhibit implicit bias-like patterns in their processing, potentially impacting real-world applications, and measurement of bias should consider processing effort in addition to model outputs. |
| Multi-Modal | Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language
  Model (Read more on [arXiv](https://arxiv.org/abs/2503.16282) or [HuggingFace](https://huggingface.co/papers/2503.16282))| Junlin Han, Runjia Li, Yun Liu, Guolei Sun, Zhaochong An | This paper introduces GFS-VL, a framework for generalized few-shot 3D point cloud segmentation (GFS-PCS) that leverages both 3D vision-language models (VLMs) and few-shot samples. The research aims to improve the performance of GFS-PCS by combining the precise, sparse annotations of few-shot samples with dense, but noisy, pseudo-labels derived from 3D VLMs. The methodology involves prototype-guided pseudo-label selection, adaptive infilling, and a novel-base mix strategy to embed support samples for novel class learning. The proposed method achieved a 43.12% HM score on ScanNet200 in the 5-shot setting, outperforming previous methods, and provides new benchmarks for GFS-PCS. Practitioners can apply this approach and benchmarks to develop robust GFS-PCS models that better generalize to novel object classes in real-world applications. |
| Computer Vision | FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields (Read more on [arXiv](https://arxiv.org/abs/2503.17095) or [HuggingFace](https://huggingface.co/papers/2503.17095))| Junyong Noh, Hangyeul Shin, Chaelin Kim, Kwan Yun | FFaceNeRF is a novel NeRF-based 3D face editing technique that enables high-fidelity, customizable face editing with only a few training samples. The main objective is to overcome the limitations of existing mask-based 3D face editing methods, which provide limited user control due to their reliance on pre-trained segmentation masks with fixed layouts. The key methodology involves employing a geometry adapter with feature injection and latent mixing for tri-plane augmentation, allowing the model to quickly adapt to desired mask layouts. The method achieves an average mIoU of 85.33% on mask generation with their proposed training data, and in A/B testing, human subjects rated the method higher in faithfulness (72.29%) than existing systems. This approach allows AI practitioners more flexible control over the regions in 3D facial editing tasks by enabling few-shot learning on custom segmentation masks. |
| Computer Vision | TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented
  Reality via 3D Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2503.17032) or [HuggingFace](https://huggingface.co/papers/2503.17032))| Tiansong Zhou, Zhonghua Jiang, Gaige Wang, Jingchuan Hu, Jianchuan Chen | This paper introduces TaoAvatar, a system for creating and rendering high-fidelity, lightweight, full-body talking avatars in real-time for augmented reality applications. The research aims to develop a 3DGS-based avatar system that can be driven by various signals, run efficiently on mobile and AR devices, and capture high-frequency appearance details. The key methodology involves binding Gaussians to a personalized clothed human parametric template, pre-training a StyleUnet-based teacher network for non-rigid deformations, distilling this knowledge into a lightweight MLP-based student network, and using blend shapes for detail compensation. TaoAvatar achieves state-of-the-art rendering quality, maintaining 90 FPS on high-definition stereo devices like the Apple Vision Pro, with a Peak Signal-to-Noise Ratio (PSNR) of 33.81 and Structural Similarity Index Measure (SSIM) of 0.9689 for full body rendering. The approach allows for the creation of realistic avatars for use in AR environments, with good performance on resource-constrained mobile devices. |
