

## Papers for 2025-03-11

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Feature-Level Insights into Artificial Text Detection with Sparse
  Autoencoders (Read more on [arXiv](https://arxiv.org/abs/2503.03601) or [HuggingFace](https://huggingface.co/papers/2503.03601))| Kristian Kuznetsov, natriistorm, razzant, plina2polina, Kushnareva | This paper investigates the interpretability of artificial text detection (ATD) using sparse autoencoders (SAEs). The research objective is to enhance ATD interpretability by extracting and analyzing features from a language model's residual stream. The key methodology involves applying SAEs to the Gemma-2-2b model's residual stream, identifying interpretable features, and evaluating them through domain/model-specific statistics, steering, and manual/LLM-based interpretation.  The SAE-derived features outperform the state-of-the-art MTL model on the COLING dataset at the 16th layer, though specific F1 scores are fragmented across layers and sub-datasets.  The results imply that ATD systems can be made more interpretable and potentially more robust by leveraging SAE-extracted features, but further research is needed for newer models. |
| Multi-Modal | MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2503.07365) or [HuggingFace](https://huggingface.co/papers/2503.07365))| wangwhcore, friskit, hflqf88888, Cierra0506, FanqingM | This paper introduces MM-Eureka, a multimodal reasoning model that extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning, successfully reproducing key characteristics observed in text-based RL systems. The research aims to investigate the effectiveness of large-scale RL in multimodal reasoning and to develop an open-source pipeline for this task. MM-Eureka employs rule-based RL with minimal training setups and difficulty-based data selection, applied to both instruction-tuned and pre-trained models.  The model, trained with 54K image-text data, achieves an average performance across 5 benchmarks surpassing a model trained with 1M data using MPO, demonstrating competitive performance and superior data efficiency. The findings imply that large-scale RL can be efficiently applied to multimodal settings, cultivating visual reasoning and reflection behaviors, offering a promising approach for enhancing multimodal reasoning capabilities. |
| Natural Language Processing | SEAP: Training-free Sparse Expert Activation Pruning Unlock the
  Brainpower of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.07605) or [HuggingFace](https://huggingface.co/papers/2503.07605))| Xun Liang, BO1022, Ki-Seki, siminniu, UglyToilet | This paper introduces SEAP, a training-free, task-adaptive pruning method for Large Language Models (LLMs) that reduces computational overhead while maintaining performance. The main objective is to investigate whether LLMs exhibit task-specific activation patterns similar to the human brain, and if so, whether these patterns can be leveraged for efficient pruning. SEAP identifies task-specific expert activation patterns and prunes the model by selectively retaining task-relevant parameters. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy; for example, at 50% pruning, SEAP surpasses WandA and FLAP by over 20% on average across several benchmarks. AI practitioners can use SEAP to optimize LLMs for deployment in resource-constrained environments, achieving significant efficiency gains without extensive retraining. |
| Multi-Modal | Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue
  Learning (Read more on [arXiv](https://arxiv.org/abs/2503.07002) or [HuggingFace](https://huggingface.co/papers/2503.07002))| Zongqing Lu, Jiazheng Liu, tellarin, sipeng9527 | This paper introduces MMDiag, a new multi-turn multimodal dialogue dataset, and DiagNote, a novel Multimodal Large Language Model (MLLM) designed to improve grounding and reasoning in multi-turn dialogues. The research aims to address the challenge of maintaining focus and integrating visual and textual information in complex, multi-turn conversations. DiagNote utilizes two interacting modules, Deliberate and Gaze, to perform Chain-of-Thought reasoning and dynamic visual grounding, mimicking human visual processing.  DiagNote, trained with MMDiag + COCO + VisCoT, achieved an average Intersection over Union (IoU) of 0.625 on grounding benchmarks, outperforming existing models. AI practitioners can leverage MMDiag and DiagNote to develop MLLMs with enhanced capabilities for multi-turn multimodal dialogue, particularly in complex scenarios requiring robust grounding and reasoning. |
| Multi-Modal | Automated Movie Generation via Multi-Agent CoT Planning (Read more on [arXiv](https://arxiv.org/abs/2503.07314) or [HuggingFace](https://huggingface.co/papers/2503.07314))| Zeyu Zhu, AnalMom, weijiawu | This paper introduces MovieAgent, a novel framework for automated movie generation from a script synopsis and character bank using a multi-agent Chain of Thought (CoT) planning approach. The main research objective is to explore and define the paradigm of automated movie/long-video generation, addressing the limitations of existing frameworks that lack automated planning. MovieAgent employs a hierarchical CoT-based multi-agent reasoning process, simulating roles like director, screenwriter, and storyboard artist, to automate scene structuring, camera settings, and cinematography.  Experiments demonstrate that MovieAgent achieves state-of-the-art performance in automated storytelling and movie generation, for example attaining a CLIP score of 22.25 and VBench Motion Smoothness of 97.84. The main implication is that AI practitioners can leverage this framework to scale automated storytelling, reduce human intervention in filmmaking, and ensure consistency in long form video generation. |
| Machine Learning | FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA
  Subparameter Updates (Read more on [arXiv](https://arxiv.org/abs/2503.07216) or [HuggingFace](https://huggingface.co/papers/2503.07216))| Sung Ju Hwang, matbambbang, Seanie-lee, Sangsang | This paper introduces FedRand, a privacy-enhancing framework for federated learning (FL) that is particularly tailored for vision-language models (VLMs). The central research objective is to mitigate the vulnerability of VLMs to membership inference attacks (MIAs) in a federated learning setting without significant performance degradation.  FedRand employs a randomized Low-Rank Adaptation (LoRA) subparameter update mechanism, where clients select and update a subset of LoRA parameters from the server, keeping the rest private, sending back to the server only the updated public subset.  Empirically, on the MSCOCO dataset, FedRand achieved a CIDEr score of 110.27 while having an AUROC of MIA of around 70, demonstrating comparable performance to FedAvg (CIDEr of 111.08) with improved robustness against MIAs. The work's main implication is that AI practitioners can enhance the privacy of federated VLM training by selectively updating and sharing model parameters, offering a trade-off between communication cost, model accuracy and resistance to membership inference attacks. |
| Natural Language Processing | DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.07067) or [HuggingFace](https://huggingface.co/papers/2503.07067))| Luming Liang, tding1, sungnyun, tianyic, jongwooko | This paper introduces DistiLLM-2, a novel contrastive approach for knowledge distillation in large language models (LLMs). The research objective is to improve the performance of small language models (sLMs) by optimizing the distillation process from larger teacher models. The key methodology is a contrastive approach that simultaneously increases the likelihood of teacher responses and decreases that of student responses, combined with optimized dataset curation and curriculum-based adaptive loss mechanisms.  Experiments show that DistiLLM-2 achieves state-of-the-art performance across various tasks; for example, on average outperforming previous methods by +2.34%, +1.95%, and +4.53% for Qwen2-1.5B, Danube2-1.8B, and Gemma2-2B, respectively, across AlpacaEval, Evol-Instruct, and UltraFeedback benchmarks. The main implication is that AI practitioners can leverage this contrastive distillation approach to build more efficient and high-performing sLMs for various applications, including preference alignment and vision-language tasks. |
| Machine Learning | FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation
  for Feature Implementation (Read more on [arXiv](https://arxiv.org/abs/2503.06680) or [HuggingFace](https://huggingface.co/papers/2503.06680))| Wei Li, lisijia0504, yangyu90, dawnmsg, CharonBony | This paper introduces FEA-Bench, a new benchmark for evaluating large language models (LLMs) on repository-level code generation for feature implementation. The main objective is to assess LLMs' ability to perform incremental development within code repositories, focusing on adding new features rather than fixing bugs. The methodology involves collecting pull requests from GitHub repositories, filtering them based on rules and intent, and pairing code changes with unit tests for verification. The best-performing LLM, DeepSeek-R1, achieved a resolved ratio of only 9.92% on the full benchmark, indicating significant challenges in this task. This implies that AI practitioners need to develop more robust methods for handling complex, repository-level code generation involving new feature additions. |
| Computer Vision | EasyControl: Adding Efficient and Flexible Control for Diffusion
  Transformer (Read more on [arXiv](https://arxiv.org/abs/2503.07027) or [HuggingFace](https://huggingface.co/papers/2503.07027))| Jiaming Liu, Yirui Yuan, wanghaofan, yiren98, zzyx | EasyControl is a novel framework designed to enhance condition-guided diffusion transformers with improved efficiency and flexibility. The main objective is to address the challenges of efficient and flexible control in Diffusion Transformer (DiT) architectures, particularly for multi-condition and customized model scenarios. The framework introduces a lightweight Condition Injection LoRA Module for isolated condition signal processing, a Position-Aware Training Paradigm for resolution flexibility, and a Causal Attention Mechanism with KV Cache to reduce latency. The full EasyControl model achieves a 58% reduction in inference time (16.3 seconds) compared to a baseline without the Position-Aware Training Paradigm and KV Cache, all while using only 15 million parameters. AI practitioners can leverage EasyControl for more efficient and flexible controllable image generation with DiT models, including zero-shot multi-condition generalization and compatibility with custom models. |
| Multi-Modal | AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via
  Reinforcement Learning and Reasoning (Read more on [arXiv](https://arxiv.org/abs/2503.07608) or [HuggingFace](https://huggingface.co/papers/2503.07608))| Qian Zhang, xinggangw, wenyuliu, Atan-0221, rb93dett | AlphaDrive is a novel framework that enhances Vision-Language Models (VLMs) for autonomous driving planning through reinforcement learning (RL) and reasoning. The research explores how to effectively apply RL and reasoning techniques, successful in general large models, to improve VLM performance in autonomous driving planning while reducing training costs. It introduces a two-stage training strategy with Group Relative Policy Optimization (GRPO)-based RL, incorporating four planning-specific rewards, and knowledge distillation from larger models. AlphaDrive significantly improves planning accuracy compared to supervised fine-tuning (SFT) alone, achieving a 77.12% accuracy and outperforming the SFT-trained model by 35.31% using only 20% of the data. This work demonstrates the potential of integrating RL and reasoning to improve multimodal planning in autonomous vehicles, showing emergent capabilities and enhanced training efficiency for practical applications. |
| Computer Vision | DreamRelation: Relation-Centric Video Customization (Read more on [arXiv](https://arxiv.org/abs/2503.07602) or [HuggingFace](https://huggingface.co/papers/2503.07602))| Shiwei Zhang, Shuaishuai0219, lloong, JacobYuan, weilllllls | DreamRelation is a novel approach for relation-centric video customization, generating personalized videos depicting user-specified relations between two subjects based on a small set of exemplar videos. The main research question is how to decouple relations and subject appearances while accurately modeling relational dynamics to enhance generalizability in video customization. The key methodology involves relational decoupling learning using a relation LoRA triplet and hybrid mask training, combined with relational dynamics enhancement via a space-time relational contrastive loss. The method achieves a Relation Accuracy of 0.4452, outperforming state-of-the-art methods. AI practitioners can leverage this approach to generate videos with customized relations and improved generalization across subject categories, advancing applications in areas like filmmaking and complex visual content comprehension. |
| Machine Learning | Agent models: Internalizing Chain-of-Action Generation into Reasoning
  models (Read more on [arXiv](https://arxiv.org/abs/2503.06580) or [HuggingFace](https://huggingface.co/papers/2503.06580))| Jitao Sang, Xinyan Wen, Jiangming Shu, tzteyang, TokerZ | This paper introduces Large Agent Models (LAMs) that internalize Chain-of-Action (CoA) generation, allowing autonomous decisions on when and how to use external tools. The research objective is to enhance reasoning models with the ability to seamlessly switch between reasoning and action, reducing reliance on external prompts.  The proposed AutoCoA framework combines supervised fine-tuning (SFT) and reinforcement learning (RL), incorporating step-level action triggering, trajectory-level CoA optimization, and an internal world model. Evaluations on open-domain QA tasks show that AutoCoA-trained models significantly outperform ReAct-based workflows, achieving, for instance, 33.9% EM and 38.5% LLM accuracy on average across multiple datasets. The key implication is that internalizing CoA generation can substantially improve task completion rates, especially in complex, multi-step reasoning tasks, suggesting a path toward more autonomous and capable AI agents. |
| Natural Language Processing | WritingBench: A Comprehensive Benchmark for Generative Writing (Read more on [arXiv](https://arxiv.org/abs/2503.05244) or [HuggingFace](https://huggingface.co/papers/2503.05244))| SHaopeng Lai, Chenliang Li, Ming Yan, Jiahao Mei, AQuarterMile | This paper introduces WritingBench, a comprehensive benchmark for evaluating large language models (LLMs) in generative writing tasks. The main objective is to address the limitations of existing benchmarks by assessing LLM performance across a wider range of writing domains and requirements. The methodology involves constructing a benchmark with 1,239 queries across 6 core writing domains and 100 subdomains, and proposing a query-dependent evaluation framework using LLMs to generate instance-specific criteria. The framework achieves 83% human alignment, significantly surpassing static-criteria baselines. The main implication is that this benchmark and evaluation framework provide a more nuanced and robust approach to assessing and improving the writing capabilities of LLMs. |
| Multi-Modal | Vision-R1: Incentivizing Reasoning Capability in Multimodal Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.06749) or [HuggingFace](https://huggingface.co/papers/2503.06749))| Zheyu Ye, Shaosheng Cao, Zijie Zhai, Bohan Jia, Wenxuan Huang | This paper introduces Vision-R1, a multimodal large language model designed to enhance reasoning capabilities in vision-language tasks. The main research objective is to explore how reinforcement learning (RL) can be effectively utilized to improve the reasoning ability of Multimodal Large Language Models (MLLMs), addressing the limitations of direct RL training. The key methodology involves constructing a high-quality multimodal Chain-of-Thought (CoT) dataset without human annotations and employing Progressive Thinking Suppression Training (PTST) with Group Relative Policy Optimization (GRPO) to mitigate overthinking.  Vision-R1-7B achieves a 73.5% accuracy on the MathVista benchmark, which is only 0.4% lower than a leading reasoning model. The main implication is that combining cold-start initialization with structured RL training can significantly improve reasoning capabilities in MLLMs, making them more effective in complex vision reasoning tasks. |
| Natural Language Processing | SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and
  Multi-dimensional Evaluation for Automated Survey Writing (Read more on [arXiv](https://arxiv.org/abs/2503.04629) or [HuggingFace](https://huggingface.co/papers/2503.04629))| Bin Wang, Renqiu Xia, Jiakang Yuan, Shiyang Feng, Xiangchao Yan | SurveyForge is a novel framework for automated survey paper generation, focusing on improving outline quality and citation accuracy. The main objective is to address the quality gap between LLM-generated surveys and human-written ones, particularly in outline structure and reference relevance. The methodology involves heuristic outline generation by analyzing human-written outlines and a memory-driven scholar navigation agent for retrieving high-quality papers. Experiments using the SurveyBench benchmark demonstrate that SurveyForge outperforms baselines like AutoSurvey, achieving a reference coverage score of 0.40 with Claude-3-Haiku, compared to AutoSurvey's 0.23. AI practitioners can use SurveyForge as a robust tool for generating high-quality, well-structured academic survey papers, significantly reducing the manual effort involved in literature review. |
| Multi-Modal | LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted
  Contrastive Learning (Read more on [arXiv](https://arxiv.org/abs/2503.04812) or [HuggingFace](https://huggingface.co/papers/2503.04812))| Jinsong Su, Jie Zhou, Fandong Meng, lqniu, zhibinlan | This paper introduces LLaVE, a new framework for training large language and vision embedding models using hardness-weighted contrastive learning to improve multimodal representation learning. The main research objective is to address the challenge that existing LMM-based embedding models struggle to distinguish hard negative pairs effectively due to overlap in similarity distributions. The key methodology involves dynamically weighting negative pairs based on their discriminative difficulty during contrastive learning, along with cross-device negative sample gathering. LLaVE-7B achieves a state-of-the-art overall score of 70.3 on the MMEB benchmark, surpassing the previous best model by 6.2 points. AI practitioners can leverage this approach to develop more robust and efficient multimodal embedding models for tasks like retrieval and clustering with improved performance in handling similar representations. |
| Natural Language Processing | MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for
  Complex Medical Reasoning (Read more on [arXiv](https://arxiv.org/abs/2503.07459) or [HuggingFace](https://huggingface.co/papers/2503.07459))| Jiapeng Chen, Jiwoong Sohn, Daniel Shao, wshi83, RTT1 | The paper introduces MedAgentsBench, a new benchmark for evaluating large language models (LLMs) and agent frameworks on complex medical reasoning tasks. The main objective is to address limitations in existing medical question-answering benchmarks by focusing on challenging questions that require multi-step reasoning, diagnosis formulation, and treatment planning. The methodology involves curating a subset of questions from seven established medical datasets, using adversarial filtering to identify difficult questions, and conducting experiments with various base models and reasoning methods. Key results show that thinking models like DEEPSEEK R1 and OPENAI 03 achieve significantly higher accuracy (15-25% improvement) on the challenging benchmark compared to traditional approaches and several agent based framework. This implies that AI practitioners should prioritize the development of advanced reasoning capabilities in LLMs for complex medical applications, and consider open-source options with superior performance. |
| Computer Vision | PE3R: Perception-Efficient 3D Reconstruction (Read more on [arXiv](https://arxiv.org/abs/2503.07507) or [HuggingFace](https://huggingface.co/papers/2503.07507))| Xinchao Wang, Shizun Wang, Jie Hu | PE3R is a novel framework for efficient and accurate 3D semantic reconstruction from 2D images, eliminating the need for 3D data like camera parameters. The main research objective is to enhance both the accuracy and efficiency of reconstructing and understanding 3D scenes from 2D images without explicit 3D information. The framework employs a feed-forward architecture with pixel embedding disambiguation, semantic field reconstruction, and global view perception to achieve rapid 3D semantic field reconstruction. PE3R demonstrates a minimum 9-fold speedup in 3D semantic field reconstruction and achieves a mIoU of 0.6531 on the ScanNet++ dataset for open vocabulary segmentation. AI practitioners can leverage this framework for scenarios requiring large-scale or real-time 3D processing with improved accuracy and efficiency, especially when 3D data is scarce or unavailable. |
| Computer Vision | Effective and Efficient Masked Image Generation Models (Read more on [arXiv](https://arxiv.org/abs/2503.07197) or [HuggingFace](https://huggingface.co/papers/2503.07197))| Jun Zhou, Jun Hu, Xiaolu Zhang, Jingyang Ou, yyyou | This paper introduces eMIGM, an efficient and effective masked image generation model that unifies masked image modeling and masked diffusion models. The research explores the design space of training and sampling in masked image generation to identify key factors contributing to performance and efficiency. The methodology involves a unified framework leveraging both masked image and masked diffusion models, with innovations like CFG with Mask and a time interval strategy for classifier-free guidance. Empirically, eMIGM achieves a FID of 1.57 on ImageNet 512x512 using only 60% of the NFE compared to state of the art models. AI practitioners can use eMIGM for high-quality, efficient image generation, with fewer computational resources. |
| Multi-Modal | Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive
  Reinforcement (Read more on [arXiv](https://arxiv.org/abs/2503.06520) or [HuggingFace](https://huggingface.co/papers/2503.06520))| Fanbin Lu, Zihao Yue, Zhisheng Zhong, Bohao Peng, Yuqi Liu | Seg-Zero is a novel framework for reasoning segmentation that derives explicit chain-of-thought reasoning through cognitive reinforcement. The research objective is to address the limitations of traditional methods that rely on supervised fine-tuning, improving out-of-domain generalization and introducing explicit reasoning. The framework uses a decoupled architecture with a reasoning model (MLLM) and a segmentation model, trained via pure reinforcement learning (GRPO) with a sophisticated reward mechanism.  Seg-Zero-7B achieves a zero-shot performance of 57.5 on the ReasonSeg benchmark, outperforming prior methods like LISA-7B by 18%. The implication is that AI practitioners can leverage reinforcement learning and explicit reasoning chains to achieve better generalization and performance in segmentation tasks without extensive labeled data. |
| Machine Learning | BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement
  for Transformers in Large-Scale Time Series Modeling (Read more on [arXiv](https://arxiv.org/abs/2503.06121) or [HuggingFace](https://huggingface.co/papers/2503.06121))| xiaol, Alic-Li | This paper introduces BlackGoose Rimer, a novel approach for large-scale time series modeling that leverages RWKV-7 as a superior alternative to Transformers. The research objective is to address the scalability challenges faced by time series models when handling large and complex datasets, similar to those encountered by large language models. The methodology integrates RWKV-7's time mix and channel mix components into a Transformer-based time series model (Timer), incorporating meta-learning into the state update mechanism. The results demonstrate that Rimer-1.6M achieves a performance improvement of 1.13x to 43.3x, a 4.5x reduction in training time, and utilizes 1/23 the number of parameters compared to Timer-37.8M, for example achieving an R^2 of 0.9991 vs 0.9755 for the ECL dataset. The implication for AI practioners is the potential to achieve better scalability, and improved efficiency and accuracy in time series modeling using RWKV-based architectures. |
| Machine Learning | Efficient Distillation of Classifier-Free Guidance using Adapters (Read more on [arXiv](https://arxiv.org/abs/2503.07274) or [HuggingFace](https://huggingface.co/papers/2503.07274))| msadat97, cristianpjensen | This paper introduces Adapter Guidance Distillation (AGD), a method for improving the efficiency of conditional diffusion models by distilling classifier-free guidance (CFG) into the base model using lightweight adapters. The main research objective is to mitigate the computational overhead of CFG, which doubles the number of neural function evaluations per inference step. AGD trains adapters on CFG-guided trajectories to simulate CFG in a single forward pass, preserving the original model weights and reducing training resource requirements. AGD achieves comparable or superior FID scores to standard CFG while doubling sampling speed; for instance, AGD achieves an FID of 5.03 on DiT compared to CFG's 5.30. The implication is that AI practitioners can achieve the benefits of CFG (improved sample quality and alignment) without the increased computational cost, making advanced diffusion models more accessible. |
| Machine Learning | This Is Your Doge, If It Please You: Exploring Deception and Robustness
  in Mixture of LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.05856) or [HuggingFace](https://huggingface.co/papers/2503.05856))| Ilija Bogunovic, Sangwoong Yoon, Llwo | This paper presents a comprehensive study of the robustness of Mixture of LLM Agents (MoA) architectures against deceptive agents. The research question is how vulnerable MoA systems are to the introduction of one or more deceptive LLM agents that deliberately provide misleading responses, and how to make them more robust. The key methodology involves introducing deceptive agents into MoA architectures, evaluating the system's performance on benchmarks like AlpacaEval 2.0 and QUALITY, and testing unsupervised defense mechanisms inspired by the historical Doge of Venice voting process. The results demonstrate that a single deceptive agent can reduce the Length-controlled Win Rate (LC WR) on AlpacaEval 2.0 from 49.2% to 37.9% in a 7-agent MoA, effectively nullifying MoA gains, and proposed defense mechanisms can recover most of this lost performance. The main implication is that AI practitioners must consider adversarial robustness when deploying multi-agent LLM systems, and that unsupervised methods can provide a significant degree of protection against deception. |
| Natural Language Processing | State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for
  State Space Models (Read more on [arXiv](https://arxiv.org/abs/2503.03499) or [HuggingFace](https://huggingface.co/papers/2503.03499))| Hyung Il Koo, Minjae Lee, Yuchen Zeng, Kevin Galim, Wonjun Kang | This paper introduces State-offset Tuning, a novel parameter-efficient fine-tuning (PEFT) method for State Space Models (SSMs) used in natural language processing. The research addresses the limited exploration of PEFT methods on SSMs and proposes state-based methods as a superior alternative to prompt-based approaches used for Transformers. State-offset Tuning directly adjusts state-related features within the SSM module at each timestep, unlike prompt-based methods that rely on external prompts.  Experiments across various datasets show that State-offset Tuning (h) achieves an average accuracy of 78.5% on the GLUE benchmark, outperforming other PEFT methods and approaching the performance of full fine-tuning. This offers a more effective and efficient way to adapt SSMs to downstream tasks, reducing computational cost and memory usage. |
| Multi-Modal | Should VLMs be Pre-trained with Image Data? (Read more on [arXiv](https://arxiv.org/abs/2503.07603) or [HuggingFace](https://huggingface.co/papers/2503.07603))| Igor Vasiljevic, Kushal Arora, Samir Yitzhak Gadre, Jean Mercat, Sedrick Keh | This paper investigates the optimal integration of image data into the pre-training of Vision-Language Models (VLMs). The central research question is how the timing and proportion of image data during pre-training affect downstream performance on vision-language and text-only tasks.  The methodology involves training a suite of 300 1B parameter models with varying amounts of text-only pre-training, image-text ratios, and fine-tuning strategies. The primary result indicates that introducing visual tokens 80% of the way through pre-training results in a 2% average improvement over introducing visual tokens after complete pre-training. The main implication is that AI practitioners should integrate image data earlier and strategically during VLM pre-training rather than as a separate post-language-model-training stage for optimal performance. |
| Multi-Modal | ProBench: Judging Multimodal Foundation Models on Open-ended
  Multi-domain Expert Tasks (Read more on [arXiv](https://arxiv.org/abs/2503.06885) or [HuggingFace](https://huggingface.co/papers/2503.06885))| Liu Liu, Bei Chen, Haoning Wu, dxli1, HelloKKMe | ProBench is a new benchmark for evaluating multimodal large language models (MLLMs) on expert-level, open-ended tasks across multiple domains. The main objective is to assess the capabilities of MLLMs in handling complex, real-world user queries that require professional expertise and advanced reasoning. The methodology involves collecting 4,000 high-quality samples from professionals across 10 fields and 56 sub-fields, and evaluating 24 models using MLLM-as-a-Judge. Results show that while top open-source models rival proprietary ones, significant challenges remain in visual perception, textual understanding, domain knowledge, and advanced reasoning, achieving 79.9% agreement with human experts using the MLLM-as-a-judge evaluation. These findings highlight the need for further research and development to improve MLLMs' performance in practical, high-value scenarios, implying that practitioners should focus on enhancing these specific capabilities for real-world applications. |
| Multi-Modal | Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by
  Learning Language-Agnostic Speech Representations (Read more on [arXiv](https://arxiv.org/abs/2503.06273) or [HuggingFace](https://huggingface.co/papers/2503.06273))| Yong Man Ro, Stavros Petridis, Chae Won Kim, Minsu Kim, JeongHun0716 | This paper introduces Zero-AVSR, a novel zero-shot audio-visual speech recognition (AVSR) framework that enables speech recognition in target languages without any audio-visual speech data. The main research objective is to expand the language support of AVSR systems beyond the languages seen during training. The key methodology involves learning language-agnostic speech representations using an Audio-Visual Romanizer, then leveraging Large Language Models (LLMs) to convert Roman text into language-specific graphemes.  The proposed Zero-AVSR framework achieves an average WER of 25.2% on the MuAViC dataset, outperforming existing multilingual AVSR methods.  This approach allows AI practitioners to develop AVSR systems with significantly broader language coverage, even for languages with no available audio-visual training data. |
| Multi-Modal | Words or Vision: Do Vision-Language Models Have Blind Faith in Text? (Read more on [arXiv](https://arxiv.org/abs/2503.02199) or [HuggingFace](https://huggingface.co/papers/2503.02199))| Bryan Hooi, Tri Cao, Ailin Deng, ryanchen42 | This paper investigates the modality preference of Vision-Language Models (VLMs) when presented with inconsistent visual and textual information, revealing a phenomenon termed "blind faith in text". The research explores how VLMs handle discrepancies between visual and textual inputs in vision-centric tasks. The methodology involves introducing textual variations (match, corruption, irrelevance) to four vision-centric tasks and evaluating ten VLMs, including proprietary and open-source models. The results show that VLMs disproportionately trust textual data over visual data, with open-source models like Qwen2-VL-7B exhibiting a significant performance drop (around 50% reduction in accuracy) under corrupted text conditions. The findings highlight the need for balanced training and careful modality interaction design in VLMs to improve their robustness and reliability in real-world applications. |
| Natural Language Processing | Detection Avoidance Techniques for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.07595) or [HuggingFace](https://huggingface.co/papers/2503.07595))| Gabi Dreo Rodosek, Joao A. G. Schneider, Florian Steuber, SinclairSchneider | This paper investigates techniques to evade detection systems for text generated by large language models (LLMs). The main research objective is to explore and evaluate methods that allow LLM-generated text to bypass state-of-the-art detection classifiers. The study employs a series of experiments involving parameter temperature adjustment, reinforcement learning, and paraphrasing to modify LLM output. Results show that paraphrasing led to a >90% evasion rate of zero-shot detectors like DetectGPT, while reinforcement learning reduces detection of BERT based systems down to 9% F1 score . AI practitioners should be aware that current LLM detection methods are vulnerable to relatively simple evasion techniques, necessitating the development of more robust detection mechanisms. |
| Multi-Modal | WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image
  Generation (Read more on [arXiv](https://arxiv.org/abs/2503.07265) or [HuggingFace](https://huggingface.co/papers/2503.07265))| Peng Jin, Bin Lin, Mengren Zheng, Munan Ning, Yuwei Niu | This paper introduces WISE, a new benchmark for evaluating the world knowledge and semantic understanding capabilities of Text-to-Image (T2I) models. The research objective is to assess how well T2I models can integrate and apply world knowledge across various domains, moving beyond simple text-image alignment. The methodology involves a benchmark of 1000 prompts across 25 sub-domains in cultural common sense, spatio-temporal reasoning, and natural science, along with a new metric, WiScore, for evaluating knowledge-image alignment. Results from testing 20 models showed that even top-performing models like FLUX.1-dev achieved an overall WiScore of only 0.50, and that dedicated T2I models generally outperform unified multimodal models, indicating significant room for improvement. The implication is that AI practitioners need to develop T2I models with improved mechanisms for incorporating and applying world knowledge in image generation. |
| Computer Vision | Novel Object 6D Pose Estimation with a Single Reference View (Read more on [arXiv](https://arxiv.org/abs/2503.05578) or [HuggingFace](https://huggingface.co/papers/2503.05578))| Hui Yang, Jin Zheng, Kai Zeng, Wei Sun, JianLiu99 | This paper introduces SinRef-6D, a novel framework for estimating the 6D pose of objects from a single reference view, eliminating reliance on CAD models or dense reference views. The research objective is to develop a scalable 6D pose estimation method that can generalize to novel objects using only a single RGB-D reference image, overcoming limitations of existing methods. The proposed method iteratively establishes point-wise alignment in the camera coordinate system using state space models (SSMs) to capture long-range dependencies and spatial information from the single reference view. Experiments on six datasets, including LineMod, demonstrate that SinRef-6D achieves 90.3% on the ADD-0.1d metric, which is on par with CAD-based and dense reference view-based methods. This enables AI practitioners to perform efficient and scalable 6D pose estimation with easier data acquisition, promoting real-world applicability, specifically in robotics and augmented reality. |
| Natural Language Processing | Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2503.04973) or [HuggingFace](https://huggingface.co/papers/2503.04973))| Fabio Petroni, Orion Weller, papotti, giulio98 | This paper introduces a task-aware key-value (KV) cache compression technique for large language models (LLMs) to improve comprehensive knowledge reasoning beyond Retrieval-Augmented Generation (RAG). The main research objective is to design a query-agnostic compression method that preserves efficiency while maintaining competitive performance compared to query-aware compression and RAG. The proposed methodology involves precomputing a compressed cache tailored to a broader task context, defined by a task description and optional few-shot examples, which is then used for all queries within that task domain.  Experiments on LongBench v2 show the approach improves accuracy by up to 7 absolute points over RAG with a 30x compression rate. The main implication is that task-aware KV cache compression offers a scalable and efficient alternative to RAG for tasks requiring synthesis of information from multiple sources, especially in long-context scenarios. |
| Computer Vision | YOLOE: Real-Time Seeing Anything (Read more on [arXiv](https://arxiv.org/abs/2503.07465) or [HuggingFace](https://huggingface.co/papers/2503.07465))| Jungong Han, Zijia Lin, Hui Chen, Lihao Liu, Ao Wang | This paper introduces YOLOE, a unified and efficient object detection and segmentation model that supports diverse open prompt mechanisms (text, visual, and prompt-free) for real-time performance. The main research objective is to develop a single model capable of detecting and segmenting arbitrary objects guided by various prompts, while maintaining high efficiency and accuracy. The key methodologies include Re-parameterizable Region-Text Alignment (RepRTA) for text prompts, Semantic-Activated Visual Prompt Encoder (SAVPE) for visual prompts, and Lazy Region-Prompt Contrast (LRPC) for prompt-free scenarios.  On LVIS, YOLOE-v8-S surpasses YOLO-Worldv2-S by 3.5 AP with 3x less training cost and 1.4x inference speedup. For AI practitioners, YOLOE offers a strong baseline for real-time, open-prompt-driven vision tasks, reducing the computational overhead typically associated with such models. |
| Reinforcement Learning | RePO: ReLU-based Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2503.07426) or [HuggingFace](https://huggingface.co/papers/2503.07426))| Jinyang Gao, Xue Wang, Kexin Huang, Junkang Wu, xiangwang1223 | This paper introduces RePO (ReLU-based Preference Optimization), a streamlined offline preference optimization algorithm for aligning large language models with human preferences. The main research objective is to develop a simpler and more efficient alternative to existing methods like RLHF, DPO, and SimPO, which face challenges in computational cost, stability, and hyperparameter tuning. RePO eliminates the need for the hyperparameter β by using a ReLU-based max-margin loss and retains reference-free reward margins. Empirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO and SimPO across multiple base models, achieving a win rate of up to 57.7% and 73.6% on AlpacaEval2 under different configurations . AI practitioners can use RePO as a simpler and more efficient method for preference optimization, reducing the complexity of hyperparameter tuning while maintaining competitive performance. |
| Multi-Modal | Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal
  LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.06362) or [HuggingFace](https://huggingface.co/papers/2503.06362))| Stavros Petridis, Minsu Kim, Umberto Cappellazzo | This paper introduces Llama-MTSK, a novel Matryoshka-based Multimodal Large Language Model (LLM) for Audio-Visual Speech Recognition (AVSR) that enables adaptive computation based on available resources. The research aims to address the trade-off between computational efficiency and recognition accuracy in AVSR systems integrated with LLMs, which is typically managed by compressing speech representations. Llama-MTSK utilizes Matryoshka Representation Learning to encode audio-visual representations at multiple granularities and introduces LoRA-based Matryoshka strategies for efficient LLM fine-tuning.  The model achieves state-of-the-art results on the LRS2 and LRS3 datasets, for instance, obtaining a Word Error Rate (WER) of 0.9 on the LRS3 dataset using an (A,V) compression rate of (4,2). AI practitioners can leverage this approach to deploy AVSR models that dynamically adjust to computational constraints without significant performance degradation. |
| Multi-Modal | Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent
  Spaces (Read more on [arXiv](https://arxiv.org/abs/2503.05283) or [HuggingFace](https://huggingface.co/papers/2503.05283))| Qixing Huang, Diego Gomez, Luca Moschella, Souhail Hadgi, teelinsan | This paper investigates the alignment between latent spaces of 3D and text encoders. The main research objective is to explore the possibility of a posteriori alignment between representations obtained from uni-modal 3D encoders and text-based feature spaces, and to compare it with vision-text alignment. The key methodology involves combining Canonical Correlation Analysis (CCA) for subspace selection with existing alignment methods like affine transformation and local CKA to project representations onto lower-dimensional subspaces. The results show that subspace projection significantly improves alignment; for example, combining affine transformation with subspace projection increases PointBert-CLIP matching accuracy from 15.8% to 30.8%. This approach provides a baseline for 3D-text cross-modal understanding, offering an alternative to explicit multimodal pre-training. |
| Computer Vision | NeuGrasp: Generalizable Neural Surface Reconstruction with Background
  Priors for Material-Agnostic Object Grasp Detection (Read more on [arXiv](https://arxiv.org/abs/2503.03511) or [HuggingFace](https://huggingface.co/papers/2503.03511))| Xudong Zheng, Wenzhe He, Chao Li, Yinghao Cai, KianYale | NeuGrasp is a novel neural surface reconstruction method for 6-DoF robotic grasping, particularly effective for transparent and specular objects. The research aims to achieve robust, material-agnostic grasp detection from sparse views within a narrow field, without depth-related supervision. NeuGrasp leverages background priors, integrates transformers and global prior volumes to aggregate multi-view features, and enhances foreground object attention through residual feature processing. In simulation experiments, NeuGrasp achieved a success rate (SR) of 86.8% and a declutter rate (DR) of 79.1% on transparent and specular objects in a packed scene using the Top Score execution strategy, outperforming existing methods. The approach offers a real-time, generalizable solution for robotic grasping, reducing the dependency on dense views and extensive per-grasp training, which are common limitations in existing NeRF-based grasping methods. |
