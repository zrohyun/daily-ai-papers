

## Papers for 2025-03-12

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural
  Vision-Language Dataset for Southeast Asia (Read more on [arXiv](https://arxiv.org/abs/2503.07920) or [HuggingFace](https://huggingface.co/papers/2503.07920))| davidanugraha, rifqifarhansyah, tackhwa, holylovenia, samuelcahyawijaya | This paper introduces SEA-VL, a new vision-language dataset designed to improve the representation of Southeast Asian cultures in AI models. The research aims to address the underrepresentation of Southeast Asia in existing vision-language datasets and explores the optimal method for collecting culturally relevant data. The authors investigate three data collection methods: crowdsourcing, web crawling, and synthetic image generation, alongside automated metadata extraction (captioning).  Image crawling achieved approximately 85% cultural relevance, outperforming image generation, and proving more cost and time-efficient than crowdsourcing, though crowdsourcing yielded the highest quality. The findings suggest AI practitioners should prioritize crawling with careful filtering for scalable data collection when dealing with underrepresented regions and cultures, while crowdsourcing can create the best training data. |
| Multi-Modal | LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through
  Two-Stage Rule-Based RL (Read more on [arXiv](https://arxiv.org/abs/2503.07536) or [HuggingFace](https://huggingface.co/papers/2503.07536))| Jie Liu, Zhiyuan You, Miaosen Zhang, Gongrui Zhang, Yingzhe Peng | This paper introduces LMM-R1, a two-stage rule-based reinforcement learning framework to enhance reasoning in 3B-parameter Large Multimodal Models (LMMs). The main objective is to improve the reasoning capabilities of compact LMMs, overcoming limitations due to architectural constraints and modality alignment. LMM-R1 employs a two-stage approach: Foundational Reasoning Enhancement (FRE) using text-only data with rule-based RL, followed by Multimodal Generalization Training (MGT) to extend these capabilities to multimodal domains. Experiments on Qwen2.5-VL-Instruct-3B show that LMM-R1 achieves average improvements of 4.83% and 4.5% over baselines in multimodal and text-only benchmarks, respectively. This demonstrates that text-based reasoning enhancement enables effective multimodal generalization, providing a data-efficient method that bypasses the need for large amounts of high-quality multimodal training data. |
| Multi-Modal | YuE: Scaling Open Foundation Models for Long-Form Music Generation (Read more on [arXiv](https://arxiv.org/abs/2503.08638) or [HuggingFace](https://huggingface.co/papers/2503.08638))| HKUST-Audio, Liam-Liu, dododododo, zhangysk, a43992899 | This paper introduces YuE, a family of open foundation models designed for long-form music generation, particularly the challenging lyrics-to-song task. The research objective is to generate high-quality music up to five minutes long, maintaining lyrical alignment, musical structure, and engaging vocals with appropriate accompaniment. The key methodology involves track-decoupled next-token prediction, structural progressive conditioning, and a multitask, multiphase pre-training recipe. The primary results demonstrate that YuE matches or surpasses some proprietary systems in musicality and vocal agility, achieving a medium vocal range of ~27 semitones, comparable to top closed-source systems. AI practitioners can leverage YuE as an open-source foundation model to advance research and applications in controllable, long-form music generation, potentially extending it to areas beyond music creation. |
| Multi-Modal | UniF^2ace: Fine-grained Face Understanding and Generation
  with Unified Multimodal Models (Read more on [arXiv](https://arxiv.org/abs/2503.08120) or [HuggingFace](https://huggingface.co/papers/2503.08120))| Liya Guo, Linrui Xu, Xuerui Qiu, delinqu, tulvgengenr | This paper introduces UniF^2ace, the first unified multimodal model (UMM) designed for fine-grained face understanding and generation. The research aims to overcome limitations of existing models that focus on coarse facial attributes and lack generation capabilities, by creating a model capable of both understanding and generating detailed facial features.  The methodology involves training UniF^2ace on a new dataset (UniF^2ace-130K) using a two-level mixture-of-experts architecture and two mutually beneficial diffusion techniques, establishing a theoretical connection between discrete diffusion score matching and masked generative models.  UniF^2ace outperforms existing UMMs and generative models, achieving a VLM-score of 88.049 and FID of 66.005 on the generation task on the UniF^2ace-130K dataset. The main implication for AI practitioners is the potential for improved performance in applications requiring detailed facial analysis and synthesis, such as identity verification, emotion recognition and virtual avatar creation, via the presented unified architecture, specialized dataset, and training approach. |
| Computer Vision | MagicInfinite: Generating Infinite Talking Videos with Your Words and
  Voice (Read more on [arXiv](https://arxiv.org/abs/2503.05978) or [HuggingFace](https://huggingface.co/papers/2503.05978))| Jiantong Zhao, Xuancheng Yang, Shitong Shao, Hongwei Yi, Owen777 | MagicInfinite is a novel diffusion Transformer (DiT) framework for generating high-fidelity, infinite-length talking head videos from a single portrait image, controlled by audio and text inputs. The main objective is to overcome limitations in existing portrait animation methods, enabling diverse character styles, varied poses, and multi-character scenes with precise speaker designation. The key methodology involves 3D full-attention mechanisms with a sliding window denoising strategy, a two-stage curriculum learning scheme for multi-modal control, and region-specific masks with adaptive loss functions.  The system achieves a 20x inference speed boost over the base model, generating a 10-second 540x540p video in 10 seconds on 8 H100 GPUs, and outperforms state-of-the-art methods like SadTalker and Hallo3 in user studies. AI practitioners can utilize this framework for generating high quality and temporally consistent talking head animations with detailed control over lip sync, identity, and expressive dynamics. |
| Multi-Modal | SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by
  Imitating Human Annotator Trajectories (Read more on [arXiv](https://arxiv.org/abs/2503.08625) or [HuggingFace](https://huggingface.co/papers/2503.08625))| Qingpei Guo, Chunluan Zhou, Hao Chen, Yuzhuo Tian, Z-MU-Z | This paper introduces SegAgent, a novel framework for fine-grained pixel-level understanding in Multi-modal Large Language Models (MLLMs) by imitating human annotation trajectories in interactive segmentation. The research objective is to assess and improve MLLMs' pixel-level comprehension by modeling segmentation as a multi-step Markov Decision Process, where models iteratively generate text-based coordinates. The key methodology involves training MLLMs on human-like annotation trajectories and adapting policy improvement methods like StaR+ and process reward modeling (PRM) guided tree search. Primary results demonstrate SegAgent achieves competitive performance on Referring Expression Segmentation (RES) datasets, with SegAgent-LLaVA+SAM achieving 79.20 mIoU on the refCOCO validation set, comparable to state-of-the-art methods. The main implication is that this approach provides a new protocol for evaluating and enhancing MLLMs' fine-grained visual capabilities, laying the foundation for future research in vision-centered, multi-step decision-making agents. |
| Multi-Modal | Seedream 2.0: A Native Chinese-English Bilingual Image Generation
  Foundation Model (Read more on [arXiv](https://arxiv.org/abs/2503.07703) or [HuggingFace](https://huggingface.co/papers/2503.07703))| Liang Li, Fanshi Li, Xiaoxia Hou, Lixue Gong, wujie10 | Seedream 2.0 is a native Chinese-English bilingual image generation foundation model that excels in generating images from text prompts in both languages, including text rendering. The main objective is to address limitations of existing models, such as model bias, limited text rendering, and insufficient understanding of Chinese cultural nuances. The key methodology involves a powerful data system, a bilingual large language model (LLM) as a text encoder, Glyph-Aligned ByT5 for character-level text rendering, Scaled ROPE for untrained resolutions, and multi-phase post-training optimizations (SFT, RLHF). The model achieves state-of-the-art performance across multiple aspects, with an overall ELO score of 1117 in English evaluations, surpassing models like Midjourney v6.1 and Ideogram 2.0. AI practitioners can leverage this model for high-fidelity, culturally-aware image generation, especially for scenarios requiring bilingual capabilities and text rendering. |
| Natural Language Processing | Gemini Embedding: Generalizable Embeddings from Gemini (Read more on [arXiv](https://arxiv.org/abs/2503.07891) or [HuggingFace](https://huggingface.co/papers/2503.07891))| Madhuri Shanbhogue, Daniel Cer, Sahil Dua, Feiyang Chen, Jinhyuk Lee | This paper introduces Gemini Embedding, a state-of-the-art text embedding model derived from Google's Gemini large language model, designed for various downstream tasks. The research objective is to create highly generalizable text embeddings for multiple languages and textual modalities, surpassing existing models in embedding quality. The methodology involves initializing the embedding model from Gemini, curating a high-quality training dataset using Gemini for data filtering and synthetic data generation, and employing a contrastive learning objective with a two-stage training pipeline (pre-finetuning and finetuning) with model souping. Evaluated on the Massive Multilingual Text Embedding Benchmark (MMTEB), Gemini Embedding achieves a mean task score of 68.32, significantly outperforming prior state-of-the-art models. AI practitioners can leverage Gemini Embedding for improved performance across a broad range of text-based applications, including classification, similarity, clustering, ranking, and retrieval, especially in multilingual contexts. |
| Computer Vision | Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled
  Sampling (Read more on [arXiv](https://arxiv.org/abs/2503.08605) or [HuggingFace](https://huggingface.co/papers/2503.08605))| Jinwoo Shin, Joon-Young Lee, Jui-Hsien Wang, Seoung Wug Oh, Subin Kim | This paper introduces SynCoS, a tuning-free inference framework for generating long, multi-event videos from text prompts using diffusion models. The main research objective is to extend text-to-video diffusion models to generate coherent long videos with multiple events while maintaining both local smoothness and global consistency. SynCoS achieves this by synchronizing denoising paths across the entire video through coupled sampling, combining reverse and optimization-based sampling with a grounded timestep and fixed baseline noise. Experimental results show that SynCoS significantly improves multi-event long video generation, achieving a global prompt fidelity of 0.341 and a local prompt fidelity of 0.354 on CogVideoX, outperforming previous approaches. This framework allows AI practitioners to generate high-quality, temporally consistent long videos with dynamic content without requiring additional model training. |
| Natural Language Processing | Implicit Reasoning in Transformers is Reasoning through Shortcuts (Read more on [arXiv](https://arxiv.org/abs/2503.07604) or [HuggingFace](https://huggingface.co/papers/2503.07604))| Deqing Yang, Siyu Yuan, Tianhe Lin, hsaest | This research paper investigates how Transformer-based language models perform implicit multi-step reasoning, particularly in mathematical problems. The main research objective is to understand why implicit reasoning in language models falls short of explicit reasoning (e.g., Chain-of-Thought) and to uncover the underlying mechanisms. The authors train GPT-2 models from scratch on a synthetic multi-step mathematical reasoning dataset and use activation patching to analyze internal reasoning processes. Results indicate models can perform step-by-step reasoning and achieve high accuracy (100% on in-distribution, 99% on one additional step, and nearly 90% requiring two additional steps, in a fixed premise order setup), but this ability is compromised when the premise order is unfixed, demonstrating reliance on shortcut learning rather than generalized reasoning. The main implication is that language models, even large ones, primarily use shortcuts for implicit reasoning, suggesting a need for new approaches to encourage more robust, generalized reasoning capabilities. |
| Computer Vision | LightGen: Efficient Image Generation through Knowledge Distillation and
  Direct Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2503.08619) or [HuggingFace](https://huggingface.co/papers/2503.08619))| Yexin Liu, Harold Haodong Chen, Haoze Zheng, Yajing Bai, Xianfeng Wu | LightGen is a novel training paradigm for efficient text-to-image generation that leverages knowledge distillation and direct preference optimization to reduce computational demands. The research objective is to develop an image generation model that achieves comparable performance to state-of-the-art (SOTA) models while significantly reducing resource requirements. LightGen distills knowledge from SOTA text-to-image models into a compact Masked Autoregressive (MAR) architecture and uses a synthetic dataset of 2M high-quality images. On the GenEval benchmark at 512x512 resolution, LightGen achieves an overall score of 0.62, nearly matching SOTA performance. This suggests AI practitioners can achieve high-quality image generation with substantially reduced dataset size, model parameters, and GPU hours. |
| Multi-Modal | OmniMamba: Efficient and Unified Multimodal Understanding and Generation
  via State Space Models (Read more on [arXiv](https://arxiv.org/abs/2503.08686) or [HuggingFace](https://huggingface.co/papers/2503.08686))| Xinggang Wang, Wenyu Liu, Qian Zhang, Bencheng Liao, Jialv Zou | OmniMamba is a novel, linear-architecture-based multimodal generation model that efficiently generates both text and images using a unified next-token prediction paradigm. The research aims to develop a unified multimodal model that achieves both training and inference efficiency, overcoming limitations of existing models that suffer from quadratic computational complexity and require massive training data. Key innovations include decoupled vocabularies, task-specific LoRA for parameter-efficient adaptation, and a decoupled two-stage training strategy. OmniMamba achieves competitive performance with JanusFlow and surpasses Show-o on multiple multimodal benchmarks, demonstrating up to a 119.2x speedup and 63% GPU memory reduction for long-sequence generation. The model's efficiency makes advanced multimodal learning more accessible, reducing the computational barriers to unified multimodal generation. |
| Reinforcement Learning | Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2503.07572) or [HuggingFace](https://huggingface.co/papers/2503.07572))| Edward Emanuel Beeching, Lewis Tunstall, Amrith Setlur, Matthew Y. R. Yang, CohenQu | This paper introduces Meta Reinforcement Fine-Tuning (MRT), a new method for optimizing test-time compute in large language models (LLMs). The research formalizes optimizing test-time compute as a meta-reinforcement learning problem, aiming to minimize cumulative regret over the output token stream. MRT trains LLMs by maximizing a dense reward bonus based on "progress," quantified by the change in likelihood of eventual success, in addition to sparse outcome rewards.  Empirically, MRT leads to a 2-3x relative gain in performance and a 1.5x gain in token efficiency for math reasoning compared to outcome-reward RL. AI practitioners can use MRT to train LLMs that better balance exploration and exploitation during test-time, leading to improvements in both performance and efficiency. |
| Multi-Modal | Video Action Differencing (Read more on [arXiv](https://arxiv.org/abs/2503.07860) or [HuggingFace](https://huggingface.co/papers/2503.07860))| Alejandro Lozano, Anita Rau, Yuhui Zhang, nicholswang, jmhb | This paper introduces Video Action Differencing (VidDiff), a novel task focused on identifying subtle differences between videos of the same action, along with a benchmark dataset called VidDiffBench. The research objective is to determine how two individuals differ when performing the same action, and to build models that can detect such fine-grained differences. The proposed VidDiff method is an agentic workflow using large language models (LLMs), contrastive language-image models (CLIP), and vision-language models (VLMs) to generate difference proposals, localize relevant frames, and perform frame differencing. The primary results show that state-of-the-art large multimodal models struggle with this task, achieving an average accuracy of only 57.7% (Gemini-1.5 Pro) in the closed-set setting compared to the proposed method's 56.3%, and lower performance in the open-set settings. The main implication is that there is significant room for improvement in fine-grained video understanding, which has practical applications in coaching, skill acquisition, and automated performance feedback. |
| Natural Language Processing | BiasEdit: Debiasing Stereotyped Language Models via Model Editing (Read more on [arXiv](https://arxiv.org/abs/2503.08588) or [HuggingFace](https://huggingface.co/papers/2503.08588))| Julian McAuley, Ningyu Zhang, Wei Xu, XinXuNLPer | This paper introduces BIASEDIT, a novel model editing method for debiasing stereotyped language models. The research objective is to efficiently eliminate stereotypical biases from language models without significantly impacting their general language modeling capabilities. BIASEDIT employs lightweight editor networks to generate parameter updates, guided by a debiasing loss and a retention loss, to perform localized edits on partial parameters. Experiments on StereoSet and Crows-Pairs datasets show BIASEDIT achieves state-of-the-art debiasing performance, reducing Stereotype Score (SS) to below 57% and often near the ideal 50%, while having minimal impact on language modeling scores (LMS). The results suggest that AI practitioners can use model editing for effective and efficient debiasing of language models, offering an alternative to more resource-intensive methods like retraining. |
| Multi-Modal | QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long
  Video Comprehension (Read more on [arXiv](https://arxiv.org/abs/2503.08689) or [HuggingFace](https://huggingface.co/papers/2503.08689))| Shukang Yin, Weizhong Huang, Xiawu Zheng, Wang Chen, Yongdong Luo | This paper introduces QuoTA, a training-free modular approach for enhancing long video understanding in large video-language models (LVLMs). The main objective is to improve visual token assignment by aligning it with task-specific requirements, addressing the limitations of existing methods that rely on post-hoc attention-based token pruning. QuoTA achieves this by implementing query-oriented frame-level importance assessment and leveraging Chain-of-Thoughts reasoning to decouple the query for more precise LVLM-based frame importance scoring. When implemented with LLaVA-Video-7B, QuoTA yields an average performance improvement of 3.2% across six benchmarks while operating within an identical visual token budget as the baseline. For AI practitioners, QuoTA offers a plug-and-play solution to improve the efficiency and accuracy of LVLMs in long video comprehension tasks, mitigating visual redundancy. |
| Computer Vision | "Principal Components" Enable A New Language of Images (Read more on [arXiv](https://arxiv.org/abs/2503.08685) or [HuggingFace](https://huggingface.co/papers/2503.08685))| Xiaojuan Qi, Jiankang Deng, Ismail Elezi, tennant, xwen99 | This paper introduces SEMANTICIST, a novel visual tokenization framework that embeds a provable PCA-like structure into the latent token space for image representation. The main research objective is to develop a tokenizer that reduces redundancy while effectively decoupling semantic information from less important low-level details, unlike existing visual tokenizers that prioritize reconstruction fidelity. The key methodology involves using a dynamic nested classifier-free guidance strategy during training to induce an orderliness bias in 1D tokens and leveraging a diffusion decoder. The experiments demonstrate that SEMANTICIST achieves state-of-the-art reconstruction FID scores on the ImageNet validation set, surpassing the previous SOTA tokenizer by almost 10% in FID.  For AI practitioners, this provides an image tokenizer with improved interpretability, better reconstruction, and efficient autoregressive generative modeling. |
| Computer Vision | RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow
  Trajectories (Read more on [arXiv](https://arxiv.org/abs/2503.07699) or [HuggingFace](https://huggingface.co/papers/2503.07699))| Xing Wang, Yuxi Ren, Yuhong Yang, Xin Xia, Huiyang Shao | RayFlow is a novel diffusion framework designed to accelerate image generation while maintaining high sample quality and controllability. The main research objective is to address the slow generation speed of diffusion models without compromising sample diversity, stability, or introducing training complexities. The key methodology involves guiding each sample along a unique path towards an instance-specific target distribution using a technique called Time Sampler for crucial timestep selection during training. Primary results shows RayFlow achieves a FID score of 4.69 in 8-step generation on the COCO-5k dataset, surpassing existing state-of-the-art methods on multiple quantitative metrics. AI practitioners can leverage RayFlow to achieve faster and more controllable image generation with diffusion models with improved training efficiency. |
| Natural Language Processing | Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents (Read more on [arXiv](https://arxiv.org/abs/2503.08684) or [HuggingFace](https://huggingface.co/papers/2503.08684))| Xiao Zhang, Liang Pang, Haiyuan Zhao, Sunhao Dai, Haoyu Wang | This paper investigates source bias in Pretrained Language Model (PLM)-based retrieval models, demonstrating that they favor documents with low perplexity, often those generated by large language models (LLMs). The research question explores the underlying causes of source bias, and how document perplexity causally affects relevance scores.  The methodology involves causal graph analysis, intervention experiments (manipulating LLM sampling temperature), and a two-stage least squares regression to quantify the causal effect. Experiments across three domains show a consistently negative correlation between perplexity and relevance scores, with a causal effect statistically significant (p-value < 0.05) in most tests; a Causal Diagnosis and Correction (CDC) inference-time debiasing method mitigates the bias, reducing average performance by less than two points. The implication is that AI practitioners using PLM-based retrievers need to be aware of and address the identified perplexity bias to ensure fair ranking between human-written and LLM-generated content. |
| Multi-Modal | ^RFLAV: Rolling Flow matching for infinite Audio Video generation (Read more on [arXiv](https://arxiv.org/abs/2503.08307) or [HuggingFace](https://huggingface.co/papers/2503.08307))| Claudio Ferrari, Tomaso Fontanini, Filippo Botti, Giuseppe Gabriele Tarollo, MaverickAlex | This paper introduces RFLAV, a novel transformer-based architecture for infinite joint audio-video (AV) generation. The research addresses the challenges of generating high-quality AV content with seamless multimodal synchronization, temporal coherence, and limitless duration. The key methodology involves a rolling rectified-flow model with a lightweight temporal fusion module for cross-modality interaction, enabling frame-by-frame synthesis without temporal compression. Experimental results demonstrate that RFLAV outperforms state-of-the-art models on the AIST++ dataset, achieving an FVD of 38.36 with 200 denoising steps. The main implication is that AI practitioners can leverage RFLAV for generating long-duration, synchronized, and high-quality audio-video content, overcoming limitations of previous approaches. |
| Other | Benchmarking AI Models in Software Engineering: A Review, Search Tool,
  and Enhancement Protocol (Read more on [arXiv](https://arxiv.org/abs/2503.05860) or [HuggingFace](https://huggingface.co/papers/2503.05860))| Maliheh Izadi, philippedebekker, RohamKoohestani | This paper presents a comprehensive review of AI4SE benchmarks, introduces a semantic search tool (BenchScout) for benchmark selection, and proposes a framework (BenchFrame) for enhancing benchmark quality. The main research objective is to address challenges in AI4SE benchmarking, including knowledge fragmentation, benchmark selection difficulty, lack of standardized approaches, and inherent benchmark limitations. The methodology involves a systematic literature review, the development of a semantic search tool using clustering and dimensionality reduction, a user study for evaluation, and the proposal of a benchmark enhancement framework with a case study applying it to HumanEval. A key result is that on the enhanced HumanEvalNext benchmark, models showed a pass@1 score reduction of 31.22% and 19.94% compared to HumanEval and HumanEvalPlus, respectively. The main implication is that AI practitioners need continuously refined benchmarks and tools like BenchScout to better guide research and provide more realistic assessments of model performance. |
| Machine Learning | Evaluating Intelligence via Trial and Error (Read more on [arXiv](https://arxiv.org/abs/2502.18858) or [HuggingFace](https://huggingface.co/papers/2502.18858))| Bo Zhang, Yiqun Liu, Jiayu Li, Jiahao Zhao, jingtao | This paper introduces "Survival Game," a novel framework for evaluating intelligence based on the number of failed attempts in a trial-and-error process, drawing inspiration from Natural Selection. The main research objective is to propose and validate an objective, task-agnostic method for assessing intelligence levels of artificial intelligence systems.  The key methodology involves quantifying intelligence as the distribution of failure counts before a correct solution is found, categorizing intelligence into Limited, Capable, and Autonomous levels based on the convergence of the expectation and variance of these counts.  Primary results show that while AI systems achieve the Autonomous Level in simple tasks like MNIST, they are mostly at the Limited Level in complex tasks, with models requiring an estimated 10^26 parameters to potentially reach Autonomous level in general language tasks. The main implication is that current AI systems, while proficient in certain narrow domains, lack a deep understanding of complex human task mechanisms and require substantial advancements to achieve robust, autonomous performance. |
| Multi-Modal | Referring to Any Person (Read more on [arXiv](https://arxiv.org/abs/2503.08507) or [HuggingFace](https://huggingface.co/papers/2503.08507))| Yuda Xiong, Tianhe Ren, Zhaoyang Zeng, Lin Wu, Qing Jiang | This paper introduces "Referring to Any Person", a new task and model for detecting individuals in an image that match a given natural language description. The research objective is to overcome limitations in existing referring expression comprehension models and benchmarks, particularly their focus on one-to-one referring and difficulty in real-world scenarios with multiple potential referents. The proposed methodology involves creating a new dataset, HumanRef, with multi-instance referring expressions, and developing a detection-oriented multimodal large language model, RexSeek, trained through a multi-stage process.  Experiments show that RexSeek achieves a DensityF1 score of 82.3 on the HumanRef benchmark, significantly outperforming existing models. AI practitioners can leverage this work to build more robust systems capable of precise, multi-instance human referring in real-world applications. |
| Computer Vision | AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion
  Models (Read more on [arXiv](https://arxiv.org/abs/2503.08417) or [HuggingFace](https://huggingface.co/papers/2503.08417))| Junyong Noh, Chaelin Kim, Seokhyeon Hong, kwanY | This paper introduces AnyMoLe, a novel method for 3D character motion in-betweening that leverages video diffusion models without requiring character-specific datasets. The main objective is to generate smooth and realistic in-between motion for arbitrary 3D characters without external training data. The method employs a two-stage frame generation process, inference-stage context adaptation (ICAdapt) for bridging the domain gap between real and rendered scenes, and a motion-video mimicking optimization technique for arbitrary joint structures. AnyMoLe outperforms existing methods on several metrics; for example a Hierarchical L2 Quaternion (HL2Q) distance of 0.0015 and 0.0019 for humanoid and non-humanoid characters, respectively. This approach allows AI practitioners to generate in-between motions for diverse characters, including those difficult to capture with motion capture systems or without any animation data. |
| Machine Learning | AI-native Memory 2.0: Second Me (Read more on [arXiv](https://arxiv.org/abs/2503.08102) or [HuggingFace](https://huggingface.co/papers/2503.08102))| Jingbo Shang, Felix Tao, Tao Gao, Xiang Ying, Jiale Wei | This paper introduces SECOND ME, an AI-native memory offload system that acts as an intelligent, persistent intermediary for user interactions, leveraging large language models (LLMs) for memory management. The main research objective is to redefine memory management by creating a system that retains, organizes, and dynamically utilizes user-specific knowledge, reducing cognitive load and interaction friction. The key methodology involves LLM-based memory parameterization, enabling structured organization, contextual reasoning, and adaptive knowledge retrieval, using techniques like supervised fine-tuning (SFT) and direct preference optimization (DPO). Primary results showed that incorporating diverse data sources with strong Chain-of-Thought (CoT) style normalization yielded the best performance, with a Memory (Self) score of 0.91 and a Memory (Third-Party) score of 0.71, also human evaluation indicated higher practical effectiveness than metrics alone. The main implication for AI practitioners is the potential to significantly enhance user interactions by integrating persistent, contextually aware, and self-optimizing memory systems, paving the way for more advanced personal AI agents. |
| Machine Learning | Mixture of Experts Made Intrinsically Interpretable (Read more on [arXiv](https://arxiv.org/abs/2503.07639) or [HuggingFace](https://huggingface.co/papers/2503.07639))| Puneet K. Dokania, Christian Schroeder de Witt, Ashkan Khakzar, Constantin Venhoff, Xingyi Yang | This paper introduces MoE-X, a Mixture-of-Experts (MoE) language model designed for intrinsic interpretability in large language models. The research aims to address the polysemanticity of neurons in large language models and improve interpretability without relying on post-hoc methods. The methodology involves rewriting the MoE layer as an equivalent sparse, large MLP, enforcing sparse activation within each expert using ReLU, and redesigning the routing mechanism to prioritize experts with the highest activation sparsity. MoE-X achieves a perplexity better than GPT-2 and a reconstruction score of 0.840 on chess tasks, surpassing sparse autoencoder-based approaches. The implication is that designing for interpretability from the outset, rather than as an afterthought, produces large language models which maintain performance, but are also directly inspectable, without further costly methods. |
| Computer Vision | NullFace: Training-Free Localized Face Anonymization (Read more on [arXiv](https://arxiv.org/abs/2503.08478) or [HuggingFace](https://huggingface.co/papers/2503.08478))| Nicu Sebe, Terence Sim, Tuomas Varanka, hkung | This paper introduces NullFace, a novel training-free method for localized face anonymization that preserves non-identity-related attributes. The main objective is to develop a face anonymization technique that obscures identity while maintaining utility and allowing users to control which facial regions are anonymized. The method utilizes a pre-trained text-to-image diffusion model, inverting the input image to recover initial noise, and then denoising it through an identity-conditioned diffusion process with modified identity embeddings.  The method achieves a re-identification rate of 0.21% on CelebA-HQ and 0.34% on FFHQ, demonstrating strong anonymization performance. AI practitioners can utilize this method for privacy-preserving applications, especially where control over specific facial feature anonymization is required without extra training overhead. |
| Natural Language Processing | Beyond Decoder-only: Large Language Models Can be Good Encoders for
  Machine Translation (Read more on [arXiv](https://arxiv.org/abs/2503.06594) or [HuggingFace](https://huggingface.co/papers/2503.06594))| Qinghong Zhang, Bei Li, Yongyu Mu, Tong Zheng, luoyingfeng | This paper explores enhancing neural machine translation (NMT) by integrating large language models (LLMs) as encoders, coupled with standard NMT decoders. The main research objective is to investigate whether LLMs can be effectively used as encoders in NMT systems to improve translation quality and efficiency, and improve generalizability. The key methodology, called LaMaTE, involves using a pre-trained LLM for encoding, an adaptor for dimensionality alignment, and a two-stage training process focusing on adaptor/decoder pre-training and subsequent fine-tuning. Results demonstrate that LaMaTE achieves comparable or better performance than baseline systems, achieving speedups ranging from 2.4x to 6.5x, and reduces the memory footprint of the KV cache by 75%. The main implication is that AI practitioners can develop more efficient and generalizable machine translation systems by leveraging the strengths of LLMs for encoding and combining them with optimized, smaller decoders. |
| Multi-Modal | VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large
  Vision-Language Models in Fact-Seeking Question Answering (Read more on [arXiv](https://arxiv.org/abs/2503.06492) or [HuggingFace](https://huggingface.co/papers/2503.06492))| Lixin Liu, Shasha Guo, Xiaodong Chen, Yihan Zhao, WYLing | This paper introduces VisualSimpleQA, a new benchmark for evaluating large vision-language models (LVLMs) in fact-seeking question answering. The research objective is to enable decoupled evaluation of LVLMs' visual and linguistic modules and to introduce well-defined difficulty criteria for sample creation. The methodology involves creating multimodal questions with corresponding text-only versions, rationales, and difficulty metrics based on visual and linguistic factors.  Experiments on 15 LVLMs show that even state-of-the-art models like GPT-4o achieve only around 60% correctness on VisualSimpleQA and 30% on the more challenging VisualSimpleQA-hard subset, indicating significant room for improvement.  This benchmark provides AI practitioners a tool for detailed, modality-specific evaluation of LVLMs, highlighting areas for improving factuality in multimodal question answering. |
