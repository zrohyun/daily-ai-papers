

## Papers for 2025-03-07

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | START: Self-taught Reasoner with Tools (Read more on [arXiv](https://arxiv.org/abs/2503.04625) or [HuggingFace](https://huggingface.co/papers/2503.04625))| BeichenZhang, jx-yang, Zhenru, mingfengxue, ChengpengLi | START (Self-Taught Reasoner with Tools) is a novel tool-integrated long Chain-of-Thought reasoning language model that enhances reasoning capabilities by leveraging external tools. The main research objective is to synergistically combine long Chain-of-Thought with Tool-Integrated Reasoning (specifically, Python interpreter invocation) to improve performance on complex reasoning tasks. The key methodology involves a self-learning framework with two main techniques: "Hint-infer", which uses strategically inserted hints to prompt tool use, and "Hint Rejection Sampling Fine-Tuning", which refines reasoning trajectories with tool invocation. START achieves an accuracy of 63.6% on the PhD-level science QA benchmark (GPQA), significantly outperforming the base model and achieving comparable results to state-of-the-art open-source models. This implies that AI practitioners can significantly improve the complex reasoning capabilities of language models by integrating external tools and employing self-learning techniques during both inference and fine-tuning. |
| Natural Language Processing | LLM as a Broken Telephone: Iterative Generation Distorts Information (Read more on [arXiv](https://arxiv.org/abs/2502.20258) or [HuggingFace](https://huggingface.co/papers/2502.20258))| Michalis Vazirgiannis, guokan-shang, mgeng, amr-mohamed | This paper investigates whether Large Language Models (LLMs) distort information through iterative generation, analogous to the 'broken telephone' game. The main research question is how repeated processing of LLM outputs affects information fidelity. The methodology involves simulating an LLM-based 'telephone game' using translation tasks, where an English document is translated into different languages and back to English over multiple iterations. The primary results show that information distortion accumulates over time, with FActScore decreasing; for instance, the EN-TH gradient for FActScore in Booksum dataset using Llama is -0.026. The main implication is that AI practitioners should be cautious of the long-term effects of LLM-mediated information propagation and implement strategies, like temperature control, to ensure the reliability of generated content in iterative workflows. |
| Multi-Modal | EgoLife: Towards Egocentric Life Assistant (Read more on [arXiv](https://arxiv.org/abs/2503.03803) or [HuggingFace](https://huggingface.co/papers/2503.03803))| Zzitang, Alarak, fesvhtr, THUdyh, Jingkang | This paper introduces EgoLife, a project focused on developing an egocentric life assistant using AI-powered wearable glasses, alongside a comprehensive dataset and benchmark for egocentric AI. The main research objective is to create an AI assistant capable of addressing practical daily life questions, recalling past events, monitoring habits, and offering personalized recommendations through multimodal data analysis. The key methodology involves collecting a 300-hour egocentric, interpersonal, multi-view, and multimodal dataset, and introducing EgoButler, a system combining EgoGPT (an omni-modal model) and EgoRAG (a retrieval-based component) for long-context question answering. Primary results show that EgoGPT (EgoIT-99K+D1) achieves an average score of 36.0 on the EgoLifeQA benchmark and EgoGPT+EgoRAG achieves 35.4 for questions over 24 hours, demonstrating the effectiveness and limitations of existing approaches. The main implication is that this work provides a resource and sets a performance benchmark that will encourage further exploration of long-term, multimodal understanding in egocentric AI assistants. |
| Natural Language Processing | LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation (Read more on [arXiv](https://arxiv.org/abs/2503.02972) or [HuggingFace](https://huggingface.co/papers/2503.02972))| Vlad Neacs, Lingyi Yang, Simi Hellsten, Karolina Korgul, Jude Khouja | This paper introduces LINGOLY-TOO, a new benchmark for evaluating the linguistic reasoning capabilities of Large Language Models (LLMs) while mitigating the effects of data exposure. The main research objective is to assess how well LLMs can reason about linguistic problems when memorization is minimized through orthographic obfuscation and templatization. The key methodology involves creating numerous question variations by dynamically obfuscating the writing systems of real languages in linguistic reasoning problems, preserving the reasoning steps but reducing the chance of overlap with pre-training data. The primary result shows that frontier models, such as Claude 3.7 Sonnet, struggle with advanced reasoning, achieving a mean exact match score of only 0.43 on obfuscated problems. The main implication is that prior data exposure significantly contributes to overestimating LLM reasoning capabilities, and practitioners should use techniques like obfuscation to obtain more accurate evaluations. |
| Natural Language Processing | HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization (Read more on [arXiv](https://arxiv.org/abs/2503.04598) or [HuggingFace](https://huggingface.co/papers/2503.04598))| Ya Wang, Breeze0417, LLIXQ, Taoer, BryceZhuo | This paper introduces HybridNorm, a novel normalization strategy for training transformer models, particularly large language models (LLMs). The research aims to address the trade-off between training stability and final model performance often encountered with standard normalization techniques like Pre-Norm and Post-Norm. HybridNorm combines QKV normalization within the attention mechanism and Post-Norm in the feed-forward network of each transformer block.  Experiments on 1.2B parameter dense models show that HybridNorm* achieves an average downstream task score of 64.15, outperforming Pre-Norm (62.99). The main implication is that HybridNorm offers a more stable and effective approach for training deep transformer models, leading to improved performance in large-scale applications. |
| Reinforcement Learning | PokéChamp: an Expert-level Minimax Language Agent (Read more on [arXiv](https://arxiv.org/abs/2503.04094) or [HuggingFace](https://huggingface.co/papers/2503.04094))| Andy Luu Nguyen, chijin, milkkarten | This paper introduces PokéChamp, a minimax language agent for Pokémon battles that achieves expert-level performance. The research objective is to develop an agent capable of strategic action proposal, accurate opponent modeling, and internal reflection on planned game trajectories within the complex, partially observable environment of Pokémon battles. PokéChamp integrates Large Language Models (LLMs) into the minimax tree search algorithm, replacing player action sampling, opponent modeling, and value function estimation modules with LLM-driven components. When powered by GPT-4o, the agent achieves a 76% win rate against the best existing LLM-based bot and an 84% win rate against the strongest rule-based bot. AI practitioners can leverage this framework to integrate LLMs with game-theoretic algorithms, addressing complex multiagent problems without requiring additional LLM training. |
| Natural Language Processing | FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion (Read more on [arXiv](https://arxiv.org/abs/2503.04222) or [HuggingFace](https://huggingface.co/papers/2503.04222))| passerqxj, OnewayLab, GGLS, Wanfq, AALF | FuseChat-3.0 is a suite of large language models (LLMs) developed by integrating the strengths of heterogeneous source LLMs into more compact target LLMs. The main objective is to enhance the performance, robustness, and generalization of smaller target LLMs by leveraging the diverse capabilities of larger, more powerful source models. The methodology involves a two-stage training pipeline: (1) supervised fine-tuning (SFT) to align target and source model distributions, and (2) Direct Preference Optimization (DPO) to apply preferences from multiple source LLMs. When using Llama-3.1-8B-Instruct as the target model, the fusion approach achieves an average improvement of 6.8 points across 14 benchmarks. This approach allows AI practitioners to create more efficient and powerful LLMs by combining the strengths of multiple models without requiring significant architectural changes or computational resources. |
| Multi-Modal | Token-Efficient Long Video Understanding for Multimodal LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.04130) or [HuggingFace](https://huggingface.co/papers/2503.04130))| zhiqilinv, MuyangLI, zhijianliu, xiuyul, jdps | This paper introduces STORM, a novel architecture for video-based multimodal large language models (Video-LLMs) designed for efficient long video understanding. The main research objective is to address the limitations of existing Video-LLMs in capturing dynamic patterns and efficiently handling long videos due to their lack of explicit temporal modeling. STORM incorporates a Mamba-based temporal encoder between the image encoder and the LLM to integrate temporal information into visual tokens, enabling token reduction strategies such as test-time sampling and training-based pooling. STORM achieves state-of-the-art results on various long video understanding benchmarks, including a 71.3% accuracy on MVBench, while reducing computation costs by up to 8x. AI practitioners can leverage STORM to develop more efficient and robust video understanding models, especially for long-form video content. |
| Multi-Modal | The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation (Read more on [arXiv](https://arxiv.org/abs/2503.04606) or [HuggingFace](https://huggingface.co/papers/2503.04606))| Xu Tan, Kai Shen, Aoxiong Yin, JunchengLi, ustcscallion | This paper introduces LanDiff, a hybrid framework for text-to-video (T2V) generation that combines the strengths of autoregressive language models and diffusion models. The main research objective is to overcome the limitations of existing T2V paradigms, specifically language models' struggles with visual quality and diffusion models' lack of semantic understanding. LanDiff employs a coarse-to-fine generation approach, using a semantic tokenizer to compress visual features, a language model to generate semantic tokens, and a streaming diffusion model to refine these into high-fidelity videos. The model achieves a score of 85.43 on the VBench T2V benchmark, surpassing state-of-the-art open-source and commercial models. AI practitioners can leverage LanDiff's architecture for improved video generation with better semantic coherence, visual quality and longer video outputs. |
| Natural Language Processing | IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval (Read more on [arXiv](https://arxiv.org/abs/2503.04644) or [HuggingFace](https://huggingface.co/papers/2503.04644))| Mingsheng Shang, yilunzhao, guo9, songtingyu | This paper introduces IFIR, a comprehensive benchmark for evaluating instruction-following information retrieval (IR) in expert domains. The main objective is to assess how well current IR systems can handle complex, domain-specific instructions, a capability underexplored in existing benchmarks. The methodology involves creating a dataset of 2,426 high-quality examples across finance, law, healthcare, and scientific literature, with instructions at varying complexity levels, and proposing a novel LLM-based evaluation metric, INSTFOL. Experiments on 15 information retrievers show that current models, including instruction-tuned ones, struggle with complex instructions, with performance declining as instruction complexity increases; however, LLM-based retrievers demonstrate more robust performance (e.g. GritLM-7B exhibits the highest INSTFOL scores). The implication is that existing training methods are insufficient for handling long, expert-domain instructions and that specialized, domain-specific approaches are needed for future IR system development. |
| Machine Learning | Identifying Sensitive Weights via Post-quantization Integral (Read more on [arXiv](https://arxiv.org/abs/2503.01901) or [HuggingFace](https://huggingface.co/papers/2503.01901))| Weiyu Huang, surfingtomchen, jt-zhang, zcliang22, yuezhouhu | This paper introduces a novel method called ReQuant, which uses Post-quantization Integral (PQI) to improve the accuracy of post-training weight quantization for large language models (LLMs). The research question focuses on how to accurately identify and prioritize sensitive weight dimensions during quantization to minimize accuracy degradation. The proposed methodology, ReQuant, leverages PQI to compute posterior sensitivity in a fine-grained manner and uses a Dense-and-Sparse detach framework, including self-adaptive outlier selection and step-wise significant weight detachment. Results show that ReQuant significantly boosts state-of-the-art post-training quantization methods, achieving a 2.66 perplexity gain on Llama 3.2 1B with QTIP. AI practitioners can use ReQuant to more efficiently compress LLMs while preserving accuracy, especially for applications with limited computational resources. |
| Natural Language Processing | L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling (Read more on [arXiv](https://arxiv.org/abs/2503.04725) or [HuggingFace](https://huggingface.co/papers/2503.04725))| Marin Soljačić, Di Luo, Zhuotao Jin, oriolmayne, zhuoc3 | This paper establishes a theoretical framework for understanding long-range dependencies in language models, focusing on the scaling of bipartite mutual information. The central research question is how a model's capacity to handle long contexts relates to its ability to store and process past information, as measured by bipartite mutual information. The authors establish a bipartite mutual information scaling law and formulate the Long-context Language Modeling (L2M) condition, linking model capacity to the scaling of its latent state. Empirical validation on transformers and state space models shows that bipartite mutual information scales as a power law (I ~ L^β , where β ranges from 0.60 to 0.76 in different datasets and LLMs), and models must satisfy the L2M condition (state size must grow at least with power-law of the bipartite mutual information) for effective long context modeling. This work implies that AI practitioners should design architectures and training methods that explicitly consider the scaling of history state size with sequence length to improve long-context modeling capabilities. |
| Natural Language Processing | Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks (Read more on [arXiv](https://arxiv.org/abs/2503.04378) or [HuggingFace](https://huggingface.co/papers/2503.04378))| Ellie Evans, Daniel Egert, Jiaqi Zeng, Zhilin Wang, odelalleau | This paper introduces a method for improving the performance of large language models (LLMs) on open-ended, general-domain tasks using dedicated Feedback and Edit models for inference-time scaling. The main research objective is to investigate whether specialized models trained to provide and utilize feedback can enhance LLM responses beyond what's achievable with existing inference-time scaling techniques. The methodology involves training separate Feedback and Edit models that iteratively refine initial model responses based on natural language feedback, similar to a human feedback loop. The proposed system, when optimally scaled, achieves a state-of-the-art performance of 92.7 on the Arena Hard benchmark, surpassing OpenAI's ol-preview-2024-09-12 (90.4) and DeepSeek R1 (92.3). The results demonstrate that this approach can significantly improve LLM performance on challenging, open-ended tasks, suggesting a valuable direction for model alignment and refinement without extensive retraining. |
| Machine Learning | Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer (Read more on [arXiv](https://arxiv.org/abs/2503.02495) or [HuggingFace](https://huggingface.co/papers/2503.02495))| Linhui Li, Jing Lian, yjyangwork | This paper introduces Union-of-Experts (UoE), a novel approach to enhance the Mixture-of-Experts (MoE) paradigm in transformers by enabling more collaborative expert interactions and improving computational efficiency. The main research objective is to address the limitations of existing MoE methods, particularly the lack of high-quality expert interactions and inefficient extension to attention blocks. The key methodology involves decomposing transformer models into equitant groups of experts, implementing selective routing on both input data and experts, and developing parallel implementation for optimized efficiency. The UoE model achieved a perplexity of 24.09 on WikiText-103 with only 1.74 TFLOPs, outperforming state-of-the-art MoE methods and demonstrating significant improvements across various tasks. AI practitioners can leverage UoE to build more efficient and powerful transformer models, especially in resource-constrained scenarios or for processing long sequences. |
| Natural Language Processing | Lost in Literalism: How Supervised Training Shapes Translationese in LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.04369) or [HuggingFace](https://huggingface.co/papers/2503.04369))| Leyang Cui, Huajian Zhang, Zhilin Wang, Ronghao Zhang, yaful | This paper investigates the prevalence and origins of translationese in large language models (LLMs) used for machine translation, focusing on how supervised fine-tuning (SFT) introduces biases that lead to unnatural translations. The main research objective is to evaluate the extent of translationese in LLM-generated text and identify its root causes during supervised training. The key methodology involves systematically evaluating translationese in LLM outputs, analyzing SFT data for biases, and proposing mitigation strategies, such as refining training references and filtering unnatural instances. Primary results show that even advanced models like GPT-4 exhibit substantial translationese patterns in over 40% of their translations, and refining training instances can significantly reduce translationese, quantified by improved perplexity, lexical density and length variability metrics.  AI practitioners should consider addressing data quality and training methodologies to minimize translationese and develop more fluent and natural LLM translation systems. |
| Machine Learning | Combining Flow Matching and Transformers for Efficient Solution of Bayesian Inverse Problems (Read more on [arXiv](https://arxiv.org/abs/2503.01375) or [HuggingFace](https://huggingface.co/papers/2503.01375))| Ekaterina Muravleva, oseledets, dsherki | This paper introduces a method for efficiently solving Bayesian inverse problems by combining Conditional Flow Matching (CFM) with a transformer-based architecture. The main research objective is to develop a generative model capable of sampling from the posterior distribution of model parameters, conditioned on a variable number of observations.  The proposed method utilizes a transformer-based CFM architecture to learn the conditional probability distribution from easily constructed samples, enabling handling of different numbers of observations.  The method achieves a relative inference error of 1.48% ± 0.71% on the SEIR problem and 2.75% ± 0.60% on the Permeability field problem with 8 observation points, significantly outperforming traditional MCMC methods in computational efficiency.  AI practitioners can apply this technique to various Bayesian inverse problems, achieving faster and more flexible solutions, especially when dealing with varying observation data. |
| Natural Language Processing | Understanding and Predicting Derailment in Toxic Conversations on GitHub (Read more on [arXiv](https://arxiv.org/abs/2503.02191) or [HuggingFace](https://huggingface.co/papers/2503.02191))| Rebekah Copeland, Robert Zita, kdamevski, rahat-rizvi, imranraad | This paper investigates conversational derailment leading to toxicity in GitHub discussions and proposes a proactive moderation approach. The main research objective is to understand the characteristics of toxic conversations on GitHub, how they derail, and to predict such derailment automatically. The key methodology involves curating a dataset of toxic and non-toxic GitHub conversations, analyzing linguistic and conversational features, and developing a Large Language Model (LLM)-based prompt strategy to generate summaries of conversation dynamics for derailment prediction.  The proposed approach, utilizing conversation trajectory summaries, achieves a 69% F1-score in predicting conversational derailment, significantly improving upon baseline methods. The research implies that AI practitioners can build automated tools for proactive moderation by identifying early signifiers of toxic conversations on platforms like GitHub. |
