

## Papers for 2025-03-20

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time
  Exploration and Exploitation (Read more on [arXiv](https://arxiv.org/abs/2503.13288) or [HuggingFace](https://huggingface.co/papers/2503.13288))| Qika, haitengzhao, changma, Meituannnnnn, xufangzhi | This paper introduces φ-Decoding, a novel inference-time optimization algorithm for large language models (LLMs) that balances exploration and exploitation during reasoning. The main research question is how to achieve superior step-value estimation and determine if deliberative planning is necessary for every step in LLM reasoning. φ-Decoding utilizes foresight sampling, estimating step values based on simulated future steps and a joint distribution derived from advantage and alignment, combined with in-width and in-depth pruning strategies for adaptive computation. Experiments show that φ-Decoding improves the average performance of LLaMA3.1-Instruct-8B by over 14% compared to auto-regressive CoT, achieving a balance between accuracy and computational efficiency. AI practitioners can leverage φ-Decoding to enhance the reasoning capabilities of LLMs at inference time without additional training or external resources. |
| Computer Vision | DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2503.15265) or [HuggingFace](https://huggingface.co/papers/2503.15265))| yikaiwang, NTU-yiwen, guangce, yejunliang23, zzzrw | DeepMesh is a novel framework for generating artist-like 3D triangle meshes using an auto-regressive approach combined with reinforcement learning. The research aims to address the limitations of existing auto-regressive methods, specifically, the constraints of limited face counts, mesh incompleteness and lack of alignment with human preferences. The method introduces an improved tokenization algorithm reducing sequence length by 72% and incorporates Direct Preference Optimization (DPO) with a scoring standard that combines human evaluation with 3D metrics. The results show that DeepMesh outperforms state-of-the-art methods in both precision and quality, with a user study indicating a 37% preference rate for meshes generated with DPO. AI practitioners can utilize DeepMesh to efficiently generate detailed and aesthetically pleasing 3D meshes conditioned on point clouds and images. |
| Multi-Modal | TULIP: Towards Unified Language-Image Pretraining (Read more on [arXiv](https://arxiv.org/abs/2503.15485) or [HuggingFace](https://huggingface.co/papers/2503.15485))| XuDong Wang, Seun Eisape, Long Lian, yala, ZinengTang | The paper introduces TULIP, a unified language-image pretraining framework addressing limitations in existing contrastive models by enhancing fine-grained visual understanding. It leverages generative data augmentation, enhanced contrastive learning, and reconstruction regularization to learn robust visual features while preserving semantic alignment. The method achieves state-of-the-art zero-shot performance on ImageNet-1K and improves RxRx1 performance by 2x compared to SigLIP in linear probing. TULIP can be used as a drop-in replacement to improve vision-language models for tasks requiring high-fidelity image understanding. |
| Multi-Modal | Cube: A Roblox View of 3D Intelligence (Read more on [arXiv](https://arxiv.org/abs/2503.15475) or [HuggingFace](https://huggingface.co/papers/2503.15475))| Karun Channa, Nishchaie Khanna, Kiran Bhat, Foundation AI Team, marcelvanworkum | This paper presents the first step towards a foundational model for 3D intelligence at Roblox, focusing on discrete tokenization of 3D shapes. The main research objective is to develop a method for representing 3D geometry that is expressive, efficient, and suitable for use in multi-modal autoregressive sequence models. The key methodology involves encoding input meshes into discrete tokens using a Perceiver-based transformer with Phased-Modulated Positional Encoding, vector quantization, and self-supervised learning. The proposed method outperforms Craftsman on the Toys4K dataset, achieving a Surface-IoU of 91.7% and a Volumetric-IoU of 94.5% compared to 68.8% and 83.6% respectively. This approach to 3D shape tokenization enables various applications like text-to-shape, shape-to-text, and text-to-scene generation, paving the way for a unified 3D foundation model. |
| Machine Learning | Efficient Personalization of Quantized Diffusion Model without
  Backpropagation (Read more on [arXiv](https://arxiv.org/abs/2503.14868) or [HuggingFace](https://huggingface.co/papers/2503.14868))| Se Young Chun, Kyungryeol Lee, Wongi Jeong, Agorium | This paper introduces ZOODiP, a memory-efficient method for personalizing quantized diffusion models without backpropagation. The research objective is to enable diffusion model personalization on memory-constrained devices, addressing the high memory demands of existing methods. ZOODiP leverages zeroth-order optimization, subspace gradient projection, and partial uniform timestep sampling on a quantized model to reduce memory usage. The method achieves comparable image and text alignment scores to prior methods while reducing training memory demand by up to 8.2x, using only 2.37GB VRAM. AI practitioners can utilize this method to deploy and fine-tune diffusion models on devices with limited memory, such as mobile phones, facilitating applications like on-device personalization. |
| Computer Vision | Temporal Regularization Makes Your Video Generator Stronger (Read more on [arXiv](https://arxiv.org/abs/2503.15417) or [HuggingFace](https://huggingface.co/papers/2503.15417))| Yajing Bai, Yexin Liu, Xianfeng Wu, Haojian Huang, Harold328 | This paper introduces FLUXFLOW, a novel temporal augmentation strategy to improve the temporal quality of generated videos. The main objective is to investigate whether controlled temporal perturbations during training can enhance temporal coherence and diversity in video generation models, without compromising spatial fidelity. FLUXFLOW operates at the data level, applying frame-level and block-level temporal shuffling to disrupt fixed temporal order. Experiments on UCF-101 and VBench show significant improvements in temporal metrics; for instance on VideoCrafter2 with a 2x1 frame shuffle, FVD improved by 19.21, and Subject consistency by 1.97, relative to a baseline that does not use FluxFlow. The results suggest that temporal augmentation is a simple and effective approach to increase video generation quality. |
| Reinforcement Learning | STEVE: AStep Verification Pipeline for Computer-use Agent Training (Read more on [arXiv](https://arxiv.org/abs/2503.12532) or [HuggingFace](https://huggingface.co/papers/2503.12532))| Chi-Wing Fu, Shu Liu, Ziqin Wei, Zhisheng Zhong, Fanbin Lu | This paper introduces STEVE, a step verification pipeline for training computer-use agents. The main objective is to improve agent training efficiency and performance in graphical user interface (GUI) manipulation by leveraging large language models (LLMs) for step-wise reward assignment. STEVE uses a large instruction set and trajectory data, verifies the correctness of each step using GPT-4o, and optimizes the agent using Kahneman & Tversky Optimization (KTO). The proposed agent achieves leading performance on the WinAgentArena benchmark, outperforming supervised finetuning and demonstrating a 23% task success rate. This approach enables more efficient and scalable training of computer-use agents by using both positive and negative actions in trajectories. |
| Multi-Modal | LEGION: Learning to Ground and Explain for Synthetic Image Detection (Read more on [arXiv](https://arxiv.org/abs/2503.15264) or [HuggingFace](https://huggingface.co/papers/2503.15264))| Weijia Li, Junyan Ye, Siwei Wen, zichenwen, khr0516 | This paper introduces LEGION, a multi-modal large language model (MLLM)-based framework for synthetic image detection, artifact localization, and explanation generation, and also explores its use as a controller for improving image generation. The main research objective is to develop a system capable of detecting and explaining artifacts in fully synthetic images, as well as leveraging this capability to enhance image generation quality. The methodology involves a multi-task framework integrating artifact detection, segmentation, and explanation, using human-expert annotated dataset (SynthScars), along with image refinement pipelines via regeneration and inpainting. The LEGION framework outperformes existing methods on multiple benchmarks, exceeding the second-best traditional method on the SynthScars by 3.31% in mIoU and 7.75% in F1 score for artifact localization. The main implication for AI practioners is that the framework can be used not only as a defender to against synthetic image generation, but also a controller to help generate high-quality realistic imagery. |
| Multi-Modal | MusicInfuser: Making Video Diffusion Listen and Dance (Read more on [arXiv](https://arxiv.org/abs/2503.14505) or [HuggingFace](https://huggingface.co/papers/2503.14505))| Steven M. Seitz, Brian Curless, Ira Kemelmacher-Shlizerman, Susung Hong | MusicInfuser is a novel approach for generating high-quality dance videos synchronized to music by adapting existing text-to-video diffusion models. The research aims to enable video diffusion models to generate videos that align with musical inputs while preserving the models' original capabilities and text-prompt responsiveness. The core methodology involves introducing lightweight music-video cross-attention and a low-rank adapter, trained on dance videos without requiring motion capture data. MusicInfuser achieves an average dance quality score of 7.95, outperforming models such as Mochi (7.70) and closely approaching ground truth data from AIST (8.01). AI practitioners can adapt this method to generate music-driven videos without the need to train entirely new multimodal models from scratch. |
| Multi-Modal | ViSpeak: Visual Instruction Feedback in Streaming Videos (Read more on [arXiv](https://arxiv.org/abs/2503.12769) or [HuggingFace](https://huggingface.co/papers/2503.12769))| Kun-Yu Lin, maybetomorrow, Lymann, PhilipC, fushh7 | This paper introduces ViSpeak, a novel task and model for visual instruction feedback in streaming video understanding, enhancing human-agent interaction. The main research objective is to enable AI agents to understand and respond to visual instructions in real-time conversational scenarios, going beyond traditional offline video analysis.  The methodology involves a new task, Visual Instruction Feedback, with seven subtasks; two new datasets, ViSpeak-Instruct and ViSpeak-Bench; and a three-stage finetuning procedure for a streaming video understanding model. The ViSpeak model achieves SOTA performance on StreamingBench (62.00) and OVO-Bench (61.08), comparable to GPT-4o. The results demonstrate the potential of visual instruction feedback in dynamic, interactive settings, providing a strong baseline and benchmark for future research in the field of streaming video understanding. |
| Natural Language Processing | GKG-LLM: A Unified Framework for Generalized Knowledge Graph
  Construction (Read more on [arXiv](https://arxiv.org/abs/2503.11227) or [HuggingFace](https://huggingface.co/papers/2503.11227))| Jun Liu, haiping Zhu, Shihao Qi, Bifan Wei, VentureZJ | This paper introduces GKG-LLM, a unified framework for constructing generalized knowledge graphs (GKGs), encompassing knowledge graphs, event knowledge graphs, and commonsense knowledge graphs. The main research objective is to address the challenges of task-specific differences and resource fragmentation in constructing these diverse graph types. The key methodology involves a three-stage curriculum learning fine-tuning framework that iteratively injects knowledge from the three graph types into Large Language Models (LLMs).  Primary results show that GKG-LLM achieves a minimum performance improvement of 9.88% on EKG sub-task datasets, and generally obtains state of art or second best across a battery of tests. The main implication for AI practitioners is a more efficient and unified approach to GKG construction, leveraging LLMs and curriculum learning to enhance performance across in-domain, out-of-distribution, and counter-task data. |
| Multi-Modal | Mitigating Visual Forgetting via Take-along Visual Conditioning for
  Multi-modal Long CoT Reasoning (Read more on [arXiv](https://arxiv.org/abs/2503.13360) or [HuggingFace](https://huggingface.co/papers/2503.13360))| Han-Jia Ye, Houwen Peng, Zhun Sun, Allen8 | This paper introduces Take-along Visual Conditioning (TVC) to mitigate visual forgetting in multi-modal large language models (MLLMs) during long-chain reasoning. The research objective is to address the decline in visual attention observed in MLLMs as reasoning progresses, leading to over-reliance on textual outputs. TVC employs a strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. The approach achieves state-of-the-art performance on five mathematical reasoning benchmarks, with an average increase of 3.4% compared to previous state-of-the-art results, demonstrating the effectiveness in enhancing multimodal reasoning. AI practitioners can leverage TVC to improve the visual grounding and long-chain reasoning capabilities of MLLMs in complex tasks requiring visual inputs. |
| Computer Vision | Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based
  Spatiotemporal Diffusion for Audio-driven Talking Portrait (Read more on [arXiv](https://arxiv.org/abs/2503.12963) or [HuggingFace](https://huggingface.co/papers/2503.12963))| Chenru Jiang, Yuyao Yan, weiguangzhao, KaiserYaoJM, ChaolongYang | This paper introduces KDTalker, a novel framework for generating realistic and dynamic talking portraits from audio input. The research objective is to improve the diversity of head poses and facial expressions in audio-driven talking head generation while maintaining lip synchronization accuracy and computational efficiency. The key methodology combines unsupervised implicit 3D keypoint-driven methods with a spatiotemporal diffusion model, and features a custom-designed spatiotemporal attention mechanism. KDTalker achieves state-of-the-art performance, with a LSE-C of 7.326 and head pose diversity of 0.760, demonstrating superior lip-sync and pose variation at 21.678 FPS. AI practitioners can use this approach for efficient, high-fidelity animation in virtual reality, digital human creation, and filmmaking. |
| Natural Language Processing | ELTEX: A Framework for Domain-Driven Synthetic Data Generation (Read more on [arXiv](https://arxiv.org/abs/2503.15055) or [HuggingFace](https://huggingface.co/papers/2503.15055))| Eugene Dmitriev, Julien Capitaine, Sofia Sedlova, Kseniia Murasheva, lavriz | ELTEX is a domain-driven framework for generating high-quality synthetic training data in specialized domains, particularly cybersecurity. The main objective is to address the scarcity of domain-specific training data that limits the performance of Large Language Models (LLMs) in specialized areas like blockchain-related cyberattack detection. The key methodology integrates explicit domain indicator extraction with dynamic prompting to preserve critical domain knowledge during the generation process. Primary results show that an ELTEX-enhanced Gemma-2B model achieved an F1-score of 0.81, competitive with GPT-4, while using significantly fewer resources. The main implication is that domain-driven synthetic data generation can bridge the performance gap between resource-efficient models and larger architectures in specialized domains, enabling more effective deployment of smaller models. |
