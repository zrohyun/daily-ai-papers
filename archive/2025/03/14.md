

## Papers for 2025-03-14

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing (Read more on [arXiv](https://arxiv.org/abs/2503.10613) or [HuggingFace](https://huggingface.co/papers/2503.10613))| Dang Nguyen, zhoutianyi, nandakiran09, advaitgupta | This paper introduces CoSTA*, a cost-sensitive toolpath agent for multi-turn image editing that combines large language models (LLMs) and A* search to find efficient tool paths. The main research question is how to combine the strengths of LLMs and graph search to find cost-efficient tool paths for complex image editing tasks, balancing quality and computational cost. The methodology involves a three-stage approach: using an LLM to create a subtask tree, pruning a graph of AI tools based on this tree, and then conducting an A* search on the subgraph. CoSTA* achieved Pareto optimality and outperformed state-of-the-art image editing models on a novel benchmark, with an overall accuracy of 0.94. The main implication for AI practitioners is that CoSTA* offers a more efficient and robust solution for complex, multi-turn image editing tasks through hierarchical planning and cost-sensitive search, enabling better trade-offs between quality and computational cost. |
| Multi-Modal | World Modeling Makes a Better Planner: Dual Preference Optimization for
  Embodied Task Planning (Read more on [arXiv](https://arxiv.org/abs/2503.10480) or [HuggingFace](https://huggingface.co/papers/2503.10480))| xpqiu, Jinlan, CyberDJ, ngc7293, sinwang | This paper introduces Dual Preference Optimization (D²PO), a new learning framework for embodied task planning that jointly optimizes state prediction and action selection in large vision-language models (LVLMs). The main research objective is to enhance LVLMs' planning capabilities by enabling them to understand environment dynamics through preference learning, moving beyond simple state-to-action mapping.  The key methodology involves a tree search mechanism for automatic data collection and a dual preference optimization approach that leverages both action selection and state prediction preference data.  Experiments on the VoTa-Bench demonstrate that D²PO significantly outperforms existing methods and GPT-4o, with a 7B-parameter model achieving a relative improvement of 31.4% in success rate and 33.0% in planning efficiency compared to SFT baselines.  The main implication for AI practitioners is that integrating world modeling objectives, specifically through joint optimization of state prediction and action selection, substantially improves the performance and efficiency of embodied task planning with LVLMs. |
| Computer Vision | Silent Branding Attack: Trigger-free Data Poisoning Attack on
  Text-to-Image Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2503.09669) or [HuggingFace](https://huggingface.co/papers/2503.09669))| Sung Ju Hwang, kiminle2, harryjo97, wchoi403, agwmon | This paper introduces the Silent Branding Attack, a novel data poisoning method that manipulates text-to-image diffusion models to generate images containing specific brand logos or symbols without requiring explicit text triggers. The main research objective is to develop and validate a data poisoning attack that stealthily embeds logos into images generated by diffusion models, bypassing the need for trigger words. The methodology involves an automated algorithm that unobtrusively injects logos into original images using logo personalization, mask generation, inpainting, and refinement techniques. The attack achieved high success rates, with a logo inclusion rate of up to 45.00% on the Midjourney dataset and 39.68% on the Tarot dataset without specific text triggers. AI practitioners should be aware of the vulnerability of text-to-image models to data poisoning attacks that can subtly manipulate outputs without requiring user-provided trigger terms, necessitating improved safeguards. |
| Machine Learning | Charting and Navigating Hugging Face's Model Atlas (Read more on [arXiv](https://arxiv.org/abs/2503.10633) or [HuggingFace](https://huggingface.co/papers/2503.10633))| yedid, LielAmar, jonkahana, nitzankur, Eliahu | This paper presents a method for charting and navigating the vast landscape of publicly available neural networks on Hugging Face, creating a 'model atlas'. The main objective is to develop a structured representation of model repositories that allows for efficient discovery, analysis, and reuse of existing models.  The key methodology involves constructing a directed acyclic graph (DAG) where nodes represent models and edges represent transformations (e.g., fine-tuning), using both documented metadata and a novel weight-based algorithm to infer undocumented relationships. The proposed algorithm achieves 78.87%, 80.44%, and 85.10% accuracy on Qwen, Llama and SD datasets, respectively, significantly outperforming baseline methods. The main implication is that a comprehensive model atlas can facilitate model comparison, trend analysis, and attribute prediction, and reduce redundant training efforts. |
| Multi-Modal | GoT: Unleashing Reasoning Capability of Multimodal Large Language Model
  for Visual Generation and Editing (Read more on [arXiv](https://arxiv.org/abs/2503.10639) or [HuggingFace](https://huggingface.co/papers/2503.10639))| zengxingyu, shilinyan, LjHuang, gogoduan, LucasFang | This paper introduces Generation Chain-of-Thought (GoT), a novel paradigm for visual generation and editing that leverages explicit semantic-spatial reasoning. The main objective is to integrate the reasoning mechanisms of multimodal large language models (MLLMs) into visual content creation, moving beyond direct prompt processing. The key methodology involves generating detailed reasoning chains with coordinates using an MLLM, then guiding a diffusion model with a Semantic-Spatial Guidance Module.  The GoT framework achieves a 0.64 overall score on the GenEval benchmark, surpassing other methods. AI practitioners can utilize this approach for enhanced control and interpretability in image generation and editing, particularly in complex scenes. |
| Machine Learning | Transformers without Normalization (Read more on [arXiv](https://arxiv.org/abs/2503.10622) or [HuggingFace](https://huggingface.co/papers/2503.10622))| Zhuang Liu, Kaiming He, ylecun, endernewton, JiachenZhu | This paper introduces Dynamic Tanh (DyT) as a drop-in replacement for normalization layers in Transformer architectures, demonstrating comparable or superior performance across various tasks. The research challenges the conventional belief that normalization layers are indispensable in deep neural networks, particularly Transformers. The key methodology involves replacing normalization layers with DyT, an element-wise operation defined as DyT(x) = tanh(ax), where 'a' is a learnable parameter. Primary results on ImageNet-1K classification using ViT-B show that DyT achieves 82.5% top-1 accuracy, compared to 82.3% with Layer Normalization (LN), demonstrating it matches or exceeds original performances. The implication of the research is that AI practitioners can potentially simplify Transformer training, improve efficiency, and gain new insights on the function of normalization layers by using DyT as a replacement. |
| Multi-Modal | GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding (Read more on [arXiv](https://arxiv.org/abs/2503.10596) or [HuggingFace](https://huggingface.co/papers/2503.10596))| wenyuliu, steelozazala, wondervictor, LianghuiZhu, RuiHu | This paper introduces GroundingSuite, a new benchmark and framework for evaluating and training models on complex pixel-level grounding tasks. The main research objective is to address limitations in existing datasets, such as limited object categories, insufficient textual diversity, and lack of high-quality annotations, by proposing an approach for various granularities. The key methodology includes an automated data annotation framework (GSSculpt) leveraging multiple Vision-Language Model (VLM) agents and creating a large-scale training dataset (GSTrain-10M) of 9.56 million expressions.  Models trained on GSTrain-10M achieved state-of-the-art results, for instance, a cIoU of 68.9 on gRefCOCO. The main implication is that AI practitioners can utilize GroundingSuite for developing and evaluating more robust and generalizable models in tasks requiring fine-grained visual-linguistic understanding. |
| Natural Language Processing | New Trends for Modern Machine Translation with Large Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2503.10351) or [HuggingFace](https://huggingface.co/papers/2503.10351))| acecamel1977, longyuewang, minghaowu, ChenyangLyu, SNF | This position paper explores the transformative potential of Large Reasoning Models (LRMs) in redefining Machine Translation (MT) systems by reframing translation as a dynamic reasoning task. The main objective is to argue that LRMs substantially transform traditional neural MT and LLM-based MT paradigms, and identify the foundational shifts they bring. The methodology involves showcasing empirical examples of LRMs in various translation scenarios, including stylized translation, document-level translation, and multimodal translation, as well as identifying the interesting phenomena and challenges. Primary evaluation uses BLEURT and COMET on commonMT; however, the paper mostly provides qualitative results. The main implication is that LRMs redefine translation systems as multilingual cognitive agents capable of reasoning about meaning beyond text, suggesting a shift towards deeper cross-cultural understanding and communication. |
| Natural Language Processing | Shifting Long-Context LLMs Research from Input to Output (Read more on [arXiv](https://arxiv.org/abs/2503.04723) or [HuggingFace](https://huggingface.co/papers/2503.04723))| mingshan, tsq2000, Zhiqiang007, bys0318, mozhu | This paper advocates for a paradigm shift in NLP research towards addressing the challenges of long-output generation in Large Language Models (LLMs). The main research objective is to highlight the under-explored domain of generating high-quality, long-form outputs, which is critical for real-world applications like novel writing and long-term planning. The methodology involves analyzing current LLM capabilities, identifying limitations in existing datasets and benchmarks, and proposing requirements for long-output LLMs. The authors found only 2 out of 104 papers in long-context research are focus on the output, which is significantly less than the requirement. The main implication for AI practitioners is the need to develop foundational LLMs tailored for generating long-form outputs, shifting the focus from solely processing extended input contexts. |
| Multi-Modal | VisualWebInstruct: Scaling up Multimodal Instruction Data through Web
  Search (Read more on [arXiv](https://arxiv.org/abs/2503.10582) or [HuggingFace](https://huggingface.co/papers/2503.10582))| Bo Li, Xiang Yue, wenhu, jiachenli-ucsb, jymmmmm | This paper introduces VisualWebInstruct, a novel approach for creating a large-scale, diverse, and high-quality multimodal instruction dataset for reasoning-focused tasks. The main research objective is to address the scarcity of high-quality training data for multimodal models, particularly for tasks requiring complex reasoning. The methodology leverages a search engine (Google Image Search and Google Lens) to identify and collect image-text pairs from the web, followed by a pipeline of content extraction, filtering, synthesis using GPT-4o, and consistency verification to build approximately 900K question-answer pairs. Models fine-tuned on VisualWebInstruct, such as MAmmoTH-VL2, showed significant performance gains, achieving a 50.4% average accuracy across seven visual reasoning benchmarks and state-of-the-art performance within the 10B parameter class on MMMU-Pro-std (40.7%). This dataset and approach can enhance VLMs' reasoning capabilities, enabling AI practitioners to train more robust models for complex multimodal tasks. |
| Computer Vision | DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture
  Design in Text to Image Generation (Read more on [arXiv](https://arxiv.org/abs/2503.10618) or [HuggingFace](https://huggingface.co/papers/2503.10618))| Rui Qian, Chen Chen, yinfeiy, tsujuifu, wenzehu | This paper introduces DiT-Air, an optimized Diffusion Transformer (DiT) architecture for text-to-image generation, demonstrating improved parameter efficiency and state-of-the-art performance. The research comprehensively investigates DiT design choices, including architecture variants, text-conditioning strategies, and training protocols, focusing on how these effect efficiency. The key methodology involves simplifying a standard DiT by processing concatenated text and noise inputs, along with comparative analyses of components like text encoders and VAEs. The proposed DiT-Air model achieves new state-of-the-art GenEval and T2I CompBench scores of 82.9 and 59.5, respectively, while using a smaller model size compared to MMDiT architecture. AI practitioners can leverage DiT-Air's design and training strategies to develop more efficient and effective text-to-image generation models, particularly in resource-constrained environments. |
| Computer Vision | Open-Sora 2.0: Training a Commercial-Level Video Generation Model in
  $200k (Read more on [arXiv](https://arxiv.org/abs/2503.09642) or [HuggingFace](https://huggingface.co/papers/2503.09642))| Xinying Guo, Tom Young, Chenhui Shen, Zangwei Zheng, Xiangyu Peng | This paper introduces Open-Sora 2.0, a commercial-level video generation model trained for only $200k, demonstrating cost-effective training of high-quality video generation models. The research objective is to show that top-performing video generation can be achieved with significantly reduced computational resources compared to existing approaches. The key methodologies include hierarchical data filtering, a novel Video DC-AE autoencoder, a DiT architecture, and a multi-stage training process optimized for efficiency using image and video conditioning.  The model achieves a win rate comparable to leading models like Runway Gen-3 Alpha in human evaluations across visual quality, prompt adherence, and motion quality; it also out performs previous open source models based on human evaluation and VBench scores. This work implies that AI practitioners can develop competitive video generation models with constrained budgets by leveraging efficient data curation, model architecture choices, training strategies, and system optimizations. |
| Computer Vision | Long Context Tuning for Video Generation (Read more on [arXiv](https://arxiv.org/abs/2503.10589) or [HuggingFace](https://huggingface.co/papers/2503.10589))| lindahua, zhenheny, Ikuinen, Brightmzb, ziyany | This paper introduces Long Context Tuning (LCT), a novel training paradigm for generating coherent multi-shot videos from pre-trained single-shot video diffusion models. The main research objective is to expand the context window of existing models to learn scene-level consistency directly from video data, addressing the limitations of current single-shot video generation methods. LCT adapts full attention mechanisms, incorporates interleaved 3D position embedding, and uses an asynchronous noise strategy to enable joint and auto-regressive shot generation. Experimental results show that LCT achieves a text alignment score of 30.14 and consistency score of 95.65, surpassing existing methods in semantic and visual consistency, demonstrated by generating videos with around 20 shots that can last 3 minutes. LCT enables the production of visually and semantically consistent multi-shot scenes and exhibits emerging capabilities, offering potential to significantly improve the quality of AI generated long videos. |
| Computer Vision | 4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.10437) or [HuggingFace](https://huggingface.co/papers/2503.10437))| hpfister, Qmh, wrencanfly, rpzhou, EthanTaylor | This paper introduces 4D LangSplat, a novel framework for constructing 4D language fields that enable time-sensitive and time-agnostic open-vocabulary queries in dynamic scenes. The research objective is to develop a method that can accurately and efficiently handle dynamic semantic changes in 4D scenes, overcoming limitations of existing methods that rely on static image-text models like CLIP. The key methodology involves leveraging Multimodal Large Language Models (MLLMs) to generate object-wise video captions, which are then encoded into pixel-aligned features, and a status deformable network to model continuous state changes over time.  The method achieves a mean Intersection over Union (mIoU) of 82.48% and mean accuracy (mAcc) of 98.01% on the HyperNeRF dataset for time-agnostic queries, significantly outperforming previous approaches. The main implication for AI practitioners is that 4D LangSplat provides a more effective and robust solution for querying and understanding dynamic, real-world environments by capturing the evolution of object semantics over time. |
| Multi-Modal | Do I look like a `cat.n.01` to you? A Taxonomy Image Generation
  Benchmark (Read more on [arXiv](https://arxiv.org/abs/2503.10357) or [HuggingFace](https://huggingface.co/papers/2503.10357))| Ekaterina Neminova, Alina Lobanova, lilaspourpre, apanc, VityaVitalich | This paper introduces a benchmark for evaluating text-to-image models on their ability to generate images representing concepts within a hierarchical taxonomy. The research question is how well existing text-to-image models can visualize taxonomic concepts of varying abstraction levels, compared to human understanding. The methodology involves evaluating 12 text-to-image models using 9 novel taxonomy-related metrics, human feedback, and pairwise evaluation with GPT-4 feedback. Results show that Playground-v2 and FLUX consistently outperform other models across various metrics and subsets; the Spearman correlation between human and GPT-4 rankings is 0.92. The implication is that text-to-image models can automate the curation of structured visual data resources, and the differing model rankings highlight the importance of taxonomy-specific evaluations. |
| Computer Vision | SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency
  Distillation (Read more on [arXiv](https://arxiv.org/abs/2503.09641) or [HuggingFace](https://huggingface.co/papers/2503.09641))| Yuyang Zhao, Shuchen Xue, Junsong Chen, xieenze, sayakpaul | SANA-Sprint is a new diffusion model designed for ultra-fast text-to-image (T2I) generation. The main research objective is to drastically reduce the inference steps of diffusion models while preserving high image quality. The key methodology involves a hybrid distillation approach, combining continuous-time consistency distillation (sCM) with latent adversarial distillation (LADD), and a training-free transformation of a pre-trained flow-matching model.  The model achieves a state-of-the-art FID of 7.59 and GenEval of 0.74 in a single step, outperforming other models while being significantly faster (0.1s on H100 for 1024x1024 images). AI practitioners can leverage SANA-Sprint for applications requiring real-time or near real-time image generation, such as interactive AI-powered consumer applications. |
| Multi-Modal | UniGoal: Towards Universal Zero-shot Goal-oriented Navigation (Read more on [arXiv](https://arxiv.org/abs/2503.10630) or [HuggingFace](https://huggingface.co/papers/2503.10630))| Ziwei Wang, Lingqing Zhao, jiwenlu, xuxw98, hangyin | This paper introduces UniGoal, a universal zero-shot framework for goal-oriented navigation across diverse goal specifications. The research objective is to develop a single model capable of handling object category, instance image, and text description goals without task-specific training or fine-tuning. The key methodology involves a uniform graph representation for both scene and goal, leveraging large language models (LLMs) for explicit graph-based reasoning, and a multi-stage scene exploration policy guided by graph matching. UniGoal achieves state-of-the-art zero-shot performance on three navigation tasks (ObjectNav, InstanceNav, TextNav), surpassing task-specific zero-shot methods and even some supervised universal methods; for example it achieved a Success Rate (SR) of 41.0% and Success weighted by Path Length (SPL) of 16.4% on the Matterport3D (MP3D) Object-goal Navigation task. AI practitioners can utilize this framework for robust and generalizable goal-oriented navigation in robots without the need for extensive task-specific data collection or training. |
| Reinforcement Learning | Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and
  Beyond (Read more on [arXiv](https://arxiv.org/abs/2503.10460) or [HuggingFace](https://huggingface.co/papers/2503.10460))| tanglifu, JunchenLiu, yyy99, duan901010, cizhenshi | This paper introduces Light-R1, a series of models trained for long chain-of-thought (COT) reasoning, along with released data and code. The main research objective is to develop compact models capable of long COT reasoning from scratch, starting with models lacking this ability. The key methodology involves a curriculum training recipe comprised of two-stage Supervised Fine-Tuning (SFT) and semi-on-policy Direct Preference Optimization (DPO), and subsequent Reinforcement Learning (specifically GRPO). The Light-R1-14B-DS model achieves a SOTA AIME24 score of 74.0 among 14B models. AI practitioners can leverage this work to train long-COT models efficiently and deploy advanced reasoning capabilities in resource-constrained settings. |
| Multi-Modal | CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance (Read more on [arXiv](https://arxiv.org/abs/2503.10391) or [HuggingFace](https://huggingface.co/papers/2503.10391))| brotherhuang, u302117, BestWishYsh, angtian, dyf | CINEMA is a novel framework for coherent multi-subject video generation that leverages Multimodal Large Language Models (MLLMs) for guidance. The research objective is to generate videos featuring multiple distinct subjects, defined by separate reference images, while ensuring temporal and spatial consistency, a challenge not fully addressed by prior methods. CINEMA utilizes MLLMs to interpret subject relationships and eliminate the need for explicit correspondences between subject images and text, and integrates a semantic alignment network (AlignerNet) and visual entity encoding to improve fidelity. Through extensive evaluations, the approach demonstrates significant improvements in subject consistency and overall video coherence, although specific quantitative metrics are not provided in this abstract. The implication is that AI practitioners can leverage CINEMA for advanced applications in storytelling, interactive media, and personalized video generation with improved handling of multiple subjects. |
| Machine Learning | Quantization for OpenAI's Whisper Models: A Comparative Analysis (Read more on [arXiv](https://arxiv.org/abs/2503.09905) or [HuggingFace](https://huggingface.co/papers/2503.09905))| allisonandreyev | This paper presents a comparative analysis of quantization techniques applied to OpenAI's Whisper speech recognition models. The study investigates the impact of quantization on model size, latency, and transcription accuracy, aiming to optimize Whisper for deployment on resource-constrained devices.  The methodology involves evaluating three Whisper model variants (Whisper, Whisper_Streaming, and whisper-timestamped) and applying different quantization methods (INT4, INT5, INT8) using the whispercpp framework, measuring Word Error Rate (WER) and latency on the LibriSpeech dataset.  Results indicate that quantization reduces model size by up to 45% and latency by 19% while preserving transcription accuracy (WER remained at 0.0199 for Base Model and INT5/INT8 quantization).  The main implication is that quantized Whisper models can enable more efficient deployment of real-time speech transcription on edge devices. |
| Computer Vision | Distilling Diversity and Control in Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2503.10637) or [HuggingFace](https://huggingface.co/papers/2503.10637))| David Bau, RohitGandikota | This paper investigates the reduced sample diversity in distilled diffusion models and proposes a method to restore diversity without compromising computational efficiency. The main research question is how to distill both diversity and control capabilities from base diffusion models to their efficient distilled variants. The key methodology involves introducing DT-Visualization to analyze diffusion model behavior and a hybrid inference approach that strategically uses the base model for the first timestep and the distilled model for the remaining steps.  The proposed diversity distillation achieves a FID score of 10.79 on the COCO-30k dataset, better than both base (12.74) and the distilled model (15.52), while maintaining the fast inference time (0.64 seconds) of the distilled model. AI practitioners can apply this approach to achieve both the speed of distilled models and the greater diversity comparable to, or better than, the original base models without any re-training. |
| Multi-Modal | R1-Onevision: Advancing Generalized Multimodal Reasoning through
  Cross-Modal Formalization (Read more on [arXiv](https://arxiv.org/abs/2503.10615) or [HuggingFace](https://huggingface.co/papers/2503.10615))| Xiaoxuan He, Yi Yang, twilightsnow, dcyin, Emilia515 | This paper introduces R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. The main objective is to improve the performance of visual-language models on complex reasoning tasks that require integrating visual and textual information. The key methodology is a cross-modal reasoning pipeline that transforms images into formal textual representations, enabling precise language-based reasoning, and a two-stage post-training strategy (supervised fine-tuning and reinforcement learning). R1-Onevision achieves state-of-the-art performance, outperforming models like GPT-4o and Qwen2.5-VL on multiple multimodal reasoning benchmarks (e.g., achieving 29.9% accuracy on MathVision, comparable to GPT-4o's 30.6%). AI practitioners can leverage R1-Onevision and its associated dataset/benchmark for developing and evaluating models that require robust multimodal reasoning, particularly in domains like education and science. |
| Computer Vision | Autoregressive Image Generation with Randomized Parallel Decoding (Read more on [arXiv](https://arxiv.org/abs/2503.10568) or [HuggingFace](https://huggingface.co/papers/2503.10568))| Huan Wang, Guoqi Li, Jinyue Yang, hp-l33 | This paper introduces ARPG, a novel visual autoregressive model for efficient and flexible image generation. The research objective is to overcome the limitations of conventional raster-order autoregressive models, particularly their inefficiency and poor generalization in tasks like inpainting. The key methodology is a "guided decoding" framework that decouples positional guidance from content representation, enabling fully random-order training and parallel inference via a shared KV cache.  On the ImageNet-1K 256x256 benchmark, ARPG achieves an FID of 1.94 with only 64 sampling steps, significantly improving throughput and reducing memory consumption compared to existing autoregressive models. AI practitioners can leverage ARPG for faster and more versatile image generation, including zero-shot generalization to tasks like image inpainting and outpainting. |
| Machine Learning | The Curse of Conditions: Analyzing and Improving Optimal Transport for
  Conditional Flow-Based Generation (Read more on [arXiv](https://arxiv.org/abs/2503.10636) or [HuggingFace](https://huggingface.co/papers/2503.10636))| Alexander Schwing, hkchengrex | This paper analyzes and improves optimal transport (OT) for conditional flow-based generative models, identifying a train-test discrepancy when using OT in conditional settings. The research objective is to address the performance degradation of minibatch OT in conditional generation due to a skewed prior distribution during training. The key methodology is a proposed conditional optimal transport (C2OT) that adds a conditional weighting term in the cost matrix to unskew the prior. Experiments on datasets like CIFAR-10 and ImageNet show C2OT outperforms flow matching (FM) and OT, achieving a 2-Wasserstein distance of 0.013±0.003 with continuous conditions, and adaptive ODE solver on the 8gaussians→moons dataset. This work implies that C2OT can improve the performance and efficiency in conditional generation tasks. |
| Multi-Modal | VisualPRM: An Effective Process Reward Model for Multimodal Reasoning (Read more on [arXiv](https://arxiv.org/abs/2503.10291) or [HuggingFace](https://huggingface.co/papers/2503.10291))| Einsiedler, Yeshenglong, Decaux, chenlj22, Weiyun1025 | VisualPRM is a novel multimodal Process Reward Model (PRM) designed to enhance the reasoning capabilities of Multimodal Large Language Models (MLLMs). The research introduces VisualPRM and investigates how to improve MLLMs' reasoning performance using Test-Time Scaling, specifically Best-of-N evaluation strategies. It involves creating a process supervision dataset, VisualPRM400K, and a benchmark, VisualProcessBench, to train and evaluate the PRM, which is formulated as a critic model estimating step-wise correctness in multimodal reasoning tasks.  Applied to InternVL2.5-78B, VisualPRM achieves a 5.9-point improvement across seven multimodal reasoning benchmarks, outperforming outcome reward models and self-consistency methods. The main implication is that AI practitioners can significantly improve MLLM reasoning performance through process-based reward modeling and test-time scaling, using provided datasets and benchmarks. |
| Other | "Silent Is Not Actually Silent": An Investigation of Toxicity on Bug
  Report Discussion (Read more on [arXiv](https://arxiv.org/abs/2503.10072) or [HuggingFace](https://huggingface.co/papers/2503.10072))| Jaydeb Sarker, imranraad | This paper investigates the prevalence and impact of toxicity in bug report discussions on GitHub. The main research objective is to explore how toxicity manifests in bug reports and its effects on bug resolution outcomes. The methodology involves a qualitative analysis of 203 bug threads, including 81 identified as toxic, sampled from 100 GitHub repositories and classified using automated toxicity detection tools (ToxiCR and LLaMA). Key findings include that toxic interactions hinder collaboration and that only 45% of toxic threads are resolved versus unknown resolved non-toxic threads, with toxicity often stemming from misaligned perceptions of bug severity. The main implication is that project maintainers should implement transparent bug severity and priority management systems to align user expectations and reduce potential toxicity. |
| Computer Vision | PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with
  Implicit Hierarchical Masked Image Modeling (Read more on [arXiv](https://arxiv.org/abs/2503.09368) or [HuggingFace](https://huggingface.co/papers/2503.09368))| Daniel Mueller-Gritschneder, Sascha Hauke, HerrSiebert, edukrom, Nikolai10 | PerCoV2 is a novel, open, ultra-low bit-rate perceptual image compression system designed for bandwidth- and storage-constrained applications. The paper introduces PerCoV2 and explores how to improve upon existing perceptual compression methods, particularly at extremely low bit-rates.  It extends the PerCo framework to the Stable Diffusion 3 ecosystem and enhances entropy coding efficiency by explicitly modeling the discrete hyper-latent image distribution using an implicit hierarchical masked image model (VAR and MaskGIT).  On the MSCOCO-30k benchmark, PerCoV2 improves all metrics at ultra-low to extreme bit-rates (0.003 – 0.03 bpp) while maintaining competitive perceptual quality compared to prior work. AI practitioners can utilize PerCoV2 for image compression tasks where extremely low bit-rates and high perceptual quality are necessary, particularly in resource-constrained environments. |
| Multi-Modal | On the Limitations of Vision-Language Models in Understanding Image
  Transforms (Read more on [arXiv](https://arxiv.org/abs/2503.09837) or [HuggingFace](https://huggingface.co/papers/2503.09837))| Saquib Sarfraz, Hasnain Ali, Ahmad Mustafa Anis | This paper investigates the limitations of Vision-Language Models (VLMs), specifically CLIP and SigLIP, in understanding basic image transformations. The main research question is whether VLMs can comprehend simple image transformations like rotations, color adjustments, and distortions. The authors created an augmented version of the Flickr8k dataset with detailed descriptions of applied transformations and conducted three experiments evaluating the model's ability to understand augmented descriptions, match augmented images with their corresponding descriptions and classify transformations. The results show that models like CLIP ViT-L/14 achieved only 43.10% accuracy in understanding augmented descriptions (Experiment 1), indicating a significant gap compared to human understanding. The findings imply that AI practitioners should be aware of these limitations when deploying VLMs in tasks requiring explicit understanding of image modifications, such as image editing, and motivate the need for models with enhanced spatial reasoning abilities. |
