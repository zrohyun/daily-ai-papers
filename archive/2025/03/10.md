

## Papers for 2025-03-10

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Unified Reward Model for Multimodal Understanding and Generation (Read more on [arXiv](https://arxiv.org/abs/2503.05236) or [HuggingFace](https://huggingface.co/papers/2503.05236))| Cheng Jin, Hao Li, Jiaqiwang, yuhangzang, CodeGoat24 | This paper introduces UNIFIEDREWARD, a unified reward model for multimodal understanding and generation assessment. The research objective is to develop a single model capable of both pairwise ranking and pointwise scoring for preference alignment across diverse visual tasks, including both image and video domains. The methodology involves training a vision-language model on a large-scale, unified human preference dataset, followed by a novel preference data construction pipeline using pair ranking and point sifting, and finally, Direct Preference Optimization (DPO) for model alignment. Experimental results demonstrate that UNIFIEDREWARD achieves a macro accuracy of 66.5% on the VLRewardBench image understanding assessment, outperforming the best baseline (62.5%). The main implication is that AI practitioners can leverage this unified reward model for more adaptable and generalizable preference learning, significantly improving performance across various visual applications. |
| Natural Language Processing | EuroBERT: Scaling Multilingual Encoders for European Languages (Read more on [arXiv](https://arxiv.org/abs/2503.05500) or [HuggingFace](https://huggingface.co/papers/2503.05500))| caiocorro, ayoubhammal, DuarteMRAlves, hgissbkh, Nicolas-BZRD | This paper introduces EuroBERT, a family of multilingual encoder models designed for European and widely spoken global languages. The main research objective is to revisit and improve the development of multilingual encoders by leveraging recent advances typically applied to decoder-only models. The methodology involves pre-training on a 5T-token multilingual dataset covering 15 languages, including code and mathematics, using a masked language modeling objective and a two-phase training pipeline with architectural improvements like grouped-query attention. The EuroBERT-2.1B model outperforms XLM-RoBERTa-XL on 7 out of 12 benchmarks, achieving a 94.8 NDCG@10 score on MIRACL. AI practitioners can use EuroBERT models as efficient and powerful alternatives to older encoder models, offering improved multilingual, mathematics, and coding capabilities. |
| Natural Language Processing | Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching (Read more on [arXiv](https://arxiv.org/abs/2503.05179) or [HuggingFace](https://huggingface.co/papers/2503.05179))| Sung Ju Hwang, jinheon, saytes | This paper introduces Sketch-of-Thought (SoT), a novel prompting framework for large language models that improves reasoning efficiency by using concise, structured intermediate steps inspired by human cognitive processes. The main objective is to reduce the computational cost of reasoning in LLMs without sacrificing accuracy, addressing the verbosity issue in traditional Chain-of-Thought (CoT) prompting. SoT employs three cognitive-inspired paradigms—Conceptual Chaining, Chunked Symbolism, and Expert Lexicons—selected dynamically by a lightweight router model based on query characteristics. Evaluations across 15 reasoning datasets show that SoT reduces token usage by up to 76% with minimal impact on accuracy, and in some cases, even improves performance. The main implication is that AI practitioners can achieve significant computational savings and improve reasoning efficiency by applying cognitive-inspired constraints to LLM prompting. |
| Natural Language Processing | Forgetting Transformer: Softmax Attention with a Forget Gate (Read more on [arXiv](https://arxiv.org/abs/2503.02130) or [HuggingFace](https://huggingface.co/papers/2503.02130))| Aaron Courville, littleowen, nikishin, zhixuan-lin | This paper introduces the Forgetting Transformer (FoX), a variant of the Transformer model that incorporates a forget gate mechanism to improve performance on tasks requiring both long and short context understanding. The main research objective is to enhance Transformers with a data-dependent mechanism for forgetting past information, similar to forget gates in recurrent sequence models. FoX achieves this by down-weighting unnormalized attention scores, and a 'Pro' block design incorporates architectural components from recurrent models. FoX outperforms the standard Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks while maintaining comparable performance on long-context downstream tasks, achieving a perplexity of 4.4 on a long-context validation set. The main implication is that FoX, with its ability to handle both long and short contexts effectively, offers a strong alternative to standard Transformers and recurrent sequence models for various NLP tasks. |
| Computer Vision | VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control (Read more on [arXiv](https://arxiv.org/abs/2503.05639) or [HuggingFace](https://huggingface.co/papers/2503.05639))| Zhaoyang Zhang, yshan2u, Ljzycmd, juxuan27, BianYx | This paper introduces VideoPainter, a novel dual-branch framework for video inpainting and editing that leverages a lightweight context encoder and diffusion transformers. The main research objective is to develop a method that can perform any-length video inpainting and editing, maintaining high ID consistency and supporting user-customized control. The key methodology involves a plug-and-play context encoder that processes masked videos and injects background guidance into pre-trained video diffusion transformers, combined with an inpainting region ID resampling technique for maintaining consistency in long videos.  VideoPainter achieves state-of-the-art performance, exemplified by a Peak Signal-to-Noise Ratio (PSNR) of 25.27 and a Structural Similarity Index Measure (SSIM) of 0.94 on the VPBench dataset. The main implication is that AI practitioners can use this efficient and versatile framework for high-quality, any-length video inpainting, editing, and restoration with enhanced user control, overcoming limitations of prior methods. |
| Reinforcement Learning | R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2503.05592) or [HuggingFace](https://huggingface.co/papers/2503.05592))| jrwen, TimothyCzp, EliverQ, Boru, XXsongLALA | This paper introduces R1-Searcher, a novel two-stage outcome-based reinforcement learning (RL) approach to enhance the search capabilities of Large Language Models (LLMs) for retrieval-augmented generation. The main research objective is to enable LLMs to autonomously invoke external search systems to access additional knowledge during reasoning, addressing limitations of relying solely on internal knowledge. The key methodology utilizes a two-stage RL framework, first incentivizing retrieval actions and then integrating them with answer accuracy, relying exclusively on outcome-based rewards without process supervision or distillation. Experiments show that R1-Searcher significantly outperforms previous RAG methods, achieving, for instance, a 48.22% improvement on HotpotQA and 21.72% improvement on 2WikiMultiHopQA over ReARTeR, a strong GPT-40-mini baseline when the Qwen-2.5-7B-Base is used. The results suggest AI practitioners can leverage this approach to improve LLM performance on knowledge-intensive tasks by effectively integrating external information sources without substantial inference overhead. |
| Multi-Modal | R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcing Learning (Read more on [arXiv](https://arxiv.org/abs/2503.05379) or [HuggingFace](https://huggingface.co/papers/2503.05379))| Xihan Wei, Liefeng, StarJiaxing | This paper introduces R1-Omni, the first application of Reinforcement Learning with Verifiable Reward (RLVR) to an omni-multimodal large language model for emotion recognition, integrating both visual and audio modalities. The research aims to enhance the model's reasoning capability, emotion recognition accuracy, and generalization ability using RLVR. The methodology leverages RLVR and Group Relative Policy Optimization (GRPO) to optimize the model, using a verifiable reward function based on accuracy and output format.  The R1-Omni model achieved a UAR of 65.83% and a WAR of 56.27% on the DFEW dataset, outperforming supervised fine-tuning, and demonstrating stronger generalization on out-of-distribution datasets. The approach offers improved explainability and robustness, valuable for optimizing multi-modal large language models in tasks involving complex reasoning. |
| Computer Vision | TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2503.05638) or [HuggingFace](https://huggingface.co/papers/2503.05638))| Mark YU, yshan2u, Doubiiu, wbhu-tc | TrajectoryCrafter is a novel approach for redirecting camera trajectories in monocular videos using diffusion models, enabling precise control over view transformations and coherent 4D content generation. The main objective is to generate high-fidelity videos from monocular inputs with user-defined camera trajectories, addressing challenges in trajectory control, 4D consistency, and robust generalization. The methodology involves a dual-stream conditional video diffusion model that integrates point cloud renders and source videos, combined with a hybrid training dataset curated from web-scale monocular videos and static multi-view datasets using a double-reprojection strategy. Evaluations on the multi-view iPhone dataset achieved a mean PSNR of 14.24, SSIM of 0.417, and LPIPS of 0.519, outperforming existing methods. AI practitioners can leverage this framework to enhance video generation, offering users more immersive experiences by allowing free redirection of camera trajectories within casual videos. |
| Reinforcement Learning | BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities (Read more on [arXiv](https://arxiv.org/abs/2503.05652) or [HuggingFace](https://huggingface.co/papers/2503.05652))| Ruohan Zhang, jiajunwu, cgokmen, yjze, yunfanj | This paper introduces BEHAVIOR ROBOT SUITE (BRS), a comprehensive framework for learning whole-body manipulation in diverse household tasks using a bimanual, wheeled robot. The main research objective is to identify and enable the key capabilities required for a robot to successfully perform everyday household activities, focusing on bimanual coordination, stable navigation, and extensive end-effector reachability. The key methodology integrates a cost-effective whole-body teleoperation interface (JoyLo) for data collection and a novel imitation learning algorithm (Whole-Body VisuoMotor Attention (WB-VIMA) policy) that models coordinated whole-body actions. The trained WB-VIMA policies achieved an average success rate of 58% and a peak success rate of 93% across five challenging household tasks in unmodified human environments. This implies that the BRS framework, combining suitable hardware, efficient data collection, and advanced modeling, represents a significant step towards deploying robots capable of whole-body manipulation for everyday household tasks. |
| Natural Language Processing | TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation (Read more on [arXiv](https://arxiv.org/abs/2503.04872) or [HuggingFace](https://huggingface.co/papers/2503.04872))| lwher1996, yuhanwuuu, xiaoqijiang, zhaoguangxiang, lincharliesun | This paper introduces the Branch-Merge distillation approach to create smaller, high-performing Large Language Models (LLMs) by improving model compression. The research objective is to address the limitations of existing distillation methods that struggle to achieve high accuracy in specialized tasks and are often computationally expensive. The methodology involves a two-phase distillation: a Branch Phase, where a large teacher model's knowledge is selectively distilled into specialized student models; and a Merge Phase, where student models are merged to transfer cross-domain knowledge. The resulting model, TinyR1-32B-Preview, outperforms its counterpart, DeepSeek-R1-Distill-Qwen-32B, in Mathematics (+5.5 points), Coding (+4.4 points), and Science (+2.9 points) on specific benchmark datasets, making a great trade-off. This approach offers AI practitioners a scalable solution to build smaller LLMs that maintain high performance while reducing computational cost and time. |
| Computer Vision | ProReflow: Progressive Reflow with Decomposed Velocity (Read more on [arXiv](https://arxiv.org/abs/2503.04824) or [HuggingFace](https://huggingface.co/papers/2503.04824))| Yu Li, Xuefei Ning, Haohang Xu, Lei Ke, Ringo1110 | ProReflow is a new flow matching training method for diffusion models that improves image and video generation quality while maintaining computational efficiency. The paper investigates limitations of existing flow matching training strategies, specifically that the original training pipeline is not optimal. To address this, the authors propose two techniques: progressive reflow, which gradually reflows the diffusion model, and aligned v-prediction, which prioritizes velocity direction matching. Experiments on SDv1.5 show that ProReflow achieves an FID of 10.70 on the MSCOCO2014 validation set with only 4 sampling steps, which is a significant improvement. This technique has the potential to improve the efficiency and sampling performance of diffusion models, enabling faster, high-quality generation. |
| Machine Learning | Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts (Read more on [arXiv](https://arxiv.org/abs/2503.05447) or [HuggingFace](https://huggingface.co/papers/2503.05447))| Yu Cheng, Tong Zhu, Xiaoye08, landisen, weigao266 | The paper introduces Linear-MoE, a production-level system for modeling and training large-scale models that integrate Linear Sequence Modeling (LSM) with Mixture-of-Experts (MoE). The main objective is to develop a system that leverages the advantages of both LSM modules for linear-complexity sequence modeling and MoE layers for sparsely activated computation, achieving high performance with efficient training. Linear-MoE combines LSM modules (linear attention, SSM, linear RNN) with MoE layers in a unified framework, and utilizes sequence parallelism and other advanced training techniques. Evaluations on model series A0.3B-2B and A1B-7B show efficiency gains (e.g., A0.3B-2B using Basic LA achieved 42.69 GB memory and 115.16 x10^3 tokens/s throughput with sequence length 2K and Batch Size 8), while maintaining competitive performance on various benchmarks. The system provides a flexible, extensible method that combines the efficiency of sparse computations and linearity of sequence models for AI practioners. |
| Natural Language Processing | An Empirical Study on Eliciting and Improving R1-like Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2503.04548) or [HuggingFace](https://huggingface.co/papers/2503.04548))| daixuancheng, Boru, ToheartZhang, EliverQ, TimothyCzp | This technical report investigates methods for developing and enhancing slow-thinking, or large reasoning models (LRMs), focusing on reinforcement learning (RL) and tool augmentation. The primary objective is to empirically explore factors influencing RL training for LRMs, including hyperparameter settings, backbone models, and training strategies. The methodology involves systematic experimentation with RL on base and fine-tuned models, and analyzing the effects of techniques like on-policy learning, dynamic KL annealing, and tool manipulation. Key results demonstrate that RL consistently improves QWEN2.5-32B base models, with accuracy increasing from 2.08% to 37.08% on the AIME 2024 benchmark and tool manipulation further boosting performance to 86.67% with greedy search. The main implication is that carefully configured RL training, along with techniques like tool augmentation, can substantially enhance the reasoning capabilities of large language models, even those already fine-tuned. |
| Natural Language Processing | SAGE: A Framework of Precise Retrieval for RAG (Read more on [arXiv](https://arxiv.org/abs/2503.01713) or [HuggingFace](https://huggingface.co/papers/2503.01713))| Jinyang Su, Guoliang Li, jt-zhang | This paper introduces SAGE, a novel framework for Retrieval-Augmented Generation (RAG) designed to improve the precision of information retrieval in question-answering tasks. The main objective is to address limitations of existing RAG systems, specifically ineffective corpus segmentation and the trade-off between noisy and missing retrieval. SAGE employs a semantic segmentation model, a gradient-based chunk selection algorithm, and a LLM self-feedback mechanism to dynamically select relevant chunks and refine the retrieved context. Experimental results show that SAGE outperforms baselines by 61.25% in QA quality and achieves a 49.41% enhancement in cost efficiency. The implication is that SAGE offers a more precise and cost-effective approach to RAG, beneficial for building more effective question-answering systems. |
| Reinforcement Learning | Learning from Failures in Multi-Attempt Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2503.04808) or [HuggingFace](https://huggingface.co/papers/2503.04808))| Jie Fu, Stephen Chung, wydu | This paper proposes a multi-attempt reinforcement learning (RL) approach for enhancing the reasoning capabilities of large language models (LLMs). The main objective is to investigate whether allowing LLMs multiple attempts to answer a question, with feedback on incorrect responses, improves performance compared to a single-attempt baseline. The methodology involves training an LLM on a multi-attempt task where the model receives feedback and iteratively refines its answers. Results show that a small LLM trained on this task improves from 45.6% to 52.5% accuracy on a math benchmark when given two attempts instead of one, significantly outperforming the single-attempt baseline. This suggests that multi-attempt RL training can improve LLMs' reasoning and self-refinement abilities, offering a valuable technique for AI practitioners. |
| Natural Language Processing | LoRACode: LoRA Adapters for Code Embeddings (Read more on [arXiv](https://arxiv.org/abs/2503.05315) or [HuggingFace](https://huggingface.co/papers/2503.05315))| bindsch, amanchadha, shollercoaster | LoRACode is a parameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) designed to enhance code embeddings for code search tasks. The main research objective is to improve the accuracy and efficiency of code retrieval across multiple programming languages by creating task-specific and language-specific adapters. The methodology involves fine-tuning pre-trained code embedding models with LoRA, reducing the number of trainable parameters to less than 2% of the base model. Experiments demonstrate an increase of up to 9.1% in Mean Reciprocal Rank (MRR) for Code2Code search and up to 86.69% for Text2Code search tasks. The results suggest that AI practitioners can use LoRACode to significantly improve code search performance with minimal computational overhead, especially leveraging language-specific adapters. |
| Multi-Modal | R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model (Read more on [arXiv](https://arxiv.org/abs/2503.05132) or [HuggingFace](https://huggingface.co/papers/2503.05132))| Minhao Cheng, Ruochen Wang, zhoutianyi, AIcell, Dolphin42 | This research paper presents VisualThinker-R1-Zero, a multimodal model that replicates the "aha moment" and emergent reasoning characteristics of DeepSeek-R1 in visual reasoning tasks, using a 2B non-SFT model. The primary objective is to demonstrate that reinforcement learning (RL) can induce sophisticated reasoning in multimodal models without supervised fine-tuning (SFT), and to investigate challenges when applying RL to instruct models. The key methodology involves applying Generalized Retrospective Policy Optimization (GRPO) directly to a Qwen2-VL-2B base model using a rule-based reward on the SAT dataset. The model achieves 59.47% accuracy on CVBench, outperforming the base model by ~30% and exceeding SFT settings by ~2%, also demonstrating self-reflection. The implication is that simple RL can autonomously induce strong multimodal reasoning without extensive supervised data, but applying RL on instruct model can lead to trivial reasoning.  |
| Computer Vision | AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM (Read more on [arXiv](https://arxiv.org/abs/2503.04504) or [HuggingFace](https://huggingface.co/papers/2503.04504))| Inpyo Hong, Sein Kwon, Kijung Lee, jyy1551, SkiddieAhn | This paper introduces AnyAnomaly, a zero-shot customizable video anomaly detection (C-VAD) system leveraging Large Vision Language Models (LVLMs). The main objective is to detect user-defined abnormal events in videos without requiring environment-specific retraining.  AnyAnomaly implements C-VAD using context-aware visual question answering with a key frame selection module, position context, and temporal context to improve understanding. The model achieves state-of-the-art results on the UBnormal dataset and a micro AUROC of 90.27% on the C-Ave dataset, outperforming other methods in generalization across datasets. AI practitioners can deploy this model for flexible anomaly detection in diverse environments without the need for extensive data collection and model retraining. |
