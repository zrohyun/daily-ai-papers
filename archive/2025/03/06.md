

## Papers for 2025-03-06

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers (Read more on [arXiv](https://arxiv.org/abs/2503.00865) or [HuggingFace](https://huggingface.co/papers/2503.00865))| LidongBing, maljunied, jhying, lukecq, Yiran0924 | The paper introduces Babel, a new open-source multilingual large language model (LLM) designed to support over 90% of global speakers by covering the top 25 languages by speaker count. The main research objective is to address the limited language coverage in existing open-source multilingual LLMs, particularly for under-resourced languages. Babel employs a layer extension technique to expand its parameter count and enhance performance, along with an LLM-based quality classifier for data curation.  Babel-83B-Base achieves an average score of 73.2 across various multilingual tasks, outperforming Qwen2.5-72B (69.8). AI practitioners can leverage Babel as a strong, inclusive multilingual LLM for tasks requiring broad language support, especially where under-resourced languages are important. |
| Multi-Modal | ABC: Achieving Better Control of Multimodal Embeddings using VLMs (Read more on [arXiv](https://arxiv.org/abs/2503.00329) or [HuggingFace](https://huggingface.co/papers/2503.00329))| Florian Kerschbaum, Benjamin Schneider, wenhu | The paper introduces ABC, a multimodal embedding model that leverages Vision-Language Models (VLMs) to integrate natural language instructions with visual information for improved control over embeddings. The main research objective is to develop a model that can effectively use natural language instructions to modify and control visual representations, addressing the limitations of existing CLIP-based approaches. The key methodology involves a two-stage training process: contrastive pretraining with mined negatives, followed by instruction fine-tuning using synthetic instructions and a lightweight adapter. The model achieves state-of-the-art performance on MSCOCO image-to-text retrieval (R@1 of 69.2) and outperforms other models on the Massive Multimodal Embedding Benchmark (MMEB). This work provides AI practitioners with a method for creating more flexible and controllable multimodal embeddings by deeply integrating visual and natural language inputs. |
| Multi-Modal | Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions (Read more on [arXiv](https://arxiv.org/abs/2503.03278) or [HuggingFace](https://huggingface.co/papers/2503.03278))| Cosmin I. Bercea, Rossella Arcucci, Wenjia Bai, Jun Li, che111 | This paper introduces a novel approach to enhance abnormality grounding in Vision Language Models (VLMs) for medical image analysis. The main research objective is to improve the performance of VLMs in detecting and localizing abnormalities in chest X-rays by leveraging decomposed medical knowledge descriptions. The method focuses on breaking down complex medical concepts into fundamental visual attributes and common visual patterns, promoting stronger alignment between textual descriptions and visual features. The proposed method, trained on a relatively small dataset (1.5% compared to the benchmark) achieves a mAP50 of 25.5% on the VinDr-CXR test set, outperforming larger models. The improved abnormality grounding, especially in zero-shot scenarios, indicates the potential of knowledge-enhanced VLMs to improve model performance and generalization in medical imaging with limited labeled data. |
| Computer Vision | GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control (Read more on [arXiv](https://arxiv.org/abs/2503.03751) or [HuggingFace](https://huggingface.co/papers/2503.03751))| Yifan Lu, Huan Ling, Jiahui Huang, Tianchang Shen, xrenaa | GEN3C is a generative video model with precise camera control and temporal 3D consistency. The paper's main objective is to generate 3D-consistent videos with accurate viewpoint control that can also handle occlusions and missing data. The method utilizes a 3D cache, represented as point clouds derived from depth estimates of input images or previously generated frames. The key result is that in single view video generation, GEN3C achieves a PSNR of 18.66 and SSIM of 0.67 on Tanks-and-Temples, outperforming prior work. The main implication is that it improves the controllability and consistency in video generation using an explicit 3D point-cloud based geometric prior, allowing for applications such as single-view/sparse view video novel view synthesis and driving simulation, which are important for graphics, movie production, and AI simulation. |
| Machine Learning | KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding (Read more on [arXiv](https://arxiv.org/abs/2503.02951) or [HuggingFace](https://huggingface.co/papers/2503.02951))| Radha Poovendran, mingyuanzhou, yyqoni, nlpyang, flydust | KodCode is a new synthetic dataset for training large language models (LLMs) in coding, comprising 447K coding questions with verified solutions and unit tests. The main objective is to address the challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for coding LLMs. The dataset is generated through a three-step pipeline: coding question synthesis, solution and test generation with self-verification, and post-training data synthesis using a reasoning model. Fine-tuning experiments on coding benchmarks demonstrate that KODCODE-tuned models achieve state-of-the-art performance, surpassing models like Qwen2.5-Coder-32B-Instruct with an average score improvement to 61.26 across standard benchmarks. This dataset provides a valuable resource for training and improving code generation capabilities of LLMs, enabling better performance on various coding tasks. |
| Natural Language Processing | CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom (Read more on [arXiv](https://arxiv.org/abs/2503.01836) or [HuggingFace](https://huggingface.co/papers/2503.01836))| Pan Zhou, Wenxuan Shen, Lingfeng Yang, shuaishuaicdp, yisenL | This paper introduces CrowdSelect, a novel framework for selecting high-quality synthetic instruction data for fine-tuning large language models (LLMs) by leveraging Multi-LLM wisdom. The main research objective is to investigate whether multi-dimensional signals derived from multiple LLMs' responses and reward scores can better reflect the various facets of instruction-response pairs for more effective data selection. CrowdSelect incorporates three foundational metrics—Difficulty, Separability, and Stability—which are combined with a clustering-based approach to maintain response diversity. Experiments demonstrate that CrowdSelect achieves state-of-the-art performance, improving performance on Arena-Hard by 4.81% and MT-bench by 11.1% when fine-tuning Llama-3.2-3b-instruct. This implies that AI practitioners can leverage Multi-LLM wisdom for more efficient and effective instruction tuning, reducing computational overhead and improving model performance. |
| Natural Language Processing | QE4PE: Word-level Quality Estimation for Human Post-Editing (Read more on [arXiv](https://arxiv.org/abs/2503.03044) or [HuggingFace](https://huggingface.co/papers/2503.03044))| Malvina Nissim, Ana Guerberof-Arenas, Grzegorz Chrupała, Vilém Zouhar, gsarti | This study, QE4PE, investigates the impact of word-level quality estimation (QE) on human post-editing of machine translation (MT) outputs in a realistic setting. The main research objective is to measure how different word-level QE highlight modalities affect editing quality, productivity, and usability for professional translators. The methodology involves 42 professional translators post-editing texts in two translation directions (English-Italian and English-Dutch) under four highlight conditions: no highlight, oracle, supervised QE, and unsupervised QE. Results indicate that highlight modalities are not predictive of edit times on their own, but a combination of domain, language, and existing differences among translators play significant roles. The study emphasizes the gap between QE accuracy and its usability in professional workflows, showing improvements of the QE systems may not address the usability of the system. |
| Natural Language Processing | Exploring Rewriting Approaches for Different Conversational Tasks (Read more on [arXiv](https://arxiv.org/abs/2502.18860) or [HuggingFace](https://huggingface.co/papers/2502.18860))| Xiang Chen, Mike Rimer, Ryan A. Rossi, Md Mehrab Tanjim, Franck-Dernoncourt | This paper systematically investigates two query rewriting approaches, rewriting and fusion, for conversational AI tasks. The main research objective is to determine whether a single query rewrite module can be universally effective across diverse conversational scenarios or if specialized modules are needed. The methodology involves a generalized framework for query rewriting and evaluating it on text-to-text generation and multimodal generative tasks (text-to-visualization). Results indicate that for conversational text-based Q&A, the query rewrite approach performs best, achieving a cosine similarity of 0.859, while query fusion is superior for conversational data analysis. The main implication is that AI practitioners should choose the appropriate rewriting/fusion strategy depending on the conversational task and underlying use case. |
| Natural Language Processing | Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective (Read more on [arXiv](https://arxiv.org/abs/2503.01933) or [HuggingFace](https://huggingface.co/papers/2503.01933))| KartikAngadi, kruthika, SyedAbdul, RakshitAralimatti | This paper introduces the Shakti Small Language Models (SLMs), a series of compact language models designed for efficient deployment on edge devices. The research objective is to address the challenges of deploying large language models on resource-constrained devices, focusing on computational demands, energy consumption, and data privacy. The methodology combines efficient architectures like Rotary Positional Embeddings and variable grouped query attention, quantization techniques, and responsible AI principles, along with pre-training, supervised fine-tuning and preference alignment methods. The results showcase strong performance on general and domain-specific benchmarks; for example, Shakti-500-Q4 achieves 583.88 tokens per second (TPS) on an NVIDIA L40s GPU. AI practitioners can leverage Shakti SLMs for real-time, privacy-preserving applications on edge devices, expanding the scope of AI deployment beyond cloud-based solutions. |
| Natural Language Processing | Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases (Read more on [arXiv](https://arxiv.org/abs/2502.20317) or [HuggingFace](https://huggingface.co/papers/2502.20317))| Ryan A. Rossi, Haoyu Han, Yongjia Lei, mhalappa, Franck-Dernoncourt | This paper proposes a novel retrieval framework, Mixture of Structural-and-Textual Retrieval (MoR), for answering queries over Text-rich Graph Knowledge Bases (TG-KBs). The main research objective is to effectively retrieve both structural and textual knowledge from TG-KBs, addressing the limitations of existing methods that often treat these knowledge types in isolation. MoR employs a Planning-Reasoning-Organizing framework, which generates textual planning graphs, interweaves structural traversal and textual matching for candidate retrieval, and uses a structure-aware reranker.  Experiments demonstrate MoR's superiority over existing methods, achieving a Hit@1 score of 58.19%, 78.34%, and 75.01% for MAG; thus achieving state-of-the-art performance. AI practitioners can leverage MoR for more effective knowledge retrieval in applications using text-rich graph knowledge bases, especially when harmonizing structural and textual information is crucial. |
| Natural Language Processing | Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.01763) or [HuggingFace](https://huggingface.co/papers/2503.01763))| Shuaiqiang Wang, Pengjie Ren, Lingyong Yan, Yuhan Wang, Zhengliang Shi | This paper introduces TOOLRET, a benchmark for evaluating information retrieval (IR) models in tool retrieval tasks for large language models (LLMs). The main objective is to assess the performance of existing IR models in retrieving relevant tools from large toolsets for LLM agents, a crucial step often simplified in current tool-use benchmarks. The methodology involves collecting 7.6k retrieval tasks and a corpus of 43k tools from existing datasets, and evaluating various IR models, including sparse, dense, and re-ranking methods. Surprisingly, even models performing strongly on conventional IR benchmarks, like the best model (NV-embedd-v1), achieve an nDCG@10 of only 33.83 on TOOLRET, indicating significant challenges in tool retrieval tasks. The main implication is that AI practitioners need to develop or optimize IR models specifically for tool retrieval to improve the performance of tool-using LLMs, as current models are not well-suited for this task. |
| Reinforcement Learning | FLAME: A Federated Learning Benchmark for Robotic Manipulation (Read more on [arXiv](https://arxiv.org/abs/2503.01729) or [HuggingFace](https://huggingface.co/papers/2503.01729))| Danica Kragic, Yuchong Zhang, Miguel Vasco, Alberta Longhini, Santiago Bou Betran | This paper introduces FLAME, a new benchmark for federated learning in robotic manipulation. The main research objective is to evaluate the feasibility and performance of federated learning strategies for training robotic manipulation policies in a distributed, privacy-preserving manner. FLAME utilizes a large-scale dataset of over 160,000 expert demonstrations across diverse simulated environments and four manipulation tasks, integrated into a FLOWER-based federated learning framework.  Experimental results show variability between different federated averaging methods; for example FedAvg achieves 2.64±0.13 RMSE on the Slide Block to Target task, while FedAvgM achieves 13.45±0.26, highlighting that no one method performs best in all tasks. The implication is that this benchmark provides a framework and initial results, pushing for the development of robust, generalized federated learning algorithms specialized for robotic manipulation. |
| Software Engineering | Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection (Read more on [arXiv](https://arxiv.org/abs/2503.01449) or [HuggingFace](https://huggingface.co/papers/2503.01449))| Hung Nguyen, Martin Weyssow, Yindu Su, Chengran Yang, Ting Zhang | This paper presents a comprehensive empirical study evaluating the effectiveness of large language models (LLMs) for multi-language software vulnerability detection (SVD). The main research objective is to investigate how well LLMs perform in detecting vulnerabilities across different programming languages (Python, Java, JavaScript) and how they compare to smaller language models (SLMs) and static application security testing (SAST) tools. The authors evaluate five open-source LLMs using prompt engineering, instruction tuning, and sequence classification fine-tuning, comparing them against five fine-tuned SLMs and two SAST tools using a dataset of 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in JavaScript . The results show that fine-tuning generally improves performance in JavaScript (e.g., up to 587.23% F1-score increase with instruction tuning), while prompt engineering is beneficial for LLMs in Python and Java, although overall performance remains challenging, and SLMs can outpeform LLMs. Thus, AI practitioners should carefully consider model and adaptation strategy based on target programming language and data characteristics and volume to effectively leverage LLMs for SVD. |
| Multi-Modal | CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs (Read more on [arXiv](https://arxiv.org/abs/2503.01378) or [HuggingFace](https://huggingface.co/papers/2503.01378))| Artyom Myshlyaev, Oleg Sautenkov, Muhammad Haris Khan, Valerii Serpiva, Artem Lykov | This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA) model and benchmark (CognitiveDroneBench) for Unmanned Aerial Vehicles (UAVs) designed to perform complex cognitive tasks. The research objective is to develop and evaluate a UAV control system capable of real-time cognitive task solving and reasoning, moving beyond simple navigation or racing tasks. The methodology involves training a 7B-parameter VLA model on a dataset of over 8,000 simulated flight trajectories, incorporating an optional Vision-Language Model (VLM) reasoning module (CognitiveDrone-R1) for enhanced task understanding. CognitiveDrone-R1 achieves a 77.2% overall success rate on the benchmark, significantly outperforming a racing-oriented VLA model (31.3%) and the base CognitiveDrone model (59.6%). This research demonstrates the feasibility and benefits of integrating advanced reasoning into UAV control, offering AI practitioners a new model, dataset, and benchmark for developing and evaluating cognitively capable robotic systems. |
| Reinforcement Learning | Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions (Read more on [arXiv](https://arxiv.org/abs/2503.00502) or [HuggingFace](https://huggingface.co/papers/2503.00502))| Peng Hang, Chen Lv, Chengkai Xu, Jiaqi Liu, FanGShiYuu | This paper introduces an LLM-driven Actor-Reasoner framework to improve the interaction and intent expression capabilities of autonomous vehicles (AVs) in interactions with human-driven vehicles (HVs). The main research objective is to address the challenges of real-time adaptability, handling the heterogeneity of human drivers, and managing the complexity of driving scenarios in AV-HV interactions. The proposed framework utilizes a parallel Actor-Reasoner architecture, where the Reasoner uses a Large Language Model (LLM) for deliberate reasoning and intent communication, and the Actor employs a two-layer memory retrieval mechanism for fast, intuitive action selection based on past experiences. Ablation studies demonstrate that the framework achieves a 94% success rate in complex intersection scenarios, significantly improving upon baseline methods. The system combines real-time, intuitive decision-making with explicit intention communication, offering a significant improvement, and making AVs safer, and more easily integrated with human-driven vehicles. |
| Natural Language Processing | SwiLTra-Bench: The Swiss Legal Translation Benchmark (Read more on [arXiv](https://arxiv.org/abs/2503.01372) or [HuggingFace](https://huggingface.co/papers/2503.01372))| Yingqiang Gao, Sina Ahmadi, Luka Nenadic, Jakob Merane, Joel Niklaus | SwiLTra-Bench is a new benchmark for evaluating machine translation systems on Swiss legal texts, including laws, headnotes, and press releases, across Switzerland's four official languages and English. The authors introduce SwiLTra-Bench and SwiLTra-Judge, and explore the research question of how well existing large language models (LLMs) perform on Swiss legal text translation, both in zero-shot and fine-tuning settings, and how the results correlate with human evaluations. The research systematically evaluates various LLMs, including specialized translation systems and frontier models, using automated metrics and human expert validation, focusing on multilingual translation quality.  The fine-tuned open SLMs quality, still lag behind the best zero-shot prompted frontier models such as Claude-3.5-Sonnet, which scores 80.66 on the GEMBA-MQM metric. By using the SwiLTra-Bench and the SwiLTra-Judge evaluation system, AI practitioners can develop and fine-tune models for robust legal text translation, supporting multilingual governance and potentially broadening access to legal information. |
