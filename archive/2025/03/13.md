

## Papers for 2025-03-13

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | TPDiff: Temporal Pyramid Video Diffusion Model (Read more on [arXiv](https://arxiv.org/abs/2503.09566) or [HuggingFace](https://huggingface.co/papers/2503.09566))| Mike Zheng Shou, Lingmin Ran | TPDiff is a novel framework designed to improve the training and inference efficiency of video diffusion models. The research objective is to address the high computational demands of video diffusion models. The key methodology, temporal pyramid, involves progressively increasing the frame rate during the diffusion process and a stage-wise training framework is designed to train the proposed model. Experiments show the proposed method achieves a 50% reduction in training cost and 1.5x improvement in inference efficiency, while achieving better generation quality in certain metrics. AI practitioners can leverage TPDiff to generate higher-quality videos more efficiently by reducing computational costs and optimizing temporal redundancy. |
| Computer Vision | Reangle-A-Video: 4D Video Generation as Video-to-Video Translation (Read more on [arXiv](https://arxiv.org/abs/2503.09151) or [HuggingFace](https://huggingface.co/papers/2503.09151))| Jong Chul Ye, Suhyeon Lee, hyeonho-jeong-video | Reangle-A-Video is a unified framework for generating synchronized multi-view videos from a single input video. The research objective is to generate multi-view videos of dynamic 4D scenes without relying on multi-view generative priors, and to support both static view transport and dynamic camera control. The method reframes multi-view video generation as video-to-video translation, leveraging image and video diffusion priors through a two-stage process: Multi-View Motion Learning using warped videos and Multi-View Consistent Image-to-Image Translation with cross-view consistency guidance. Extensive experiments show that Reangle-A-Video surpasses existing methods like Generative Camera Dolly and NVS-solver; for example, a FID score of 53.448 and FVD of 2690.9 in the static view transport are reported, outperforming alternative methods. This approach establishes a new, effective methodology for multi-view video generation, offering AI practitioners a way to create 4D videos without needing large-scale 4D datasets. |
| Natural Language Processing | Block Diffusion: Interpolating Between Autoregressive and Diffusion
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.09573) or [HuggingFace](https://huggingface.co/papers/2503.09573))| Zhixuan Qi, Zhihan Yang, Justin T Chiu, Aaron Gokaslan, Marianne Arriola | This paper introduces Block Diffusion Language Models (BD3-LMs), a new class of language models that interpolates between autoregressive and diffusion models. The main research objective is to address the limitations of discrete diffusion models in language modeling, such as fixed-length generation, lack of KV caching, and lower performance compared to autoregressive models. BD3-LMs achieve this by defining an autoregressive distribution over blocks of tokens and using discrete denoising diffusion within each block, combined with custom training algorithms and data-driven noise schedules to reduce gradient variance. The results show that BD3-LMs achieve a new state-of-the-art perplexity among diffusion models on the LM1B dataset (28.23 PPL at block size 4), support arbitrary-length generation, and enable KV caching. For AI practitioners, BD3-LMs offer a promising approach to combine the benefits of diffusion and autoregressive models, potentially leading to more efficient and controllable language generation systems. |
| Computer Vision | RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling (Read more on [arXiv](https://arxiv.org/abs/2503.09601) or [HuggingFace](https://huggingface.co/papers/2503.09601))| Sagie Benaim, Guy Yariv, Itay Chachy | RewardSDS is a novel approach that improves score distillation sampling (SDS) by incorporating a reward-weighted sampling mechanism for tasks such as text-to-3D and text-to-image generation. The main research objective is to achieve fine-grained alignment between generated outputs and user intent, overcoming a key limitation of standard SDS. The key methodology involves weighting noise samples during the SDS process based on alignment scores from a pre-trained reward model, prioritizing gradients from samples yielding high-reward outputs. Evaluations on text-to-image generation using ImageReward showed an increase in LLM-Grader score from 6.74 (SDS Baseline) to 7.19 with RewardSDS. The implication for AI practitioners is that RewardSDS offers a plug-and-play approach for improved alignment and control in various generation tasks using diffusion models. |
| Reinforcement Learning | GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based
  VLM Agent Training (Read more on [arXiv](https://arxiv.org/abs/2503.08525) or [HuggingFace](https://huggingface.co/papers/2503.08525))| Zongqing Lu, Yuanchun Shi, Junliang Xing, Yijun Yang, Tong Wei | This paper introduces Guided Thought Reinforcement (GTR), a framework to prevent "thought collapse" in reinforcement learning (RL)-based training of Vision-Language Model (VLM) agents. The research investigates the problem of thought collapse, where RL training fails to incentivize chain-of-thought reasoning in VLMs, leading to performance degradation.  GTR mitigates this by using an automated corrector model that evaluates and refines the agent's reasoning at each RL step, combining thought correction with RL-based optimization.  Experiments on the 24-points card game demonstrate that GTR achieves 3-5 times higher task success rates compared to SoTA models. The main implication is that process-level guidance, specifically through automated thought correction, significantly improves the performance and generalization of VLM agents in complex visual environments. |
| Natural Language Processing | More Documents, Same Length: Isolating the Challenge of Multiple
  Documents in RAG (Read more on [arXiv](https://arxiv.org/abs/2503.04388) or [HuggingFace](https://huggingface.co/papers/2503.04388))| Gabriel Stanovsky, Michael Hassid, Nir Mazor, Shahar Levy, LihiShalmon | This paper investigates the impact of the number of retrieved documents on the performance of Large Language Models (LLMs) in Retrieval-Augmented Generation (RAG) systems, while controlling for context length. The central research question is how LLM performance is affected by the number of retrieved documents, assuming a fixed input length. The authors constructed custom datasets derived from a multi-hop QA task, varying the number of distractor documents while maintaining a fixed context size and the position of relevant information. Results show that increasing the number of documents can reduce performance by up to 10% for some models (Llama-3.1 and Gemma-2), while Qwen-2 remained largely unaffected. The implication is that RAG systems should consider the number of retrieved documents as a factor affecting performance, independent of context length. |
| Multi-Modal | Motion Anything: Any to Motion Generation (Read more on [arXiv](https://arxiv.org/abs/2503.06955) or [HuggingFace](https://huggingface.co/papers/2503.06955))| Rui Zhao, Danning Li, Wei Mao, Yiran Wang, SteveZeyuZhang | The paper "Motion Anything: Any to Motion Generation" introduces a framework for generating controllable human motion from multimodal conditions such as text, music, or a combination of both. The main research objective is to address limitations in existing conditional motion generation models, specifically their inability to prioritize dynamic frames/body parts and effectively integrate multiple conditioning modalities. The key methodology involves an attention-based mask modeling approach within an autoregressive framework, adaptively encoding multimodal conditions for fine-grained spatial and temporal control. The method achieves a 15% improvement in FID on HumanML3D compared to state-of-the-art methods, and also introduces the new Text-Music-Dance (TMD) dataset. AI practitioners can utilize this framework and dataset for improved controllable and multi-modal human motion generation, which can improve performance and versatility of motion generation, in areas such as, Film Production, AR/VR and embodied AI. |
| Machine Learning | Quantizing Large Language Models for Code Generation: A Differentiated
  Replication (Read more on [arXiv](https://arxiv.org/abs/2503.07103) or [HuggingFace](https://huggingface.co/papers/2503.07103))| Gabriele Bavota, Saima Afrin, Antonio Mastropaolo, mdiipenta, Devy1 | This paper presents a differentiated replication of prior work on quantizing large language models (LLMs) for code generation, exploring more recent models, extreme quantization levels, and diverse calibration datasets. The main research objective is to investigate the impact of low-bit quantization, calibration dataset choices, and model size on the code generation ability of LLMs. The methodology involves quantizing CodeLlama and DeepSeek-Coder models to 8, 4, 3, and 2 bits using AQLM, evaluating performance on MultiPL-E and McEval benchmarks, and experimenting with different calibration datasets.  The primary result is that 4-bit quantization reduces memory footprint by 70% with negligible performance loss, whereas more extreme quantization (2 and 3 bits) results in a manageable loss using specialized calibration data. AI practitioners can deploy very large models and reduce the memory footprint and thus, cost, for LLM-based code generation tools by safely leveraging quantization down to 4 bits. |
| Multi-Modal | VLog: Video-Language Models by Generative Retrieval of Narration
  Vocabulary (Read more on [arXiv](https://arxiv.org/abs/2503.09402) or [HuggingFace](https://huggingface.co/papers/2503.09402))| Mike Zheng Shou, KevinQHLin | VLog is a novel video understanding framework that defines video narrations as vocabulary, going beyond subword vocabularies in existing generative video-language models. The paper introduces VLog to address how a video model can be tailored to meet specific requirements, prioritizing task-specific efficiency over generalist models. VLog features a generative retrieval model, a hierarchical vocabulary derived from video narrations, and a vocabulary update strategy leveraging generative models. Experiments on the VidCap-Eval dataset show VLog achieving a CIDEr score of 87.3 and Recall@1 of 5.0 in the casual retrieval setting, with 20x speedup compare to generative model. VLog suggests AI practitioners could achieve efficient and accurate video narrations via vocabulary-based video understanding, balancing contextual accuracy and computational speed. |
| Natural Language Processing | Cost-Optimal Grouped-Query Attention for Long-Context LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.09579) or [HuggingFace](https://huggingface.co/papers/2503.09579))| Maosong Sun, Zhiyuan Liu, Xu Han, Yutong Wu, chen-yingfa | This paper introduces a cost-optimal Grouped-Query Attention (GQA) mechanism for long-context large language models (LLMs). The main objective is to minimize both the computational and memory costs of LLMs while maximizing their language modeling capabilities, particularly in long-context scenarios. The key methodology involves decoupling the number of attention heads from the hidden dimension, extending existing scaling laws to account for context length and attention head configurations, and empirically establishing the relationship between language modeling loss and these configurations. Results indicate, using a Llama-3.2-1B, that for a 128k context length, using fewer attention heads and a larger model size reduces inference FLOPs and memory by approximately 50% with the same loss. This suggests that common GQA configurations are suboptimal, providing valuable guidance for creating efficient LLMs with longer effective contexts. |
| Computer Vision | Alias-Free Latent Diffusion Models:Improving Fractional Shift
  Equivariance of Diffusion Latent Space (Read more on [arXiv](https://arxiv.org/abs/2503.09419) or [HuggingFace](https://huggingface.co/papers/2503.09419))| Xingang Pan, Shuai Yang, Zeqi Xiao, SingleZombie | This paper introduces Alias-Free Latent Diffusion Models (AF-LDM) to improve the shift-equivariance of diffusion models for more consistent image generation and editing. The research addresses the instability and inconsistency in Latent Diffusion Models (LDMs) when the input noise or latent space undergoes small shifts. The key methodology involves redesigning attention modules to be shift-equivariant, proposing an equivariance loss to suppress feature bandwidth, and using cross-frame attention during both training and inference. The AF-LDM achieves significantly higher shift-equivariance, with an image SPSNR of 28.06 compared to 12.38 for standard LDM on the FFHQ dataset, and is robust to irregular warping. AI practitioners can apply AF-LDM to improve the consistency and stability of applications requiring consistent image/video generation like, image editing and video processing. |
| Natural Language Processing | Self-Taught Self-Correction for Small Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.08681) or [HuggingFace](https://huggingface.co/papers/2503.08681))| Irina Nikishina, Chris Biemann, VityaVitalich | This paper introduces the Self-Taught Self-Correction (STaSC) algorithm, enabling small language models (SLMs) to improve their outputs iteratively through self-generated data without external supervision. The research objective is to investigate the self-correction capabilities of SLMs using iterative fine-tuning on data generated solely by the model itself. The STaSC algorithm extends existing self-correction methods by incorporating flexible design choices like initial answer exploration, correction filtering, and iterative fine-tuning mechanisms. Results on the Natural Questions dataset show that SLMs can learn self-correction and, with the STaSCEIF configuration, the model improved it initial answers, reporting a maximal accuracy(r(Ŷ1)) of 0.392 ± 0.024 using Phi3-mini. This work demonstrates that SLMs can learn self-correction, thereby enhancing inference efficiency and enabling model improvement without reliance on external resources or larger models. |
| Natural Language Processing | MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented
  Generation System (Read more on [arXiv](https://arxiv.org/abs/2503.09600) or [HuggingFace](https://huggingface.co/papers/2503.09600))| Simin Niu, Hanyu Wang, Zhaoxin Fan, Zhiyuan Ji, Robot2050 | This paper introduces MoC, a framework for improving text chunking in Retrieval-Augmented Generation (RAG) systems. The main research objective is to develop a more effective text chunking method that addresses the limitations of traditional and semantic chunking approaches, and to devise evaluation metrics to quantify the quality of chunking. The key methodology involves a granularity-aware Mixture-of-Chunkers (MoC) framework, which uses a three-stage processing mechanism, including a router, meta-chunkers, and a post-processing algorithm, to generate structured chunking regular expressions. Primary results show that the Meta-chunker-1.5B achieves a BLEU-Avg score of 0.2760 and ROUGE-L score of 0.4445 on the CRUD (Single-hop) dataset, outperforming other baseline methods. AI practitioners can utilize MoC framework to improve the performance of RAG systems by enabling better text chunking, which influences the accuracy of retrieval and generated content. |
| Multi-Modal | Multimodal Language Modeling for High-Accuracy Single Cell
  Transcriptomics Analysis and Generation (Read more on [arXiv](https://arxiv.org/abs/2503.09427) or [HuggingFace](https://huggingface.co/papers/2503.09427))| Xiang Wang, Junfeng Fang, Sihang Li, Jiaqi Yang, Yaorui Shi | This paper introduces scMMGPT, a unified pre-trained language model (PLM) for joint cell and text modeling in single-cell analysis. The research objective is to address the limitations of existing PLMs in handling both single-cell RNA sequencing (scRNA-seq) data and free text, overcoming information loss and inadequate pre-training. The methodology integrates state-of-the-art cell and text PLMs, leverages dedicated cross-modal projectors, and undergoes extensive pre-training on a large dataset of 27 million cells. Primary results show scMMGPT achieves an 84% relative improvement in textual discrepancy for cell description generation, and a 20.5% accuracy boost for cell type annotation, compared to baseline. For AI practioners, the implication is that the approach offers a novel framework for effectively fusing cellular and textual, which can enhance performance across various downstream, single-cell multi-modal tasks. |
| Multi-Modal | When Large Vision-Language Model Meets Large Remote Sensing Imagery:
  Coarse-to-Fine Text-Guided Token Pruning (Read more on [arXiv](https://arxiv.org/abs/2503.07588) or [HuggingFace](https://huggingface.co/papers/2503.07588))| Qi Zhu, Kang Wu, Xue Yang, Yingying Zhang, Junwei Luo | This paper proposes a text-guided token pruning method for efficient vision-language understanding of large Remote Sensing Images (RSIs). The main research objective is to balance image detail and computational cost when processing gigapixel RSIs with Large Vision-Language Models (LVLMs). The key methodology introduces a Region Focus Module (RFM) for text-aware region localization and a coarse-to-fine token pruning strategy using a Dynamic Image Pyramid (DIP). The proposed method achieved an average accuracy of 32.16% on the newly proposed LRS-VQA benchmark, outperforming existing high-resolution strategies. The implication is that AI practitioners can more efficiently process large RSIs with improved accuracy by leveraging the proposed dynamic, text-guided token pruning, instead of relying only on pre-defined grid methods. |
| Other | Multi Agent based Medical Assistant for Edge Devices (Read more on [arXiv](https://arxiv.org/abs/2503.05397) or [HuggingFace](https://huggingface.co/papers/2503.05397))| Pragya Sahu, Jagdish Samant, Chinmay Kulkarni, Shivam Akhouri, Sakharam Gawade | This paper introduces an on-device, multi-agent healthcare assistant designed to address privacy, latency, and internet dependency issues in healthcare applications. The main research objective is to develop a system that overcomes the limitations of cloud-based solutions by utilizing smaller, task-specific agents on edge devices. The key methodology involves a multi-agent architecture with specialized models for intelligent diagnosis, appointment booking, emergency services, and other healthcare tasks, integrated with a user-friendly application. The primary result is a fine-tuned planner and caller agents that achieve an average RougeL score of 85.5 for planning and 96.5 for calling, while being lightweight enough for edge deployment. The main implication is that AI practitioners can utilize this approach to develop user-centric healthcare solutions that combine the benefits of on-device systems and multi-agent architectures, enhancing privacy, scalability, and optimized local execution. |
| Computer Vision | Monte Carlo Diffusion for Generalizable Learning-Based RANSAC (Read more on [arXiv](https://arxiv.org/abs/2503.09410) or [HuggingFace](https://huggingface.co/papers/2503.09410))| Tong Zhang, Wei Ke, Chen Zhao, Jiale Wang | This paper introduces a novel diffusion-based training paradigm for learning-based RANSAC to improve its generalization ability to out-of-distribution data. The research objective is to address the limited generalization of existing learning-based RANSAC methods when applied to data generated by different algorithms. The key methodology involves incorporating Monte Carlo sampling into a diffusion process to simulate diverse noisy data distributions, and a multi-stage randomization mechanism to enhance data diversity. Primary results show significant improvements in generalization; for example, on ScanNet, the method improves AUC @20° by 12% on LoFTR when compared to a model trained on SIFT. The implication for AI practitioners is a more robust and generalizable RANSAC approach that can handle variations in input data sources without retraining. |
