

## Papers for 2025-03-21

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | One-Step Residual Shifting Diffusion for Image Super-Resolution via
  Distillation (Read more on [arXiv](https://arxiv.org/abs/2503.13358) or [HuggingFace](https://huggingface.co/papers/2503.13358))| agoxandr, skushneryuk, ngushchin, kekchpek, apryc1 | This paper introduces a new distillation method called Residual Shifting Distillation (RSD) for efficient image super-resolution (SR) using diffusion models. The main research objective is to accelerate diffusion-based SR models while maintaining high perceptual quality and fidelity, and reducing computational costs compared to text-to-image (T2I) based methods. RSD trains a student network to produce images such that a fake ResShift model trained on them coincides with the teacher model, achieving one-step restoration.  The proposed method outperforms the teacher ResShift model and other distillation baselines on several real-world and synthetic datasets, achieving a MUSIQ score of 69.172 on RealSet65. AI practitioners can leverage RSD for fast and efficient SR, achieving a better trade-off between perceptual quality, fidelity, and computational efficiency compared to existing methods. |
| Natural Language Processing | Stop Overthinking: A Survey on Efficient Reasoning for Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2503.16419) or [HuggingFace](https://huggingface.co/papers/2503.16419))| andrewwen, HongyiLiuAI, jy-yuan, JiamuZhang, yangsui | This paper presents a structured survey on efficient reasoning in Large Language Models (LLMs), specifically addressing the "overthinking phenomenon" where models generate unnecessarily lengthy reasoning steps. The main research objective is to investigate and categorize current methods for optimizing reasoning length while preserving the reasoning capabilities of LLMs. The key methodologies include model-based approaches (RL with length rewards, SFT with variable-length CoT data), reasoning output-based approaches (latent representation compression, dynamic reasoning paradigms), and input prompt-based approaches (prompt-guided length control, prompt difficulty routing). The paper highlights that methods like Speculative Rejection can reduce computational overhead by up to 30% (not a specific mentioned metric but implied from evaluations such as presented in section relating to figure 8). AI practitioners can leverage these techniques to reduce computational costs and improve the responsiveness of LLMs in real-world applications, though specific quantitative improvements vary considerably across methods and use cases. |
| Computer Vision | Unleashing Vecset Diffusion Model for Fast Shape Generation (Read more on [arXiv](https://arxiv.org/abs/2503.16302) or [HuggingFace](https://huggingface.co/papers/2503.16302))| Huiwenshi, wangfuyun, cocacola, qikahh, ZeqiangLai | This paper introduces FlashVDM, a framework designed to accelerate 3D shape generation using Vecset Diffusion Models (VDMs). The research focuses on accelerating both the diffusion sampling and VAE decoding processes within VDMs, which are traditionally slow. The proposed methodology includes Progressive Flow Distillation for faster diffusion sampling and a lightning vecset decoder with Adaptive KV Selection, Hierarchical Volume Decoding, and Efficient Network Design for faster VAE decoding. FlashVDM achieves a 45x speedup in VAE decoding and over 32x overall speedup, reducing inference time to approximately 1 second per shape while maintaining comparable performance with state-of-the-art. These accelerations can allow for faster, high quality, 3D assets generation, enabling practitioners to explore interactive applications of 3D generative models. |
| Natural Language Processing | Survey on Evaluation of LLM-based Agents (Read more on [arXiv](https://arxiv.org/abs/2503.16416) or [HuggingFace](https://huggingface.co/papers/2503.16416))| Yilun Zhao, Guy Uziel, Lilach Eden, lihaoxin2020, Asaf-Yehudai | This paper provides a comprehensive survey of evaluation methodologies for LLM-based agents, a paradigm shift in AI enabling autonomous systems. The main objective is to systematically analyze and map the rapidly evolving landscape of agent evaluation, identifying current trends, limitations, and future research directions. The methodology involves analyzing evaluation benchmarks and frameworks across four critical dimensions: fundamental agent capabilities, application-specific benchmarks, benchmarks for generalist agents, and agent evaluation frameworks. Key findings reveal a shift towards more realistic and challenging evaluations with continuously updated benchmarks, yet gaps remain, particularly in assessing cost-efficiency, safety, and robustness. The main implication for AI practitioners is the need for developing fine-grained, scalable, and safety-conscious evaluation methods to support the responsible development of LLM-based agents in real-world applications. |
| Computer Vision | DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2503.14487) or [HuggingFace](https://huggingface.co/papers/2503.14487))| Mingwu Zheng, Xintao Wang, Haotian Yang, Ziyang Yuan, MingleiShi | This paper introduces DiffMoE, a novel Mixture-of-Experts (MoE) based architecture for diffusion transformers that improves performance and scalability in visual generation tasks. The main research objective is to address the limitations of existing MoE approaches in diffusion models, particularly their restricted token accessibility and fixed computational patterns. DiffMoE incorporates a batch-level global token pool and a capacity predictor to enable dynamic token selection and global token interaction during training, promoting specialized expert behavior.  DiffMoE achieves state-of-the-art performance on the ImageNet benchmark with a FID50K score of 2.30/2.13 (DDPM/Flow, cfg=1.5) outperforming dense models and achieving comparable performance while being more parameter efficient. AI practitioners can leverage DiffMoE's improved scaling efficiency and performance for various diffusion model applications, particularly in class-conditional and text-to-image generation. |
| Computer Vision | Scale-wise Distillation of Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2503.16397) or [HuggingFace](https://huggingface.co/papers/2503.16397))| Dmitry Baranchuk, Artem Babenko, Denis Kuznedelev, Nikita Starodubcev | This paper presents Scale-wise Distillation (SwD), a novel framework for distilling diffusion models that generates images by progressively increasing spatial resolution during the sampling process. The main research objective is to improve the efficiency of diffusion models by performing sampling steps at smaller scales without compromising generation quality. The key methodology integrates scale-wise generation into existing diffusion distillation methods based on distribution matching and introduces a novel patch loss for finer-grained distribution similarity.  Experimental results show that SwD approaches the inference times of two full-resolution steps and significantly outperforms counterparts under the same computational budget, evidenced by a FID score of 13.6 on the MJHQ dataset using a 6-step scale-wise configuration. The main implication is that AI practitioners can achieve significant speed improvements in diffusion-based image generation by operating at lower resolutions during early sampling stages. |
| Multi-Modal | Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning (Read more on [arXiv](https://arxiv.org/abs/2503.15558) or [HuggingFace](https://huggingface.co/papers/2503.15558))| Hannah Brandon, Alisson Azzolini, NVIDIA, zhuoliny, fferroni | This paper introduces Cosmos-Reason1, a family of multimodal large language models designed for physical AI reasoning, encompassing both physical common sense and embodied decision-making. The main research objective is to develop models capable of understanding the physical world from video input and generating appropriate embodied decisions through long chain-of-thought reasoning. The methodology involves defining ontologies for physical common sense and embodied reasoning, curating large-scale multimodal datasets, and training models in four stages: vision pre-training, general supervised fine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL).  Physical AI SFT improves the backbone VLM's performance on proposed benchmarks by over 10%, and subsequent Physical AI RL boosts accuracy by a further 8.2%.  The results imply that creating specialized, domain-specific data and using reinforcement learning with rule-based verifiable rewards can significantly enhance models' physical reasoning capabilities, important steps for building advanced Physical AI systems. |
| Natural Language Processing | MathFusion: Enhancing Mathematic Problem-solving of LLM through
  Instruction Fusion (Read more on [arXiv](https://arxiv.org/abs/2503.16212) or [HuggingFace](https://huggingface.co/papers/2503.16212))| Honglin Lin, Yu Li, Zhuoshi Pan, Lijun Wu, Qizhi Pei | MathFusion is a novel framework designed to enhance the mathematical reasoning capabilities of Large Language Models (LLMs) through cross-problem instruction synthesis. The main objective is to improve LLMs' ability to solve complex, multi-step mathematical problems by leveraging the relational structures inherent in mathematical knowledge. MathFusion implements this through three fusion strategies: sequential, parallel, and conditional fusion, which generate new training data by combining existing problems. Experimental results demonstrate that MathFusion boosts performance by 18.0 points in accuracy across diverse benchmarks, using only 45K additional synthetic instructions. This approach offers AI practitioners a data-efficient method to substantially improve mathematical reasoning in LLMs. |
| Computer Vision | InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity (Read more on [arXiv](https://arxiv.org/abs/2503.16418) or [HuggingFace](https://huggingface.co/papers/2503.16418))| Hao Kang, Zichuan Liu, Yumin Jia, Qing Yan, Liming Jiang | InfiniteYou (InfU) is a novel framework for flexible and high-fidelity identity-preserved image generation using Diffusion Transformers (DiTs). The research aims to address limitations of existing methods, such as insufficient identity similarity, poor text-image alignment, and low generation quality, especially when adapting DiTs for this task. The key methodology involves InfuseNet, which injects identity features into the DiT base model via residual connections, combined with a multi-stage training strategy using synthetic single-person-multiple-sample (SPMS) data. The proposed InfU outperforms baselines across all dimensions, achieving an ID Loss of 0.209, demonstrating superior identity preservation. AI practitioners can leverage InfU's plug-and-play design and compatibility with existing methods to enhance identity-preserved image generation with improved quality and text-image alignment. |
| Multi-Modal | Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2503.16257) or [HuggingFace](https://huggingface.co/papers/2503.16257))| Huan Wang, Can Qin, Yang Sui, Haoxuan You, KD-TAO | This paper introduces VidKV, a novel plug-and-play 1.x-bit KV cache quantization method for Video Large Language Models (VideoLLMs) to address the inference bottleneck caused by substantial KV cache memory consumption. The research investigates the limits of low-bit KV cache quantization in VideoLLMs, particularly below 2 bits. VidKV employs a mixed-precision quantization scheme: for the key cache, 2-bit quantization is used for anomalous channels and 1-bit quantization with FFT for normal channels; for the value cache, 1.58-bit quantization is used, with optional token protection for semantically salient tokens.  Empirically, VidKV compresses the KV cache to 1.5-bit and 1.58-bit precision with almost no performance drop compared to FP16 on benchmarks like VideoDC and MovieChat, and achieves an accuracy of 2.92 on Qwen2.5-VL-7B. These optimizations will allow AI practitioners to deploy and scale VideoLLMs much more efficiently. |
| Multi-Modal | JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play
  Visual Games with Keyboards and Mouse (Read more on [arXiv](https://arxiv.org/abs/2503.16365) or [HuggingFace](https://huggingface.co/papers/2503.16365))| Yitao Liang, Xiaojian Ma, Kaichen He, Zihao Wang, Muyao Li | The paper introduces JARVIS-VLA, a novel Vision-Language-Action (VLA) model that leverages visual language post-training to enhance decision-making in open-world environments like Minecraft. The research aims to improve VLA models' understanding of environments and task-related knowledge, in addition to action generation, for enhanced generalization and flexibility. The core methodology, ActVLP, involves a three-stage post-training process: refining language models with world knowledge, aligning vision and language, and then performing imitation learning on trajectory data. JARVIS-VLA achieved a 70% average success rate across diverse Minecraft tasks, a 40% improvement over baseline agents, and outperforming traditional imitation learning-based policies. This suggests that integrating visual-language tasks into the training pipeline of VLA models enhances their decision-making and action generation capabilities, offering practitioners a new approach for developing more robust agents. |
| Natural Language Processing | CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners (Read more on [arXiv](https://arxiv.org/abs/2503.16356) or [HuggingFace](https://huggingface.co/papers/2503.16356))| Shumin Deng, Jia-Chen Gu, Jizhan Fang, Yunzhi Yao, Ningyu | This paper introduces CaKE (Circuit-aware Knowledge Editing), a novel method to improve the generalization of knowledge updates in large language models (LLMs) for multi-hop reasoning tasks. The research addresses the issue that existing knowledge editing methods, while effective on isolated facts, fail to consistently propagate updates to downstream reasoning tasks. CaKE leverages strategically curated data, guided by a circuits-based analysis, to stimulate the model to develop appropriate reasoning circuits that utilize the modified knowledge. Experimental results demonstrate that CaKE improves multi-hop reasoning accuracy by an average of 20% on the MQUAKE dataset compared to existing knowledge editing methods.  AI practitioners can apply CaKE to update LLMs more effectively, improving their performance on complex reasoning tasks that rely on updated information. |
| Computer Vision | Ultra-Resolution Adaptation with Ease (Read more on [arXiv](https://arxiv.org/abs/2503.16322) or [HuggingFace](https://huggingface.co/papers/2503.16322))| Xinchao Wang, Zhenxiong Tan, Songhua Liu, Ruonan Yu | This paper proposes URAE, a set of guidelines for adapting text-to-image diffusion models to ultra-high resolutions (e.g., 4K) with limited data and computational resources. The main research question is how to make ultra-resolution adaptation of text-to-image models easier and more accessible. The key methodology, URAE, involves leveraging synthetic data for improved training convergence, tuning minor components of weight matrices for parameter efficiency, and disabling classifier-free guidance during adaptation for guidance-distilled models. The results show that URAE achieves comparable 2K-generation performance to state-of-the-art closed-source models like FLUX1.1 [Pro] Ultra (FID of 29.44 on HPD prompts), and sets new benchmarks for 4K resolution generation. AI practitioners can adapt existing models to higher resolutions using less data and computation by applying the URAE guidelines. |
| Computer Vision | Expert Race: A Flexible Routing Strategy for Scaling Diffusion
  Transformer with Mixture of Experts (Read more on [arXiv](https://arxiv.org/abs/2503.16057) or [HuggingFace](https://huggingface.co/papers/2503.16057))| Xun Zhou, Defa Zhu, Ziyu Wang, FetchFortune, yyk-wew | This paper introduces Race-DiT, a novel Mixture of Experts (MoE) model for diffusion transformers in visual generation, featuring a flexible routing strategy called Expert Race. The main research objective is to enhance model scalability and performance in diffusion transformers by improving routing strategies. The key methodology involves allowing tokens and experts to compete and selecting the top candidates, along with per-layer regularization and router similarity loss for improved training.  Experimental results on ImageNet show that Race-DiT achieves a FID score of 8.03, CMMD of .4587 and CLIP score of 23.09, showcasing significant performance gains compared to baseline methods. AI practitioners can utilize this approach to scale diffusion models more effectively, achieving better performance with comparable computational resources. |
| Computer Vision | MagicMotion: Controllable Video Generation with Dense-to-Sparse
  Trajectory Guidance (Read more on [arXiv](https://arxiv.org/abs/2503.16421) or [HuggingFace](https://huggingface.co/papers/2503.16421))| Qi Dai, Hui Zhang, Rui Wang, Zhen Xing, quanhaol | MagicMotion is a novel image-to-video generation framework that enables trajectory control through dense-to-sparse conditions for generating high-quality videos. The research addresses the limitations of existing trajectory-controllable video generation methods, which struggle with complex object movements and multi-object motion control. MagicMotion utilizes a Trajectory ControlNet architecture, progressive training strategy, and a latent segment loss, enabling it to control object motion via masks, bounding boxes, and sparse boxes. The model, evaluated on the MagicBench dataset (also introduced in this paper), achieves a Mask_IoU of 91.57% and a Box_IoU of 87.75% in the first stage, showcasing superior trajectory adherence and object consistency compared to prior methods. This framework allows AI practitioners to produce high-quality, controllable videos with precise object motion, broadening the applicability of video generation models in various real-world scenarios. |
| Natural Language Processing | Why Do Multi-Agent LLM Systems Fail? (Read more on [arXiv](https://arxiv.org/abs/2503.13657) or [HuggingFace](https://huggingface.co/papers/2503.13657))| Bhavya Chopra, Lakshya A. Agrawal, Shuyi Yang, Melissa Z. Pan, Mert Cemri | This paper presents a comprehensive study of failure modes in Multi-Agent LLM Systems (MAS). The main research objective is to understand why MASs fail and to develop a taxonomy of these failures. The methodology involves analyzing over 150 execution traces from five popular MAS frameworks using Grounded Theory, with six expert human annotators and inter-annotator agreement studies achieving a Cohen's Kappa score of 0.88. Primary results show the identification of 14 unique failure modes organized into 3 categories. This taxonomy, named MASFT, provides a structured framework for understanding and designing more robust MAS, offering important implications for practitioners seeking reliability in multi-agent systems. |
| Computer Vision | 1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering (Read more on [arXiv](https://arxiv.org/abs/2503.16422) or [HuggingFace](https://huggingface.co/papers/2503.16422))| Xinchao Wang, Xingyi Yang, Qiuhong Shen, nopyyh | This paper introduces 4DGS-1K, a novel, compact, and high-speed representation for dynamic scene rendering based on 4D Gaussian Splatting. The research addresses the high storage requirements and slow rendering speeds of existing 4D Gaussian Splatting (4DGS) methods. The key methodology involves a two-step pruning approach: (1) pruning short-lifespan Gaussians using a spatial-temporal variation score, and (2) filtering inactive Gaussians using a key-frame based temporal filter.  The proposed method achieves a 41x reduction in storage and 9x faster rasterization speed on complex dynamic scenes compared to vanilla 4DGS, enabling rendering at over 1000 FPS. AI practitioners can leverage 4DGS-1K for efficient and high-fidelity dynamic scene modeling, particularly in real-time applications. |
| Multi-Modal | M3: 3D-Spatial MultiModal Memory (Read more on [arXiv](https://arxiv.org/abs/2503.16413) or [HuggingFace](https://huggingface.co/papers/2503.16413))| Jianglong Ye, Xuanbin Peng, Ri-Zhao Qiu, Yuchen Song, Xueyan Zou | This paper introduces M3, a 3D spatial multimodal memory system designed to retain information about medium-sized static scenes using video sources for visual perception. The research objective is to create a memory system capable of storing multi-granular information about a scene and addressing computational constraints and information loss present in previous feature splatting works. M3 integrates 3D Gaussian Splatting with foundation models, utilizing principal scene components and Gaussian memory attention for efficient training and inference.  Quantitative evaluations show that M3 outperforms previous methods such as F-3DGS in feature similarity on CLIP (Cosine similarity 0.3260 vs 0.3435 on Drjohnson Dataset), and downstream tasks, demonstrating better memorization and generalizability. AI practitioners can leverage M3 for enhanced spatial reasoning and multimodal understanding in applications such as robotics, potentially using a distilled format for edge devices. |
| Machine Learning | XAttention: Block Sparse Attention with Antidiagonal Scoring (Read more on [arXiv](https://arxiv.org/abs/2503.16428) or [HuggingFace](https://huggingface.co/papers/2503.16428))| Song Han, Junxian Guo, Guangxuan Xiao, Ruyi Xu, songhan | XAttention is a plug-and-play framework designed to accelerate long-context inference in Transformer models by using block-sparse attention. The paper introduces XAttention and asks if a block-sparse attention mechanism can dramatically accelerate long-context Transformers without compromising accuracy. The key innovation is using the sum of antidiagonal values in the attention matrix as a proxy for block importance, enabling efficient identification and pruning of non-essential blocks. XAttention achieves accuracy comparable to full attention on benchmarks like RULER, LongBench, VideoMME, and VBench, with up to 13.5x acceleration in attention computation during pre-filling. This method enables AI practitioners to deploy more scalable and efficient long-context Transformer models, particularly beneficial for multimodal applications. |
| Machine Learning | Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on
  Compressed Spatial Tokens (Read more on [arXiv](https://arxiv.org/abs/2503.16278) or [HuggingFace](https://huggingface.co/papers/2503.16278))| Zhifeng Gao, Lin Yao, Haowei Lin, Shuqi Lu, guolinke | Uni-3DAR is a unified framework for 3D structural generation and understanding (3D GU) that integrates tasks via autoregressive prediction. The main research objective is to bridge the gap between 3D structural generation and understanding tasks, which have largely evolved independently, and unify them within an autoregressive framework. The core methodology involves a novel hierarchical tokenization that compresses 3D space using an octree, combined with a two-level subtree compression strategy and a masked next-token prediction mechanism.  Uni-3DAR surpasses previous state-of-the-art diffusion models in microscopic 3D GU tasks, achieving up to 256% relative improvement on PXRD-guided crystal structure prediction and delivering inference speeds up to 21.8x faster. For AI practitioners, this implies a more efficient and unified approach for various 3D understanding and generation tasks, applicable across diverse domains like molecules, proteins, and crystals. |
| Multi-Modal | CLS-RL: Image Classification with Rule-Based Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2503.16188) or [HuggingFace](https://huggingface.co/papers/2503.16188))| Kaipeng Zhang, Jike Zhong, Ming Li, yuxianglai117, stzhao | This paper introduces CLS-RL, a novel rule-based reinforcement learning approach for image classification using Multimodal Large Language Models (MLLMs). The research explores how to effectively fine-tune MLLMs for image classification in a few-shot setting, addressing catastrophic forgetting issues observed with supervised fine-tuning (SFT). CLS-RL uses verifiable signals (class names) as rewards and formats the reward to encourage a 'thinking' process, optimizing MLLMs through policy gradients.  Experiments on eleven datasets demonstrate that CLS-RL outperforms SFT in most datasets, achieving, for example, an 80.15% harmonic mean accuracy on base-to-new generalization, compared to SFT's 69.03%. The findings suggest that RL-based fine-tuning is a promising approach for enhancing MLLM's image classification capabilities, and surprisingly, reducing reasoning steps can even improve performance. |
| Computer Vision | LHM: Large Animatable Human Reconstruction Model from a Single Image in
  Seconds (Read more on [arXiv](https://arxiv.org/abs/2503.10625) or [HuggingFace](https://huggingface.co/papers/2503.10625))| Weichao Shen, Peihao Li, Xiaodong Gu, Lingteng Qiu, DyrusQZ | This paper introduces LHM, a feed-forward model for reconstructing animatable 3D human avatars from single images in seconds. The main objective is to create a generalizable model capable of producing high-fidelity, animatable 3D humans from a single image, overcoming the limitations of existing methods in terms of speed, generalization, and animation capability.  The methodology leverages a multimodal transformer architecture to fuse 3D geometric features and 2D image features, representing the avatar using 3D Gaussian Splatting, and incorporates a head feature pyramid encoding for enhanced facial detail preservation. The model, trained primarily using widely available video data, achieves a PSNR of 25.183, SSIM of 0.951, LPIPS of 0.029 and Face consistency of 0.035 on a synthetic test dataset. AI practitioners can use this model for efficient and high-quality animatable 3D human avatar generation from single images, enabling applications in AR/VR and virtual content creation. |
| Computer Vision | Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video
  Diffusion (Read more on [arXiv](https://arxiv.org/abs/2503.15851) or [HuggingFace](https://huggingface.co/papers/2503.15851))| Chua Tat-Seng, Fan Hehe, Ma Fan, zhenglin | This paper introduces Zero-1-to-A, a novel method for generating high-fidelity, animatable 4D head avatars from a single image using a pre-trained video diffusion model. The research aims to overcome the limitations of existing methods that rely on extensive training data or suffer from spatial and temporal inconsistencies when distilling from video diffusion models. Zero-1-to-A employs a symbiotic generation approach that iteratively constructs a consistent video dataset and optimizes avatars, combined with a progressive learning strategy that sequences learning from simple to complex scenarios. Experiments demonstrate that Zero-1-to-A significantly improves fidelity, animation quality, and rendering speed achieving a 0.1 and 0.05 improvement in ViT-L/14 and ViT-B/32 evaluations respectively,compared to state-of-the-art methods. The main implication is that Zero-1-to-A offers a data-efficient solution with higher accuracy for creating lifelike 4D avatars without the need of real or synthetic training datasets that are expensive and difficult to collect. |
| Machine Learning | Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling (Read more on [arXiv](https://arxiv.org/abs/2503.15567) or [HuggingFace](https://huggingface.co/papers/2503.15567))| Kenji Kawaguchi, Sihang Li, Yi Zhao, Zhiyuan Liu, Yanchen Luo | This paper introduces UAE-3D and UDM-3D, a new approach for 3D molecule generation using a unified latent space. The main research question is whether a unified generative model can seamlessly integrate all modalities of 3D molecule generation (atom types, chemical bonds, and 3D coordinates). The key methodology is a multi-modal variational auto-encoder (UAE-3D) that compresses 3D molecules into a unified latent space, combined with a Diffusion Transformer (UDM-3D) for latent diffusion modeling. Results show that UAE-3D achieves near-zero reconstruction error (100% atom/bond accuracy and 2E-4 coordinate RMSD) and establishes new benchmarks in both de novo and conditional 3D molecule generation. For AI practitioners, this approach offers a more efficient and accurate method for 3D molecule generation by simplifying the handling of multi-modality and equivariance. |
| Computer Vision | Tokenize Image as a Set (Read more on [arXiv](https://arxiv.org/abs/2503.16425) or [HuggingFace](https://huggingface.co/papers/2503.16425))| Shuyang Gu, Han Hu, Mengde Xu, Zigang Geng | This paper introduces TokenSet, a novel paradigm for image generation that uses set-based tokenization and distribution modeling, departing from traditional sequential approaches. The central research objective is to investigate whether dynamic allocation of coding capacity based on regional semantic complexity improves image generation compared to uniform spatial compression. The key methodology involves representing images as unordered token sets, employing a dual transformation mechanism to convert sets into fixed-length sequences, and proposing Fixed-Sum Discrete Diffusion for set distribution modeling. The TokenSet approach achieved a reconstruction rFID of 2.74 and a generation gFID of 5.56 on the ImageNet dataset at 256x256 resolution, demonstrating improved performance over baselines. The results indicate that TokenSet representation enables more robust and semantically aware image generation, offering AI practitioners a new approach to visual generation beyond traditional sequential token paradigms. |
| Computer Vision | NuiScene: Exploring Efficient Generation of Unbounded Outdoor Scenes (Read more on [arXiv](https://arxiv.org/abs/2503.16375) or [HuggingFace](https://huggingface.co/papers/2503.16375))| Angel X. Chang, Qinghong Han, rexleeppp | This paper introduces NuiScene, a method for efficient and unbounded outdoor scene generation, addressing challenges specific to outdoor environments. The main research objective is to explore how to generate expansive outdoor scenes with varying heights and diverse styles efficiently. The key methodology involves compressing scene chunks into vector sets using a VAE and training a diffusion model for explicit outpainting, enabling faster inference. The vector set diffusion model achieved an FPD of 0.571 and a KPD of 0.951, outperforming triplane-based models, and facilitates blending different environment styles like rural houses and skyscrapers. The framework shows promising capability for content generation by allowing joint training of heterogenous scene data for future applications. |
| Natural Language Processing | Fin-R1: A Large Language Model for Financial Reasoning through
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2503.16252) or [HuggingFace](https://huggingface.co/papers/2503.16252))| Jinyi Niu, Lingfeng Zeng, Fangqi Lou, Xin Guo, Zhaowei Liu | This paper introduces Fin-R1, a large language model specifically designed for financial reasoning. The main objective is to address challenges in applying LLMs to complex financial problems, such as fragmented data, uncontrollable reasoning logic, and weak business generalization. The researchers built a high-quality dataset (Fin-R1-Data) and employed a two-stage training framework involving Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). Fin-R1, with 7 billion parameters, achieves an average score of 75.2 on financial benchmarks, outperforming other large-scale reasoning LLMs and achieving state-of-the-art scores in ConvFinQA (85.0) and FinQA (76.0). This demonstrates the model's potential to improve accuracy and interpretability in financial AI applications, offering a lightweight and effective solution for financial reasoning tasks. |
| Computer Vision | SALT: Singular Value Adaptation with Low-Rank Transformation (Read more on [arXiv](https://arxiv.org/abs/2503.16055) or [HuggingFace](https://huggingface.co/papers/2503.16055))| Mohammad Yaqub, Hu Wang, Mohammed Elseiagy, Abdelrahman Elsayed, Sarim-Hash | This paper introduces SALT, a novel parameter-efficient fine-tuning (PEFT) method for adapting the Segment Anything Model (SAM) to medical image segmentation. The main objective is to address the limitations of existing PEFT methods like LoRA and SVD-based approaches, which struggle to balance parameter efficiency and adaptation capability in the medical domain. SALT selectively adapts the most influential singular values of weight matrices using trainable scale and shift parameters, while applying low-rank updates to the remaining subspace, synergizing SVD and LoRA. Evaluated on five medical datasets, SALT outperforms state-of-the-art PEFT methods by 2% to 5% in Dice score with only 3.9% trainable parameters. This suggests that AI practioners can leverage SALT for efficient and effective adaptation of foundation models to specialized domains like medical imaging, achieving robust performance even with limited data. |
| Computer Vision | MotionStreamer: Streaming Motion Generation via Diffusion-based
  Autoregressive Model in Causal Latent Space (Read more on [arXiv](https://arxiv.org/abs/2503.15451) or [HuggingFace](https://huggingface.co/papers/2503.15451))| Liang Pan, Ke Fan, Huaijin Pi, Shunlin Lu, lxxiao | MotionStreamer is a novel framework for text-conditioned streaming human motion generation that produces realistic and diverse motions from sequential text prompts. The main objective is to generate human motion incrementally while dynamically adapting to online text inputs and maintaining semantic coherence. The key methodology is the integration of a diffusion head with an autoregressive model to predict continuous motion latents, along with a causal motion compressor (Causal TAE) for online decoding. The method achieves a FID score of 10.724 on the HumanML3D test set, outperforming existing approaches. AI practitioners can use MotionStreamer for various real-time applications, like video games and robotics, enabling online multi-round generation and dynamic motion composition. |
| Computer Vision | Make Your Training Flexible: Towards Deployment-Efficient Video Models (Read more on [arXiv](https://arxiv.org/abs/2503.14237) or [HuggingFace](https://huggingface.co/papers/2503.14237))| Yi Wang, Xiangyu Zeng, Tianxiang Jiang, Kunchang Li, Chenting Wang | This paper introduces FluxViT, a novel framework for training deployment-efficient video models that can adapt to varying computational budgets. The main research objective is to optimize the trade-off between accuracy and computation in video models by addressing the inherent redundancy in video data. The key methodology, termed Flux, leverages flexible sampling and token selection to maximize input information across different budgets, integrated with a new data augmentation technique.  Results show that FluxViT-S outperforms the previous state-of-the-art small-scale model, InternVideo2-S, by 2.2% on K400 under standard computation constraints and achieves comparable performance to much larger models at reduced cost. AI practitioners can use this framework to train video models that are both more robust and adaptable to various deployment scenarios with different computational constraints. |
| Computer Vision | MagicID: Hybrid Preference Optimization for ID-Consistent and
  Dynamic-Preserved Video Customization (Read more on [arXiv](https://arxiv.org/abs/2503.12689) or [HuggingFace](https://huggingface.co/papers/2503.12689))| Hongwei Yi, Tianyang Wang, Xi Xiao, Lifan Jiang, Hengjia Li | MagicID is a novel framework for video identity customization that aims to generate high-fidelity videos that maintain consistent identity and exhibit significant dynamics based on user reference images. The main objective is to overcome identity degradation over extended video lengths and reduced dynamics during training, challenges faced by existing approaches relying on self-reconstruction with static images. The key methodology involves constructing pairwise preference video data with explicit identity and dynamic rewards and using a hybrid sampling strategy for preference learning.  The primary result is that MagicID achieves a Face Similarity score of 0.600, significantly outperforming existing methods, while also improving dynamic degree and temporal consistency. AI practitioners can utilize MagicID to develop improved video customization tools with better ID preservation and more dynamic output, particularly in areas like film and video production. |
| Reinforcement Learning | Reinforcement Learning for Reasoning in Small LLMs: What Works and What
  Doesn't (Read more on [arXiv](https://arxiv.org/abs/2503.16219) or [HuggingFace](https://huggingface.co/papers/2503.16219))| Chris Ngo, quyanh | This study investigates the effectiveness of reinforcement learning (RL) for enhancing reasoning capabilities in small language models (LLMs) under resource-constrained settings. The main research objective is to determine how small LLMs behave when fine-tuned with RL under strict computational limits and whether their reasoning performance can be improved using an approach similar to DeepSeek-R1. The key methodology involves adapting the Group Relative Policy Optimization (GRPO) algorithm and curating a compact, high-quality mathematical reasoning dataset for training a 1.5-billion-parameter model.  A primary result is that the model's AIME24 accuracy reached 46.7% with a $42 training cost, surpassing the o1-preview. AI practitioners can leverage these findings to develop cost-effective, reasoning-capable LLMs in resource-limited environments, although optimization instability and length constraints should be carefully considered. |
| Computer Vision | Improving Autoregressive Image Generation through Coarse-to-Fine Token
  Prediction (Read more on [arXiv](https://arxiv.org/abs/2503.16194) or [HuggingFace](https://huggingface.co/papers/2503.16194))| Michael Qizhe Shieh, Kaipeng Zhang, Ziyao Guo | This paper proposes a novel coarse-to-fine token prediction framework to improve autoregressive image generation. The main research objective is to address the vocabulary redundancy issue in large codebooks used by Vector Quantized Variational Autoencoders (VQ-VAE), which complicates autoregressive modeling. The proposed method first clusters similar tokens using k-means, then employs a two-stage prediction: an autoregressive model predicts coarse cluster labels, and an auxiliary model predicts fine-grained token labels conditioned on the coarse labels. Experiments on ImageNet demonstrate that the method achieves an average improvement of 59 points in Inception Score compared to baselines, as well as faster sampling speeds. The core implication is that AI practitioners can leverage this technique to achieve higher-quality and more efficient autoregressive image generation by effectively managing vocabulary size. |
| Natural Language Processing | Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging
  Fabricated Claims with Humorous Content (Read more on [arXiv](https://arxiv.org/abs/2503.16031) or [HuggingFace](https://huggingface.co/papers/2503.16031))| Sunil Saumya, Shankar Biradar, UVSKKR | This paper introduces the Deceptive Humor Dataset (DHD), a novel multilingual benchmark for studying humor derived from fabricated claims and misinformation. The main research objective is to understand how humor intertwines with deception and to establish a foundation for analyzing humor in deceptive contexts. The dataset was synthetically generated using ChatGPT-4o, comprising 9,000 humor-infused comments in multiple languages and code-mixed variants, labeled with satire levels and humor categories.  The best performing model for Satire Level Classification was mBART(51.00 accuracy), and for Humor Attribute classification, the best model was BERT (40.44 accuracy), although the performance indicates considerable room for improvement.  AI practitioners can use this dataset to develop and benchmark models capable of detecting and analyzing deceptive humor, which has implications for addressing the spread of misinformation. |
| Computer Vision | VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting
  Generation with Flexible Pose and Multi-View Joint Modeling (Read more on [arXiv](https://arxiv.org/abs/2503.15855) or [HuggingFace](https://huggingface.co/papers/2503.15855))| Hyungjin Chung, Byung-Hoon Kim, Hyelin Nam, Byeongjun Park, Hyojun Go | VideoRFSplat is a direct text-to-3D model that generates realistic 3D Gaussian Splatting (3DGS) for unbounded real-world scenes leveraging video generation techniques. The main research objective is to develop a model capable of generating diverse camera poses and unbounded spatial extents of real-world scenes from text prompts, while ensuring generalization and avoiding instability issues common in previous methods. The proposed method employs a dual-stream architecture with a dedicated pose generation model attached to a pre-trained video generation model, combined with an asynchronous sampling strategy for joint modeling of multi-view images and camera poses.  VideoRFSplat achieves a FID of 30.33 and CLIPScore of 33.0 on the MVImgNet, outperforming existing methods that depend on post-hoc refinement via score distillation sampling. This provides AI practitioners with a new approach for direct 3D scene generation without requiring expensive refinement steps, improving the efficiency and quality for the described task. |
| Machine Learning | BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space
  Complexity? (Read more on [arXiv](https://arxiv.org/abs/2503.15242) or [HuggingFace](https://huggingface.co/papers/2503.15242))| Gabriel Synnaeve, Benoit Sagot, Baptiste Roziere, pierrechambon | This paper introduces BIGO(BENCH), a new benchmark for evaluating the ability of large language models (LLMs) to generate code with specified time and space complexity constraints. The main research objective is to assess how well LLMs can understand and control computational complexity, a critical skill for software development that is often overlooked in current evaluations. The methodology involves a novel coding benchmark with tooling to infer algorithmic complexity from Python function profiling, including a dataset of 3,105 coding problems and 1,190,250 solutions annotated with complexity labels.  Evaluation of state-of-the-art LLMs reveals that while reasoning models like DeepSeek R1 achieve high accuracy (over 70%) on programming contests, they struggle significantly with complexity constraints, achieving only 4.8% accuracy (pass@1) on generating solutions under specific time complexity constraints. The main implication is that AI practitioners need to develop new benchmarks that go beyond simple code generation to cover higher order reasoning around efficiency to advance reasoning capabilities of LLMs in coding tasks. |
| Multi-Modal | See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language
  Balance to Mitigate Dominant Modality Bias (Read more on [arXiv](https://arxiv.org/abs/2503.13834) or [HuggingFace](https://huggingface.co/papers/2503.13834))| YoungBin Kim, Juhwan Choi, Eunju Lee, MiHyeon Kim, JuneHyoung Kwon | This paper introduces BALGRAD, a novel framework to mitigate dominant modality bias in vision-language (VL) models. The main objective is to address the issue where VL models over-rely on a specific modality, particularly when one modality is impaired. The methodology includes inter-modality gradient reweighting and inter-task gradient projection, adjusting KL divergence based on each modality's contribution and aligning task directions. Experiments on UPMC Food-101, Hateful Memes, and MM-IMDb datasets show that BALGRAD effectively balances modality contributions; for example, on UPMC Food-101 BALGRAD improved weak modality (text) performance by 12.5%p compared to the baseline. AI practitioners can utilize BALGRAD for more robust and balanced multimodal learning, particularly in scenarios with imperfect or missing data. |
| Machine Learning | AIMI: Leveraging Future Knowledge and Personalization in Sparse Event
  Forecasting for Treatment Adherence (Read more on [arXiv](https://arxiv.org/abs/2503.16091) or [HuggingFace](https://huggingface.co/papers/2503.16091))| Hassan Ghasemzadeh, Diane J. Cook, ab9mamun | This paper introduces Adherence Forecasting and Intervention with Machine Intelligence (AIMI), a knowledge-guided system for forecasting medication adherence. The primary research objective is to improve the accuracy of treatment adherence forecasting by leveraging smartphone sensors, previous medication history, and, crucially, future knowledge. AIMI employs CNN and LSTM-based models, with various input feature combinations, and uses incremental learning for resource-constrained training. LSTM models achieved an accuracy of 0.932 and an F-1 score of 0.936, demonstrating the positive impact of future knowledge and personalized training. AI practitioners can leverage AIMI's insights regarding feature engineering and personalization for developing more effective and timely intervention tools, especially in the context of sparse event forecasting. |
