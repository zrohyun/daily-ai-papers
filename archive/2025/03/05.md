

## Papers for 2025-03-05

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | MPO: Boosting LLM Agents with Meta Plan Optimization (Read more on [arXiv](https://arxiv.org/abs/2503.02682) or [HuggingFace](https://huggingface.co/papers/2503.02682))| sujianli, songff, Adagio, Rsy24, xwm | This paper introduces Meta Plan Optimization (MPO), a framework to enhance the planning capabilities of Large Language Model (LLM)-based agents. The main objective is to improve agent performance in interactive planning tasks by incorporating explicit, high-level guidance through meta plans. MPO leverages meta plans to assist agent planning and optimizes these plans based on feedback from task execution, using Monte Carlo sampling and Direct Preference Optimization (DPO). Experiments on ALFWorld and ScienceWorld benchmarks show significant performance improvements, with agents achieving up to 100% performance gains and surpassing existing baselines. MPO offers a plug-and-play solution that boosts task completion efficiency and generalization, indicating substantial implications to building more reliable LLM-based agent. |
| Natural Language Processing | Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.02846) or [HuggingFace](https://huggingface.co/papers/2503.02846))| Kai Chen, Chengqi Lyu, lindahua, ZwwWayne, vanilla1116 | This paper introduces Mask-DPO, a novel method for improving the factual accuracy of Large Language Models (LLMs) responses. The main research objective is to develop a fine-grained factuality alignment technique that reduces hallucinations in LLMs more effectively than existing response-level preference learning methods. Mask-DPO incorporates sentence-level factuality as mask signals within the Direct Preference Optimization (DPO) framework, selectively learning from factually correct sentences and avoiding penalties on factual content in less-preferred samples.  Experimental results show that Mask-DPO significantly improves the factuality of Llama3.1-8B-Instruct, achieving a 77.53% factuality score on the ANAH test set, surpassing the 53.44% score of Llama3.1-70B-Instruct, and demonstrating strong generalization to out-of-domain datasets. This method offers a more precise and generalizable approach to factuality alignment, which helps the model better align with external knowledge, making it a valuable tool for AI practitioners aiming to enhance the reliability of LLM-generated content. |
| Reinforcement Learning | LADDER: Self-Improving LLMs Through Recursive Problem Decomposition (Read more on [arXiv](https://arxiv.org/abs/2503.00735) or [HuggingFace](https://huggingface.co/papers/2503.00735))| akiray1, TamasSimonds | LADDER is a framework that enables Large Language Models (LLMs) to autonomously improve their problem-solving capabilities through self-guided learning. The main research objective is to develop a method for LLMs to improve mathematical reasoning without relying on curated datasets or human feedback. The key methodology is recursive problem decomposition, where the model generates and solves progressively simpler variants of complex problems, verified through numerical integration, and refined using reinforcement learning.  LADDER improved a Llama 3.2 3B model's accuracy on undergraduate-level integration problems from 1% to 82% and enabled a Qwen2.5 7B model to achieve 90% on the MIT Integration Bee qualifying exam. The main implication is that self-directed strategic learning can achieve significant capability improvements without architectural scaling or human supervision, and can be scaled up at inference time. |
| Natural Language Processing | Wikipedia in the Era of LLMs: Evolution and Risks (Read more on [arXiv](https://arxiv.org/abs/2503.02879) or [HuggingFace](https://huggingface.co/papers/2503.02879))| Yao Wan, fjchendp, mgeng, sdzzxyl, hsm316 | This paper presents a thorough analysis of the impact of Large Language Models (LLMs) on Wikipedia, examining its evolution and exploring potential risks. The main research question is how LLMs have already impacted Wikipedia and how they might influence the broader Natural Language Processing (NLP) community. The authors analyze page views, word frequency, and linguistic style changes in Wikipedia articles, and simulate the effects of LLMs on machine translation and retrieval-augmented generation (RAG) tasks. The results indicate that LLMs have influenced Wikipedia articles, with an impact of approximately 1%-2% in certain categories, potentially leading to inflated machine translation scores and reduced RAG effectiveness. AI practitioners should be aware of the potential for LLM-generated content to influence the quality and reliability of widely used resources like Wikipedia, necessitating careful consideration of future risks. |
| Multi-Modal | MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents (Read more on [arXiv](https://arxiv.org/abs/2503.01935) or [HuggingFace](https://huggingface.co/papers/2503.01935))| mikewang, ShuyiGuo, Thomas-X-Yang, zhaochenhong, Leozkl | The paper introduces MultiAgentBench, a comprehensive benchmark for evaluating LLM-based multi-agent systems across diverse, interactive scenarios. The main objective is to assess not only task completion but also the quality of collaboration and competition in multi-agent settings. The key methodology involves a framework named MARBLE, which supports various coordination protocols and innovative strategies like group discussion and cognitive planning, utilizing milestone-based key performance indicators. Primary results indicate that gpt-4o-mini achieves the highest average task score, graph structure performs best among coordination protocols in research scenarios, and cognitive planning improves milestone achievement rates by 3%. The main implication for AI practitioners is the provision of a robust benchmark and framework to design, evaluate, and optimize multi-agent systems, especially in dynamic and socially complex environments. |
| Machine Learning | PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization (Read more on [arXiv](https://arxiv.org/abs/2503.01328) or [HuggingFace](https://huggingface.co/papers/2503.01328))| Min Lin, Xinyi Wan, JialinLi, huanggx-sea, QPHutu | The paper introduces PipeOffload, a method to improve the scalability of pipeline parallelism (PP) for large language model training by optimizing memory usage. It addresses the challenge of high activation memory consumption by leveraging memory offloading, aiming to minimize pipeline bubbles. The key methodology involves a selective offload strategy and integration with other techniques to jointly optimize throughput and memory. Experiments show that per-device activation memory effectively reduces with an increasing number of stages, yielding up to a 19% acceleration with lower memory consumption compared to tensor parallelism (TP), assuming the memory transfer cost is low enough. |
| Natural Language Processing | Iterative Value Function Optimization for Guided Decoding (Read more on [arXiv](https://arxiv.org/abs/2503.02368) or [HuggingFace](https://huggingface.co/papers/2503.02368))| Ruizhe Chen, jokephp, ab3223323, lljhbxt, zhliu | The paper introduces Iterative Value Function Optimization (IVO) for improved guided decoding in language models. It aims to enhance the accuracy of value function estimation, critical for effective guided decoding, by addressing limitations of existing methods. IVO employs Monte Carlo Value Estimation to reduce variance and Iterative On-Policy Optimization to improve value estimation through trajectories from value-guided policies. Experiments on summarization, multi-turn dialogue, and instruction following show IVO achieves 77.52% GPT-4 win rates on multi-turn dialogue against the base policy.  The framework enables more efficient and effective alignment of language models with desired behaviors. |
| Natural Language Processing | FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling (Read more on [arXiv](https://arxiv.org/abs/2502.14856) or [HuggingFace](https://huggingface.co/papers/2502.14856))| yuxuanli, zwl96, hyx21, ThonyPan, Achazwl | This paper introduces FR-Spec, a frequency-ranked speculative sampling framework designed to accelerate the generation process of large-vocabulary language models (LLMs). The main research objective is to address the reduced efficiency gains of speculative sampling in LLMs with large vocabularies, particularly the computational overhead of the LM Head. FR-Spec optimizes draft candidate selection by constraining the draft search to a frequency-prioritized token subset, reducing LM Head computation. Experiments show an average 1.12x speedup over the state-of-the-art speculative sampling method EAGLE-2 across multiple datasets. This provides a practical solution for improving the efficiency of deploying LLMs, especially on resource-constrained devices. |
| Natural Language Processing | SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking (Read more on [arXiv](https://arxiv.org/abs/2503.00955) or [HuggingFace](https://huggingface.co/papers/2503.00955))| Thanh T. Tran, ThanhDi, TienAnh, xuandin, DavidNguyen | This paper introduces SemViQA, a novel Vietnamese fact-checking framework designed to improve both accuracy and efficiency in detecting misinformation. The main research objective is to address the challenges of semantic ambiguity, homonyms, and complex linguistic structures in Vietnamese, which existing methods often struggle with. The methodology integrates Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC), balancing precision and speed using a hybrid approach of TF-IDF and a Question Answering Token Classifier (QATC). The system achieves state-of-the-art results with 78.97% strict accuracy on ISE-DSC01 and 80.82% on ViWikiFC, and a 7x speed improvement. For AI practitioners, SemViQA offers a new benchmark for Vietnamese fact verification, providing a robust solution, particularly in low-resource language scenarios. |
| Multi-Modal | UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface (Read more on [arXiv](https://arxiv.org/abs/2503.01342) or [HuggingFace](https://huggingface.co/papers/2503.01342))| windmillknight, Shawnee-bxy, Haiyang-W, chenweix7, kanashi6 | This paper introduces UFO, a unified framework for fine-grained visual perception tasks such as detection and segmentation, using an open-ended language interface. The main research question is how to effectively integrate fine-grained perception tasks into multimodal large language models (MLLMs) without relying on task-specific designs and architectures.  UFO transforms all perception targets into the language space and uses a novel embedding retrieval approach for segmentation, bridging the gap between fine-grained perception and vision-language tasks. After multi-task training, UFO outperforms previous state-of-the-art generalist models by 12.3 mAP on COCO instance segmentation and 3.3 mIoU on ADE20K semantic segmentation. This approach allows AI practitioners to more easily build generalist models capable of handling a wide range of vision and vision-language tasks without complex task-specific modules. |
| Natural Language Processing | ATLaS: Agent Tuning via Learning Critical Steps (Read more on [arXiv](https://arxiv.org/abs/2503.02197) or [HuggingFace](https://huggingface.co/papers/2503.02197))| Yuxuan Huang, Ming Li, Zhixun Chen, zhoutianyi, YaliDU | This paper introduces ATLAS, a method for fine-tuning Large Language Model (LLM) agents by focusing on critical steps within expert trajectories. The research objective is to improve the efficiency and generalization capabilities of LLM agents by training them on a reduced set of important actions, rather than entire trajectories. ATLAS identifies critical steps using an oracle LLM based on criteria such as plan creation, critical observation, critical action, and self-correction. The experiments show that an LLM finetuned on only ~30% critical steps selected by ATLAS outperforms the LLM finetuned on all steps and achieves over a 5% average improvement compared to models fine-tuned with unfiltered data on held-in tasks. This method allows AI practitioners to train more effective and generalizable LLM agents with reduced computational cost and training data. |
| Reinforcement Learning | Language Models can Self-Improve at State-Value Estimation for Better Search (Read more on [arXiv](https://arxiv.org/abs/2503.02878) or [HuggingFace](https://huggingface.co/papers/2503.02878))| rittera, emendes3 | This paper introduces self-taught lookahead (STL), a self-supervised method that improves language-model-controlled search by leveraging state-transition dynamics to train a value model. The main objective is to develop a method for training value models for multi-step reasoning tasks without relying on costly ground truth rewards or human demonstrations. STL bootstraps an initial value function and leverages the environment's state-transition dynamics to train a value model that predicts the next best action, resulting state, and a value rationale. Results on WebShop show that STL improves performance by 20% and reduces costs by 37x compared to previous LLM-based tree search methods. This technique enables efficient training and deployment of agents, particularly in interactive domains where data collection is expensive. |
| Computer Vision | RectifiedHR: Enable Efficient High-Resolution Image Generation via Energy Rectification (Read more on [arXiv](https://arxiv.org/abs/2503.02537) or [HuggingFace](https://huggingface.co/papers/2503.02537))| Liang Hou, dizhang, wileewang, PaulSHEN1, YZCS | The paper introduces RectifiedHR, a training-free framework for efficient high-resolution image generation. It aims to address the performance degradation of diffusion models when generating images beyond their training resolution. The method employs a noise refresh strategy and an energy rectification technique based on adjusting the classifier-free guidance hyperparameter. Experiments show RectifiedHR achieves competitive performance, obtaining a CLIP score of 33.756 in 2048x2048 resolution and significantly reducing inference time compared to existing training-free baselines. The proposed method offers AI practitioners an efficient way to generate high-resolution images without retraining, addressing the limitations of existing diffusion models. |
| Computer Vision | SPIDER: A Comprehensive Multi-Organ Supervised Pathology Dataset and Baseline Models (Read more on [arXiv](https://arxiv.org/abs/2503.02876) or [HuggingFace](https://huggingface.co/papers/2503.02876))| Ekaterina Ivanova, alpchel, mgvz | This paper introduces SPIDER, a large, publicly available, multi-organ, patch-level histopathology dataset with expert-validated annotations, and associated baseline models. The main research objective is to address the scarcity of diverse, high-quality, public datasets in computational pathology, which limits the development of robust AI models. The methodology involves a semi-automatic annotation pipeline to create patch-level labels across multiple organs (Skin, Colorectal, Thorax), using a foundation model (Hibou-L) for feature extraction and an attention-based classification head. The baseline models achieved high classification performance, with an F1-score of 0.937 for Skin, 0.915 for Colorectal, and 0.960 for Thorax. The release of this dataset and baseline models can accelerate research, and enable a wider range of applications, like WSI segmentation, improving diagnostic accuracy and efficiency in digital pathology. |
| Multi-Modal | Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content (Read more on [arXiv](https://arxiv.org/abs/2503.02357) or [HuggingFace](https://huggingface.co/papers/2503.02357))| Zicheng Zhang, GTZhai, a9108, sl2782087, wcain | This paper introduces Q-Eval-100K, a large-scale dataset for evaluating text-to-vision models, and Q-Eval-Score, a unified evaluation framework. The main objective is to develop a comprehensive and reliable method for assessing both the visual quality and text-image/video alignment of generated content. The methodology involves collecting 100K instances (60K images and 40K videos) with 960K human annotations, designing context prompts for LMMs, and employing a Vague-to-Specific strategy for long-text alignment. Q-Eval-Score achieves a superior performance with a model-level SRCC of 0.949 on image visual quality and 0.969 on image alignment. AI practitioners can leverage this dataset and framework to improve text-to-vision models, especially regarding visual quality which currently lags behind alignment performance. |
| Reinforcement Learning | IterPref: Focal Preference Learning for Code Generation via Iterative Debugging (Read more on [arXiv](https://arxiv.org/abs/2503.02783) or [HuggingFace](https://huggingface.co/papers/2503.02783))| Ruihang, yangyu90, Jianwen2003, CharonBony, Ringo1110 | This paper introduces IterPref, a novel preference alignment framework for code generation that mimics human iterative debugging to refine Code Language Models (LLMs). The main objective is to improve the accuracy of Code LLMs beyond supervised fine-tuning by leveraging relative quality comparisons and focusing on correcting specific errors. IterPref explicitly locates error regions and aligns the corresponding tokens via a tailored Direct Preference Optimization (DPO) algorithm using a new dataset, CodeFlow, where samples are iteratively refined until passing tests. Experiments show that IterPref improves performance; for example it achieves a 29.7% pass@1 score on BigCodeBench Complete Hard, matching some larger models. This approach can offer AI practitioners a method for improving code generation accuracy by focusing model training on specific error corrections within an iterative debugging process. |
| Machine Learning | AppAgentX: Evolving GUI Agents as Proficient Smartphone Users (Read more on [arXiv](https://arxiv.org/abs/2503.02268) or [HuggingFace](https://huggingface.co/papers/2503.02268))| Chi Zhang, Wenjia Jiang, xuyang, ChenxiSong, yyzhuang2 | The paper introduces AppAgentX, an evolutionary framework for GUI agents to improve operational efficiency while maintaining intelligence. It addresses the inefficiency of step-by-step reasoning in LLM-based agents by incorporating a memory mechanism that records task execution history. The framework identifies repetitive action sequences and evolves high-level actions as shortcuts. Experimental results on multiple benchmark tasks demonstrate that AppAgentX significantly outperforms existing methods in both efficiency and accuracy. For example, AppAgentX reduced the average required number of steps from 9.1 to 5.7, while the step execution time is reduced from 23 to 16 seconds, demonstrating its efficiency for AI practitioners. |
