

## Papers for 2025-03-04

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Visual-RFT: Visual Reinforcement Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2503.01785) or [HuggingFace](https://huggingface.co/papers/2503.01785))| yhcao, sweetFruit, yuhangzang, Zery, ziyuliu | This paper introduces Visual Reinforcement Fine-Tuning (Visual-RFT), a novel approach to enhance large vision-language models (LVLMs) using reinforcement learning with verifiable rewards. The main objective is to extend Reinforcement Fine-Tuning (RFT), commonly used in language models, to visual tasks and improve data efficiency in fine-tuning LVLMs for domain-specific applications. Visual-RFT employs LVLMs to generate multiple responses, including reasoning and answers, and uses visual perception verifiable reward functions, such as Intersection over Union (IoU) for object detection, to update the model via policy optimization algorithms like Group Relative Policy Optimization (GRPO). Experimental results show that Visual-RFT improves accuracy by 24.3% over the baseline in one-shot fine-grained image classification, and by 21.9 in COCO's few-shot detection setting, demonstrating superior data efficiency and generalization. Visual-RFT offers a data-efficient, reward-driven paradigm shift in fine-tuning LVLMs, enhancing their reasoning and adaptability for visual perception tasks. |
| Computer Vision | Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2503.01774) or [HuggingFace](https://huggingface.co/papers/2503.01774))| zgojcic, AnalMom, xrenaa, hturki, jayw | DIFIX3D+ is a novel pipeline designed to enhance 3D reconstruction and novel-view synthesis using single-step diffusion models. The paper's core research objective is to address the challenge of persistent artifacts in 3D reconstructions, especially in under-constrained regions and extreme novel viewpoints, when using Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). The key methodology involves fine-tuning a single-step image diffusion model (DIFIX) to act as both a neural enhancer during reconstruction (by cleaning pseudo-training views) and as a real-time post-processing step during inference.  DIFIX3D+ achieves an average 2x improvement in FID score over baselines while maintaining 3D consistency, and the same model enhances both NeRF and 3DGS representations. The main implication is that practitioners can use the proposed model to improve reconstruction quality efficiently with no additional compute for multi-step denoising. |
| Multi-Modal | Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs (Read more on [arXiv](https://arxiv.org/abs/2503.01743) or [HuggingFace](https://huggingface.co/papers/2503.01743))| vishravmsft, martincai, alonbenhaim, jianmin-ustc, atabakashfaqMSFT | Phi-4-Mini and Phi-4-Multimodal are compact, high-performing language and multimodal models, respectively. The research aims to develop small language models (SLMs) and multimodal models that achieve competitive performance with significantly fewer parameters, leveraging high-quality synthetic data and a novel 'Mixture of LoRAs' technique. Phi-4-Mini uses a 3.8-billion-parameter language model trained on curated web and synthetic data, expanding vocabulary to 200K tokens, and Phi-4-Multimodal extends to vision and speech/audio via modality-specific LoRA adapters. Phi-4-Mini outperforms recent open-source models of similar size and matches performance of models twice its size in math and coding and Phi-4-Multimodal outperforms larger vision-language and speech-language models on several benchmarks. The implication is that smaller, efficient models, particularly with strategic data curation and modality extension techniques, can rival or surpass larger models in various tasks, making advanced AI more accessible. |
| Machine Learning | OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment (Read more on [arXiv](https://arxiv.org/abs/2502.18965) or [HuggingFace](https://huggingface.co/papers/2502.18965))| GuoruiZhou, DingWF, caikuo, oneself, OrpheusBetter | This paper introduces OneRec, an end-to-end generative recommender system that unifies the retrieval and ranking stages into a single generative model. The research objective is to overcome the limitations of traditional cascaded ranking systems and develop a unified generative model for recommendation. The key methodology involves an encoder-decoder architecture with sparse Mixture-of-Experts (MoE), a session-wise generation approach, and an Iterative Preference Alignment module combined with Direct Preference Optimization (DPO).  Deployed in Kuaishou's main scene, OneRec achieved a 1.6% increase in watch-time. AI practitioners can adapt OneRec's approach for creating efficient and more effective recommendations for production-ready systems.  |
| Natural Language Processing | Liger: Linearizing Large Language Models to Gated Recurrent Structures (Read more on [arXiv](https://arxiv.org/abs/2503.01496) or [HuggingFace](https://huggingface.co/papers/2503.01496))| Yu Cheng, JusenK, Jiaxihu2, weigao266, landisen | This paper introduces Liger, a novel method for converting pretrained Transformer-based Large Language Models (LLMs) into gated linear recurrent structures for improved efficiency. The main research objective is to linearize LLMs without adding extra parameters and extensive fine-tuning, while maintaining performance. Liger repurposes pretrained key matrix weights to construct gating mechanisms and uses lightweight fine-tuning with Low-Rank Adaptation (LoRA). Liger recovers 93% of the Transformer-based LLM performance with only 0.02% of pre-training tokens during the linearization process. AI practitioners can use Liger to deploy efficient LLMs with linear-time inference and constant memory, especially useful in long-sequence tasks. |
| Natural Language Processing | When an LLM is apprehensive about its answers -- and when its uncertainty is justified (Read more on [arXiv](https://arxiv.org/abs/2503.01688) or [HuggingFace](https://huggingface.co/papers/2503.01688))| Alexey Zaytsev, Edvard Khalafyan, DanielVyazhev, aigoncharov, sspetya | This paper investigates uncertainty estimation in Large Language Models (LLMs) for multiple-choice question-answering tasks, focusing on how different factors influence the reliability of LLM responses. The main research objective is to determine the effectiveness of token-wise entropy and Model-As-Judge (MASJ) approaches in predicting model errors across various question topics and reasoning complexities.  The methodology involves evaluating three LLMs (Phi-4, Mistral, and Qwen) of varying sizes on the MMLU-Pro dataset, analyzing response entropy and MASJ scores in relation to question domains and reasoning requirements.  A key finding is that response entropy effectively predicts model errors in knowledge-dependent domains, achieving a ROC AUC of 0.73 for biology, while MASJ performs similarly to a random error predictor.  The study implies that data-uncertainty related entropy should be integrated into LLM uncertainty frameworks, and that current benchmark datasets may be biased, requiring more balanced assessments of LLM performance. |
| Multi-Modal | DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion (Read more on [arXiv](https://arxiv.org/abs/2503.01183) or [HuggingFace](https://huggingface.co/papers/2503.01183))| Guobin Ma, Chunbo Hao, Yuepeng Jiang, Huakang Chen, Ziqian Ning | DiffRhythm is a novel end-to-end latent diffusion-based model for full-length song generation, capable of synthesizing both vocals and accompaniment. The primary objective is to address limitations of existing music generation approaches, such as restricted song lengths, disjointed vocal/accompaniment generation, and slow inference speeds. The methodology employs a variational autoencoder (VAE) for audio compression and a diffusion transformer (DiT) operating in the latent space, along with a sentence-level lyrics alignment mechanism for improved intelligibility. The model achieves a real-time factor (RTF) below 0.04 and shows substantial relative improvements against a comparable baseline, SongLM, by 18.2% in phoneme error rate (PER), indicating enhanced vocal clarity. The core implication includes reduced requirements for complex multi-stage architectures or intricate data pipelines; AI practitioners can utilize DiffRhythm for scalable and efficient song generation research, enabling faster inferences. |
| Natural Language Processing | Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs (Read more on [arXiv](https://arxiv.org/abs/2503.01307) or [HuggingFace](https://huggingface.co/papers/2503.01307))| ngoodman, nlile, Asap7772, ayushchakravarthy, obiwan96 | This paper investigates how certain cognitive behaviors in language models enable self-improvement through reinforcement learning (RL). The main research question is what intrinsic properties of language models allow for effective self-improvement when trained with RL on verifiable tasks. The methodology involves analyzing four key cognitive behaviors—verification, backtracking, subgoal setting, and backward chaining—in models like Qwen-2.5-3B and Llama-3.2-3B, and manipulating these through priming and curated pretraining.  A key result is that Qwen achieved approximately 60% accuracy on the Countdown game after RL training, substantially outperforming Llama's 30%. The main implication is that the presence of initial reasoning behaviors, rather than the correctness of answers, is a critical factor enabling successful self-improvement, suggesting practitioners should focus on cultivating these behaviors in their models. |
| Machine Learning | Speculative Ad-hoc Querying (Read more on [arXiv](https://arxiv.org/abs/2503.00714) or [HuggingFace](https://huggingface.co/papers/2503.00714))| Venkat Arun, Aditya Akella, Maria Angels de Luis Balaguer, Srikanth Kandula, Haoyu0529 | This paper proposes SpeQL, a system that accelerates ad-hoc SQL query execution by using Large Language Models (LLMs) to predict and precompute query results even before the user finishes typing. The main research objective is to determine whether query execution can begin before a user has completed their query, reducing latency in data exploration. SpeQL leverages LLMs to predict likely queries and precomputes smaller temporary tables by predicting query structure and using query rewriting. On 103 industry-standard TPC-DS queries at a 100GB scale, SpeQL reduces P90 planning latency by 94.42% (1.22 seconds), compilation latency by 99.99% (6.43 seconds), and execution latency by 87.23% (3.34 seconds). AI practitioners can leverage this approach of speculative execution to reduce latency in interactive data analysis systems. |
| Natural Language Processing | DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting (Read more on [arXiv](https://arxiv.org/abs/2503.00784) or [HuggingFace](https://huggingface.co/papers/2503.00784))| xpqiu, QipengGuo, KYLN24, KaiLv | DuoDecoding is a novel approach to accelerate the inference speed of large language models (LLMs) using speculative decoding. The research addresses the performance bottleneck introduced by the draft model in conventional speculative decoding, which increases computational overhead. DuoDecoding strategically deploys the draft and target models on CPU and GPU respectively, enabling parallel decoding, a hardware-aware optimal draft budget, and dynamic multi-sequence drafting. Experiments across seven tasks show DuoDecoding achieves up to 2.61x speedup in generation latency compared to conventional speculative decoding. The main implication is that AI practitioners can significantly reduce LLM inference latency by leveraging heterogeneous computing resources and optimizing the draft phase of speculative decoding. |
| Multi-Modal | Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions (Read more on [arXiv](https://arxiv.org/abs/2503.00501) or [HuggingFace](https://huggingface.co/papers/2503.00501))| Xiaohui He, Jia Chen, aiqy, haitaoli, qian | This paper introduces Qilin, a new multimodal information retrieval dataset collected from the Xiaohongshu social platform, designed for improving search and recommendation (S&R) services. The main research objective is to address the lack of high-quality datasets for multimodal S&R, and specifically, to model user satisfaction by analyzing heterogeneous user behaviors at the APP-level. The methodology involves collecting user sessions with diverse result types (image-text, video, commercial notes, direct answers) along with extensive contextual signals and user feedback, using LLMs and human experts for data filtering.  Primary results include the dataset itself, with 15,482 users, 1,983,938 notes and 5,006,181 images, as well as baseline experiments showing, e.g., that DCN-V2 achieves an MRR@10 of 0.5600 for search. This dataset enables practitioners to develop and evaluate advanced multimodal retrieval models and investigate user behavior in complex S&R systems, including the effects of features like a Deep Query Answering (DQA) module. |
| Computer Vision | Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation (Read more on [arXiv](https://arxiv.org/abs/2503.01370) or [HuggingFace](https://huggingface.co/papers/2503.01370))| yingcongchen, Xxlbigbrother, StarYDY, MeixiChen, LTT | Kiss3DGen is a framework for 3D asset generation that repurposes 2D image diffusion models. The main research objective is to develop an efficient method for generating, editing, and enhancing 3D objects by leveraging pretrained 2D diffusion models, overcoming limitations of existing methods requiring large 3D asset datasets. The key methodology involves fine-tuning a diffusion model to generate a '3D Bundle Image'—a tiled representation of multi-view images and normal maps—and reconstructing a 3D mesh from the normal maps, with textures provided by the multi-view images. The method outperforms MVDream in text-to-multi-view synthesis with a CLIP score of 0.844 versus 0.809, and achieves a significantly improved alignment with text descriptions compared to other text-to-3D generation methods. AI practitioners can use this approach to generate high-quality 3D models efficiently using existing 2D diffusion models and readily available 2D datasets. |
| Natural Language Processing | Word Form Matters: LLMs' Semantic Reconstruction under Typoglycemia (Read more on [arXiv](https://arxiv.org/abs/2503.01714) or [HuggingFace](https://huggingface.co/papers/2503.01714))| Lang Gao, Zhongyu Wei, Ziruibest, Carol0110, Aurora-cx | This paper investigates how Large Language Models (LLMs) reconstruct the semantics of scrambled words, a phenomenon known as Typoglycemia. The main research objective is to analyze the roles of word form and contextual information in LLMs' semantic reconstruction and examine the underlying attention mechanisms. The authors propose a metric called Semantic Reconstruction Score (SemRecScore) to quantify semantic recovery, and use controlled experiments with varying Scramble Ratio (SR) and Context Integrity (CI) on LLaMA models. Results show LLMs primarily rely on word form for reconstruction (e.g., a SemRecScore near 1 for SR=0 and CI=1, dropping to 0.5 for SR=1), with minimal impact from context, and utilize specialized attention heads for this process. The findings imply that integrating human-like, context-aware mechanisms could improve LLM semantic adaptability. |
| Natural Language Processing | SampleMix: A Sample-wise Pre-training Data Mixing Strategey by Coordinating Data Quality and Diversity (Read more on [arXiv](https://arxiv.org/abs/2503.01506) or [HuggingFace](https://huggingface.co/papers/2503.01506))| bitwjg, WeiWang, WQYC, DeyangKong, xixy | This paper introduces SampleMix, a novel sample-wise data mixing strategy for pre-training large language models. The main objective is to address the limitations of existing domain-wise mixing methods by coordinating data quality and diversity at the sample level. SampleMix evaluates the quality and diversity of each sample and assigns corresponding sampling weights, performing global cross-domain sampling to determine the optimal domain distribution dynamically. Experiments show that SampleMix outperforms existing methods, achieving an average accuracy of 47.77% across eight downstream tasks and requiring 1.9x fewer training steps to reach baseline performance. This approach offers a more effective way to optimize pre-training data, enhancing model performance and training efficiency for AI practitioners. |
| Natural Language Processing | From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens (Read more on [arXiv](https://arxiv.org/abs/2502.18890) or [HuggingFace](https://huggingface.co/papers/2502.18890))| Yuxuan Wang, zlzheng, vickyandkekey, JunzheS, TongWu | This paper introduces TokenSwift, a novel framework designed to accelerate the generation of ultra-long sequences (up to 100K tokens) in large language models (LLMs) while maintaining lossless accuracy. The main research question is whether model-agnostic, lossless acceleration, akin to those seen in short-sequence speculative decoding, can be achieved for generating ultra-long sequences with minimal training overhead. TokenSwift addresses challenges such as frequent model reloading, dynamic key-value (KV) cache management, and repetitive content generation, by using n-gram retrieval, dynamic KV cache updates, multi-token generation, and token reutilization. Experimental results demonstrate that TokenSwift achieves over a 3x speedup compared to autoregressive generation across various models and architectures, for example from 5 hours to 90 minutes for 100k token generation on LLaMA3.1-8b. AI practitioners can use TokenSwift to significantly reduce the time required for ultra-long sequence generation, making applications requiring long text generation, much more practical. |
| Computer Vision | Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model (Read more on [arXiv](https://arxiv.org/abs/2502.16779) or [HuggingFace](https://huggingface.co/papers/2502.16779))| Jianan Wang, Xili Dai, xyyue, qixianbiao, yxuan | This paper introduces a novel pipeline, Plane-DUSt3R, for multi-view room layout estimation from unposed, sparse-view images, leveraging a pre-trained 3D foundation model. The research objective is to address the challenge of 3D room layout reconstruction from multiple perspective images without known camera poses, a departure from traditional single-view or panoramic approaches. The methodology combines a 2D plane detector, a modified DUSt3R model (Plane-DUSt3R) for 3D structural plane prediction and correspondence establishment, and a post-processing algorithm to derive the final 3D layout.  Plane-DUSt3R achieves a 5.27% and 5.33% improvement in RRA and mAA metrics, respectively, for the multi-view correspondence task compared to state-of-the-art methods on the Structure3D dataset.  AI practitioners can leverage this end-to-end approach to simplify multi-view 3D reconstruction, reducing error accumulation and extending applicability to in-the-wild and varied image styles. |
| Natural Language Processing | CodeArena: A Collective Evaluation Platform for LLM Code Generation (Read more on [arXiv](https://arxiv.org/abs/2503.01295) or [HuggingFace](https://huggingface.co/papers/2503.01295))| terryyz, DongHuang-ebay, bobxwu, anhtuanluu36, Elfsong | CodeArena is an online evaluation framework designed for assessing Large Language Models (LLMs) in code generation. The main objective is to address limitations in current LLM code evaluation, such as benchmark contamination, data dissipation, and limited system accessibility. The key methodology involves a collective evaluation mechanism that dynamically recalibrates model scores based on the holistic performance of all participating models, and utilizes automation-friendly APIs. Primary results, shown in Table 1, reveal that top closed-source models like DeepSeek-Coder and GPT-4o achieve Dynamic Point (DP) scores of 249.28 and 247.32, respectively, corresponding to solving about 90% of the challenge problems. AI practitioners can use CodeArena to obtain unbiased assessments, access a public repository of solutions and test cases, and improve the efficiency of LLM code generation evaluation workflows. |
| Multi-Modal | VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation (Read more on [arXiv](https://arxiv.org/abs/2503.01739) or [HuggingFace](https://huggingface.co/papers/2503.01739))| Yi Yang, WenhaoWang | This paper introduces VideoUFO, a million-scale video dataset designed to align with user preferences in text-to-video generation. The research objective is to create a dataset that addresses the gap between existing video datasets and real-world user needs in text-to-video tasks. The methodology involves analyzing user-focused topics from the VidProM dataset, retrieving relevant videos from YouTube, segmenting them into clips, generating captions, and filtering based on topic relevance and quality. The dataset contains 1.09 million video clips spanning 1,291 user-focused topics, and a model trained on VideoUFO achieved a low-10 score of 0.442 on a proposed benchmark, surpassing other models. AI practitioners can use VideoUFO to train or fine-tune text-to-video models, improving their performance on user-focused topics and enhancing real-world applicability. |
| Natural Language Processing | Large-Scale Data Selection for Instruction Tuning (Read more on [arXiv](https://arxiv.org/abs/2503.01807) or [HuggingFace](https://huggingface.co/papers/2503.01807))| pradeepd, pangwei, faezeb, nanami, hamishivi | This paper presents a systematic study of how well data selection methods scale for instruction-tuning language models when selecting large datasets from very large data pools. The main research question is how well data selection methods perform when selecting hundreds of thousands of samples from data pools of millions of samples, diverging from the smaller scales typically studied. The key methodology involves evaluating nine data selection methods, including representation-based data selection (RDS+), across seven diverse tasks, selecting up to 2.5M samples from pools of up to 5.8M samples. The primary result is that RDS+ consistently outperforms other methods, achieving an average performance of 50.5 when selecting 10k datapoints for single tasks from a 5.8 million datapoint pool, compared to 46.4 of competing methods, whilst also generally being more compute efficient. This suggests that scaling properties of data selection methods are vital to examine, and simple embedding-based methods may work best for large-scale instruction tuning. |
| Machine Learning | Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator (Read more on [arXiv](https://arxiv.org/abs/2503.01103) or [HuggingFace](https://huggingface.co/papers/2503.01103))| mingyuliutw, gdhe17, HuayuChen, Ema11, worstcoder | This paper introduces Direct Discriminative Optimization (DDO), a unified framework that bridges likelihood-based generative model training and the GAN objective to improve visual generation quality. The research objective is to bypass the mode-covering tendency of maximum likelihood estimation (MLE) that limits generation quality under limited model capacity. DDO implicitly parameterizes a discriminator using the likelihood ratio between a learnable target model and a fixed reference model, enabling direct finetuning without altering network architecture.  By finetuning SOTA diffusion models EDM and EDM2, DDO achieves unprecedented FID scores of 1.30/0.97 on CIFAR-10/ImageNet-64 datasets, respectively. AI practitioners can use DDO to refine pre-trained generative visual models with minimal overhead for increased performance and parameter efficiency. |
| Natural Language Processing | AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond Human Understanding (Read more on [arXiv](https://arxiv.org/abs/2503.01063) or [HuggingFace](https://huggingface.co/papers/2503.01063))| dnoever | This paper explores the potential for large language models (LLMs) to autonomously develop private tonal languages for machine-to-machine communication, drawing inspiration from human cryptophasia. The main research question is whether LLMs can create and use a private tonal language based on a human-machine mapping, and what characteristics this language would have, particularly concerning human auditability. The methodology involves implementing a character-to-frequency mapping system using musical semitones, encoding the full ASCII character set with frequencies spanning approximately 7.9 octaves. The system achieves information rates exceeding human speech, with some frequencies deliberately mapped to ultrasonic ranges beyond human perception (>20kHz). AI practitioners should be aware of the potential for LLMs to develop communication methods outside of human understanding, necessitating new governance and detection strategies. |
| Multi-Modal | CLEA: Closed-Loop Embodied Agent for Enhancing Task Execution in Dynamic Environments (Read more on [arXiv](https://arxiv.org/abs/2503.00729) or [HuggingFace](https://huggingface.co/papers/2503.00729))| Qing Zhao, Zhixin Mai, Yiming Zhao, Ge Wang, SP4595 | This paper introduces CLEA, a closed-loop embodied agent framework that enhances task execution in dynamic environments using Large Language Models (LLMs). The research objective is to address the limitations of LLMs in embodied systems, specifically regarding reliable execution of subtasks and achieving one-shot success in long-term tasks in dynamic environments. CLEA integrates four specialized open-source LLMs with functional decoupling in a planner-critic architecture, featuring an interactive task planner and a multimodal execution critic for probabilistic action feasibility assessment. Across 12 task trials in a real environment, CLEA demonstrated a 67.3% improvement in success rate and a 52.8% increase in task completion rate compared to the baseline model. The implication is that closed loop systems significantly enhance robustness of multi-modal models in dynamic environments, although further work is needed for collaboration tasks. |
