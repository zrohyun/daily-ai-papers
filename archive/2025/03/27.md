

## Papers for 2025-03-27

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Dita: Scaling Diffusion Transformer for Generalist
  Vision-Language-Action Policy (Read more on [arXiv](https://arxiv.org/abs/2503.19757) or [HuggingFace](https://huggingface.co/papers/2503.19757))| TTTTTony, MIASANMIA, robot-haonan, TianyiZhang0213, zhihou | This paper introduces Dita, a scalable diffusion Transformer framework designed for generalist vision-language-action (VLA) robot policy learning. The objective is to overcome limitations of prior VLA models constrained by compact action heads or discretization, enabling better adaptation to heterogeneous action spaces. Dita employs a causal Transformer that directly denoises continuous action sequences using a unified multimodal diffusion process with in-context conditioning on raw visual tokens and language instructions. Dita achieves state-of-the-art or competitive results, such as an 82.4% average success rate on the LIBERO benchmark (outperforming OpenVLA by nearly 6%) and demonstrates robust 10-shot real-world adaptation (63.8% success on two-step tasks). The work provides AI practitioners with a simple, lightweight (334M parameters), and open-source baseline showing the effectiveness of integrated, in-context diffusion denoising within Transformers for complex multi-modal robotic control. |
| Multi-Modal | Qwen2.5-Omni Technical Report (Read more on [arXiv](https://arxiv.org/abs/2503.20215) or [HuggingFace](https://huggingface.co/papers/2503.20215))| JialinWang, chenkq, bluelike, jinzheng-he, ZhifangGuo | Qwen2.5-Omni is an end-to-end multimodal model capable of processing text, images, audio, and video, generating real-time text or speech responses. The paper addresses the challenge of unified multimodal understanding and generation, focusing on synchronizing audio and video inputs and preventing modality interference. The model introduces TMRoPE for synchronized temporal representation and a Thinker-Talker architecture for simultaneous text and speech generation. Results show state-of-the-art performance on OmniBench and comparable performance with Qwen2.5-VL, along with WER of 1.42% on seed-tts-eval test-zh. The model provides AI practitioners with a unified framework for developing applications requiring simultaneous multimodal input and real-time text and speech output. |
| Multi-Modal | LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning? (Read more on [arXiv](https://arxiv.org/abs/2503.19990) or [HuggingFace](https://huggingface.co/papers/2503.19990))| Leoxing, KennyUTC, zengyh1900, favourisnotyou, KexianTang | This paper introduces LEGO-Puzzles, a benchmark designed to evaluate the multi-step spatial reasoning capabilities of Multimodal Large Language Models (MLLMs). The main objective is to assess MLLMs' proficiency in spatial understanding and sequential reasoning through tasks based on LEGO constructions. The methodology involves a dataset of 1,100 visual question-answering (VQA) samples across 11 tasks and related image generation tasks, evaluating 20 state-of-the-art MLLMs. Key findings reveal significant limitations, with even the best models like GPT-4o achieving only 57.7% accuracy, far below human performance (93.6%), exposing critical deficiencies in current models. The main implication is the underscored need for substantial advancements in multimodal spatial reasoning for AI systems. |
| Multi-Modal | Wan: Open and Advanced Large-Scale Video Generative Models (Read more on [arXiv](https://arxiv.org/abs/2503.20314) or [HuggingFace](https://huggingface.co/papers/2503.20314))| HermanZ, chenweix7, chaojiemao, baoleai, ang-annng | This paper presents Wan, a comprehensive open-source suite of large-scale video generative models built upon the diffusion transformer paradigm. The primary objective is to push the boundaries of video generation by achieving leading performance, comprehensive capabilities, and consumer-grade efficiency, bridging the gap between open-source and closed-source models. Wan employs innovations including a novel spatio-temporal VAE, scalable pre-training on billions of images/videos, data curation techniques, and optimized Diffusion Transformer architecture leveraging Flow Matching and umT5 for text encoding. The 14B parameter Wan model demonstrates state-of-the-art results, outperforming competitors on benchmarks like Wan-Bench (0.724 weighted score) and VBench (86.22% total score), while the 1.3B model achieves high efficiency (8.19GB VRAM). By open-sourcing models and detailed methodologies, the work aims to accelerate community-driven advancements in video generation technology. |
| Computer Vision | Unconditional Priors Matter! Improving Conditional Generation of
  Fine-Tuned Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2503.20240) or [HuggingFace](https://huggingface.co/papers/2503.20240))| Jaihoon Kim, Minhyuk, phillipinseoul, prinphunya | This paper introduces a training-free method to enhance the conditional generation quality of fine-tuned diffusion models by utilizing superior unconditional priors from base models. The primary objective is to address the performance degradation observed in fine-tuned models where the jointly learned unconditional noise prediction under Classifier-Free Guidance (CFG) becomes poor. The core technique involves replacing the fine-tuned model's unconditional noise prediction with that of its original base model (or another model with strong priors) during CFG sampling. Experiments show significant improvements; for instance, applying this method to Zero-1-to-3 with SD2.1 priors improved novel view synthesis PSNR from 16.647 to 17.801. The key implication for practitioners is that leveraging readily available, higher-quality unconditional priors from base models during inference can substantially improve the output of fine-tuned conditional diffusion models without retraining. |
| Natural Language Processing | Open Deep Search: Democratizing Search with Open-source Reasoning Agents (Read more on [arXiv](https://arxiv.org/abs/2503.20201) or [HuggingFace](https://huggingface.co/papers/2503.20201))| speedyarda, ljirwin, pchiniya, cabxyz, salzubi401 | This paper introduces Open Deep Search (ODS), an open-source framework designed to enhance Large Language Models (LLMs) with sophisticated web search and reasoning capabilities. The primary objective is to close the performance gap between proprietary AI search solutions and their open-source counterparts by providing a transparent and extensible alternative. ODS combines an 'Open Search Tool' for advanced web information retrieval (including query rephrasing and context augmentation) with an 'Open Reasoning Agent' (using ReAct or CodeAct) that orchestrates tool use and complex reasoning based on user queries and retrieved context. ODS significantly improves performance on benchmarks; for instance, ODS-v2 combined with DeepSeek-R1 achieved 75.3% accuracy on FRAMES, surpassing the GPT-4o Search Preview baseline by 9.7%. The main implication for practitioners is the availability of a powerful, open-source framework to build state-of-the-art search AI systems, democratizing advanced search capabilities. |
| Multi-Modal | GenHancer: Imperfect Generative Models are Secretly Strong
  Vision-Centric Enhancers (Read more on [arXiv](https://arxiv.org/abs/2503.19480) or [HuggingFace](https://huggingface.co/papers/2503.19480))| yshan2u, yxgeee, aether25, tttoaster, msj9817 | This paper introduces GenHancer, demonstrating that imperfect generative models can significantly enhance the fine-grained visual perception capabilities of discriminative models like CLIP. The primary objective is to investigate the optimal conditions under which generative models improve visual representations, challenging the assumption that perfect reconstruction is necessary. GenHancer employs a two-stage post-training strategy using lightweight denoisers conditioned solely on the global class token for self-supervised reconstruction, followed by LoRA-based fine-tuning of the visual encoder. This approach consistently outperforms prior methods, achieving notable gains such as a 6.0% improvement over baseline OpenAICLIP on the MMVP-VLM benchmark. The key implication is that targeted use of even imperfect generative models, focusing on specific conditioning and training configurations, is highly effective for enhancing vision-centric tasks in multi-modal AI. |
| Multi-Modal | BizGen: Advancing Article-level Visual Text Rendering for Infographics
  Generation (Read more on [arXiv](https://arxiv.org/abs/2503.20672) or [HuggingFace](https://huggingface.co/papers/2503.20672))| YuanYuhui, kevinlin311tw, bohanChen, Marseclipse, wukeming11 | This paper introduces BizGEN, a novel framework for generating high-quality business infographics and slides featuring accurate article-level visual text rendering within ultra-dense layouts. The primary objective is to overcome the limitations of existing models in handling long context lengths (article-level prompts) and the scarcity of suitable training data for complex business content generation. Key methodologies include the creation of a large-scale dataset (INFOGRAPHICS-650K) via a retrieval-augmented engine and a layout-guided cross-attention mechanism with layout-conditional CFG to manage dense layouts and region-specific prompts flexibly. BizGEN significantly outperforms previous SOTA models like FLUX and SD3 on the custom BIZEVAL benchmark, achieving over 25% absolute improvement in text spelling accuracy for infographics with 20+ layers. For AI practitioners, BizGEN offers a robust approach and dataset for generating complex, text-rich visual documents requiring precise layout and textual adherence, advancing multi-modal content creation capabilities. |
| Multi-Modal | Gemini Robotics: Bringing AI into the Physical World (Read more on [arXiv](https://arxiv.org/abs/2503.20020) or [HuggingFace](https://huggingface.co/papers/2503.20020))| abalakrishna123, TravisAStrong, montse90, jalayrac, saminda | This paper introduces the Gemini Robotics family, AI models derived from Gemini 2.0, designed to equip robots with enhanced embodied reasoning and dexterous physical interaction capabilities. The primary objective is to translate the advancements in large multimodal models into physically grounded AI agents capable of general-purpose robotics tasks. The methodology involves developing Gemini Robotics-ER, enhancing Gemini 2.0 for embodied reasoning (validated on the new ERQA benchmark), and creating Gemini Robotics, a Vision-Language-Action (VLA) model fine-tuned on diverse robot action data for direct, low-latency control. Key results show Gemini Robotics-ER achieving state-of-the-art on ERQA (54.8% accuracy with CoT), the generalist Gemini Robotics model solving numerous dexterous tasks out-of-the-box, outperforming baselines in generalization, and specialized versions reaching high success rates (e.g., 100% on lunch-box packing) and adapting rapidly (<100 demos). For AI practitioners, this work validates leveraging large multimodal models as a foundation for building general-purpose robots with strong generalization and dexterity through targeted fine-tuning and adaptation. |
| Natural Language Processing | MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree
  Search (Read more on [arXiv](https://arxiv.org/abs/2503.20757) or [HuggingFace](https://huggingface.co/papers/2503.20757))| armanc, chenzhao, yilunzhao, AlexCCtop | This paper introduces MCTS-RAG, a novel framework combining Monte Carlo Tree Search (MCTS) with Retrieval-Augmented Generation (RAG) to enhance the reasoning capabilities of small language models (SLMs) on knowledge-intensive tasks. The main objective is to address the limitations of standard RAG (suboptimal knowledge integration) and MCTS reasoning (reliance solely on internal knowledge) by dynamically integrating structured search with adaptive external knowledge retrieval. MCTS-RAG iteratively explores reasoning paths using MCTS, incorporating specific retrieval actions (query generation, decomposition, reflection) at decision points, and uses retrieved information to guide the search and evaluate states. Experimental results demonstrate significant performance gains, such as over 20% improvement on ComplexWebQA using Llama 3.1-8B compared to baselines, enabling SLMs to approach the performance of much larger models like GPT-4. For AI practitioners, this work provides a method to effectively scale inference-time computation to boost the reasoning performance of smaller, more accessible language models on complex, knowledge-heavy tasks. |
| Computer Vision | AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset (Read more on [arXiv](https://arxiv.org/abs/2503.19462) or [HuggingFace](https://huggingface.co/papers/2503.19462))| Yunhong Wang, XihuiLiu, YaohuiW, AriaChen, aejion | The paper introduces AccVideo, a novel distillation method to significantly accelerate video diffusion models using a synthetically generated dataset, SynVid. Its main objective is to reduce the computationally expensive inference steps required by video diffusion models by overcoming challenges like useless data points in existing distillation approaches. The key methodology involves generating a synthetic dataset of valid denoising trajectories from a pretrained teacher model, applying trajectory-based few-step guidance to learn a faster mapping, and employing an adversarial training strategy to enhance video quality. AccVideo achieves an 8.5x improvement in generation speed compared to its teacher model (HunyuanVideo), reducing inference time for a 5s, 720x1280 video from 3234s to 380s, while maintaining comparable quality. This provides AI practitioners with an efficient technique to speed up video diffusion models for high-resolution generation without needing original training data. |
| Multi-Modal | ViLBench: A Suite for Vision-Language Process Reward Modeling (Read more on [arXiv](https://arxiv.org/abs/2503.20271) or [HuggingFace](https://huggingface.co/papers/2503.20271))| cihangxie, xianft, alihiker, Helicopt, PahaII | This paper introduces VILBENCH, a suite for benchmarking and advancing vision-language process reward modeling (PRM). The primary objective is to evaluate existing Vision-Large Language Models (VLLMs) as reward models, create a challenging benchmark (VILBENCH) requiring fine-grained step-wise rewards, and develop an improved vision-language PRM. Key methodologies include benchmarking seven VLLMs as Output Reward Models (ORMs) and PRMs, filtering existing benchmarks to create VILBENCH (600 examples), collecting 73.6K process reward data points (ViLReward-73K) via Monte Carlo Tree Search (MCTS), and training a 3B parameter PRM called ViLPRM. The results show neither ORM nor PRM consistently outperforms the other, GPT-4o achieves only 27.3% accuracy on VILBENCH, and the trained ViLPRM improves selection accuracy by an average of 3.3% over standard Chain-of-Thought on VILBENCH. For practitioners, this work underscores the limitations of using general VLLMs directly as PRMs and provides a benchmark, dataset, and model (ViLPRM) to facilitate the development of more effective, specialized vision-language reward models for complex reasoning tasks. |
| Machine Learning | LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior
  Accuracy Preservation (Read more on [arXiv](https://arxiv.org/abs/2503.19950) or [HuggingFace](https://huggingface.co/papers/2503.19950))| Pingyi Luo, Bingsheng He, deciding, Zicong99, Concyclics | LogQuant introduces a novel log-distributed 2-bit quantization technique for LLM KV Cache compression, aiming to reduce memory footprint while preserving accuracy better than existing methods. The core methodology involves selectively quantizing tokens based on a base-2 logarithmic sparsity pattern derived from observed attention spike distributions, combined with position-agnostic attention calculation for efficiency. Key results show LogQuant improves throughput by 25% and batch size by 60% compared to a BF16 baseline under the same memory constraints, while significantly outperforming comparable 2-bit techniques like KiVi (e.g., 40%-200% accuracy gains on Math/Code tasks). For AI practitioners, LogQuant provides a more effective way to deploy memory-intensive LLMs, especially for long-context tasks, by enabling substantial compression with superior accuracy retention. |
| Multi-Modal | ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving
  Systems (Read more on [arXiv](https://arxiv.org/abs/2503.20756) or [HuggingFace](https://huggingface.co/papers/2503.20756))| xzwnlp, bozhong, xiangchen-dvi, JizhanFang, Chenxiwang | This paper introduces ADS-Edit, a novel multimodal knowledge editing dataset designed to update Large Multimodal Models (LMMs) for Autonomous Driving Systems (ADS). The main objective is to address the challenges LMMs face in ADS, such as misunderstanding traffic knowledge and handling complex road conditions, by enabling targeted, efficient knowledge updates without full retraining. The methodology involves constructing the ADS-Edit benchmark using data from LingoQA, DriveLM, and CODA-LM across video, multi-view, and single-image modalities and three scenario types (perception, understanding, decision making), followed by evaluating four knowledge editing baselines. Primary results indicate high single-edit reliability for memory-based methods (e.g., GRACE achieved 100%) and strong generality (around 90% for WISE and Prompt), although lifelong editing reveals performance degradation and challenges in locality. This work highlights knowledge editing as a viable technique for adapting LMMs in dynamic ADS contexts and provides a dedicated benchmark for evaluating these methods. |
| Multi-Modal | Beyond Words: Advancing Long-Text Image Generation via Multimodal
  Autoregressive Models (Read more on [arXiv](https://arxiv.org/abs/2503.20198) or [HuggingFace](https://huggingface.co/papers/2503.20198))| Min Li, Lijuan, zyang39, linjieli222, Awiny | This paper introduces LongTextAR, a novel multimodal autoregressive model specifically designed for generating high-quality images containing long, coherent text. The main objective is to overcome the limitations of existing text-to-image models which typically fail to accurately render text beyond short phrases. The key methodology involves identifying image tokenization as a bottleneck, introducing a specialized text-focused binary tokenizer (TextBinarizer), and building the LongTextAR model upon it using an autoregressive transformer architecture. Experiments demonstrate superior performance, achieving 69.5% OCR accuracy on long text generation, significantly outperforming models like SD3.5 Large (52.3%). For AI practitioners, this work offers a robust approach for controllable long-text image generation and underscores the importance of specialized tokenizers for specific multimodal tasks like document or presentation synthesis. |
| Computer Vision | Attention IoU: Examining Biases in CelebA using Attention Maps (Read more on [arXiv](https://arxiv.org/abs/2503.19846) or [HuggingFace](https://huggingface.co/papers/2503.19846))| Vikram V. Ramaswamy, Olga Russakovsky, tyleryzhu, serianni | This paper introduces Attention-IoU, a metric using attention maps to quantify bias in computer vision classification models by analyzing internal representations. The objective is to identify spurious correlations where models rely on confounding features rather than target features, going beyond traditional accuracy-based bias measures. The methodology involves calculating Attention-IoU scores comparing GradCAM-generated attention maps against ground-truth feature masks (mask score) or attention maps of protected attributes (heatmap score), validated on Waterbirds and applied to CelebA. Key results show Attention-IoU accurately reflects bias in Waterbirds (e.g., bird mask score decreased from 0.72 to 0.42 as bias increased, mirroring WGA decrease) and reveals complex biases in CelebA, such as identifying potential hidden confounders for the Blond_Hair attribute not evident from label correlations alone (Kendall τ = 0.007 for heatmap score vs. MCC change). The main implication is that Attention-IoU offers a finer-grained view of *how* models exhibit bias, potentially enabling more targeted debiasing techniques. |
| Computer Vision | Self-Supervised Learning of Motion Concepts by Optimizing
  Counterfactuals (Read more on [arXiv](https://arxiv.org/abs/2503.19953) or [HuggingFace](https://huggingface.co/papers/2503.19953))| Kevin Feigelis, Rahul Venkatesh, Seungwoo Kim, Stefan Stojanov, kmeisthax | This paper introduces Opt-CWM, a self-supervised method for estimating optical flow and occlusion by optimizing counterfactual probes applied to a pre-trained next-frame video prediction model. The main objective is to extract motion concepts from unlabeled real-world videos, avoiding reliance on synthetic data or fixed heuristics by leveraging large-scale video representations. The key methodology involves parameterizing perturbations with a learnable network (MLPθ) and training it jointly with a sparse flow-conditioned predictor (Ψflow) using a bootstrapped, self-supervised reconstruction objective derived from an asymmetric masking principle. Opt-CWM achieves state-of-the-art performance on real-world benchmarks like TAP-Vid First (DAVIS), outperforming previous self-supervised methods with metrics such as AJ 47.53 and OF1 60.74. The primary implication is that optimizing counterfactuals provides a scalable, heuristic-free approach for self-supervised extraction of motion and potentially other visual properties from pre-trained video models. |
| Machine Learning | Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.16870) or [HuggingFace](https://huggingface.co/papers/2503.16870))| kw1jjang, Rock222, AndrewAhn, ya-mehdi, Anshumann | This paper introduces 'Random Sampling Knowledge Distillation' (RS-KD), a sparse, importance-sampling-based method to accelerate offline knowledge distillation for Large Language Model (LLM) pre-training. The objective is to overcome the bias, miscalibration, and prohibitive storage costs associated with naive sparse distillation approaches like Top-K logit caching. RS-KD randomly samples teacher logits according to their probability distribution, providing unbiased estimates, preserving expected gradients, and requiring significantly less storage. Experiments demonstrate that RS-KD achieves performance comparable to full distillation (e.g., matching LM loss and 0-shot scores on 3B models using ~12 unique tokens) with marginal overhead (<10%) compared to cross-entropy training and superior calibration versus Top-K methods (e.g., 0.2% ECE vs 4.7% ECE for Top-K 12). This technique allows practitioners to efficiently pre-train smaller LLMs using pre-computed teacher logits, significantly reducing storage and compute needs. |
| Computer Vision | DINeMo: Learning Neural Mesh Models with no 3D Annotations (Read more on [arXiv](https://arxiv.org/abs/2503.20220) or [HuggingFace](https://huggingface.co/papers/2503.20220))| Alan Yuille, Weijie Guo, wufeim, guofeng1123 | This paper introduces DINeMo, a novel neural mesh model for category-level 3D pose estimation trained without requiring any 3D annotations. The primary objective is to leverage pseudo-correspondences from large visual foundation models (like SD-DINO) to overcome the dependency on costly 3D labels. DINeMo employs a bidirectional pseudo-correspondence generation method that refines initial noisy correspondences by considering both local appearance features and global object context (estimated pose). Experimental results demonstrate state-of-the-art zero-shot performance on PASCAL3D+ car pose estimation, achieving 92.8% Acc@π/6 and narrowing the gap with fully-supervised methods by 67.3%. The main implication is enabling scalable and efficient training of 3D object models using abundant unlabeled images, significantly reducing the annotation burden. |
| Computer Vision | Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred
  Image (Read more on [arXiv](https://arxiv.org/abs/2503.17358) or [HuggingFace](https://huggingface.co/papers/2503.17358))| r0nn13, jerredchen | This paper proposes a novel framework to estimate instantaneous camera velocity from a single motion-blurred image, effectively acting as a virtual IMU. The objective is to robustly estimate camera motion during fast movements where traditional methods fail, by treating motion blur as an informative signal. The methodology involves predicting a dense motion flow field and monocular depth map from the blurred image, followed by solving a linear least squares system to recover instantaneous rotational (ω) and translational (v) velocities. Evaluations demonstrate state-of-the-art results, achieving an average RMSE of (1.22, 0.91, 1.76) rad/s for ω and (1.11, 1.03, 0.92) m/s for v, outperforming methods like MASt3R and COLMAP and running in real-time. The key implication for AI practitioners is the potential to leverage traditionally problematic motion blur within single images for accurate and efficient motion estimation in dynamic applications like robotics and AR/VR. |
| Computer Vision | PathoHR: Breast Cancer Survival Prediction on High-Resolution
  Pathological Images (Read more on [arXiv](https://arxiv.org/abs/2503.17970) or [HuggingFace](https://huggingface.co/papers/2503.17970))| Rundong Xue, Jiaxuan Xiao, Jun Liu, Shiru Wang, Yang Luo | PathoHR is a novel pipeline designed for accurate breast cancer survival prediction by enhancing feature learning from high-resolution pathological images. The main objective is to address challenges posed by tumor heterogeneity and computational costs in analyzing Whole Slide Images (WSIs) for survival prediction. Key methodologies include patch-wise feature extraction, incorporating a plug-and-play high-resolution Vision Transformer (ViTAR) for enhanced representation, and systematically evaluating similarity metrics for adaptive token merging. Experiments demonstrated that PathoHR using smaller 16x16 patches (e.g., achieving 0.907 AUC with cosine similarity) could outperform baseline methods using larger 24x24 patches (0.8 AUC), significantly reducing computational overhead. This implies AI practitioners can develop more accurate and efficient computational pathology models by integrating enhanced high-resolution processing and optimized feature learning, rather than solely relying on larger input patches. |
