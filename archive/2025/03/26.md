

## Papers for 2025-03-26

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | Long-Context Autoregressive Video Modeling with Next-Frame Prediction (Read more on [arXiv](https://arxiv.org/abs/2503.19325) or [HuggingFace](https://huggingface.co/papers/2503.19325))| Mike Zheng Shou, Weijia Mao, Yuchao Gu | This paper introduces Frame AutoRegressive (FAR), a novel baseline for long-context autoregressive video modeling using next-frame prediction. The primary objective is to investigate long-context video modeling and address challenges like visual redundancy, poor temporal extrapolation with existing positional embeddings (RoPE), and high computational costs for long sequences. FAR employs a frame-wise flow matching objective with causal spatiotemporal attention, introduces 'stochastic clean context' training to mitigate train-inference discrepancies, proposes 'FlexRoPE' for improved test-time temporal extrapolation (up to 16x), and uses 'long short-term context modeling' for efficient training on long videos. FAR achieves state-of-the-art performance, demonstrating better convergence than video diffusion transformers (e.g., FAR-L FVD 280 vs TATS FVD 420 on UCF-101 128x128 unconditional) and superior results in long video prediction (e.g., lower LPIPS than TECO on DMLab). For AI practitioners, FAR offers a strong and scalable autoregressive framework for video generation that effectively handles long temporal dependencies while managing computational resources. |
| Multi-Modal | CoMP: Continual Multimodal Pre-training for Vision Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2503.18931) or [HuggingFace](https://huggingface.co/papers/2503.18931))| Yu-Gang Jiang, Zuxuan Wu, Wujian Peng, Lingchen Meng, Row11n | This paper introduces CoMP, a continual multimodal pre-training pipeline to enhance existing Vision Foundation Models (VFMs) for native resolution processing and improved cross-modal alignment. The primary objective is to adapt VFMs, regardless of their original pre-training paradigm, to better handle varying image sizes and produce visual representations more aligned with language models. Key methodologies include C-ROPE (Continual Rotary Position Embedding) for resolution flexibility and an Alignment Loss to explicitly align visual features with the language space using LLM embeddings as prototypes. CoMP demonstrates significant improvements, with CoMP-SigLIP achieving 66.7 on ChartQA and 75.9 on DocVQA while maintaining 87.4% accuracy on ImageNet-1K. For AI practitioners, CoMP offers a way to upgrade existing VFMs, making them more effective 'eyes' for Large Multimodal Models by improving native resolution handling and feature alignment. |
| Multi-Modal | Exploring Hallucination of Large Multimodal Models in Video
  Understanding: Benchmark, Analysis and Mitigation (Read more on [arXiv](https://arxiv.org/abs/2503.19622) or [HuggingFace](https://huggingface.co/papers/2503.19622))| Yue Liu, Baolong Bi, Jingyi Tang, Jiashu Qu, Hongcheng Gao | This paper introduces HAVEN, a benchmark to evaluate hallucinations in Large Multimodal Models (LMMs) for video understanding, and proposes methods for mitigation. The primary objective is to systematically study the video hallucination problem in LMMs, which is more complex than static image hallucination. Key methodologies include constructing the HAVEN benchmark along three dimensions (causes, aspects, question formats), analyzing 16 LMMs, and developing a mitigation strategy using Supervised Reasoning Fine-Tuning (SRFT) and Thinking-based Direct Preference Optimization (TDPO). The proposed SRFT+TDPO method improved baseline accuracy by 7.65% on hallucination evaluation and reduced the consistency bias score by 4.5%. For AI practitioners, this research provides a structured approach and benchmark (HAVEN) to assess LMM reliability in video tasks and offers a training strategy to reduce video-based hallucinations. |
| Computer Vision | Inference-Time Scaling for Flow Models via Stochastic Generation and
  Rollover Budget Forcing (Read more on [arXiv](https://arxiv.org/abs/2503.19385) or [HuggingFace](https://huggingface.co/papers/2503.19385))| Minhyuk Sung, Jisung Hwang, Taehoon Yoon, Jaihoon Kim | This paper proposes an inference-time scaling approach for pretrained flow models, enhancing sample quality and alignment via stochastic generation and adaptive budget forcing. The objective is to enable effective reward-based particle sampling for typically deterministic flow models to better satisfy complex user preferences, particularly in image generation. Key methods involve converting the ODE process to an SDE (Stochastic Differential Equation), using a Variance-Preserving (VP) interpolant instead of a linear one to boost diversity, and applying Rollover Budget Forcing (RBF) for adaptive compute (NFE) allocation across timesteps. Results demonstrate significant improvements; for instance, RBF with VP-SDE achieves a VQAScore of 0.925 on compositional text-to-image generation, outperforming baseline flow models (0.726) and diffusion models. This allows practitioners to boost existing flow model performance at inference time without retraining, offering an efficient way to align generation with specific rewards or criteria. |
| Multi-Modal | Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection
  with Artifact Explanation (Read more on [arXiv](https://arxiv.org/abs/2503.14905) or [HuggingFace](https://huggingface.co/papers/2503.14905))| Zichen Wen, Hengrui Kang, Peilin Feng, Junyan Ye, Siwei Wen | This paper introduces FakeVLM, a large multimodal model specialized for detecting general synthetic images and DeepFakes while providing natural language explanations for identified artifacts, alongside a new dataset, FakeClue. The main objective is to overcome the interpretability limitations of traditional detectors and the performance gaps of general LMMs in synthetic image detection. FakeVLM leverages a LLaVA-v1.5 architecture with a CLIP vision encoder and a Vicuna LLM, fully fine-tuned on the FakeClue dataset which contains over 100,000 images annotated with artifact details using multiple LMMs. FakeVLM achieves high performance, notably 0.986 Accuracy and 0.981 F1-score on FakeClue, and 0.843 Accuracy on the LOKI benchmark, demonstrating comparable results to expert models without needing additional classifiers. This work shows that specialized, fine-tuned LMMs can provide both accurate and interpretable synthetic image detection. |
| Multi-Modal | Scaling Vision Pre-Training to 4K Resolution (Read more on [arXiv](https://arxiv.org/abs/2503.19903) or [HuggingFace](https://huggingface.co/papers/2503.19903))| Sifei Liu, Yao Lu, Han Cai, Boyi Li, Baifeng Shi | This paper introduces PS3 (Pre-training with Scale-Selective Scaling), a method enabling efficient CLIP-style vision pre-training at 4K resolution with near-constant computational cost. The main objective is to overcome the quadratic/quartic scaling cost that limits existing vision pre-training to low resolutions, hindering high-resolution visual perception. PS3 achieves this by selectively processing local image regions based on saliency or text prompts and contrasting their high-resolution features with detailed local captions, rather than using global image representations. Key results show that the resulting MLLM, VILA-HD, significantly improves high-resolution visual understanding, outperforming baselines like AnyRes and S² with up to 4.3x fewer tokens, and achieving state-of-the-art results on a new 4KPro benchmark with a 14.5% improvement over GPT-4O and 2.96x speedup over Qwen2-VL. For AI practitioners, PS3 demonstrates a viable path to train high-resolution perception capabilities into vision-language models efficiently, enabling better performance on tasks requiring fine-grained visual detail. |
| Natural Language Processing | Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time
  Thinking (Read more on [arXiv](https://arxiv.org/abs/2503.19855) or [HuggingFace](https://huggingface.co/papers/2503.19855))| Yunjie Ji, Shuaiting Chen, Haotian Wang, Sitong Zhao, Xiaoyu Tian | This paper introduces Multi-round Thinking, a simple test-time scaling technique to enhance Large Language Model (LLM) reasoning by iteratively refining answers. The objective is to improve model performance by prompting reconsideration using only the previous final answer, addressing limitations of methods constrained by long-context processing or complex reinforcement learning training. The key methodology involves feeding the original prompt concatenated with the previous round's final answer back to the model for re-evaluation. Experiments show consistent improvements across various benchmarks; for example, QwQ-32B's accuracy on AIME 2024 increased from 80.3% (Round 1) to 82.1% (Round 2) pass@1. This approach offers AI practitioners a practical, training-free method to boost LLM reasoning capabilities at inference time. |
| Multi-Modal | CoLLM: A Large Language Model for Composed Image Retrieval (Read more on [arXiv](https://arxiv.org/abs/2503.19910) or [HuggingFace](https://huggingface.co/papers/2503.19910))| Son Tran, Mubarak Shah, Ashish Tawari, Jinyu Yang, Chuong Huynh | CoLLM presents a novel Large Language Model (LLM)-based framework for Composed Image Retrieval (CIR) that addresses data scarcity and model limitations by synthesizing training data on-the-fly. The primary objective is to improve CIR performance, particularly for complex queries, without relying on expensive, manually annotated reference-modification-target triplets. Key methodologies include dynamically generating triplets from image-caption pairs using Spherical Linear Interpolation (Slerp) for reference embeddings and template-based text interpolation for modifications, alongside leveraging LLMs for deep multimodal fusion of image and text queries. CoLLM achieves state-of-the-art results, significantly outperforming prior methods on benchmarks like CIRR and Fashion-IQ, with models trained on the introduced MTCIR dataset yielding up to 15% performance gains (e.g., achieving 45.8 Recall@1 on CIRR). For AI practitioners, this work demonstrates an effective approach to train robust CIR models using abundant image-caption data and highlights the capability of LLMs for sophisticated multimodal understanding and fusion. |
| Multi-Modal | MDocAgent: A Multi-Modal Multi-Agent Framework for Document
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2503.13964) or [HuggingFace](https://huggingface.co/papers/2503.13964))| Yun Li, Tong Sun, Ruiyi Zhang, Peng Xia, Siwei Han | MDocAgent is a novel multi-modal, multi-agent framework designed for enhanced document question answering (DocQA). The primary objective is to overcome the limitations of single-modal systems by effectively integrating and reasoning over both textual and visual information within complex documents. Its methodology employs parallel text and image Retrieval-Augmented Generation (RAG) pipelines feeding context to five specialized agents (general, critical, text, image, summarizing) that collaborate through analysis and synthesis stages. Preliminary results demonstrate an average accuracy improvement of 12.1% over state-of-the-art methods on benchmarks like MMLongBench and LongDocURL using top-1 retrieval. For AI practitioners, MDocAgent showcases a promising architecture using collaborative specialized agents and multi-modal RAG to tackle complex document understanding tasks requiring detailed cross-modal integration. |
| Computer Vision | Latent Space Super-Resolution for Higher-Resolution Image Generation
  with Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2503.18446) or [HuggingFace](https://huggingface.co/papers/2503.18446))| Seon Joo Kim, Jinwoo Kim, Sangmin Han, Jinho Jeong | This paper proposes LSRNA, a framework enhancing high-resolution image generation by performing super-resolution directly in the latent space of diffusion models. The objective is to overcome manifold deviation and detail loss associated with latent-space and RGB-space reference upsampling in reference-based generation pipelines. LSRNA combines a learned Latent space Super-Resolution (LSR) module for manifold alignment and Region-wise Noise Addition (RNA) guided by edge detection to boost high-frequency details in the upsampled latent reference. Integrating LSRNA significantly improves existing methods like DemoFusion, achieving superior quantitative results (e.g., pFID improved from 32.89 to 29.12 at 4096x4096 resolution) and notably faster inference speeds (e.g., 3x speedup for DemoFusion). For AI practitioners, LSRNA provides an effective method to generate higher-quality megapixel images more efficiently using existing diffusion models, highlighting the critical role of learned latent upsampling for reference guidance. |
| Reinforcement Learning | ReSearch: Learning to Reason with Search for LLMs via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2503.19470) or [HuggingFace](https://huggingface.co/papers/2503.19470))| Chenzheng Zhu, Yijie Zhou, Haoze Sun, Tianpeng Li, Mingyang Chen | ReSearch introduces a framework training Large Language Models (LLMs) to reason using external search via reinforcement learning, eliminating the need for supervised reasoning step data. The main objective is to enable LLMs to dynamically decide when and how to perform searches within a reasoning chain for complex, multi-hop questions. Key methodology involves using Group Relative Policy Optimization (GRPO) to optimize the LLM's policy, treating search operations as integral components guided by text-based thinking and rewarded based on final answer correctness. Trained ReSearch models demonstrate significant effectiveness, achieving average improvements over baselines of up to 14.82% in Exact Match and 15.46% in LLM-as-a-Judge scores for 32B parameter models across multiple benchmarks. The main implication is that reinforcement learning offers a scalable approach to instill complex reasoning and dynamic tool-use capabilities in LLMs without explicit supervision on intermediate steps, even eliciting reflection. |
| Natural Language Processing | LookAhead Tuning: Safer Language Models via Partial Answer Previews (Read more on [arXiv](https://arxiv.org/abs/2503.19041) or [HuggingFace](https://huggingface.co/papers/2503.19041))| Mengshu Sun, Lin Yuan, Yujie Luo, Mengru Wang, Kangwei Liu | This paper introduces LookAhead Tuning, a method to enhance the safety of large language models (LLMs) during fine-tuning by previewing partial answer prefixes. The primary objective is to mitigate the degradation of safety alignment often caused by fine-tuning, even with benign data, while preserving task performance. The key methodology involves modifying the training data by either appending the initial tokens of the ground-truth answer (Real Answer) or a generic prefix (Virtual Answer) to the input instruction, thereby minimizing perturbations to the model's initial token distributions. Experiments show that LookAhead Tuning (virtual) significantly improves safety (e.g., achieving 98.03% average RSR compared to 82.88% for Vanilla FT) while maintaining or improving task utility across datasets like GSM8K and SAMSum. The main implication for AI practitioners is that LookAhead Tuning offers a simple, low-resource, and effective data-centric approach to adapt LLMs safely without sacrificing performance. |
| Computer Vision | Frequency Dynamic Convolution for Dense Image Prediction (Read more on [arXiv](https://arxiv.org/abs/2503.18783) or [HuggingFace](https://huggingface.co/papers/2503.18783))| Ying Fu, Chenggang Yan, Liang Li, Lin Gu, CharlesChen2023 | This paper introduces Frequency Dynamic Convolution (FDConv), a novel method enhancing dynamic convolutions by learning frequency-diverse weights within a fixed parameter budget. The objective is to mitigate the high parameter cost and limited frequency adaptability observed in traditional dynamic convolution methods, where parallel weights often exhibit similar frequency responses. FDConv employs Fourier Disjoint Weight (FDW) to create diverse weights in the Fourier domain, Kernel Spatial Modulation (KSM) for fine-grained spatial adjustment, and Frequency Band Modulation (FBM) for spatially variant frequency tuning. Applied to ResNet-50 on COCO object detection, FDConv achieves 39.4 Apbox with only a +3.6M parameter increase, outperforming prior methods like ODConv which required +65.1M parameters for 39.2 Apbox. This work provides AI practitioners an efficient module to improve the adaptability of vision models for dense prediction tasks by incorporating frequency-domain insights without significant parameter overhead. |
| Multi-Modal | LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary
  Semantic Segmentation (Read more on [arXiv](https://arxiv.org/abs/2503.19777) or [HuggingFace](https://huggingface.co/papers/2503.19777))| Giorgos Tolias, Jiří Matas, Yannis Kalantidis, Vladan Stojnić | The paper introduces LPOSS/LPOSS+, a training-free approach improving open-vocabulary semantic segmentation by refining Vision-and-Language Model (VLM) predictions using label propagation (LP). Its objective is to enhance coarse VLM patch predictions for accurate pixel-level segmentation, especially near boundaries, without requiring model training. The methodology employs patch-level LP using affinities derived from a separate Vision Model (VM) for contextual refinement (LPOSS), followed by a pixel-level LP step for boundary sharpness (LPOSS+). LPOSS+ achieves state-of-the-art results among training-free methods, attaining an average mIoU of 42.1% across eight benchmark datasets using ViT-B/16 backbones. AI practitioners can leverage this approach to significantly improve open-vocabulary segmentation quality from off-the-shelf VLMs without expensive fine-tuning. |
| Machine Learning | Gumbel-Softmax Flow Matching with Straight-Through Guidance for
  Controllable Biological Sequence Generation (Read more on [arXiv](https://arxiv.org/abs/2503.17361) or [HuggingFace](https://huggingface.co/papers/2503.17361))| Alexander Tong, Yinuo Zhang, Sophia Tang, pranamanam | This paper introduces Gumbel-Softmax Flow Matching (FM) and Score Matching (SM), novel generative frameworks for discrete biological sequences operating on the continuous simplex. The primary objective is to develop a scalable and controllable method for generating high-quality DNA, proteins, and peptides by learning smooth interpolations between noise and target data using a time-dependent Gumbel-Softmax distribution. Key methodologies include deriving velocity fields (FM) or score functions (SM) based on this interpolant and proposing Straight-Through Guided Flows (STGFlow), a training-free classifier guidance technique leveraging straight-through estimators. The framework demonstrates state-of-the-art performance, achieving a low MSE of 0.029 in conditional DNA promoter design and generating peptides with improved VINA docking scores (e.g., -6.5 kcal/mol for 4EZN) compared to known binders. For AI practitioners, this work offers a robust approach for controllable discrete sequence generation via continuous relaxations on the simplex, with STGFlow providing an efficient method for incorporating external guidance without retraining. |
| Computer Vision | Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID (Read more on [arXiv](https://arxiv.org/abs/2503.17237) or [HuggingFace](https://huggingface.co/papers/2503.17237))| wish44165 | This paper presents a strong baseline for multi-UAV tracking in thermal infrared video using YOLOv12 and BoT-SORT-ReID. Its objective is to create a straightforward yet effective workflow for tracking UAVs in challenging thermal conditions, leveraging recent advances in detection and tracking. The core methodology combines YOLOv12 for detection and BoT-SORT (with ReID for MOT) for tracking, applying specific training and inference strategies optimized for the Anti-UAV Challenge dataset. The approach significantly outperforms the official challenge baseline, achieving a MOTA score of 0.7609 on Track 3 (compared to 0.3747). This work offers AI practitioners a robust foundation for thermal UAV tracking, highlighting key performance factors like detector/tracker choice and input image resolution for further optimization. |
| Multi-Modal | When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only
  Training For Human-Centered Decision Making (Read more on [arXiv](https://arxiv.org/abs/2503.16965) or [HuggingFace](https://huggingface.co/papers/2503.16965))| Yu Yin, Jing Li, Zhe Hu | This paper investigates enhancing Visual Language Models (VLMs) for human-centered decision-making by leveraging text-only training, demonstrating a novel self-improvement mechanism. The primary objective is to address the counterintuitive finding that Large Language Models (LLMs) outperform their VLM counterparts on these tasks and to develop an efficient method to improve VLM reasoning without requiring paired image-text data. The key methodology involves fine-tuning VLMs using only synthesized textual data generated by LLMs (GPT-4 or smaller counterparts), strengthening their language components, and transferring these abilities to multimodal inference. Primary results show significant performance gains through this text-only training, such as Qwen2-VL accuracy increasing from 80.32% to 83.15% on the VIVA benchmark, and demonstrate VLM self-improvement using data from similar-scale LLM counterparts. The main implication for AI practitioners is the potential for efficient and scalable VLM enhancement via text-only self-improvement, reducing reliance on large multimodal datasets or teacher models. |
| Multi-Modal | Towards a Unified Copernicus Foundation Model for Earth Vision (Read more on [arXiv](https://arxiv.org/abs/2503.11849) or [HuggingFace](https://huggingface.co/papers/2503.11849))| Thomas Dujardin, Adam J. Stewart, Chenying Liu, Zhitong Xiong, Yi Wang | This paper presents Copernicus-FM, a unified foundation model framework designed to process diverse Earth observation data from multiple Copernicus Sentinel missions. The primary objective is to overcome limitations of existing models by creating a scalable and versatile system capable of handling various spectral, non-spectral, and metadata inputs for holistic Earth system understanding. The methodology involves curating a large-scale pretraining dataset (Copernicus-Pretrain), developing a model (Copernicus-FM) with dynamic hypernetworks and metadata encoding, and establishing a comprehensive benchmark (Copernicus-Bench). Results show Copernicus-FM significantly outperforms baselines across 15 downstream tasks, achieving, for example, an RMSE of 789.4 on the AQ-O3-S5P air quality task compared to 1755.6 for the prior SOTA, demonstrating the effectiveness of unifying diverse sensor data and metadata. For AI practitioners, this work highlights a pathway towards building flexible, multi-modal foundation models for complex geospatial data, enabling integrated analysis across different Earth observation domains. |
