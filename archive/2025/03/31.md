

## Papers for 2025-03-31

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through
  Lightweight Vocabulary Adaptation (Read more on [arXiv](https://arxiv.org/abs/2503.19693) or [HuggingFace](https://huggingface.co/papers/2503.19693))| Roi Reichart, ehoffer, eyalbd, nitay, itaynakash | This paper introduces AdaptiVocab, an end-to-end approach for adapting Large Language Model (LLM) vocabularies to enhance efficiency in focused, low-resource domains. The main objective is to reduce latency and computational costs by optimizing the tokenization process without compromising performance. AdaptiVocab replaces low-frequency tokens with domain-specific n-gram tokens based on a savings score, initializes new embeddings using exponential weighting, and employs lightweight fine-tuning on embedding layers and the first/last transformer layers. Results on 7B LLMs across three domains demonstrate over 25% reduction in token usage for input and output, maintaining generation quality and end-task performance comparable to standard fine-tuning. For AI practitioners, this presents a resource-efficient method to improve LLM inference speed in specialized applications without altering model architecture. |
| Reinforcement Learning | Exploring Data Scaling Trends and Effects in Reinforcement Learning from
  Human Feedback (Read more on [arXiv](https://arxiv.org/abs/2503.22230) or [HuggingFace](https://huggingface.co/papers/2503.22230))| amusingchao, qingping95, zhengwu07, glnbyte, Swtheking | This paper explores data scaling bottlenecks in Reinforcement Learning from Human Feedback (RLHF) for large language models (LLMs), focusing on mitigating reward hacking and improving response diversity. The primary objective is to enhance RLHF performance by addressing these data-driven limitations through novel prompt construction and training strategies. Key methodologies include a hybrid reward system combining Reasoning Task Verifiers (RTV) and Generative Reward Models (GenRM) with ground truth, a 'Pre-PPO' prompt selection method targeting challenging prompts, and prioritizing math/coding tasks early in training. Results demonstrate significant performance gains over baseline PPO, achieving up to a +1.4 overall score improvement on challenging benchmarks, with RTV showing the highest resistance to reward hacking, followed by GenRM with ground truth. The main implication for practitioners is the critical importance of careful data curation and strategic prompt/task scheduling in overcoming performance barriers and improving scalability in RLHF. |
| Machine Learning | Think Before Recommend: Unleashing the Latent Reasoning Power for
  Sequential Recommendation (Read more on [arXiv](https://arxiv.org/abs/2503.22675) or [HuggingFace](https://huggingface.co/papers/2503.22675))| Xu Chen, Jun Xu, TengShi, KID-22, TangJiakai5704 | This paper introduces ReaRec, a novel framework enhancing sequential recommendation (SeqRec) models through multi-step implicit reasoning during inference time. The primary objective is to improve the modeling of complex user preference dynamics and long-tail items by increasing computational depth beyond traditional direct inference paradigms. ReaRec autoregressively feeds the last hidden state back into the sequence encoder multiple times, incorporating specialized position embeddings and optimized via two proposed learning strategies: Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL). Experiments demonstrate significant performance improvements across various backbones and datasets, achieving an average gain of 7.49% across metrics with only 3.51% additional latency for two reasoning steps, and post-hoc analysis reveals a potential performance ceiling increase of 30-50%. The main implication for practitioners is that strategically increasing inference-time computation through multi-step reasoning can significantly boost the performance of existing recommender models with minimal architectural changes. |
| Machine Learning | A Survey of Efficient Reasoning for Large Reasoning Models: Language,
  Multimodality, and Beyond (Read more on [arXiv](https://arxiv.org/abs/2503.21614) or [HuggingFace](https://huggingface.co/papers/2503.21614))| Elliott, weigao266, Warrieryes, yaful, Xiaoye08 | This survey provides a comprehensive overview of recent efforts to improve the reasoning efficiency of Large Reasoning Models (LRMs) across language, multimodality, and beyond. The primary objective is to address the tendency of LRMs to generate excessively long and often redundant reasoning traces, proposing a focus on maximizing "intelligence per token". The paper systematically reviews methods across the LRM lifecycle, including pre-training, supervised fine-tuning (SFT), reinforcement learning (RL), and inference, defining reasoning efficiency η(M) based on solution quality Q and computational cost C (Eq. 1). It highlights inefficiencies, noting for example that an LRM used 1248 tokens for a simple math problem compared to 30 tokens by its instruct counterpart (Figure 1). The key implication for AI practitioners is the critical need to develop and apply techniques that enhance reasoning efficiency for the practical deployment, scalability, and cost-effectiveness of advanced AI systems. |
| Multi-Modal | ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation (Read more on [arXiv](https://arxiv.org/abs/2503.22194) or [HuggingFace](https://huggingface.co/papers/2503.22194))| Jihyun Lee, Minhyuk, 32V, daehyeonchoi, myhong | This paper introduces ORIGEN, the first zero-shot method enabling 3D orientation grounding for multiple objects across diverse categories in text-to-image generation. The primary objective is to provide precise control over 3D object orientation, a capability lacking in previous spatial grounding methods that mainly focus on 2D positioning. ORIGEN utilizes a reward-guided sampling approach based on Langevin dynamics, guided by a reward function derived from a pretrained 3D orientation estimation model (OrientAnything) and applied to a one-step generative flow model, incorporating adaptive time rescaling for faster convergence. Quantitative results demonstrate ORIGEN's superiority, achieving 87.1% accuracy (Acc.@22.5°) on the MS-COCO-Single orientation grounding benchmark, significantly outperforming existing training-based and test-time guidance methods. For AI practitioners, ORIGEN offers a novel, training-free approach to enhance the controllability of text-to-image models by explicitly grounding 3D object orientation. |
| Computer Vision | Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal
  Consistency (Read more on [arXiv](https://arxiv.org/abs/2503.20785) or [HuggingFace](https://huggingface.co/papers/2503.20785))| skhu101, GuangcongWang, FrozenBurning, Inso, tqliu | Free4D introduces a tuning-free framework for 4D scene generation from a single image while enforcing spatial-temporal consistency. The paper addresses the challenge of generating dynamic 3D scenes from limited observations by distilling pre-trained foundation models for consistent 4D scene representation. The method involves animating an input image using image-to-video diffusion models, initializing 4D geometric structures, and applying an adaptive guidance mechanism and latent replacement strategy. Free4D achieves high consistency, reaching a 96.8% consistency score on the VBench benchmark, and offers real-time, controllable rendering. This allows AI practitioners to generate high-quality 4D scenes without extensive training or fine-tuning. |
| Multi-Modal | Perceptually Accurate 3D Talking Head Generation: New Definitions,
  Speech-Mesh Representation, and Evaluation Metrics (Read more on [arXiv](https://arxiv.org/abs/2503.20308) or [HuggingFace](https://huggingface.co/papers/2503.20308))| taehyunoh, akasha9890, backryun, Han-EunGi, Chae-Yeon | This paper introduces a framework for generating perceptually accurate 3D talking heads by defining new criteria, proposing a novel speech-mesh representation, and developing corresponding evaluation metrics. The primary objective is to enhance the perceptual alignment between speech signals and 3D facial movements based on three key criteria: Temporal Synchronization, Lip Readability, and Expressiveness. The authors propose a two-stage training process to learn a synchronized speech-mesh representation, utilizing it as a plug-and-play perceptual loss for existing models, and introduce three new metrics: Mean Temporal Misalignment (MTM), Perceptual Lip Readability Score (PLRS), and Speech-Lip Intensity Correlation Coefficient (SLCC). Experiments demonstrate that using this perceptual loss significantly improves performance across all three criteria, for example, boosting FaceFormer's PLRS from 0.368 to 0.463 on the VOCASET dataset. The main implication for AI practitioners is the provision of a method and metrics to explicitly optimize and evaluate the perceptual realism of talking heads beyond standard geometric error, particularly capturing nuances like speech intensity correlation with lip motion. |
| Multi-Modal | PHYSICS: Benchmarking Foundation Models on University-Level Physics
  Problem Solving (Read more on [arXiv](https://arxiv.org/abs/2503.21821) or [HuggingFace](https://huggingface.co/papers/2503.21821))| armanc, jsous, henryL7, yilunzhao, Carrie777 | The paper introduces PHYSICS, a comprehensive benchmark designed to evaluate foundation models on university-level physics problem-solving. Its primary objective is to assess the multi-step reasoning, mathematical proficiency, and domain-specific knowledge integration capabilities of AI models using 1,297 challenging, open-ended problems derived from PhD-qualifying exams. The methodology involves expert annotation of problems and solutions, and a robust automated evaluation system utilizing SymPy for mathematical verification and GPT-4o for assessing correctness, including natural language aspects. Key results show significant limitations in current models, with the top-performing model (o3-mini) achieving only 59.9% accuracy, highlighting a substantial gap compared to human expert performance. This implies that advanced scientific problem-solving remains a major challenge for AI, necessitating improvements in reasoning frameworks and knowledge integration. |
| Computer Vision | Segment Any Motion in Videos (Read more on [arXiv](https://arxiv.org/abs/2503.22268) or [HuggingFace](https://huggingface.co/papers/2503.22268))| Nan Huang, qianqian68, akanazawa, kurtkeutzer, chenfengx | This paper presents a novel approach for segmenting moving objects in videos by integrating long-range trajectory motion cues, DINO-based semantic features, and leveraging SAM2 for mask densification. The main objective is to achieve robust Moving Object Segmentation (MOS), distinguishing true object motion from camera motion and handling challenges like deformation and occlusion. Key methodologies include Spatio-Temporal Trajectory Attention for encoding motion patterns and Motion-Semantic Decoupled Embedding for balancing motion and semantic information, followed by an Iterative Prompting strategy with SAM2. The method demonstrates state-of-the-art performance, achieving an F-score of 91.0% on the DAVIS2016 MOS benchmark and 83.6% on the fine-grained DAVIS17-Moving benchmark. For AI practitioners, this work offers an effective technique for detailed dynamic scene analysis by uniquely combining long-term tracking, semantic understanding, and promptable segmentation models. |
| Computer Vision | Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal
  Bridging (Read more on [arXiv](https://arxiv.org/abs/2503.22236) or [HuggingFace](https://huggingface.co/papers/2503.22236))| Xiaoyang Guo, Jiahao Chang, Yushuang Wu, Chongjie Ye, LUZITENG | This paper introduces Hi3DGen, a novel framework for generating high-fidelity 3D geometry from single 2D images by leveraging normal maps as an intermediate representation. The primary objective is to address the challenge of reproducing fine-grained geometric details often lost in direct image-to-3D mapping due to domain gaps and image ambiguities. The key methodology involves three components: an image-to-normal estimator (NiRNE) using noise injection and dual-stream training for sharp normal prediction, a normal-to-geometry approach (NoRLD) employing normal-regularized latent diffusion learning, and a high-quality synthesized 3D dataset (DetailVerse) for training. Experiments show Hi3DGen's superiority, with the NiRNE component achieving a state-of-the-art Normal Error (NE) of 21.837 on the LUCES-MV dataset. For AI practitioners, this work highlights the effectiveness of using intermediate geometric representations like normal maps and specialized training strategies with synthesized data to significantly boost the fidelity of 3D generation from images. |
| Multi-Modal | OThink-MR1: Stimulating multimodal generalized reasoning capabilities
  via dynamic reinforcement learning (Read more on [arXiv](https://arxiv.org/abs/2503.16081) or [HuggingFace](https://huggingface.co/papers/2503.16081))| Changwang Zhang, Feng Liu, Yuting Zhang, Zhiyuan Liu, jwanglux | This paper introduces OThink-MR1, a framework enhancing Multimodal Large Language Models (MLLMs) with dynamic reinforcement learning for improved generalized reasoning. The primary objective is to overcome the limitations of supervised fine-tuning (SFT) and static reinforcement learning (RL) strategies, specifically their poor cross-task generalization and suboptimal exploration-exploitation balance in multimodal settings. The key methodology involves Group Relative Policy Optimization with a dynamic Kullback-Leibler divergence strategy (GRPO-D), which adaptively adjusts regularization during training. GRPO-D demonstrates superior performance, achieving over 61.63% average relative improvement compared to SFT in cross-task generalization evaluations, indicating effective knowledge transfer between different multimodal tasks. The main implication is that dynamic RL approaches like GRPO-D can significantly boost the generalization capabilities of MLLMs across diverse tasks, reducing the reliance on task-specific data and training. |
| Computer Vision | Your ViT is Secretly an Image Segmentation Model (Read more on [arXiv](https://arxiv.org/abs/2503.19108) or [HuggingFace](https://huggingface.co/papers/2503.19108))| Giuseppe Averta, Narges Norouzi, Alexander Hermans, Niccolò Cavagnero, Tommie Kerssies | This paper introduces the Encoder-only Mask Transformer (EoMT), demonstrating that plain Vision Transformers (ViTs), with sufficient scale and pre-training, can perform image segmentation effectively without complex task-specific add-ons. The research questions the necessity of components like adapters and decoders, hypothesizing they become redundant with large, pre-trained models. The methodology involves systematically removing these components from a baseline (ViT-Adapter + Mask2Former) and repurposing ViT blocks in EoMT to directly process segmentation queries, using a mask annealing technique for efficient inference. EoMT achieves competitive accuracy (e.g., 56.0 Panoptic Quality on COCO val2017 with ViT-L) while being significantly faster (up to 4x, 128 FPS vs 29 FPS) than the baseline. This suggests practitioners may achieve better efficiency by scaling the core ViT rather than adding auxiliary architectural complexity for segmentation. |
| Multi-Modal | 4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2503.17827) or [HuggingFace](https://huggingface.co/papers/2503.17827))| mhelhoseiny, ajhamdi, TonNew, bing-li-ai, vxuanz | This paper introduces 4D-Bench, the first benchmark designed to evaluate Multi-modal Large Language Models (MLLMs) on 4D object understanding. The primary objective is to assess MLLM capabilities in comprehending dynamic 3D objects (4D objects) over time, specifically addressing tasks requiring multi-view spatial-temporal reasoning. The methodology involves creating 4D object Question Answering (QA) and captioning tasks using rendered multi-view videos of dynamic 3D assets and evaluating various MLLMs. Key findings show that even state-of-the-art models like GPT-4o perform poorly on 4D object QA (63% accuracy vs. 91% human baseline), struggling significantly with temporal understanding and object counting compared to appearance understanding. This highlights a substantial gap in current MLLMs' ability to handle complex 4D data, indicating a need for further advancements in multi-view spatial-temporal reasoning capabilities. |
| Natural Language Processing | A Refined Analysis of Massive Activations in LLMs (Read more on [arXiv](https://arxiv.org/abs/2503.22329) or [HuggingFace](https://huggingface.co/papers/2503.22329))| Fabian Güra, akanyaani, nilabhra, louisowen6 | This paper presents a refined analysis of massive activations across diverse Large Language Model (LLM) architectures, challenging prior assumptions about their universal detrimental nature and mitigation efficacy. The research aims to systematically investigate the characteristics and impact of these activations and evaluate various mitigation strategies, including Attention KV Bias, Target Variance Rescaling (TVR), and Dynamic Tanh (DyT). Using intervention analysis on pre-trained models and retraining experiments on GPT-2 and LLaMA-1B, the study measures perplexity and downstream task performance. Key findings reveal that massive activations are not always harmful, Attention KV bias is ineffective for LLaMA-1B, but hybrid methods like KV Bias + TVR (mean downstream accuracy 52.0 vs 50.3 baseline) and DyT + TVR (50.3 mean accuracy) successfully balance activation mitigation and performance preservation. The primary implication for AI practitioners is that massive activation behavior and mitigation effectiveness are architecture-dependent, necessitating careful evaluation, with hybrid strategies like TVR combinations offering robust solutions relevant to quantization and numerical stability. |
| Computer Vision | SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling (Read more on [arXiv](https://arxiv.org/abs/2503.21732) or [HuggingFace](https://huggingface.co/papers/2503.21732))| Lp256, pookiefoof, bennyguo, zouzx, XianglongHe | SparseFlex introduces a novel sparse-structured isosurface representation for high-fidelity, arbitrary-topology 3D shape modeling. The primary objective is to enable differentiable, high-resolution (up to 1024^3) mesh reconstruction and generation directly from rendering losses, handling complex geometries including open surfaces and interiors, which challenge existing methods. Key methodologies include the SparseFlex representation, combining Flexicubes' accuracy with a memory-efficient sparse voxel structure, and a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering. Experiments demonstrate state-of-the-art reconstruction accuracy, achieving an ~82% reduction in Chamfer Distance and ~88% increase in F-score compared to previous methods on benchmark datasets. For AI practitioners, SparseFlex offers an advanced representation and training paradigm for creating detailed, high-resolution 3D assets with complex topologies using efficient rendering-based supervision. |
| Natural Language Processing | ReFeed: Multi-dimensional Summarization Refinement with Reflective
  Reasoning on Feedback (Read more on [arXiv](https://arxiv.org/abs/2503.21332) or [HuggingFace](https://huggingface.co/papers/2503.21332))| jasoncai, hwany-j, Myyhlee, hyang0503, hamzzi | This paper introduces ReFeed, a pipeline for multi-dimensional summarization refinement using reflective reasoning on feedback to enhance faithfulness, completeness, and conciseness simultaneously. The main objective is to address challenges in multi-dimensional refinement, such as quality trade-offs, feedback ordering bias, and noisy feedback, which are limitations of existing methods. ReFeed's methodology involves distilling reflective reasoning capabilities from a large reasoning model into a lightweight model using a newly created large-scale dataset, SumFeed-CoT, which incorporates Long-CoT reasoning on multi-dimensional feedback. Experiments show ReFeed achieves significant improvements, yielding an average quality score increase of +8.4 (reaching 75.3) across the three dimensions compared to original summaries, and demonstrates robustness against feedback noise and order variations. The key implication is that incorporating reflective reasoning and simultaneous multi-dimensional feedback processing is crucial for effective text refinement, mitigating trade-offs better than simpler receptive or sequential approaches. |
| Multi-Modal | MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via
  Reasoning Agentic Workflow (Read more on [arXiv](https://arxiv.org/abs/2503.18968) or [HuggingFace](https://huggingface.co/papers/2503.18968))| Yueming Jin, Chang Han Low, morson, ZiyueWang | This paper introduces MedAgent-Pro, a reasoning agentic workflow for evidence-based multi-modal medical diagnosis. The primary objective is to overcome the limitations of existing Multi-modal Large Language Models (MLLMs), such as hallucinations and lack of quantitative analysis, for reliable clinical diagnosis. The methodology employs a hierarchical structure where a task-level planner agent uses retrieved clinical guidelines (via RAG) to create diagnostic plans, and case-level tool agents (specialized vision models, LLMs) execute these plans on multi-modal patient data to analyze indicators. MedAgent-Pro significantly outperformed baseline MLLMs, achieving 90.4% mACC on glaucoma diagnosis compared to 53.4% for LLaVa-Med. The main implication for AI practitioners is that structured, evidence-based agentic workflows integrating specialized tools and knowledge retrieval can enhance the reliability and explainability of MLLMs for complex, safety-critical applications like medical diagnosis. |
| Computer Vision | X^{2}-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time
  Tomographic Reconstruction (Read more on [arXiv](https://arxiv.org/abs/2503.21779) or [HuggingFace](https://huggingface.co/papers/2503.21779))| yixuanyuan, XGGNet, Fanzhiwen, CaiYuanhao, vortex778 | This paper introduces X²-Gaussian, a framework for continuous-time 4D Computed Tomography (CT) reconstruction using dynamic radiative Gaussian splatting. The primary objective is to overcome the limitations of traditional discrete phase-binning methods and the reliance on external respiratory gating devices in 4D CT. The key methodology involves modeling anatomical dynamics with a spatiotemporal encoder-decoder architecture predicting time-varying Gaussian deformations and employing a self-supervised, physiology-driven periodic consistency loss to learn breathing cycles directly from projections. Experimental results demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR gain over traditional FDK methods and a 2.25 dB improvement over the prior R²-GS method on the DIR dataset. For AI practitioners in medical imaging, this work offers a high-fidelity, hardware-free approach to continuous 4D dynamic imaging, enhancing motion analysis for clinical applications like radiotherapy. |
| Multi-Modal | On Large Multimodal Models as Open-World Image Classifiers (Read more on [arXiv](https://arxiv.org/abs/2503.21851) or [HuggingFace](https://huggingface.co/papers/2503.21851))| Yiming Wang, Enrico Fini, paolorota, massimilianom, altndrr | This paper evaluates the performance of Large Multimodal Models (LMMs) on open-world image classification tasks where categories are not predefined. The primary objective is to rigorously assess LMM classification capabilities in this open-world setting and characterize the types of errors made. The methodology involves formalizing the task, introducing four evaluation metrics (Text Inclusion, Llama Inclusion, Semantic Similarity, Concept Similarity), and testing 13 LMMs across 10 benchmarks with varying granularity. Key results show LMMs generally outperform contrastive open-world baselines like CaSED on metrics such as Llama Inclusion (e.g., Qwen2VL 7B achieves 60.3 average LI) but still lag significantly behind closed-world models like CLIP, particularly struggling with very fine-grained classification (often 0.0 Text Inclusion). The main implication is that while LMMs show promise for recognizing diverse concepts, practitioners need to address challenges in granularity and fine-grained discrimination, potentially through tailored prompting or reasoning, for reliable open-world classification. |
| Computer Vision | Reconstructing Humans with a Biomechanically Accurate Skeleton (Read more on [arXiv](https://arxiv.org/abs/2503.21751) or [HuggingFace](https://huggingface.co/papers/2503.21751))| Qixing Huang, Etienne Vouga, Xiaowei Zhou, geopavlakos, IsshikiHugh | This paper introduces HSMR (Human Skeleton and Mesh Recovery), a method for reconstructing 3D humans, including a biomechanically accurate skeleton and surface mesh, from a single input image. The primary objective is to estimate parameters for the SKEL model, which incorporates realistic joint constraints, directly from images, addressing the limitations of anatomically inaccurate models like SMPL. The methodology involves training a transformer network to regress SKEL parameters, generating initial training data by converting existing SMPL pseudo-ground truth, and iteratively refining these labels using an optimization process (SKELify) guided by 2D keypoints. HSMR achieves competitive performance on standard benchmarks and significantly outperforms state-of-the-art SMPL-based methods like HMR2.0 on challenging datasets with extreme poses, such as improving PA-MPJPE on MOYO by over 10mm (79.6 vs 90.4). For AI practitioners, this work offers a way to generate more realistic and biomechanically valid 3D human poses, reducing unnatural joint rotations often seen with simpler models. |
