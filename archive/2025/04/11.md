

## Papers for 2025-04-11

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Kimi-VL Technical Report (Read more on [arXiv](https://arxiv.org/abs/2504.07491) or [HuggingFace](https://huggingface.co/papers/2504.07491))| dongliangwang, congcongwang, DuChenZhuang, tzzcl, xingbowei | This paper introduces Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) designed for multimodal reasoning, long-context understanding, and agent capabilities while activating only 2.8B parameters. The main objective is to develop a scalable and efficient open-source VLM that rivals larger models in diverse vision-language tasks, including long-context processing and high-resolution image understanding. Kimi-VL employs a 16B MoE language model (2.8B active) combined with a native-resolution MoonViT vision encoder, trained through extensive multi-stage pre-training and subsequent fine-tuning, with an advanced 'Thinking' variant utilizing long-CoT SFT and RL. Key results include achieving 83.2 on InfoVQA, 64.5 on LongVideoBench (with 128K context), and the Thinking variant reaching 61.7 on MMMU and 71.3 on MathVista, demonstrating strong performance despite its efficiency. For AI practitioners, Kimi-VL presents a viable, parameter-efficient open-source alternative for complex multimodal tasks, highlighting the potential of MoE architectures in VLM development. |
| Multi-Modal | VCR-Bench: A Comprehensive Evaluation Framework for Video
  Chain-of-Thought Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.07956) or [HuggingFace](https://huggingface.co/papers/2504.07956))| lovesnowbest, Lin-Chen, Osilly, ChthollyTree, yukunqi | This paper introduces VCR-Bench, a novel benchmark designed to comprehensively evaluate the Video Chain-of-Thought (CoT) reasoning capabilities of Large Vision-Language Models (LVLMs). The primary objective is to address the lack of rigorous evaluation frameworks that can assess the entire reasoning process and differentiate between perception and reasoning failures in video understanding. VCR-Bench utilizes a dataset of 859 videos with 1,034 question-answer pairs, annotated with stepwise CoT rationales tagged for perception or reasoning, and evaluates models across seven task dimensions using a proposed CoT score based on step recall and precision. Experiments reveal significant limitations in current LVLMs, with the top-performing model (o1) achieving only a 62.8% CoT score and 56.7% accuracy, indicating a key bottleneck in temporal-spatial perception. VCR-Bench offers AI practitioners a standardized framework to diagnose specific weaknesses in LVLMs for complex video reasoning tasks, underscoring the critical need for improved CoT capabilities. |
| Computer Vision | VisualCloze: A Universal Image Generation Framework via Visual
  In-Context Learning (Read more on [arXiv](https://arxiv.org/abs/2504.07960) or [HuggingFace](https://huggingface.co/papers/2504.07960))| mingming8688, cosumosu25, JonsonYan, RuoyiDu, lzyhha | This paper introduces VisualCloze, a universal image generation framework leveraging visual in-context learning to handle diverse tasks within a single model. The primary objective is to overcome the limitations of task-specific models and ambiguous language instructions by enabling models to learn task specifications directly from visual demonstrations. The key methodology involves formulating image generation as an infilling problem on a grid layout containing in-context examples and a query, fine-tuning a pre-trained infilling model (FLUX.1-Fill-dev) on a novel graph-structured dataset (Graph200K) designed to increase task density. Quantitative results demonstrate the effectiveness of in-context learning; for instance, on the depth-to-image task, increasing in-context examples from 0 to 2 reduces RMSE from 10.31 to 9.68. The main implication for AI practitioners is that visual in-context learning offers a promising approach for building more versatile and generalizable image generation models by utilizing visual demonstrations for task specification, potentially reducing reliance on complex text prompts and leveraging strong priors from infilling models. |
| Multi-Modal | MM-IFEngine: Towards Multimodal Instruction Following (Read more on [arXiv](https://arxiv.org/abs/2504.07957) or [HuggingFace](https://huggingface.co/papers/2504.07957))| yhcao, sweetFruit, KennyUTC, yuhangzang, ChrisDing1105 | This paper presents MM-IFEngine, a pipeline for generating high-quality multimodal instruction-following (IF) training data, and MM-IFEval, a challenging benchmark to evaluate MLLMs' IF capabilities. The main objective is to address the scarcity of diverse, complex training data and benchmarks for multimodal IF, which limits current MLLM performance in precise instruction execution. The methodology involves a three-stage pipeline (Image Filter, Task Generation, Constraints Integration) using LLMs like GPT-4o to create the MM-IFInstruct-23k (SFT) and MM-IFDPO-23k (DPO) datasets, and introduces the MM-IFEval benchmark with 32 constraint types and a hybrid evaluation system. Fine-tuning Qwen2-VL-7B on MM-IFDPO-23k demonstrated notable improvements, achieving gains of +10.2% on MM-IFEval, +7.6% on MIA-Bench, and +12.3% on IFEval. For AI practitioners, MM-IFEngine offers a way to generate richer training data, while MM-IFEval provides a more comprehensive evaluation framework for assessing and improving the crucial instruction-following abilities of MLLMs. |
| Natural Language Processing | DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.07128) or [HuggingFace](https://huggingface.co/papers/2504.07128))| parishadbehnam, miladink, vaibhavad, arkilpatel, spaidartaigar | This paper introduces "Thoughtology", a systematic analysis of the reasoning chains, or "thoughts", generated by the Large Reasoning Model (LRM) DeepSeek-R1. The primary objective is to understand the structure, characteristics, and limitations of DeepSeek-R1's reasoning across various dimensions including thought length scaling, long context processing, faithfulness, safety, cultural aspects, and parallels with human cognition. The methodology involves defining a taxonomy for reasoning steps (Problem Definition, Bloom Cycle, Reconstruction Cycle(s), Final Decision), annotating reasoning chains across diverse tasks (math, QA, safety, NLP benchmarks), and conducting quantitative and qualitative analyses. Key results show that DeepSeek-R1 follows a structured process often involving redundant 'rumination', exhibits a non-monotonic relationship between thought length and performance (e.g., accuracy peaks then declines on AIME-24), shows high faithfulness but struggles with extremely long contexts (95% on 120k NIH), and possesses significant safety vulnerabilities (ASR increases from 30% to 72.5% on HarmBench with generated jailbreaks). For AI practitioners, this implies that while LRMs demonstrate complex reasoning, controlling thought length, ensuring faithfulness, improving efficiency, and addressing safety concerns are crucial challenges requiring further investigation before reliable deployment. |
| Computer Vision | HoloPart: Generative 3D Part Amodal Segmentation (Read more on [arXiv](https://arxiv.org/abs/2504.07943) or [HuggingFace](https://huggingface.co/papers/2504.07943))| Lp256, zouzx, KevinHuang, bennyguo, yhyang-myron | This paper introduces HoloPart, a method for 3D part amodal segmentation which decomposes 3D shapes into complete, semantically meaningful parts, including occluded geometry. The primary objective is to overcome the limitations of standard 3D part segmentation that only captures visible surface patches, enabling applications requiring complete part information. The key methodology involves a two-stage approach: first obtaining incomplete part segments using existing segmentation techniques, and then using the proposed novel diffusion-based model, HoloPart, equipped with local and context-aware attention, to complete these segments into full 3D parts, leveraging a pretrained generative prior. HoloPart significantly outperforms prior shape completion methods on new benchmarks based on ABO and PartObjaverse-Tiny datasets (e.g., achieving 0.764 IoU on ABO compared to 0.565 for a finetuned VAE baseline). For AI practitioners, this work provides a way to generate complete part geometry from segmented shapes, facilitating downstream tasks like geometry editing, animation, and material assignment. |
| Machine Learning | C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization
  for Test-Time Expert Re-Mixing (Read more on [arXiv](https://arxiv.org/abs/2504.07964) or [HuggingFace](https://huggingface.co/papers/2504.07964))| Ziyue Li, zhoutianyi, Lzy01241010 | C3PO introduces a test-time optimization framework for Mixture-of-Experts (MoE) Large Language Models (LLMs) that dynamically re-mixes expert pathways to enhance performance. The objective is to overcome the severe sub-optimality of default expert routing learned during pretraining by adapting pathways for individual test samples using collaborative information from reference examples. The key methodology involves optimizing routing weights via surrogate objectives based on successful neighbor samples, employing techniques like Neighborhood Gradient Descent (NGD), and efficiently focusing updates on critical layers and core experts. C3PO significantly improves accuracy, boosting base MoE models by 7-15% across benchmarks (e.g., OLMoE-1B-7B average accuracy increased by 9.3% to 79.2%), enabling smaller MoE models to surpass larger dense models. This presents practitioners with an efficient test-time strategy to enhance MoE model performance by dynamically optimizing computational pathways without retraining. |
| Machine Learning | MOSAIC: Modeling Social AI for Content Dissemination and Regulation in
  Multi-Agent Simulations (Read more on [arXiv](https://arxiv.org/abs/2504.07830) or [HuggingFace](https://huggingface.co/papers/2504.07830))| Marzyeh Ghassemi, saadia, elisakreiss, salmannyu, genglinliu | This paper introduces MOSAIC, a novel multi-agent simulation framework using Large Language Model (LLM)-powered agents to model social network dynamics, content dissemination, and regulation. The main objective is to analyze emergent social behaviors, particularly concerning misinformation spread, and evaluate different content moderation strategies within a simulated environment. The methodology combines generative LLM agents instantiated with diverse personas, a directed social graph for interactions (liking, sharing, flagging), and mechanisms for evaluating community-based, third-party, and hybrid fact-checking. Key findings include that a hybrid fact-checking approach achieved the best trade-off with an F1 score of 0.612, and contrary to human studies, simulated misinformation did not spread faster than factual news; moderation strategies also increased overall user engagement. The main implication for AI practitioners is the demonstration of generative agent simulations as a viable tool for studying complex online social behaviors, testing intervention strategies, and assessing risks associated with AI in social contexts, although agents' reasoning traces may not fully align with collective behavior. |
| Multi-Modal | SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual
  Reasoning Self-Improvement (Read more on [arXiv](https://arxiv.org/abs/2504.07934) or [HuggingFace](https://huggingface.co/papers/2504.07934))| furongh-lab, kevinlin311tw, linjieli222, zyang39, russwang | This paper presents a data-efficient method for enhancing visual reasoning in Vision Language Models (VLMs) through self-improvement without knowledge distillation. The primary objective is to demonstrate that reinforcement fine-tuning (RFT) with a small, strategically selected dataset can significantly boost VLM reasoning performance. The key methodology involves repurposing Monte Carlo Tree Search (MCTS) to quantify training sample difficulty based on the number of iterations required by the VLM to solve the problem, filtering an initial 70k dataset down to 11k challenging samples for RFT. The resulting model, ThinkLite-VL-7B, achieves state-of-the-art 75.1% accuracy on MathVista using only 11k samples, improving the base Qwen2.5-VL-7B-Instruct's average performance by 7% across eight benchmarks. The main implication for AI practitioners is that MCTS-guided difficulty-based sample selection enables highly data-efficient RFT for VLMs, improving reasoning capabilities substantially with less data and no external knowledge distillation. |
| Multi-Modal | Scaling Laws for Native Multimodal Models Scaling Laws for Native
  Multimodal Models (Read more on [arXiv](https://arxiv.org/abs/2504.07951) or [HuggingFace](https://huggingface.co/papers/2504.07951))| Joshua Susskind, Matthieu Cord, Victor Guilherme Turrisi da Costa, Enrico Fini, Mustafa Shukor | This paper investigates the scaling properties of native multimodal models (NMMs) trained from scratch, comparing early and late fusion architectures. The primary objective is to determine if late-fusion approaches hold an inherent advantage over early-fusion NMMs and to establish their respective scaling laws. The study involves training 457 models across various architectures (early/late fusion, dense/MoE), sizes, and data mixtures, fitting power laws relating validation loss to compute (FLOPs), model parameters (N), and training data (D). Key findings reveal that early and late fusion NMMs achieve comparable performance at scale (e.g., loss scaling exponents L ∝ C^-0.0492 vs L ∝ C^-0.0494), but early fusion is more parameter-efficient and performs better at lower compute budgets; sparse MoE models further enhance early-fusion performance. The results suggest that early-fusion NMMs, particularly with sparsity, are a competitive and potentially more efficient alternative for practitioners, challenging the common reliance on late-fusion systems assembled from pre-trained unimodal models. |
| Multi-Modal | Towards Visual Text Grounding of Multimodal Large Language Model (Read more on [arXiv](https://arxiv.org/abs/2504.04974) or [HuggingFace](https://huggingface.co/papers/2504.04974))| Franck-Dernoncourt, YfZ, JoshuaGu, zhangry868, MingLiiii | This paper introduces TRIG, a novel task, benchmark (TRIG-Bench), and dataset to evaluate and improve the visual text grounding capabilities of Multimodal Large Language Models (MLLMs) on text-rich document images. The primary objective is to address the significant limitations of current MLLMs in accurately localizing textual evidence within documents for tasks like question-answering. Key methodology involves an OCR-LLM-human pipeline for benchmark creation (800 manually verified QA pairs) and synthetic data generation (90k instances), alongside proposing instruction-tuning and embedding-based methods. Evaluation shows existing models struggle significantly in OCR-free grounding (e.g., GPT-4o achieves 5.26% average pixel-level IoU), while the proposed instruction-tuned method substantially improves this to 29.98% average IoU. The main implication for AI practitioners is that visual text grounding in documents remains a challenging, under-explored area requiring specialized datasets and fine-tuning for MLLMs to produce verifiable and trustworthy outputs. |
| Computer Vision | MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular
  Detection (Read more on [arXiv](https://arxiv.org/abs/2504.06801) or [HuggingFace](https://huggingface.co/papers/2504.06801))| R. Venkatesh Babu, Jogendra Kundu, Sarthak Vora, Srinjay Sarkar, RishubhPar | MonoPlace3D improves monocular 3D object detection through realistic, scene-aware data augmentation by learning where and how to place synthetic objects. The main objective is to automatically determine plausible 3D placement parameters (position, dimensions, orientation) for inserting objects realistically into background scenes. The core methodology involves training a Scene-Aware Placement Network (SA-PlaceNet) to learn a distribution over plausible 3D bounding boxes for a given scene, and then rendering objects according to sampled locations using synthetic assets enhanced by ControlNet. Evaluations demonstrate significant accuracy improvements on KITTI and NuScenes; for example, MonoDLE's AP40@IoU=0.7 (Easy) on KITTI increased from 17.45 to 22.49, and performance comparable to using 100% real data was achieved with only 50% real data plus augmentation. This implies that learning context-aware 3D object placement is a critical factor for effective data augmentation in 3D detection, significantly boosting performance and data efficiency. |
| Multi-Modal | Compass Control: Multi Object Orientation Control for Text-to-Image
  Generation (Read more on [arXiv](https://arxiv.org/abs/2504.06752) or [HuggingFace](https://huggingface.co/papers/2504.06752))| R. Venkatesh Babu, Vaibhav Agrawal, sachi1, RishubhPar | Compass Control introduces a method for precise, multi-object 3D orientation control in text-to-image diffusion models. The research aims to overcome the limitations of text-only prompts by allowing explicit orientation specification for individual objects in generated scenes. Its key methodology involves conditioning the diffusion model with orientation-aware 'compass tokens' predicted by a lightweight encoder, coupled with a 'Coupled Attention Localization' (CALL) mechanism that constrains cross-attention maps to specific object regions. Quantitatively, the method achieves state-of-the-art orientation control, significantly reducing Angular Error compared to baselines (e.g., 0.198 vs 0.385 for single objects) and shows strong generalization to unseen objects and complex scenes. For AI practitioners, this provides a user-friendly interface for fine-grained 3D control in image generation without requiring dense 3D data or multi-view inputs. |
| Computer Vision | TAPNext: Tracking Any Point (TAP) as Next Token Prediction (Read more on [arXiv](https://arxiv.org/abs/2504.05579) or [HuggingFace](https://huggingface.co/papers/2504.05579))| Viorica Patraucean, Skanda Koppula, Yi Yang, Carl Doersch, Artem Zholus | TAPNext is a novel causal model that formulates Tracking Any Point (TAP) in videos as online, sequential masked token decoding. The research aims to create a simpler, scalable, and high-performance TAP model by removing complex tracking-specific inductive biases and heuristics present in prior methods. TAPNext employs a TRecViT architecture, jointly processing image patch tokens and point track tokens (with masking for unknown future points) using interleaved spatial Transformer and temporal State-Space Model (SSM) layers for causal, per-frame prediction via token imputation. The model achieves new state-of-the-art results among online trackers, with the BootsTAPNext-B variant obtaining 78.5 Average Jaccard (AJ) on the DAVIS First benchmark, outperforming previous online methods while operating with minimal frame latency. This work demonstrates that effective point tracking can be achieved using general sequence modeling principles without specialized heuristics, suggesting a path towards more scalable video correspondence models where complex behaviors emerge from end-to-end training. |
