

## Papers for 2025-04-04

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Machine Learning | Advances and Challenges in Foundation Agents: From Brain-Inspired
  Intelligence to Evolutionary, Collaborative, and Safe Systems (Read more on [arXiv](https://arxiv.org/abs/2504.01990) or [HuggingFace](https://huggingface.co/papers/2504.01990))| KaitaoSong, JinlinW, Peiyan, xinfeng1i, Bang-UdeM-Mila | This survey provides a comprehensive overview of advances and challenges in foundation agents, framing them within a modular, brain-inspired architecture. The main objective is to structure the exploration of intelligent agents by examining their core components (cognition, perception, action, memory, world models, reward, emotion), self-enhancement mechanisms, collaborative multi-agent systems, and the critical aspects of safety and security. The methodology involves systematically mapping agent modules to human brain functionalities, reviewing relevant literature across these interconnected parts, and drawing parallels between biological and artificial intelligence. While the paper synthesizes the state-of-the-art across various components and categorizes AI research progress (L1-L3) for brain functionalities, it does not present novel quantitative results but rather identifies key research gaps and opportunities. The primary implication for AI practitioners is the advocacy for integrated, modular, and brain-inspired design principles to build more capable, adaptive, collaborative, and safe foundation agents. |
| Multi-Modal | Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual
  Editing (Read more on [arXiv](https://arxiv.org/abs/2504.02826) or [HuggingFace](https://huggingface.co/papers/2504.02826))| Rethinker, GTZhai, KexianTang, zpy777, PhoenixZ | This paper introduces RISEBench, the first benchmark specifically designed to evaluate Reasoning-Informed viSual Editing (RISE) capabilities in Large Multi-modality Models (LMMs). The primary objective is to assess how well LMMs perform complex visual edits that require understanding temporal, causal, spatial, and logical reasoning, beyond simple pixel manipulation. RISEBench uses curated image-instruction pairs across these four reasoning categories and evaluates models on Instruction Reasoning, Appearance Consistency, and Visual Plausibility using both human and LMM-as-a-judge evaluations. Results show that while GPT-4o-Native significantly outperforms others (35.9% accuracy overall), even state-of-the-art models struggle, particularly with logical reasoning, indicating a key area for future research. For practitioners, RISEBench provides a framework to systematically evaluate and compare the reasoning abilities of LMMs in visual editing tasks, guiding development towards more capable systems. |
| Multi-Modal | GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image
  Generation (Read more on [arXiv](https://arxiv.org/abs/2504.02782) or [HuggingFace](https://huggingface.co/papers/2504.02782))| shawnxyh, BestWishYsh, SereinH, liweijia, Yejy53 | This paper introduces GPT-ImgEval, a comprehensive benchmark to evaluate OpenAI's GPT-4o model's capabilities in image generation and editing. The main objective is to quantitatively and qualitatively diagnose GPT-4o's performance across generation quality (GenEval), editing proficiency (Reason-Edit), and world knowledge-informed synthesis (WISE), while also probing its architecture and limitations. Methodology involves evaluating GPT-4o on these benchmarks using custom automation scripts and employing a classification model to infer its internal architecture based on generated outputs. Key results show GPT-4o significantly outperforms prior models, achieving an overall score of 0.84 on GenEval and a GPT Score of 0.929 on Reason-Edit, indicating strong control, quality, and reasoning capabilities. The main implication for AI practitioners is providing a standardized evaluation framework, insights into GPT-4o's specific strengths (e.g., compositional reasoning) and weaknesses (e.g., inconsistencies, non-English text), and evidence suggesting a hybrid AR-diffusion architecture, guiding future multimodal model development and assessment. |
| Reinforcement Learning | Rethinking RL Scaling for Vision Language Models: A Transparent,
  From-Scratch Framework and Comprehensive Evaluation Scheme (Read more on [arXiv](https://arxiv.org/abs/2504.02587) or [HuggingFace](https://huggingface.co/papers/2504.02587))| Pengfei, IanZhong, Ryan1122, steffichern, ManTle | This paper introduces a transparent, from-scratch reinforcement learning (RL) framework (MAYE) and a comprehensive evaluation scheme for Vision Language Models (VLMs) to enhance reproducibility and analysis. The primary objective is to overcome the limitations of complex, opaque RL toolkits often used in VLM research by providing an accessible four-step pipeline and standardized evaluation metrics. The methodology involves implementing the RL pipeline using basic libraries like Transformers and FSDP2, validating it on Qwen-VL models across visual reasoning datasets (mm_math5k, geometry3k), and using a novel scheme to track dynamics like accuracy, response length, and reflection. Key results show RL consistently surpasses supervised fine-tuning (SFT) in generalization, achieving up to a 1.76x peak accuracy increase on mm_math5k compared to the Vanilla model, even when SFT uses high-quality data. For practitioners, the work offers a reproducible baseline and robust evaluation tools for developing and comparing RL-based VLMs, highlighting RL's potential for improving generalization in multi-modal reasoning. |
| Multi-Modal | SkyReels-A2: Compose Anything in Video Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2504.02436) or [HuggingFace](https://huggingface.co/papers/2504.02436))| raul678, ruiwang, diqiu7, Debang, onion | SkyReels-A2 introduces a controllable video generation framework using diffusion transformers to synthesize videos by composing visual elements from reference images and text prompts. The main objective is to tackle the elements-to-video (E2V) task, generating coherent videos that maintain fidelity to multiple visual references (characters, objects, backgrounds) and follow textual descriptions. Key methodologies involve constructing a text-reference-video triplet dataset and designing a joint image-text embedding model to inject multi-element representations via distinct semantic and spatial feature streams into a video diffusion transformer. Evaluated using the proposed A2-Bench, SkyReels-A2 shows comparable performance to closed-source models, achieving strong element fidelity, exemplified by an Object Consistency score of 0.809. This provides AI practitioners with an open-source framework for precise, multi-element video composition, enabling applications like AI drama and virtual e-commerce. |
| Multi-Modal | Scaling Analysis of Interleaved Speech-Text Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.02398) or [HuggingFace](https://huggingface.co/papers/2504.02398))| adiyoss, MajoRoth, hassid, gallilmaimon | This paper investigates the scaling properties of interleaved speech-text language models (SLMs), finding they scale more efficiently than textless counterparts. The primary objective was to determine if joint speech-text SLMs, particularly those initialised from pre-trained text models (TextLMs), exhibit better scaling dynamics. The methodology involved training dozens of interleaved SLMs (e.g., Qwen2.5-based) of varying sizes and compute budgets, using a mix of real and synthetic data, and analysing performance using ISO-FLOP curves and downstream speech semantic metrics. Results show that interleaved SLMs scale favourably with compute, suggesting a larger portion of the budget should be allocated to model size over training tokens compared to textless SLMs; the final 7B model achieved strong results (e.g., 88.3 multi-speaker tSC) with less compute than some prior methods. For practitioners, this implies prioritising larger models from high-quality TextLM families and incorporating synthetic data is crucial for efficient interleaved SLM training. |
| Multi-Modal | ShortV: Efficient Multimodal Large Language Models by Freezing Visual
  Tokens in Ineffective Layers (Read more on [arXiv](https://arxiv.org/abs/2504.00502) or [HuggingFace](https://huggingface.co/papers/2504.00502))| xphan, sanmusunrise, luyaojie, chenjiawei-icip, yuanqianhao | This paper introduces ShortV, a training-free method to enhance the computational efficiency of Multimodal Large Language Models (MLLMs) by exploiting layer-wise redundancy in visual token processing. The main objective is to reduce the high computational overhead of MLLMs by identifying layers where transformations on visual tokens have minimal impact on the final output. ShortV utilizes a novel metric, Layer Contribution (LC), based on KL divergence, to identify these ineffective layers and subsequently freezes visual token updates within them, creating sparse 'ShortV layers'. Experiments demonstrate that ShortV can freeze visual tokens in approximately 60% of layers (e.g., achieving a 50% FLOPs reduction on LLaVA-NeXT-13B) with negligible performance degradation, offering practitioners a way to significantly improve MLLM inference efficiency without retraining. |
| Multi-Modal | Audio-visual Controlled Video Diffusion with Masked Selective State
  Spaces Modeling for Natural Talking Head Generation (Read more on [arXiv](https://arxiv.org/abs/2504.02542) or [HuggingFace](https://huggingface.co/papers/2504.02542))| Jun Zhou, Zixiang Zhou, danxuhk, xuzn, HarlanHong | ACTalker is an end-to-end video diffusion framework for generating natural talking head videos controlled by single or multiple (audio, facial motion) signals. The primary research objective is to enable simultaneous multi-signal control without conflicts and support single-signal generation flexibility. Key methods include a parallel-control mamba layer using Masked Selective State Spaces (Mask-SSM) for spatio-temporal feature manipulation, a mask-drop strategy to assign signals to specific facial regions, and a gating mechanism for control switching. Results show the method produces natural-looking videos, achieving a Sync-C score of 5.737 on CelebV-HD under audio-visual control and outperforming prior methods on video quality metrics like FVD-Inc. For AI practitioners, this work provides a robust method for multi-modal talking head synthesis, enabling finer, conflict-free control over generated animations using diverse driving signals. |
| Machine Learning | ZClip: Adaptive Spike Mitigation for LLM Pre-Training (Read more on [arXiv](https://arxiv.org/abs/2504.02507) or [HuggingFace](https://huggingface.co/papers/2504.02507))| gueraf, nilabhra, louisowen6, akanyaani | This paper introduces ZClip, an adaptive gradient clipping algorithm to mitigate loss spikes and instability during Large Language Model (LLM) pre-training. The objective is to dynamically adjust the clipping threshold based on recent gradient norm statistics, avoiding issues associated with fixed thresholds or heuristic methods. ZClip utilizes z-score-based anomaly detection, tracking the mean and standard deviation of gradient norms via Exponential Moving Averages (EMA), and applies reciprocal clipping to adjust anomalous gradients. Empirical results on a 1B LLaMA model show ZClip enables stable training at higher learning rates (e.g., 3.0e-3) where fixed clipping fails, reaching a baseline loss 18.6B tokens faster, and eliminates spikes while improving downstream performance (e.g., +1.55% WinoGrande accuracy over fixed clipping at lr=1e-3). ZClip offers AI practitioners a method to enhance training stability, potentially reduce training time and compute costs by enabling more aggressive learning rates, and minimize manual intervention. |
| Reinforcement Learning | Inference-Time Scaling for Generalist Reward Modeling (Read more on [arXiv](https://arxiv.org/abs/2504.02495) or [HuggingFace](https://huggingface.co/papers/2504.02495))| Chong Ruan, Shirong Ma, Runxin Xu, Peiyi Wang, Zijun Liu | This paper introduces Self-Principled Critique Tuning (SPCT) to improve the inference-time scalability and quality of generalist generative reward models (GRMs) for large language models. The research aims to enhance reward modeling (RM) by enabling better performance-compute scaling through increased inference compute and effective learning methods for general queries. The key methodology involves adopting pointwise GRM for flexibility and training it with SPCT, which uses rejective fine-tuning and rule-based online RL to generate adaptive principles and critiques, complemented by parallel sampling and a meta RM for inference scaling. Results show DeepSeek-GRM-27B with SPCT and meta RM-guided voting achieves top performance (72.8% overall on RM benchmarks with 32 samples), significantly outperforming baselines and demonstrating effective inference-time scaling over training-time scaling. This work suggests AI practitioners can improve LLM performance via scalable generalist reward models that benefit from increased inference compute. |
| Machine Learning | Efficient Model Selection for Time Series Forecasting via LLMs (Read more on [arXiv](https://arxiv.org/abs/2504.02119) or [HuggingFace](https://huggingface.co/papers/2504.02119))| Hongjie Chen, Franck-Dernoncourt, ryanrossi, tiankaiy, wwdd7718 | This paper proposes leveraging Large Language Models (LLMs) for efficient model selection in time series forecasting, bypassing the need for traditional, computationally expensive performance matrices. The main objective is to evaluate if LLMs, via zero-shot prompting, can accurately recommend suitable forecasting models and hyperparameters for unseen datasets. The methodology involves querying LLMs like Llama, GPT, and Gemini with prompts containing time series data (and optionally meta-features or Chain-of-Thought reasoning) to select a model from a defined space. Results show the LLM approach, particularly Llama 3.2, significantly outperforms meta-learning baselines (e.g., achieving 7.27% hit@10 accuracy versus 4.10% for ISAC) and reduces inference time by up to 89x compared to naïve evaluation, while achieving competitive forecasting accuracy (MSE). The key implication is that LLMs provide a scalable, training-free alternative for automating time series model selection, reducing computational overhead. |
| Machine Learning | Instruction-Guided Autoregressive Neural Network Parameter Generation (Read more on [arXiv](https://arxiv.org/abs/2504.02012) or [HuggingFace](https://huggingface.co/papers/2504.02012))| Sung Ju Hwang, Song Chong, Bruno Andreis, bedio | This paper introduces Instruction-Guided Parameter Generation (IGPG), an autoregressive framework to synthesize neural network parameters conditioned on task instructions and architecture specifications. The primary objective is to overcome limitations of existing methods, such as poor scalability and lack of inter-layer coherence, by unifying parameter generation across diverse tasks and architectures. IGPG employs a Vector Quantized Variational Autoencoder (VQ-VAE) to encode parameters into discrete tokens and an autoregressive transformer model to generate these tokens sequentially, ensuring coherence and adapting to task/architecture inputs. Experiments demonstrate that IGPG achieves competitive or superior performance, for instance attaining 49.06% accuracy on STL-10 after 25 epochs, outperforming SANE and matching D2NWG, while scaling effectively to larger models unlike prior methods. The main implication is that IGPG provides a flexible tool for generating tailored initializations, potentially accelerating model adaptation, selection, and fine-tuning for new tasks. |
| Reinforcement Learning | Interpreting Emergent Planning in Model-Free Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2504.01871) or [HuggingFace](https://huggingface.co/papers/2504.01871))| David Krueger, Usman Anwar, Stephen Chung, agaralon, tuphs | This paper provides the first mechanistic evidence that model-free reinforcement learning agents can learn to perform internal planning by analyzing a DRC agent playing Sokoban. The study investigates whether a model-free agent, specifically Deep Repeated ConvLSTM (DRC), internally formulates and utilizes plans for decision-making, despite lacking an explicit world model. Using concept-based interpretability, the authors probe the agent's internal states (ConvLSTM cell states) for planning-relevant concepts (Agent Approach Direction CA, Box Push Direction CB), analyze plan formation dynamics over time, and perform interventions on activations to establish causality. Results demonstrate that linear probes decode these planning concepts with high accuracy (e.g., Macro F1 > 0.8 for layer 3 probes vs < 0.1 baseline), qualitative analysis reveals an iterative, parallelized bidirectional planning process, and interventions successfully steer agent behavior (e.g., 80.6% success rate for Box-Shortcut intervention on Layer 3). The main implication is that complex planning capabilities can emerge spontaneously in model-free RL agents, offering insights into the internal mechanisms potentially underlying emergent reasoning in advanced AI systems. |
| Machine Learning | GenPRM: Scaling Test-Time Compute of Process Reward Models via
  Generative Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.00891) or [HuggingFace](https://huggingface.co/papers/2504.00891))| Saputello, dmux, ChetKao, iseesaw, RyanLiu112 | This paper introduces GenPRM, a generative process reward model designed to enhance Large Language Model (LLM) verification by incorporating explicit reasoning and code verification, enabling effective test-time scaling (TTS). The primary research objective is to investigate how generative modeling can improve the process supervision capabilities of Process Reward Models (PRMs) while facilitating test-time compute scaling. Key methodologies include developing a generative PRM that performs Chain-of-Thought (CoT) reasoning and code verification for each step, utilizing Relative Progress Estimation (RPE) and a rationale synthesis framework for high-quality data generation, and applying TTS via majority voting over multiple generated reasoning paths. Experiments show that GenPRM significantly outperforms prior PRMs; notably, a 7B GenPRM achieves an 80.5 average F1 score on ProcessBench using majority voting (Maj@8), surpassing the larger Qwen2.5-Math-PRM-72B, and a 1.5B GenPRM outperforms GPT-4o through TTS. For AI practitioners, this work presents a new paradigm for PRMs as generative critics capable of TTS, offering improved verification and refinement capabilities for LLMs, especially on complex reasoning tasks. |
| Multi-Modal | Scaling Laws in Scientific Discovery with AI and Robot Scientists (Read more on [arXiv](https://arxiv.org/abs/2503.22444) or [HuggingFace](https://huggingface.co/papers/2503.22444))| Zhenting Wang, Renjun Xu, Huazhe Xu, Heng Zhang, universea | This paper introduces the Autonomous Generalist Scientist (AGS) concept, integrating agentic AI and embodied robotics to automate the entire scientific research lifecycle. The main objective is to propose the AGS framework to overcome current research limitations, accelerate multi-disciplinary discovery, and explore potential new scaling laws for knowledge generation driven by autonomous systems. The proposed methodology involves a modular multi-agent system combining large language models (LLMs) for virtual tasks (e.g., literature review, hypothesis generation, writing) and embodied robots for physical experimentation, coordinated through interaction and reflection mechanisms across defined automation levels (0-5). As a conceptual work, the paper does not provide quantitative results from a deployed AGS but hypothesizes the emergence of new scaling laws potentially leading to research output exceeding human capacity, illustrated via projections (e.g., Fig. 6). The key implication for AI practitioners is the challenge and opportunity in creating highly integrated, multi-modal AI and robotic systems capable of autonomous, end-to-end scientific reasoning and experimentation in both virtual and physical domains. |
| Multi-Modal | Sparse Autoencoders Learn Monosemantic Features in Vision-Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2504.02821) or [HuggingFace](https://huggingface.co/papers/2504.02821))| Zeynep Akata, Serge Belongie, Quentin Bouniot, Shyamgopal Karthik, Mateusz Pach | This paper investigates applying Sparse Autoencoders (SAEs) to enhance the interpretability and steerability of Vision-Language Models (VLMs). The primary objective is to quantitatively evaluate if SAEs induce more monosemantic features in VLM vision representations and to demonstrate control over multimodal LLM outputs. Methodologically, the authors train SAEs on CLIP encoder activations, propose a vision-specific Monosemanticity Score (MS) based on activation-weighted embedding similarity, and intervene on SAE neurons within LLaVA. Key results show SAEs significantly increase neuron monosemanticity (e.g., MS improves from 0.48 in the base VLM to 0.81 in the SAE for a specific neuron), reveal hierarchical concept structures, and enable successful steering, yielding a relative improvement of ~22% in similarity between activating images and steered output words. The main implication for practitioners is that SAEs provide an effective unsupervised, post-hoc technique for interpreting and controlling multimodal models without retraining. |
| Natural Language Processing | Whisper-LM: Improving ASR Models with Language Models for Low-Resource
  Languages (Read more on [arXiv](https://arxiv.org/abs/2503.23542) or [HuggingFace](https://huggingface.co/papers/2503.23542))| Ibon Saratxaga, Eva Navas, inmahernaez, zuazo | This paper presents Whisper-LM, an approach to enhance Automatic Speech Recognition (ASR) for low-resource languages by integrating language models with fine-tuned Whisper models. The primary objective is to improve Whisper's performance and out-of-distribution (OOD) robustness, particularly for languages underrepresented in its training data. The methodology involves fine-tuning Whisper models on specific languages and then integrating statistical n-gram models (KenLM) and Large Language Models (LLMs) during inference by adjusting beam search scores. Key results show significant Word Error Rate (WER) reductions, with n-gram models providing improvements up to 51% for in-distribution datasets and 34% for OOD datasets, while LLMs offered more modest but consistently robust gains across diverse linguistic contexts. For AI practitioners, the main implication is that augmenting powerful ASR models like Whisper with external linguistic knowledge via language models is a viable strategy to boost performance and robustness, especially in low-resource scenarios, but requires careful parameter optimization. |
