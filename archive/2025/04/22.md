

## Papers for 2025-04-22

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Reinforcement Learning | Learning to Reason under Off-Policy Guidance (Read more on [arXiv](https://arxiv.org/abs/2504.14945) or [HuggingFace](https://huggingface.co/papers/2504.14945))| Zhi Wang, ganqu, huzican, yaful, Elliott | This paper introduces LUFFY, a framework to enhance large reasoning models by integrating off-policy guidance into reinforcement learning. The primary objective is to overcome the limitations of purely on-policy zero-RL methods, which restrict learning to a model's own outputs and hinder the acquisition of abilities beyond initial capabilities. LUFFY achieves this by dynamically balancing imitation and exploration, combining off-policy demonstrations with on-policy rollouts, and employing policy shaping via regularized importance sampling to prevent superficial imitation. The method demonstrated significant improvements, achieving an average gain of over +7.0 points across six math benchmarks and +6.2 points on out-of-distribution tasks compared to previous zero-RL approaches. For AI practitioners, LUFFY offers a scalable and effective approach to train more generalizable reasoning models by leveraging external, off-policy knowledge alongside self-exploration. |
| Reinforcement Learning | FlowReasoner: Reinforcing Query-Level Meta-Agents (Read more on [arXiv](https://arxiv.org/abs/2504.15257) or [HuggingFace](https://huggingface.co/papers/2504.15257))| P2333, bhooi, dreamerdeo, yueliu1999, HongchengGao | This paper introduces FLOWREASONER, a reasoning-driven meta-agent designed to automate the generation of query-specific multi-agent systems. The primary objective is to enhance the adaptability of multi-agent systems by creating personalized workflows for individual user queries, moving beyond static, task-level approaches. The methodology involves first endowing the meta-agent with basic reasoning capabilities via supervised fine-tuning (SFT) by distilling from a large model, followed by reinforcement learning (RL) enhancement using external execution feedback guided by a multi-purpose reward signal considering performance, complexity, and efficiency. Key results show FLOWREASONER significantly outperforms baselines, achieving a 10.52% higher accuracy than o1-mini across three code benchmarks. The main implication for AI practitioners is the potential to build more adaptive, scalable, and efficient multi-agent systems tailored dynamically to user needs, particularly in domains allowing external execution feedback. |
| Multi-Modal | Eagle 2.5: Boosting Long-Context Post-Training for Frontier
  Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.15271) or [HuggingFace](https://huggingface.co/papers/2504.15271))| WonminByeon, deahuang, lulidong, RealZhiqiLi, cg1177 | Eagle 2.5 introduces a family of frontier vision-language models (VLMs) specifically designed for enhanced long-context multimodal understanding. The main objective is to address the limitations of existing VLMs in processing extended visual contexts, such as long videos and high-resolution or multiple images, by developing a generalist framework. Key methodologies include two novel training techniques, Automatic Degrade Sampling (ADS) and Image Area Preservation (IAP), progressive mixed post-training to handle varying context lengths, efficiency optimizations, and a new dataset, Eagle-Video-110K, for long-video understanding. The primary result highlighted is Eagle 2.5-8B achieving 72.4% accuracy on the Video-MME benchmark with 512 input frames, comparable to much larger models like GPT-40 and Qwen2.5-VL-72B. For AI practitioners, Eagle 2.5 provides a robust and more parameter-efficient approach to building VLMs capable of handling long-context visual inputs effectively. |
| Reinforcement Learning | ToolRL: Reward is All Tool Learning Needs (Read more on [arXiv](https://arxiv.org/abs/2504.13958) or [HuggingFace](https://huggingface.co/papers/2504.13958))| Cheng Qian, Gokhantur, XtremSup, Merlin-Hongru, emrecanacikgoz | This paper presents a comprehensive study on reward design for tool selection and application tasks within the Reinforcement Learning (RL) paradigm for Large Language Models (LLMs). The primary objective is to determine the optimal RL reward design for improving LLM tool-using capabilities, particularly in scenarios where Supervised Fine-Tuning (SFT) struggles. The key methodology involves systematically exploring reward strategies across types, scales, granularity, and temporal dynamics, proposing a principled reward design, and training LLMs using Group Relative Policy Optimization (GRPO). Empirical evaluations show the proposed approach achieves up to a 17% improvement over base models and a 15% gain over SFT models on tool use benchmarks. The main implication is that thoughtful, fine-grained reward design is critical for effectively training LLMs via RL to enhance their tool use and generalization performance. |
| Computer Vision | SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video
  Generation via Spherical Latent Representation (Read more on [arXiv](https://arxiv.org/abs/2504.14396) or [HuggingFace](https://huggingface.co/papers/2504.14396))| joyfull78, sungwon95, YeolJoo, TaewoongKang, mpark | SphereDiff introduces a tuning-free framework for generating high-quality, seamless 360-degree panoramic images and videos using pre-trained diffusion models. The primary objective is to overcome the severe distortions and discontinuities, especially near the poles, associated with traditional equirectangular projection (ERP) representations without requiring model fine-tuning. The key methodology involves defining a uniform spherical latent representation, extending MultiDiffusion to this spherical space, employing dynamic spherical latent sampling to interface with standard diffusion models, and using distortion-aware weighted averaging during projection. SphereDiff significantly outperforms baseline methods in quantitative evaluations, achieving top scores in distortion (3.238 for images) and end continuity (4.892 for images) metrics, and shows strong performance in video metrics like motion smoothness (0.9956). This provides AI practitioners with a robust method to generate high-fidelity panoramic content for immersive applications by directly utilizing existing diffusion models. |
| Computer Vision | StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on
  3D Gaussians (Read more on [arXiv](https://arxiv.org/abs/2504.15281) or [HuggingFace](https://huggingface.co/papers/2504.15281))| Cailin Zhuang, Yiying12, unpackableorange, wchengad, xuanyangz | StyleMe3D introduces a novel framework for applying diverse artistic styles to 3D Gaussian Splatting (3D GS) representations while preserving geometric integrity and ensuring stylistic consistency. The primary objective is to overcome limitations of existing 3D stylization techniques, such as texture fragmentation and semantic misalignment, especially for non-photorealistic styles. The methodology integrates multiple novel components including Dynamic Style Score Distillation (DSSD) leveraging Stable Diffusion, Contrastive Style Descriptor (CSD) for localized textures, Simultaneously Optimized Scale (SOS) for multi-scale detail, and a 3D Gaussian Quality Assessment (3DG-QA) prior, optimizing only RGB attributes to maintain geometry. StyleMe3D demonstrates superior performance over state-of-the-art methods on NeRF synthetic and tandt db datasets, achieving a PSNR of 18.015 and LPIPS of 0.174. For AI practitioners, this work offers a systematic approach to high-quality 3D artistic stylization, enabling applications in virtual worlds and digital art by bridging photorealistic 3D GS with artistic rendering while maintaining real-time performance. |
| Natural Language Processing | X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents (Read more on [arXiv](https://arxiv.org/abs/2504.13203) or [HuggingFace](https://huggingface.co/papers/2504.13203))| hamidpalangi, mparvez, genglinliu, liweijiang, salmannyu | This paper presents X-Teaming, a scalable multi-agent framework for systematically identifying multi-turn jailbreak vulnerabilities in language models (LMs) and developing defenses. The primary objective is to address the safety risks posed by harmful intent distributed across multiple conversational turns, a largely underexplored area compared to single-turn safety. X-Teaming employs collaborative agents for strategic attack planning (persona, context, approach, trajectory), adaptive execution, verification, and TextGrad-based prompt optimization when models refuse harmful requests. The framework achieves state-of-the-art multi-turn attack success rates, reaching up to 98.1% on various LMs and 96.2% against Claude 3.7 Sonnet, while also generating a large-scale (30K examples) multi-turn safety dataset, XGuard-Train. For AI practitioners, X-Teaming offers crucial tools and insights for developing more robust defenses against sophisticated conversational attacks, thereby advancing the multi-turn safety alignment of LMs. |
| Multi-Modal | UFO2: The Desktop AgentOS (Read more on [arXiv](https://arxiv.org/abs/2504.14603) or [HuggingFace](https://huggingface.co/papers/2504.14603))| rujiawang, liqul, duchao, shilhe, vyokky | UFO2 introduces a multi-agent AgentOS for Windows, leveraging multimodal LLMs and deep OS integration to enable practical, system-level desktop automation. The objective is to overcome the limitations of existing Computer-Using Agents (CUAs), such as fragile screenshot-based interaction and disruptive execution, by developing a robust, deeply integrated automation system. Key methodologies include a multi-agent architecture (HostAgent/AppAgents), a hybrid UIA-vision control detection pipeline, a unified GUI-API action layer, continuous knowledge integration via RAG, speculative multi-action execution, and a non-disruptive Picture-in-Picture interface. Evaluations demonstrate substantial improvements over prior CUAs, achieving over 10% higher task completion rates and reducing LLM inference costs by up to 51.5% via speculative execution. The main implication is that deep OS integration, combined with hybrid GUI-API interaction and multimodal perception, offers a scalable path towards reliable, user-aligned desktop automation agents. |
| Multi-Modal | Seeing from Another Perspective: Evaluating Multi-View Understanding in
  MLLMs (Read more on [arXiv](https://arxiv.org/abs/2504.15280) or [HuggingFace](https://huggingface.co/papers/2504.15280))| Shengbang Tong, yubei, chengtim, ch-chenyu, danielchyeh | This paper introduces the All-Angles Bench benchmark to evaluate multi-view understanding capabilities in Multi-Modal Large Language Models (MLLMs). The primary objective is to assess MLLMs' ability to integrate visual information across different viewpoints, focusing on geometric consistency and cross-view correspondence, crucial for embodied agents. The methodology involves a new benchmark, All-Angles Bench, comprising over 2,100 human-annotated question-answer pairs across 90 real-world scenes, testing six specific multi-view reasoning tasks. Results show a significant performance gap between MLLMs and humans; for instance, humans achieve 88.9% accuracy in camera pose estimation, while even state-of-the-art MLLMs lag significantly, indicating poor geometric understanding and cross-view correspondence, especially with occlusion. The main implication is that current MLLMs require significant improvements, potentially through domain-specific refinements or architectural changes, to achieve reliable multi-view understanding for real-world applications like robotics and navigation. |
| Machine Learning | LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient
  Training of Code LLMs (Read more on [arXiv](https://arxiv.org/abs/2504.14655) or [HuggingFace](https://huggingface.co/papers/2504.14655))| Yan Wang, Yunhui Xia, chuyi777, jasonkleinlove, Swtheking | This paper introduces LeetCodeDataset, a benchmark for evaluating and training code-generation LLMs using temporally split LeetCode Python problems. The primary objective is to address the lack of reasoning-focused coding benchmarks and self-contained, high-quality training datasets for competition-level coding. The methodology involves curating 2,869 LeetCode problems with rich metadata, 100+ test cases each, and a pre/post July 2024 temporal split for contamination-free evaluation and supervised fine-tuning (SFT). Key results show reasoning models like DeepSeek-R1 significantly outperform others (pass@1 65.23% on the test set), and SFT with only 2.6K model-generated solutions achieves performance comparable to models trained on 110K samples. The main implication is that LeetCodeDataset offers a robust resource for reliable benchmarking and highly efficient SFT for developing advanced code generation models. |
| Multi-Modal | InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to
  Deliberative Reasoners (Read more on [arXiv](https://arxiv.org/abs/2504.14239) or [HuggingFace](https://huggingface.co/papers/2504.14239))| Xavier Hu, Yuhang Liu, xiaotianhan, xieck13, pengxiang | This paper introduces InfiGUI-R1, an MLLM-based GUI agent, and the Actor2Reasoner framework designed to transform agents from reactive actors into deliberative reasoners for complex GUI tasks. The main objective is to imbue GUI agents with explicit reasoning, planning, and error recovery capabilities, moving beyond reliance on implicit reasoning or predefined templates. The key methodology is a two-stage training approach: Reasoning Injection using Spatial Reasoning Distillation to integrate visual-spatial understanding, followed by Deliberation Enhancement using Reinforcement Learning with Sub-goal Guidance and Error Recovery Scenario Construction to improve planning and self-correction. Experimental results show InfiGUI-R1-3B achieves state-of-the-art performance with 87.5% average accuracy on the ScreenSpot grounding benchmark, competitive with larger models. The primary implication for AI practitioners is the provision of a structured framework and techniques to develop more capable and robust GUI agents capable of deliberate reasoning for complex task automation. |
| Natural Language Processing | EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2504.15133) or [HuggingFace](https://huggingface.co/papers/2504.15133))| Linear-Matrix-Probability, HaomingXu, xukewei, Saberlve, xzwnlp | This paper introduces EasyEdit2, a user-friendly framework designed for plug-and-play, test-time steering and editing of Large Language Model (LLM) behaviors. The main objective is to provide an accessible method for controlling diverse LLM attributes like safety, sentiment, factuality, and reasoning patterns during inference without altering the model's parameters. EasyEdit2's key methodology involves modules for generating steering vectors (using techniques like Contrastive Activation Addition - CAA, LM-Steer, etc.) and applying these vectors to influence model activations or embeddings during the forward pass. Empirical results show effectiveness, for instance, the CAA method improved the safety defense rate on Gemma-2-9B to 64.72% compared to a 58.29% baseline. For AI practitioners, EasyEdit2 offers a practical, low-code tool to achieve fine-grained, adjustable control over LLM outputs, enhancing reliability and customization for specific applications. |
| Multi-Modal | LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration
  Benchmark (Read more on [arXiv](https://arxiv.org/abs/2504.13805) or [HuggingFace](https://huggingface.co/papers/2504.13805))| dkeeeee, Yuxiang007, zhimingc, Pengxiangzhao, lgy0404 | This paper introduces LearnAct, a few-shot learning framework for mobile GUI agents, alongside the LearnGUI benchmark designed for demonstration-based learning evaluation. The primary objective is to improve agent generalization in diverse, unseen mobile scenarios by leveraging a small number of human demonstrations rather than relying on massive pre-training datasets. The methodology involves the LearnAct multi-agent system (DemoParser, KnowSeeker, ActExecutor) that processes demonstrations (screenshots and actions) to extract, retrieve, and apply knowledge for task execution based on visual GUI input and instructions. Key results show significant performance gains; for example, a single demonstration boosted Gemini-1.5-Pro's offline accuracy from 19.3% to 51.7%, and LearnAct enhanced UI-TARS-7B-SFT's online task success rate from 18.1% to 32.8%. This work suggests that demonstration-based learning offers a practical path towards more adaptable and personalized mobile GUI agents for AI practitioners. |
| Computer Vision | LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping (Read more on [arXiv](https://arxiv.org/abs/2504.08902) or [HuggingFace](https://huggingface.co/papers/2504.08902))| Vinicius C. Azevedo, Jingwei Tang, coffeeweb2907, ssancho, pascalchang87 | LookingGlass presents a method for generating high-quality anamorphic optical illusions using latent text-to-image models, ensuring both the direct and transformed views are meaningful. The objective is to create these ambiguous images by synchronizing generation across complex spatial transformations (e.g., via mirrors/lenses). The core methodology involves Laplacian Pyramid Warping (LPW), a frequency-aware image-space technique using pyramids to handle distortions while preserving details, integrated with adaptations for latent generative models like VAE encoding/decoding and residual correction. Quantitatively, LookingGlass surpasses previous methods on challenging transforms, achieving an FID of 130.27 on the cylindrical mirror task, outperforming baselines like SyncTweedies (138.69). This work enables AI practitioners to create sophisticated generative perceptual illusions from text, effectively managing complex image distortions within latent generative frameworks. |
| Machine Learning | DRAGON: Distributional Rewards Optimize Diffusion Generative Models (Read more on [arXiv](https://arxiv.org/abs/2504.15217) or [HuggingFace](https://huggingface.co/papers/2504.15217))| Somayeh Sojoudi, Jonah Casebeer, Njb, Bai-YT | DRAGON introduces a versatile framework for fine-tuning generative models, particularly diffusion models, using a broad spectrum of reward functions including distributional ones. The main objective is to optimize models towards desired output distributions or specific metrics, overcoming limitations of traditional instance-level feedback methods like RLHF or DPO. DRAGON collects on-policy generations, uses a target reward function (potentially constructed via cross-modal exemplars) to create positive and negative demonstration sets, and optimizes the model contrastively. Evaluated on text-to-music diffusion, DRAGON achieved an 81.45% average win rate across 20 diverse reward functions and improved human-perceived quality (60.95% win rate) using only an exemplar set without human preference annotations. This provides practitioners a flexible approach to optimize generative models directly for distributional metrics (like FAD or diversity) and design rewards using reference examples, reducing the need for extensive human feedback. |
| Computer Vision | Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls
  for Video Generation (Read more on [arXiv](https://arxiv.org/abs/2504.14899) or [HuggingFace](https://huggingface.co/papers/2504.14899))| Shikai Li, yanweifuture, Alex-snow, theFoxofSky, ewrfcas | This paper presents Uni3C, a unified framework for precise 3D-enhanced control over both camera trajectories and human motion in video generation, leveraging foundational video diffusion models (VDMs). The main objective is to overcome the limitations of separate control mechanisms and the need for extensive, jointly annotated data by unifying control signals within a 3D world representation. Uni3C introduces PCDController, a lightweight plug-and-play module using unprojected point clouds for robust camera control, and a global 3D alignment strategy for integrating scenic point clouds and SMPL-X human models during inference. Experiments show Uni3C significantly improves controllability, with the PCDController achieving superior camera precision (e.g., achieving an ATE of 0.102 on the custom benchmark) compared to methods relying solely on Plücker rays, while enabling effective joint camera and human motion control. For AI practitioners, Uni3C offers a method to add precise, unified 3D control to large VDMs without full retraining, facilitating complex video generation tasks and motion transfer. |
| Computer Vision | TAPIP3D: Tracking Any Point in Persistent 3D Geometry (Read more on [arXiv](https://arxiv.org/abs/2504.14717) or [HuggingFace](https://huggingface.co/papers/2504.14717))| Katerina Fragkiadaki, Bowei Zhang, aharley, lkeab | TAPIP3D introduces a novel approach for long-term 3D point tracking in RGB or RGB-D videos by utilizing persistent 3D geometry. The main objective is to improve tracking accuracy and robustness, especially under large camera movements and occlusions, by operating in a camera-stabilized 3D world space. The key methodology involves lifting 2D features into a 3D spatio-temporal feature cloud, canceling camera motion, and employing a novel Local Pair Attention mechanism within a transformer architecture for iterative trajectory refinement. Results demonstrate state-of-the-art performance, significantly outperforming prior 3D trackers on benchmarks like TAPVid3D (e.g., achieving an average AJ3D of 18.8 vs 17.8 for DELTA with MegaSaM depth) and DexYCB-Pt (AJ3D of 30.3 vs 26.4 for DELTA with sensor depth). The main implication for AI practitioners is that explicitly modeling and tracking in a stabilized 3D world coordinate system enhances tracking performance compared to 2D or camera-centric 3D approaches. |
| Multi-Modal | An LMM for Efficient Video Understanding via Reinforced Compression of
  Video Cubes (Read more on [arXiv](https://arxiv.org/abs/2504.15270) or [HuggingFace](https://huggingface.co/papers/2504.15270))| Yuan Yao, Ji Qi, chuats, acharkq, bys0318 | This paper introduces Quicksviewer, a Large Multimodal Model (LMM) designed for efficient video understanding through reinforced, nonuniform compression of video content into cubes. The primary objective is to mitigate computational inefficiency in LMMs by dynamically adapting video perception based on varying temporal information density. The key methodology involves a cubing network using Gumbel Softmax to partition videos into nonuniform temporal cubes based on semantic changes, followed by a unified 3D resampler to generate a fixed number of tokens per cube, enabling high compression (e.g., 45x overall). Quicksviewer outperforms a fixed partitioning baseline by up to 8.72% in accuracy and achieves state-of-the-art results on benchmarks like Video-MME (56.9 score) using significantly fewer tokens per frame (up to 5%) and only 0.8M training samples. The main implication for AI practitioners is that dynamic, content-aware temporal compression offers a path towards more efficient and scalable LMMs for processing lengthy videos. |
| Natural Language Processing | RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary
  Quality-Diversity Search (Read more on [arXiv](https://arxiv.org/abs/2504.15047) or [HuggingFace](https://huggingface.co/papers/2504.15047))| Truong-Son Hy, tnngo2, quyanh | This paper introduces RainbowPlus, a novel framework for enhancing adversarial prompt generation for Large Language Models (LLMs) using an evolutionary quality-diversity approach. The main objective is to overcome limitations of existing red-teaming methods by simultaneously optimizing for attack success rate (quality) and prompt diversity. RainbowPlus employs an adaptive quality-diversity (QD) search extending MAP-Elites, featuring a multi-element archive to store diverse prompts per niche and a comprehensive fitness function for concurrent multi-prompt evaluation. Experiments show RainbowPlus achieves a superior average Attack Success Rate (ASR) of 81.1% on the HarmBench dataset across twelve LLMs, surpassing AutoDAN-Turbo by 3.9%, while being significantly faster and generating substantially more diverse prompts (Diverse-Score ≈ 0.84). The primary implication for AI practitioners is that RainbowPlus offers a more scalable, efficient, and effective method for LLM vulnerability assessment and red-teaming. |
| Reinforcement Learning | NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.13941) or [HuggingFace](https://huggingface.co/papers/2504.13941))| yejinchoinka, ericnyberg, ekmb, shrimai19, SieraL | This paper proposes NEMOTRON-CROSSTHINK, a framework to enhance Large Language Model (LLM) reasoning capabilities across diverse domains beyond mathematics using Reinforcement Learning (RL). The main objective is to generalize RL-based self-learning by systematically incorporating varied data sources and formats while managing the challenge of verifiable rewards in non-deterministic domains. Key methods include curating multi-domain synthetic and real-world QA data, applying structured answer templates, filtering for verifiable responses, optimizing data blending strategies, and employing Group Relative Policy Optimization (GRPO) for RL training. The approach demonstrates significant accuracy improvements on both math (MATH-500: +30.1%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%), alongside enhanced response efficiency (28% fewer tokens). The primary implication is that strategically blending multi-domain, multi-format data within an RL framework can yield more accurate, efficient, and generalizable reasoning LLMs. |
| Computer Vision | CoMotion: Concurrent Multi-person 3D Motion (Read more on [arXiv](https://arxiv.org/abs/2504.12186) or [HuggingFace](https://huggingface.co/papers/2504.12186))| Stephan R. Richter, Alejandro Newell, vkoltun, lahavl, peiyun-hu-apple | CoMotion introduces a novel online approach for concurrently detecting and tracking detailed 3D poses for multiple people from a single monocular camera stream. Its objective is to maintain temporally coherent pose estimates in crowded and occluded scenes without future frame lookahead. The method utilizes a recurrent, attention-based model that directly updates existing tracks from new image features via cross-attention and is trained on a heterogeneous mix of datasets with pseudo-labeling. CoMotion achieves state-of-the-art pose accuracy and significantly improves tracking, boosting MOTA by 14% on PoseTrack21 over prior work while being substantially faster. This provides AI practitioners with an efficient, online-capable system for robust multi-person 3D motion analysis in challenging real-world videos, complete with released code and weights. |
