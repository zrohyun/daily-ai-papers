

## Papers for 2025-04-01

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | TextCrafter: Accurately Rendering Multiple Texts in Complex Visual
  Scenes (Read more on [arXiv](https://arxiv.org/abs/2503.23461) or [HuggingFace](https://huggingface.co/papers/2503.23461))| Nikai Du, yingtai, jzzzzk, Chenzzzzzz, zhen-nan | This paper presents TextCrafter, a training-free framework designed to accurately render multiple text instances within complex visual scenes generated by diffusion models. The main objective is to overcome challenges in Complex Visual Text Generation (CVTG), such as text distortion, omission, and blurriness, particularly when multiple texts are involved. TextCrafter employs a progressive three-stage methodology involving Instance Fusion for text-carrier alignment, Region Insulation for layout initialization and interference reduction, and Text Focus for enhancing text fidelity via attention control. Experiments on the newly introduced CVTG-2K benchmark show TextCrafter achieves state-of-the-art performance, notably improving average Word Accuracy to 0.7370, a significant gain over baselines like FLUX (0.4965). For AI practitioners, TextCrafter offers an effective approach to generate images with significantly more accurate and complex textual content without requiring model retraining. |
| Multi-Modal | MoCha: Towards Movie-Grade Talking Character Synthesis (Read more on [arXiv](https://arxiv.org/abs/2503.23307) or [HuggingFace](https://huggingface.co/papers/2503.23307))| Luczzz, daixl1992, FelixXu, haoyum1997, lim142857 | This paper introduces MoCha, an end-to-end diffusion transformer model designed for generating movie-grade talking character videos directly from speech and text inputs without auxiliary conditions. The main objective is to synthesize realistic, full-body characters capable of synchronized speech, expressive emotions, complex actions, and multi-character, turn-based dialogue. Key methodologies include a novel speech-video window attention mechanism for precise lip-sync, a joint training strategy leveraging both speech-labeled and text-only video data for improved generalization, and structured prompt templates with character tagging for coherent multi-character interactions. Quantitative results on the MoCha-Bench benchmark show MoCha significantly outperforms baselines, achieving a human evaluation score of 3.85 out of 4 for lip-sync quality compared to the next best score of 2.45. The main implication for AI practitioners is that MoCha provides a new standard and framework for generating controllable, narrative-driven character animations, advancing capabilities in automated cinematic storytelling and virtual avatar creation. |
| Natural Language Processing | What, How, Where, and How Well? A Survey on Test-Time Scaling in Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.24235) or [HuggingFace](https://huggingface.co/papers/2503.24235))| nancy-zwx, demolei, RubinSun, silentspring2, DonJoey | This paper presents a comprehensive survey on test-time scaling (TTS) techniques for enhancing large language model (LLM) performance during inference. The primary objective is to create a unified framework for understanding the rapidly growing field of TTS, analyzing how allocating additional computation at inference time can improve LLM capabilities. The authors propose a four-dimensional taxonomy (what to scale, how to scale, where to scale, how well to scale) and use it to systematically review existing methods, applications, and evaluation metrics. While the survey itself doesn't introduce new empirical results, it synthesizes findings from numerous studies, referencing metrics like Pass@k and Cons@k used in benchmarks like MATH-500 to evaluate TTS effectiveness, often showing performance improvements with increased inference compute. The main implication for AI practitioners is a structured understanding of the TTS landscape, offering practical guidelines for deployment and identifying key challenges and future directions for maximizing LLM utility. |
| Reinforcement Learning | Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement
  Learning on the Base Model (Read more on [arXiv](https://arxiv.org/abs/2503.24290) or [HuggingFace](https://huggingface.co/papers/2503.24290))| Xiangyu Zhang, Qi Han, djiang, YinminZhang, reign12 | Open-Reasoner-Zero (ORZ) introduces an open-source implementation for scaling up reasoning-oriented reinforcement learning (RL) directly on base large language models (LLMs). The primary objective is to demonstrate and democratize a scalable, simple, and accessible approach to large-scale reasoning RL, exploring scaling phenomena without complex regularization. The methodology employs vanilla Proximal Policy Optimization (PPO) with Generalized Advantage Estimation (GAE λ=1, γ=1) and simple rule-based binary rewards, notably without KL regularization, applied directly to Qwen-2.5 base models. Key results show ORZ-32B achieving superior performance over DeepSeek-R1-Zero-Qwen-32B on benchmarks like MATH500 (92.2% vs 91.6%) with only 1/10th the training steps, and strong generalization (e.g., 74.4% on MMLU_PRO). The main implication for practitioners is that effective scaling of reasoning via RL can be achieved with minimalist, robust methods like vanilla PPO, suggesting compute and data scale are more critical than algorithmic complexity, with the open-source release facilitating broader research and application. |
| Multi-Modal | RIG: Synergizing Reasoning and Imagination in End-to-End Generalist
  Policy (Read more on [arXiv](https://arxiv.org/abs/2503.24388) or [HuggingFace](https://huggingface.co/papers/2503.24388))| Haian Huang, Zhonghan Zhao, GaoangWang, pppppM, ZwwWayne | This paper introduces RIG, an end-to-end generalist policy that synergizes textual reasoning and visual imagination (future frame prediction) for embodied agents within a single Transformer model. The primary objective is to overcome the limitations of prior works that separate these capabilities, aiming to improve learning efficiency, generalization, and robustness in complex environments like Minecraft. RIG is trained end-to-end using a progressive data collection strategy that enriches existing trajectories with reasoning and simulated 'dream-review' sequences, enabling joint learning of reasoning, low-level actions, and next-image generation. Experimental results demonstrate significant improvements, achieving state-of-the-art performance on embodied tasks (e.g., 3.29x improvement on benchmarks) with over 17x greater sample efficiency compared to previous methods (using 111 hours of video data). The key implication for practitioners is that integrating reasoning and imagination synergistically within a unified end-to-end model enhances policy robustness, generalization, and enables test-time scaling through lookahead mechanisms. |
| Machine Learning | Effectively Controlling Reasoning Models through Thinking Intervention (Read more on [arXiv](https://arxiv.org/abs/2503.24370) or [HuggingFace](https://huggingface.co/papers/2503.24370))| Prateek Mittal, Jiachen T. Wang, cxiang, tongwu2020 | This paper introduces Thinking Intervention, a novel paradigm for controlling reasoning-enhanced large language models (LLMs) by strategically modifying their explicit intermediate thinking steps. The primary objective is to achieve more fine-grained and transparent control over model behavior beyond traditional prompting, specifically for tasks requiring complex reasoning, instruction following, and safety alignment. The key methodology involves inserting or revising specific token sequences directly into the LLM's reasoning chain, often triggered by monitoring specific patterns like `<think>` tags, without needing model retraining. Results demonstrate significant improvements over baseline prompting, achieving up to 6.7% higher accuracy in instruction-following (IFEVAL), a 15.4% improvement in reasoning about instruction hierarchies (SEP), and a 40.0% increase in refusal rates for unsafe prompts (XSTEST) with DeepSeek R1 models. For AI practitioners, this presents a lightweight, training-free technique to enhance the control, reliability, and alignment of reasoning LLMs by directly intervening in their internal processing. |
| Natural Language Processing | Query and Conquer: Execution-Guided SQL Generation (Read more on [arXiv](https://arxiv.org/abs/2503.24364) or [HuggingFace](https://huggingface.co/papers/2503.24364))| sfc-mwydmuch, Borchmann | This paper proposes an execution-guided self-consistency method to improve accuracy and cost-efficiency in text-to-SQL generation. The main objective is to bridge the gap between potential (pass@k) and actual (pass@1) accuracy by reliably selecting the semantically best SQL query from multiple candidates, overcoming structural variance issues. The key methodology involves sampling multiple SQL queries, executing them (or comparing execution plans), and using execution-based similarity metrics within a Minimum Bayes Risk (MBR) framework to choose the most consistent output. Primary results show that this method enabled a 7B Qwen Coder model to improve accuracy by nearly 10% on BIRD-SQL (reaching ~54.8% exec@30), achieving performance comparable to much larger models like O1 at up to 30 times lower inference cost. The main implication for AI practitioners is that execution-guided self-consistency offers a practical, scalable, and cost-effective approach to significantly enhance text-to-SQL performance, particularly for smaller models. |
| Multi-Modal | SketchVideo: Sketch-based Video Generation and Editing (Read more on [arXiv](https://arxiv.org/abs/2503.23284) or [HuggingFace](https://huggingface.co/papers/2503.23284))| dizhang, WeicaiYe, Xintao, fuhongbo, Okrin | SketchVideo introduces a unified framework for generating and editing videos using sparse keyframe sketches and text prompts, leveraging a DiT-based architecture. The primary objective is to enable fine-grained spatial, geometric, and motion control in video synthesis and editing, addressing limitations of text-only or dense image conditioning. Key methodologies include a memory-efficient skip residual structure with sketch control blocks, an inter-frame attention mechanism for propagating sparse conditions, and a video insertion module with latent fusion for editing consistency. Experiments show superior performance, achieving the lowest LPIPS (27.56 for generation, 9.74 for editing) and highest CLIP scores (98.31 for generation, 98.34 for editing) compared to baselines like SparseCtrl and AnyV2V. For AI practitioners, this work offers a novel approach for controllable video manipulation using intuitive sketch inputs, enhancing control over generated content's geometry and dynamics. |
| Multi-Modal | TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud
  Detection (Read more on [arXiv](https://arxiv.org/abs/2503.24115) or [HuggingFace](https://huggingface.co/papers/2503.24115))| Kai Wu, Jingpeng Wang, HuangMinhua, WDong, JimmyMa99 | This paper introduces TeleAntiFraud-28k, the first open-source, audio-text slow-thinking dataset specifically designed for telecom fraud detection. The main objective is to address the lack of high-quality multimodal training data by creating a dataset integrating audio signals and reasoning-oriented text analysis to improve automated fraud detection systems. The dataset was constructed using a three-pronged methodology: processing privacy-preserved real call recordings (ASR+TTS), LLM-based self-instruction for semantic enhancement, and multi-agent adversarial synthesis for scenario diversity. Key results demonstrate that a fine-tuned Large Audio Language Model (AntiFraud-Qwen2Audio) trained on this dataset achieved a significantly improved average F1 score of 83.00% (including 84.78% F1 for fraud detection) on the associated TeleAntiFraud-Bench benchmark, validating the dataset's effectiveness. This work provides AI practitioners with a foundational resource and benchmark for developing and evaluating multimodal models capable of slow-thinking reasoning for complex anti-fraud tasks, emphasizing the benefit of integrating audio features. |
| Natural Language Processing | Efficient Inference for Large Reasoning Models: A Survey (Read more on [arXiv](https://arxiv.org/abs/2503.23077) or [HuggingFace](https://huggingface.co/papers/2503.23077))| jiaheng233, Bibaolong, HongyuChen, HongchengGao, yueliu1999 | This survey reviews efficient inference techniques for Large Reasoning Models (LRMs), focusing on reducing token inefficiency while maintaining reasoning quality. The primary objective is to categorize and analyze methods that optimize the deliberative reasoning process inherent in LRMs, addressing challenges like high token consumption and inference time. The paper proposes a taxonomy dividing methods into explicit compact Chain-of-Thought (CoT) and implicit latent CoT, evaluating their performance and efficiency through empirical analysis. Results on GSM8K show methods like implicit SoftCoT achieving 85.81% accuracy (zero-shot), demonstrating efficiency gains, though often trading interpretability for reduced token usage compared to explicit methods. For AI practitioners, this work highlights key strategies, open challenges (controllability, interpretability, safety), and potential future directions like new architectures for deploying efficient LRMs, informing choices between interpretability, performance, and computational cost. |
| Machine Learning | Classical Planning with LLM-Generated Heuristics: Challenging the State
  of the Art with Python Code (Read more on [arXiv](https://arxiv.org/abs/2503.18809) or [HuggingFace](https://huggingface.co/papers/2503.18809))| jendrikseipp, andregrahl, abcorrea | This paper demonstrates how Large Language Models (LLMs) can automatically generate effective domain-dependent heuristics as Python code for classical planning problems. The primary objective was to investigate if LLM-generated heuristics could outperform standard domain-independent ones and compete with state-of-the-art learning approaches. The methodology involves prompting an LLM multiple times (n=25 suggested) to create a pool of candidate heuristic functions, evaluating them on training tasks using Greedy Best-First Search (GBFS) within the Pyperplan planner, and selecting the best one based on coverage and agile score. Results show that the selected LLM heuristics (specifically from DeepSeek R1) solve substantially more unseen test tasks (373 out of 720) compared to the hFF baseline (243 out of 720) in Pyperplan, and are even competitive with state-of-the-art learned heuristics implemented in optimized C++ (371 out of 720). This implies that LLMs offer a viable approach for automatically generating high-quality, domain-specific search guidance for classical planners, potentially reducing manual effort. |
| Reinforcement Learning | Expanding RL with Verifiable Rewards Across Diverse Domains (Read more on [arXiv](https://arxiv.org/abs/2503.23829) or [HuggingFace](https://huggingface.co/papers/2503.23829))| zptu, haitaominlp, douvleplus, freesunshine0316, yudian | This paper investigates the extension of Reinforcement Learning with Verifiable Rewards (RLVR) to diverse domains beyond traditional mathematics and coding tasks. The main objective is to improve the reasoning capabilities of Large Language Models (LLMs) across fields like medicine, chemistry, and economics using RLVR, even with unstructured reference answers. The key methodology involves training a distilled, cross-domain generative reward model (RM-7B) without domain-specific annotations and incorporating model-based soft scoring instead of purely binary rewards. Experiments show that fine-tuning a 7B base model using RL algorithms against this reward model achieves policies outperforming state-of-the-art open-source models like Qwen2.5-72B-Instruct by up to 8.0% accuracy across various domains. The main implication is that a moderately sized, distilled generative reward model can serve as an effective and scalable cross-domain verifier for RL, enhancing robustness even with noisy or weak labels. |
| Multi-Modal | Progressive Rendering Distillation: Adapting Stable Diffusion for
  Instant Text-to-Mesh Generation without 3D Data (Read more on [arXiv](https://arxiv.org/abs/2503.21694) or [HuggingFace](https://huggingface.co/papers/2503.21694))| Zhen Lei, Xiangyu Zhu, Rongyuan Wu, DarklordLeto, ZhiyuanthePony | This paper presents Progressive Rendering Distillation (PRD), a novel training scheme to adapt the 2D text-to-image model Stable Diffusion (SD) for fast text-to-3D mesh generation without 3D ground-truth data. The primary objective is to overcome the limitations of insufficient high-quality 3D training data by distilling knowledge from multi-view 2D diffusion models into an SD-based native 3D generator. PRD progressively denoises a latent code over few steps, decoding it into a 3D Triplane representation at each step, guided by score distillation from multiple teachers (SD, MVDream, RichDreamer) and employing a parameter-efficient adapter (PETA) adding only 2.5% trainable parameters. The resulting model, TriplaneTurbo, generates high-fidelity textured meshes in 1.2 seconds, outperforming prior work in speed and quality (CLIP Score 68.2, R@1 32.3). This approach offers AI practitioners a way to create efficient text-to-3D systems by adapting powerful 2D models without needing extensive 3D datasets. |
| Reinforcement Learning | TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through
  Task Tokenization (Read more on [arXiv](https://arxiv.org/abs/2503.19901) or [HuggingFace](https://huggingface.co/papers/2503.19901))| BoDai, WenjiaWang, frankzydou, Zeshi209, lianganimation | TokenHSI introduces a unified transformer-based policy for synthesizing diverse, physically plausible human-scene interactions (HSI) using physics simulation. The main objective is to unify multiple foundational HSI skills (like following, sitting, climbing, carrying) within a single network and enable flexible, efficient adaptation to novel complex tasks, such as skill composition or variations in objects and terrain. The key methodology involves using separate tokenizers for shared humanoid proprioception and distinct task states, combined via a masking mechanism within a transformer encoder, allowing adaptation by training only new task tokenizers and lightweight adapter layers. Experiments show high success rates on foundational skills (e.g., 92.2%±6.7 on Carry) and superior performance on skill composition tasks compared to baselines (e.g., 99.2%±0.1 on Climb+Carry), demonstrating effective multi-task learning and adaptation. For AI practitioners, TokenHSI offers a versatile and extensible framework for creating physically simulated characters capable of complex interactions, reducing the need for multiple specialized controllers and full fine-tuning for new tasks. |
| Multi-Modal | KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large
  Vision-Language Models in the Korean Language (Read more on [arXiv](https://arxiv.org/abs/2503.23730) or [HuggingFace](https://huggingface.co/papers/2503.23730))| lastdefiance20, yoonshik1205 | This paper introduces KOFFVQA, a novel free-form Visual Question Answering (VQA) benchmark designed for evaluating Large Vision-Language Models (VLMs) specifically in the Korean language. The primary objective is to overcome the limitations of existing benchmarks, such as reliance on pre-determined answers or subjective LLM/VLM judges, by enabling objective evaluation of open-ended responses. The key methodology involves using 275 curated image-question pairs, each accompanied by detailed, objective, pre-defined grading criteria and partial scores, which guide an LLM judge to reliably score VLM outputs. Evaluation of 47 VLMs on KOFFVQA reveals varied performance across models and subcategories, with the proposed criteria-based evaluation demonstrating higher consistency (e.g., mean standard deviation of 0.127-0.398 for KOFFVQA vs. 0.426-0.584 for KOFFVQA-GT across judges) and accuracy (e.g., 89.3%-95.8% correct grading for KOFFVQA vs. 91.8%-92.9% for KOFFVQA-V) compared to baseline-comparison or VLM-as-judge methods. The main implication for AI practitioners is the provision of a more reliable and objective benchmark for assessing the nuanced capabilities and potential limitations of VLMs in Korean free-form VQA tasks. |
| Multi-Modal | UPME: An Unsupervised Peer Review Framework for Multimodal Large
  Language Model Evaluation (Read more on [arXiv](https://arxiv.org/abs/2503.14941) or [HuggingFace](https://huggingface.co/papers/2503.14941))| Zheyuan Liu, Yibing, yuehuang, MunanNing, 77Hui | This paper introduces UPME, an unsupervised peer review framework for evaluating Multimodal Large Language Models (MLLMs) without human-annotated Q&A pairs. The main objective is to address the limitations of existing MLLM evaluation methods, such as high human workload and inherent biases in automated approaches. UPME employs a peer review mechanism where MLLMs automatically generate questions for images, evaluate answers from other models using a novel vision-language scoring system (assessing correctness, visual understanding/reasoning, and image-text correlation), and iteratively refine model scores and weights via consistency optimization. Experimental results show that UPME achieves high alignment with human evaluations, reaching a Pearson correlation of 0.944 on the MMstar dataset and 0.814 on ScienceQA. The key implication for AI practitioners is the availability of a scalable, less biased, and unsupervised evaluation framework for MLLMs that closely mirrors human preferences. |
| Computer Vision | Easi3R: Estimating Disentangled Motion from DUSt3R Without Training (Read more on [arXiv](https://arxiv.org/abs/2503.24391) or [HuggingFace](https://huggingface.co/papers/2503.24391))| Anpei Chen, Andreas Geiger, Yuliang Xiu, faneggg, rover-xingyu | Easi3R presents a training-free approach adapting the static 3D reconstruction model DUSt3R for dynamic 4D scene understanding by disentangling motion from its internal attention maps. The main objective is to achieve robust dynamic object segmentation, camera pose estimation, and 4D point map reconstruction from videos containing motion, without needing retraining or fine-tuning on dynamic datasets. The key methodology involves analyzing aggregated cross-attention maps from DUSt3R during inference to identify dynamic regions and then applying attention re-weighting in a second inference pass to improve robustness. Quantitative results show significant improvements, for example achieving a JM score of 57.7 on DAVIS-16 dynamic object segmentation (using MonST3R backbone, without SAM2), outperforming prior methods trained on dynamic data. The main implication is that practitioners can adapt existing static models for dynamic scenes via inference-time attention manipulation, bypassing the need for extensive dynamic datasets. |
| Computer Vision | MeshCraft: Exploring Efficient and Controllable Mesh Generation with
  Flow-based DiTs (Read more on [arXiv](https://arxiv.org/abs/2503.23022) or [HuggingFace](https://huggingface.co/papers/2503.23022))| Xiaoshui Huang, Zexiang Liu, Di Huang, Junyi Chen, Xianglong He | The paper introduces MeshCraft, a novel framework for efficient and controllable 3D mesh generation using flow-based diffusion transformers. It addresses the limitations of slow generation speeds and uncontrollable face numbers in existing mesh auto-regressive techniques. MeshCraft leverages a transformer-based VAE to encode and decode meshes into continuous face-level tokens, coupled with a flow-based diffusion transformer conditioned on the number of faces. Experiments on ShapeNet demonstrate that MeshCraft achieves state-of-the-art results while being 35x faster than MeshGPT. This approach offers AI practitioners a more efficient and controllable method for 3D mesh creation, potentially relieving artists from time-consuming manual processes. |
| Machine Learning | Bridging Evolutionary Multiobjective Optimization and GPU Acceleration
  via Tensorization (Read more on [arXiv](https://arxiv.org/abs/2503.20286) or [HuggingFace](https://huggingface.co/papers/2503.20286))| Ran Cheng, Kebin Sun, Naiwei Yu, Hao Li, ZhenyuLiang | This paper introduces a tensorization methodology to accelerate evolutionary multiobjective optimization (EMO) algorithms on GPUs. The research aims to bridge the gap between EMO algorithms and advanced computing devices by transforming EMO data structures and operations into tensor representations. The key methodology involves tensorizing three representative EMO algorithms (NSGA-III, MOEA/D, HypE) and evaluating them on a novel multiobjective robot control benchmark using a GPU-accelerated physics engine. Experiments demonstrate speedups of up to 1113× compared to CPU-based counterparts while maintaining solution quality. The tensorization methodology offers a versatile approach for AI practitioners to leverage GPU acceleration in EMO algorithms, facilitating efficient handling of computationally intensive real-world applications. |
| Machine Learning | Decoupling Angles and Strength in Low-rank Adaptation (Read more on [arXiv](https://arxiv.org/abs/2503.18225) or [HuggingFace](https://huggingface.co/papers/2503.18225))| Zeynep Akata, Leander Girrbach, Massimo Bini | The paper introduces Decoupled Low-rank Adaptation (DeLoRA), a new parameter-efficient fine-tuning (PEFT) method. DeLoRA aims to improve the robustness of low-rank adaptation by decoupling angular learning from adaptation strength through normalization and scaling of learnable low-rank matrices. The method enhances robustness without significantly compromising performance, as demonstrated by matching or exceeding the performance of LoRA and ETHER on image generation and LLM adaptation tasks. In subject-driven image generation, DeLoRA achieves a DINO score of 0.693, outperforming baseline methods. This robustness enhances adaptability across various settings and simplifies the application of PEFT methods for AI practitioners. |
| Natural Language Processing | Entropy-Based Adaptive Weighting for Self-Training (Read more on [arXiv](https://arxiv.org/abs/2503.23913) or [HuggingFace](https://huggingface.co/papers/2503.23913))| Wei Wang, Mingyu Derek Ma, Yihe Deng, Xiaoxuan Wang | The paper introduces Entropy-Based Adaptive Weighting for Self-Training (EAST), a novel method to enhance the reasoning capabilities of large language models. EAST addresses the challenge of uniformly treating self-generated data by prioritizing uncertain data during self-training using an entropy-based weighting strategy. The key methodology involves a mapping function with a tunable parameter to assign higher weights to data where the model exhibits greater uncertainty. Evaluated on GSM8K, EAST achieves a further 1-2% performance boost over the vanilla method and around a 1% gain over the backbone model on MATH. The approach allows practitioners to improve reasoning abilities of models by focusing on more informative examples during self-training. |
