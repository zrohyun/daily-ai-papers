

## Papers for 2025-04-30

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with
  Diverse Modalities and Granularities (Read more on [arXiv](https://arxiv.org/abs/2504.20734) or [HuggingFace](https://huggingface.co/papers/2504.20734))| Sung Ju Hwang, Soyeong Jeong, jinheon, KangsanKim71, wgcyeo | This paper introduces UniversalRAG, a Retrieval-Augmented Generation (RAG) framework designed to operate over multiple corpora with diverse modalities (text, image, video) and granularities (e.g., paragraph, document, clip, full video). The main objective is to overcome the limitations of existing RAG systems, which are often restricted to single modalities or suffer from modality gaps when unifying heterogeneous corpora. UniversalRAG employs a modality-aware routing mechanism that dynamically selects the most appropriate modality-specific corpus and granularity level for retrieval, avoiding problematic unified embedding spaces. Evaluations across 8 benchmarks show UniversalRAG significantly outperforms baselines; for instance, using InternVL-2.5, the T5-Large trained router achieved an average score of 39.36 across benchmarks, surpassing unified (31.15) and modality-specific approaches. This work implies that dynamically routing queries to modality- and granularity-specific corpora offers a more effective strategy for grounding large vision-language models in diverse real-world information needs. |
| Reinforcement Learning | Reinforcement Learning for Reasoning in Large Language Models with One
  Training Example (Read more on [arXiv](https://arxiv.org/abs/2504.20571) or [HuggingFace](https://huggingface.co/papers/2504.20571))| Baolin, renll, ZhiyuanZeng, hushqyang, ypwang61 | This paper demonstrates that Reinforcement Learning with Verifiable Reward (RLVR) using only a single training example (1-shot RLVR) can significantly enhance the mathematical reasoning capabilities of Large Language Models (LLMs). The main objective was to investigate the minimal data requirements for RLVR while maintaining high performance compared to training on large datasets. The key methodology involved applying RLVR, specifically using algorithms like GRPO, with just one or two carefully selected math problems (sometimes ranked by historical variance score) to train LLMs like Qwen2.5-Math-1.5B. Remarkably, using a single example boosted MATH500 performance from 36.0% to 73.6%, matching the results obtained from a 1.2k example dataset. This implies that substantial reasoning capabilities may already exist in base LLMs and can be effectively activated with extreme data efficiency using RLVR, suggesting a need to re-evaluate data curation and training strategies. |
| Natural Language Processing | ReasonIR: Training Retrievers for Reasoning Tasks (Read more on [arXiv](https://arxiv.org/abs/2504.20595) or [HuggingFace](https://huggingface.co/papers/2504.20595))| pangwei, sewon, Muennighoff, volpato30, rulins | This paper introduces REASONIR-8B, a novel bi-encoder retriever specifically trained for reasoning-intensive information retrieval (IR) and retrieval-augmented generation (RAG) tasks. The primary objective is to overcome the limitations of existing retrievers, which are typically trained on factual queries and perform poorly on complex reasoning tasks requiring broader background knowledge retrieval. The key methodology involves REASONIR-SYNTHESIZER, a synthetic data generation pipeline creating challenging queries, relevant positive documents, and plausible hard negatives, combined with contrastive training on a mixture of synthetic and public data, using LLAMA3.1-8B as the base model. REASONIR-8B achieves state-of-the-art results on the BRIGHT reasoning-intensive IR benchmark (e.g., 36.9 nDCG@10 with an LLM reranker) and improves performance on RAG tasks like MMLU and GPQA by 6.4% and 22.6% respectively over closed-book baselines. For AI practitioners, this work demonstrates an effective recipe for training specialized retrievers for reasoning tasks using targeted synthetic data generation, improving performance in complex RAG applications. |
| Reinforcement Learning | Toward Evaluative Thinking: Meta Policy Optimization with Evolving
  Reward Models (Read more on [arXiv](https://arxiv.org/abs/2504.20157) or [HuggingFace](https://huggingface.co/papers/2504.20157))| Chanwoo Park, dykang, machineteacher, zaemyung | This paper introduces Meta Policy Optimization (MPO), a framework to enhance reinforcement learning-based alignment for large language models (LLMs) by dynamically evolving reward model prompts. The main objective is to address reward hacking vulnerabilities and reduce the manual prompt engineering overhead associated with using LLMs as reward models (RLAIF). MPO employs a meta-reward model that performs meta-analysis, refinement, and merging to adapt the standard reward model's evaluation rubric during training based on the policy model's outputs and context. Experiments demonstrate that MPO achieves superior performance, for instance, attaining an Elo score of 1196 on an essay writing task compared to 966 for a fixed-prompt PPO baseline. The key implication is that MPO provides a more robust, adaptive, and automated method for LLM alignment, potentially mitigating reward hacking and easing reliance on handcrafted reward prompts. |
| Multi-Modal | TesserAct: Learning 4D Embodied World Models (Read more on [arXiv](https://arxiv.org/abs/2504.20995) or [HuggingFace](https://huggingface.co/papers/2504.20995))| Junyan Li, Hongxin Zhang, Qiao Sun, yilundu, anyeZHY | TesserAct introduces a 4D embodied world model that predicts the dynamic evolution of 3D scenes conditioned on visual input and text instructions. The main objective is to learn spatially and temporally consistent 4D world representations to overcome the limitations of 2D models for embodied agent tasks. Key methodology involves extending datasets with depth and normal (DN) information, fine-tuning a video diffusion model to generate RGB-DN videos, and reconstructing coherent 4D point clouds using a novel optimization algorithm with consistency losses. The model demonstrates improved 4D scene generation, achieving a lower Chamfer L1 distance on synthetic data (0.0811) compared to baselines, and enhances downstream robotic planning performance. This implies that leveraging richer 4D representations learned from RGB-DN data can significantly improve the capabilities of embodied AI systems requiring 3D geometric understanding. |
| Multi-Modal | YoChameleon: Personalized Vision and Language Generation (Read more on [arXiv](https://arxiv.org/abs/2504.20998) or [HuggingFace](https://huggingface.co/papers/2504.20998))| Yong Jae Lee, Trung Bui, Jing Shi, Krishna Kumar Singh, Thao Nguyen | Yo'Chameleon introduces a method to personalize Large Multimodal Models (LMMs) for both vision and language generation using only 3-5 images of a novel concept. The primary objective is to enable LMMs to handle user-specific visual and textual queries, overcoming challenges like catastrophic forgetting and cross-modality performance degradation. The methodology leverages dual soft-prompt tuning (one set of tokens for vision generation, another for language tasks) optimized via a self-prompting mechanism, and incorporates a 'soft-positive' image strategy using visually similar negative examples to improve few-shot generation. Yo'Chameleon significantly outperforms prompting baselines, achieving 0.845 recognition accuracy and 0.783 CLIP image similarity for generation, while using fewer tokens (32) compared to detailed text or image prompting. This approach allows AI practitioners to develop LMMs that can effectively understand and generate personalized content across modalities with minimal user input. |
| Natural Language Processing | Certified Mitigation of Worst-Case LLM Copyright Infringement (Read more on [arXiv](https://arxiv.org/abs/2504.16046) or [HuggingFace](https://huggingface.co/papers/2504.16046))| Daniel Khashabi, Benjamin Van Durme, Marc Marone, Jiacan Yu, jackzhang | This paper introduces BLOOMSCRUB, an inference-time method providing certified mitigation against worst-case copyright infringement by large language models (LLMs). The primary objective is to prevent the generation of long verbatim quotes from copyrighted sources, addressing a key legal risk, while preserving text quality and utility. BLOOMSCRUB employs an iterative process using a Bloom filter to efficiently detect verbatim quotes exceeding a length threshold (Ï„) and then uses a separate LLM guided by dynamic prompts to rewrite the infringing segments. Experimental results demonstrate significant reduction in infringement risk; for instance, on the NewsSpan dataset, BLOOMSCRUB reduced the percentage of outputs containing quotes longer than 100 characters (%R > Q(100)) to 0.0%, compared to 20.0% for the vanilla model, while maintaining comparable utility scores (e.g., QA F1). This work implies that lightweight, post-hoc techniques can effectively offer certified guarantees against specific forms of copyright infringement in deployed LLMs without requiring direct model modification or logit access. |
| Machine Learning | The Leaderboard Illusion (Read more on [arXiv](https://arxiv.org/abs/2504.20879) or [HuggingFace](https://huggingface.co/papers/2504.20879))| Daniel D'Souza, Alex Wang, Yiyang Nan, Shivalika Singh, yuntian-deng | This paper critically evaluates the Chatbot Arena leaderboard, identifying systemic biases and distortions in its ranking of large language models. The primary objective is to assess the reliability of Arena rankings by investigating practices such as undisclosed private testing, selective score reporting, data access asymmetries, and model deprecation. The methodology combines analysis of historical data (2M battles, 243 models), scraped battle data, simulations, and controlled experiments submitting model variants to the Arena. Key results show that private testing (e.g., 27 variants by Meta) and selective disclosure inflate scores, proprietary providers receive disproportionately more data (OpenAI/Google ~20% each vs ~30% for 83 open-weight models combined), and Arena data access yields up to 112% relative performance gains on its distribution. The main implication is that current Arena dynamics risk promoting leaderboard overfitting and gamification over genuine model progress, necessitating reforms for transparent and fair benchmarking. |
| Multi-Modal | ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting (Read more on [arXiv](https://arxiv.org/abs/2504.20630) or [HuggingFace](https://huggingface.co/papers/2504.20630))| Tao Jin, Zhiyuan Zhu, Changhao Pan, Wenxiang Guo, AaronZ345 | This paper introduces ISDrama, a novel framework and the MRSDrama dataset for generating continuous, multi-speaker immersive spatial drama with dramatic prosody from multimodal prompts (text, audio, video, pose). The primary objective is to create a unified system capable of simultaneously modeling complex spatial dynamics (including Doppler effects) and expressive prosody, addressing limitations of cascaded approaches. ISDrama employs a contrastive learning-based Multimodal Pose Encoder for unified pose representation and a flow-based Mamba-Transformer (Immersive Drama Transformer) with Mixture-of-Experts (Drama-MOE) and context-consistent classifier-free guidance for generation. Experimental results demonstrate ISDrama's superiority over baselines, achieving lower objective errors (e.g., IPD MAE of 0.008 with geometric prompts vs 0.009 for the best baseline) and higher subjective scores (e.g., MOS-P of 4.41 with textual prompts). For AI practitioners, ISDrama provides a method to generate rich, immersive audio-visual experiences by effectively fusing spatial and prosodic information from diverse inputs, relevant for VR/AR and creative applications. |
| Multi-Modal | X-Fusion: Introducing New Modality to Frozen Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.20996) or [HuggingFace](https://huggingface.co/papers/2504.20996))| Yijun Li, Siddharth Srinivasan Iyer, Xun Huang, Thao Nguyen, Sicheng Mo | This paper introduces X-Fusion, a framework for extending frozen pre-trained Large Language Models (LLMs) with vision capabilities while preserving their original language competence. The primary objective is to efficiently integrate new modalities like vision into LLMs without costly retraining or performance degradation in language tasks. X-Fusion employs a dual-tower architecture where the LLM weights are frozen (text tower), and a separate, trainable vision tower processes visual inputs, with features fused at intermediate layers for joint understanding and generation. Experiments show X-Fusion outperforms baselines, achieving a FID score of 14.20 on text-to-image generation compared to 19.10 for a single-tower approach, while maintaining the base LLM's MMLU score. The key implication is that this dual-tower approach provides an efficient and scalable method for building unified multimodal models by leveraging existing LLMs. |
| Multi-Modal | Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional
  Talking Portrait Generation (Read more on [arXiv](https://arxiv.org/abs/2504.18087) or [HuggingFace](https://huggingface.co/papers/2504.18087))| Xiaobin Hu, FeiFan Xu, Chuming Lin, Weipeng Tan, ChengmingX | This paper introduces DICE-Talk, a novel diffusion-based framework for generating emotionally expressive talking portraits while preserving identity. The primary objective is to overcome limitations in existing methods by better utilizing audio cues, preventing identity leakage in emotion representations, and modeling inter-emotion correlations. Key methodologies include a cross-modal disentangled emotion embedder representing emotions as identity-agnostic Gaussian distributions, a correlation-enhanced conditioning module leveraging learnable emotion banks, and an emotion discrimination objective during diffusion. DICE-Talk significantly outperforms prior art in emotion accuracy on datasets like MEAD (achieving an Emo-Score of 0.4865 with prompt conditioning) while maintaining competitive lip-sync. For AI practitioners, this work facilitates the creation of more realistic digital humans capable of expressing nuanced, identity-preserving emotions. |
| Natural Language Processing | TreeHop: Generate and Filter Next Query Embeddings Efficiently for
  Multi-hop Question Answering (Read more on [arXiv](https://arxiv.org/abs/2504.20114) or [HuggingFace](https://huggingface.co/papers/2504.20114))| Xuming Hu, Shuliang Liu, Jinghuai Ou, Zhonghao Li, kpzhang1028 | TreeHop introduces an efficient embedding-level framework for multi-hop question answering (MHQA) without requiring LLM query rewriting. The primary objective is to mitigate the high computational costs and latency of traditional iterative Retrieval-Augmented Generation (RAG) methods in MHQA tasks. TreeHop achieves this via a streamlined 'Retrieve-Embed-Retrieve' loop, updating query embeddings using a lightweight gated cross-attention mechanism (UpdateGate) and employing rule-based pruning strategies. Experimental results show TreeHop rivals advanced methods in recall across three MHQA datasets while reducing query latency by approximately 99% and using significantly fewer parameters (25 million). This presents a faster and more cost-effective approach for deploying RAG systems in knowledge-intensive scenarios. |
