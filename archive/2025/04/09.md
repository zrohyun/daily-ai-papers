

## Papers for 2025-04-09

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | OmniSVG: A Unified Scalable Vector Graphics Generation Model (Read more on [arXiv](https://arxiv.org/abs/2504.06263) or [HuggingFace](https://huggingface.co/papers/2504.06263))| Jiaxu Zhang, Xianfang Zeng, Yiying Yang, CH3COOK, wchengad | OmniSVG introduces a unified framework leveraging pre-trained Vision-Language Models (VLMs) for generating high-quality, complex Scalable Vector Graphics (SVGs). The main objective is to address the limitations of existing methods, such as low complexity or high computational cost, by enabling end-to-end multimodal SVG generation (Text-to-SVG, Image-to-SVG, Character-Reference SVG). The key methodology involves parameterizing SVG commands and coordinates into discrete tokens, decoupling structural logic from low-level geometry for efficient training using a VLM like Qwen2.5-VL, supported by a new large-scale dataset (MMSVG-2M). Quantitative results demonstrate superiority over prior work; for example, OmniSVG(7B) achieves a CLIP score of 0.3164 and HPS of 0.253 on the MMSVG-Illustration Text-to-SVG task, outperforming methods like SVGDreamer and Chat2SVG. For AI practitioners, OmniSVG presents a scalable and versatile approach for complex vector graphics synthesis, showing the potential of VLMs for structured graphical output beyond pixel-based generation. |
| Multi-Modal | Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought (Read more on [arXiv](https://arxiv.org/abs/2504.05599) or [HuggingFace](https://huggingface.co/papers/2504.05599))| Jiangbo Pei, Yichen Wei, Xiaokun Wang, Chris, Yi Peng | Skywork R1V is introduced as a multimodal reasoning model extending R1-series Large Language Models (LLMs) to visual modalities using an efficient transfer method. The main objective is to achieve strong multimodal reasoning, particularly in complex logical and mathematical tasks, by leveraging existing LLM capabilities without retraining foundational models. Key methodologies include a lightweight visual projector for transfer, a hybrid optimization strategy combining Iterative Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO), and an adaptive-length Chain-of-Thought distillation for generating reasoning data. The 38B parameter model demonstrates competitive performance, achieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista. For AI practitioners, this paper presents an effective technique for adapting powerful LLMs for multimodal reasoning tasks efficiently, with model weights released publicly. |
| Multi-Modal | An Empirical Study of GPT-4o Image Generation Capabilities (Read more on [arXiv](https://arxiv.org/abs/2504.05979) or [HuggingFace](https://huggingface.co/papers/2504.05979))| Zhuoran Zhao, Sixiang Chen, donghao-zhou, QingyuShi, BryanW | This paper presents an empirical study evaluating the image generation capabilities of GPT-4o across diverse multimodal tasks. The primary objective is to benchmark GPT-4o against contemporary open-source and commercial models, assessing its strengths and weaknesses in potentially unifying text and image generation within a single framework. The methodology involves extensive qualitative comparisons across over 20 tasks within text-to-image, image-to-image, image-to-3D, and image-to-X categories, analyzing performance based on visual quality, consistency, knowledge, and instruction adherence as detailed in Table 1. Key findings highlight GPT-4o's exceptional text rendering, compositional generalization, prompt following, spatial reasoning, and broad image transformation capabilities; however, specific quantitative metrics for GPT-4o's generation performance are not provided in this primarily qualitative study, and limitations include inconsistent generation, hallucination, and data bias. For AI practitioners, this study suggests that large-scale unified models like GPT-4o show significant promise, especially for instruction-rich tasks, but highlights the ongoing importance of architecture, data scale, and addressing limitations like consistency and bias. |
| Natural Language Processing | Hogwild! Inference: Parallel LLM Generation via Concurrent Attention (Read more on [arXiv](https://arxiv.org/abs/2504.06261) or [HuggingFace](https://huggingface.co/papers/2504.06261))| Vage Egiazarian, George Yakushev, Alina Shutova, Roman Garipov, Gleb Rodionov | This paper introduces Hogwild! Inference, a parallel LLM inference method utilizing concurrent attention for faster generation. The research aims to expedite LLM inference by enabling collaborative generation through a shared Key-Value cache. The method allows parallel LLM instances to synchronize via a concurrently-updated attention cache and decide how to collaborate. Preliminary experiments show the method maintains reasoning abilities while incorporating progress from other instances, and it accelerates problem-solving compared to single-threaded inference. This approach offers a promising avenue for enabling effective collaboration between multiple LLM instances without additional fine-tuning. |
| Multi-Modal | Less-to-More Generalization: Unlocking More Controllability by
  In-Context Generation (Read more on [arXiv](https://arxiv.org/abs/2504.02160) or [HuggingFace](https://huggingface.co/papers/2504.02160))| Fei Ding, Yufeng Cheng, Mengqi Huang, wuwx, fenfan | This paper introduces UNO, a subject-to-image model leveraging Diffusion Transformers for enhanced controllability in single and multi-subject image generation via in-context learning. The primary objective is to address data scalability and subject expansibility limitations in customized image generation, particularly for multi-subject scenarios. Key methodologies include a novel model-data co-evolution paradigm featuring a progressive synthetic data curation pipeline based on DiT's in-context generation and the UNO model architecture incorporating progressive cross-modal alignment and Universal Rotary Position Embedding (UnoPE). UNO achieves state-of-the-art results, notably attaining a DINO score of 0.760 and CLIP-I score of 0.835 on the DreamBench single-subject benchmark. The main implication for AI practitioners is the demonstration of a scalable, tuning-free approach to achieve high-fidelity, controllable multi-subject image generation by systematically synthesizing high-quality training data and evolving model capabilities. |
| Natural Language Processing | COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for
  Alignment with Human Values (Read more on [arXiv](https://arxiv.org/abs/2504.05535) or [HuggingFace](https://huggingface.co/papers/2504.05535))| Siwei Wu, M-A-P Team, Liam-Liu, aaabiao, JinChengRen | This paper introduces COIG-P, a large-scale (1,006k pairs), high-quality Chinese preference dataset for aligning LLMs with human values, generated via an LLM-based pipeline without human intervention. The main objective was to address the scarcity, limited domain coverage, and validation issues of existing Chinese preference datasets. The methodology involved crawling 92k queries, using 15 LLMs for response generation and 8 LLMs for scoring to create chosen-rejected pairs across 6 domains, alongside developing a Chinese Reward Model (CRM) and benchmark (CRBench). Experiments showed that training models on COIG-P led to significant performance improvements (2% to 12% gains on AlignBench for Qwen2/2.5 and Infinity-Instruct-3M-0625 series models) compared to baselines. For AI practitioners, COIG-P provides a substantial resource for improving LLM alignment in Chinese contexts more efficiently and cost-effectively than manual annotation, complemented by a CRM for reward modeling tasks. |
| Computer Vision | Tuning-Free Image Editing with Fidelity and Editability via Unified
  Latent Diffusion Model (Read more on [arXiv](https://arxiv.org/abs/2504.05594) or [HuggingFace](https://huggingface.co/papers/2504.05594))| Ming-Hsuan Yang, Mike Zheng Shou, Yuchao Gu, Lan Chen, Qi Mao | The paper introduces UnifyEdit, a tuning-free method for text-based image editing that balances fidelity and editability. It addresses the challenge of over- or under-editing by employing self-attention preservation and cross-attention alignment constraints during diffusion latent optimization. An adaptive time-step scheduler dynamically adjusts constraint influence to resolve gradient conflicts. Experiments demonstrate UnifyEdit's superiority, achieving a CLIP score of 22.49 for color changes while maintaining structural integrity. The method offers AI practitioners a robust, adaptable approach for image editing that explicitly manages the fidelity-editability trade-off without task-specific fine-tuning. |
| Natural Language Processing | Generative Evaluation of Complex Reasoning in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.02810) or [HuggingFace](https://huggingface.co/papers/2504.02810))| Baizhou Huang, Ruilin Yan, Xiangyu Wang, YitaoLiang, pkuHaowei | The paper introduces KUMO, a generative evaluation framework for assessing complex reasoning in LLMs. It aims to address the issue of benchmark contamination by dynamically generating diverse reasoning tasks using LLMs and symbolic engines. The methodology involves creating multi-turn, partially observable tasks across 100 domains, requiring genuine generalization rather than memorization. Evaluations of 23 LLMs on 5,000 KUMO tasks reveal that many outperform university-level performance on easy tasks; reasoning-scaled LLMs reach university-level performance on complex reasoning tasks, achieving up to 0.9+ Pearson correlation coefficients with real-world benchmarks. KUMO offers a robust, enduring assessment tool for LLM reasoning, resistant to data contamination. |
| Multi-Modal | V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric
  Capabilities in Multimodal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.06148) or [HuggingFace](https://huggingface.co/papers/2504.06148))| Alex Jinpeng Wang, Ping Yu, Zhengyuan Yang, Linjie Li, Fengx1nn | The paper introduces V-MAGE, a game-based evaluation framework to assess visual reasoning capabilities in multimodal large language models (MLLMs). It aims to address the limitations of existing benchmarks by using visually rich, dynamic game environments. V-MAGE features five diverse games with over 30 handcrafted levels, testing positioning, trajectory tracking, timing, visual memory, and long-term planning. Evaluation of state-of-the-art MLLMs revealed significant performance gaps compared to humans, highlighting challenges in visual perception and reasoning. V-MAGE offers AI practitioners a more realistic and interactive benchmark for developing MLLMs with robust visual reasoning and decision-making skills. |
| Multi-Modal | CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs
  with Controllable Puzzle Generation (Read more on [arXiv](https://arxiv.org/abs/2504.00043) or [HuggingFace](https://huggingface.co/papers/2504.00043))| William W. Cohen, Bill Yuchen Lin, Langlin Huang, Chengsong Huang, Jixuan Leng | The paper introduces CrossWordBench, a new benchmark for evaluating reasoning in LLMs and LVLMs using crossword puzzles.  It aims to assess the dynamic interplay between textual clues and visual grid constraints. The benchmark uses a controllable puzzle generation framework to produce diverse puzzles and evaluation strategies.  Evaluations of over 20 models revealed that reasoning LLMs outperform non-reasoning models and leverage crossing-letter constraints effectively, while LVLMs struggle with grid parsing, showing a strong correlation between puzzle-solving performance and grid-parsing accuracy (r=0.94).  The findings provide insights into the limitations of current models and offer a framework for creating multimodal reasoning tasks for future evaluations. |
| Computer Vision | HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned
  Guidance (Read more on [arXiv](https://arxiv.org/abs/2504.06232) or [HuggingFace](https://huggingface.co/papers/2504.06232))| Tong Wu, Pan Zhang, Yujie Zhou, Pengyang Ling, Jiazi Bu | The paper introduces HiFlow, a novel training-free and model-agnostic framework designed to improve high-resolution image generation in text-to-image models. It addresses the challenge of generating high-resolution images by establishing a virtual reference flow in high-resolution space and aligning the high-resolution sampling flow with this reference. HiFlow guides the generation through initialization, direction, and acceleration alignment, resulting in improved low-frequency consistency, structure preservation, and detail fidelity. Experiments demonstrate that HiFlow achieves superior image quality compared to state-of-the-art methods, achieving a FID of 45.01 at 4096x4096 resolution. This framework allows AI practitioners to enhance the resolution capabilities of pre-trained flow models without additional training, leading to more detailed and coherent high-resolution images. |
| Natural Language Processing | Accelerate Parallelizable Reasoning via Parallel Decoding within One
  Sequence (Read more on [arXiv](https://arxiv.org/abs/2503.20533) or [HuggingFace](https://huggingface.co/papers/2503.20533))| Yijiong Yu | This paper introduces "Parallel Decoding in One Sequence," a method to accelerate reasoning in Large Language Models (LLMs) for tasks involving parallelizable steps. The main objective is to overcome the computational expense and latency of generating long, sequential reasoning chains by exploiting inherent task parallelism. The key methodology involves identifying parallel steps, marking them with special tokens, decoding their content simultaneously within a single sequence using a modified attention mask and position IDs, and then concatenating the results. Experimental results demonstrate significant decoding speedups, such as nearly doubling speed on a retrieval task (e.g., from 21.2 to 40.5 tokens/s for Qwen2.5-14b) and achieving over 60% speedup on QA tasks, while largely maintaining answer quality. For AI practitioners, this method offers a way to reduce inference time for parallelizable reasoning tasks without increased memory usage or KV cache recomputation. |
