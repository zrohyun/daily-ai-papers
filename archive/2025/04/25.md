

## Papers for 2025-04-25

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Step1X-Edit: A Practical Framework for General Image Editing (Read more on [arXiv](https://arxiv.org/abs/2504.17761) or [HuggingFace](https://huggingface.co/papers/2504.17761))| Peng Xing, Yucheng Han, Shiyu Liu, skicy, wchengad | This paper introduces Step1X-Edit, an open-source framework designed for general-purpose, instruction-based image editing. The primary objective is to bridge the significant performance gap observed between existing open-source editing algorithms and state-of-the-art closed-source multimodal models like GPT-40 and Gemini2 Flash. Key methodology involves using a Multimodal Large Language Model (MLLM) to interpret the reference image and text instruction, generating a latent embedding that guides a DiT-style diffusion decoder, trained on a novel large-scale dataset generated via a custom pipeline. Evaluation on the proposed GEdit-Bench benchmark shows Step1X-Edit substantially outperforms open-source baselines (e.g., achieving a VIEScore G_O of 6.813 vs 3.231 for AnyEdit on the EN-Intersection subset evaluated by GPT-4.1) and approaches the capabilities of proprietary systems. The main implication for AI practitioners is the release of a high-performing, open-source model and benchmark that significantly advances general image editing capabilities and facilitates further research. |
| Multi-Modal | RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image
  Generation (Read more on [arXiv](https://arxiv.org/abs/2504.17502) or [HuggingFace](https://huggingface.co/papers/2504.17502))| Michal Sokolik, Brian Gordon, Yonatan Bitton, Hagai Taitelbaum, lovodkin93 | This paper introduces REFVNLI, a cost-effective automatic metric for evaluating subject-driven text-to-image (T2I) generation by assessing both subject preservation and textual alignment in a single prediction. The primary objective is to overcome the limitations of existing evaluation methods, which often assess only one aspect, misalign with human judgments, or rely on expensive API calls. REFVNLI employs a fine-tuned Vision-Language Model (PaliGemma) trained on a large-scale dataset derived from video reasoning benchmarks and image perturbations, using automatically generated positive and negative examples for both subject consistency and textual alignment. Key results show REFVNLI outperforms or matches baselines across multiple benchmarks, achieving up to 6.4-point gains in textual alignment and 8.5-point gains in subject consistency, and aligning with human preferences over 87% on rare concepts. For AI practitioners, REFVNLI provides a scalable and reliable tool to evaluate and advance subject-driven T2I models, especially for personalized generation and consistent character rendering. |
| Machine Learning | Paper2Code: Automating Code Generation from Scientific Papers in Machine
  Learning (Read more on [arXiv](https://arxiv.org/abs/2504.17192) or [HuggingFace](https://huggingface.co/papers/2504.17192))| Sung Ju Hwang, Seongyun Lee, jinheon, iaminju | This paper introduces PaperCoder, a multi-agent Large Language Model (LLM) framework designed to automate the generation of functional code repositories directly from machine learning scientific papers. The primary objective is to address the research reproducibility challenge caused by the frequent unavailability of code implementations corresponding to published papers. PaperCoder employs a structured three-stage pipeline involving planning (high-level roadmap, architecture design, dependency identification, configuration), analysis (interpreting implementation details), and generation (modular, dependency-aware code production) executed by specialized collaborating agents. Evaluations on the Paper2Code and PaperBench benchmarks show PaperCoder significantly outperforms baselines, achieving a 44.26% replication score on PaperBench Code-Dev, and human evaluations indicate 77% of its generated repositories were rated best by original paper authors. The main implication for AI practitioners is the potential to significantly reduce the manual effort required for reimplementing research, thereby accelerating scientific validation and progress. |
| Multi-Modal | Breaking the Modality Barrier: Universal Embedding Learning with
  Multimodal LLMs (Read more on [arXiv](https://arxiv.org/abs/2504.17432) or [HuggingFace](https://huggingface.co/papers/2504.17432))| Yanzhao Zhang, Xingjun Wang, Ziyong Feng, Tiancheng Gu, Kaichengalex | This paper introduces UniME, a novel two-stage framework leveraging Multimodal Large Language Models (MLLMs) to learn universal and discriminative embeddings for diverse vision-language tasks. The primary objective is to overcome limitations of existing methods like CLIP and standard MLLMs in generating transferable multimodal representations with strong discriminative and compositional capabilities. The UniME methodology involves textual discriminative knowledge distillation to enhance the MLLM's language component, followed by hard negative enhanced instruction tuning with false negative filtering and hard negative sampling. UniME achieves state-of-the-art results, demonstrating significant improvements on the MMEB benchmark (e.g., 66.6% overall score, +3.3% over VLM2Vec) and various retrieval tasks. For AI practitioners, this work presents a method to create powerful, universal multimodal embeddings from MLLMs applicable across various downstream tasks, improving performance without task-specific fine-tuning. |
| Multi-Modal | Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery
  Simulation (Read more on [arXiv](https://arxiv.org/abs/2504.17207) or [HuggingFace](https://huggingface.co/papers/2504.17207))| Leonidas Guibas, Mikaela Angelina Uy, Chanho Park, Jihyeon Je, Phillip Y. Lee | This paper introduces Abstract Perspective Change (APC), a framework enhancing perspective-aware spatial reasoning in Vision-Language Models (VLMs) by simulating mental imagery. The primary objective is to overcome the strong egocentric bias in existing VLMs and enable reasoning from arbitrary viewpoints (allocentric reasoning). The key methodology involves using vision foundation models to create a coarse 3D scene abstraction, transforming its coordinate system to the specified reference viewpoint, and then re-prompting the VLM with this egocentric representation, either numerically or visually. Experiments show significant improvements; for instance, on the COMFORT++ left/right task, APC achieves up to 89.67% accuracy, vastly outperforming baseline VLMs (e.g., LLaVA-OneVision at 55.33%) and novel-view-synthesis methods. For AI practitioners, this work demonstrates that explicit scene abstraction and perspective transformation can effectively imbue VLMs with allocentric spatial reasoning capabilities, crucial for robust environmental interaction and understanding, without complex model retraining or view synthesis. |
| Natural Language Processing | QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM
  Pretraining (Read more on [arXiv](https://arxiv.org/abs/2504.16511) or [HuggingFace](https://huggingface.co/papers/2504.16511))| Yifan Zhang, Zhimiao Yu, Binbin Liu, Weidong Zhou, Fengze Liu | This paper introduces QuaDMix, a unified framework designed to jointly optimize data quality and diversity for efficient Large Language Model (LLM) pretraining. The core objective is to address the inherent trade-off between these two critical metrics, often overlooked by methods that optimize them separately. QuaDMix employs multiple quality criteria scorers and domain classification, integrating these through a parameterized merging function and applying a subsequent parameterized sampling function to determine data selection probabilities; parameter optimization is accelerated via proxy model experiments and a LightGBM regressor. Experimental results demonstrate that QuaDMix achieves an average performance improvement of 7.2% across multiple benchmarks compared to baseline strategies. The main implication for AI practitioners is the highlighted necessity and feasibility of jointly optimizing data quality and diversity for improved LLM pretraining efficiency, offering a structured approach via the QuaDMix framework. |
| Multi-Modal | Token-Shuffle: Towards High-Resolution Image Generation with
  Autoregressive Models (Read more on [arXiv](https://arxiv.org/abs/2504.17789) or [HuggingFace](https://huggingface.co/papers/2504.17789))| Chih-Yao Ma, Hao Tang, Haoyu Ma, Peize Sun, Xu Ma | This paper introduces Token-Shuffle, a novel method to enable efficient high-resolution image generation using autoregressive (AR) models within Multimodal Large Language Models (MLLMs). The primary objective is to overcome the limitations imposed by the large number of visual tokens required for high-resolution images in traditional AR approaches, which hinders training and inference efficiency. The core methodology involves 'token-shuffle' to merge spatially local visual tokens along the channel dimension leveraging visual vocabulary redundancy, reducing the token count for Transformer computation, and 'token-unshuffle' to restore spatial arrangement for output, all within a unified next-token prediction framework. Key results show their 2.7B parameter model achieves a 0.77 overall score on the GenAI-benchmark's hard prompts, outperforming LlamaGen by 0.18 and LDM by 0.15, while enabling 2048x2048 image generation. For AI practitioners, Token-Shuffle presents a computationally efficient technique to significantly scale the resolution capabilities of AR image generation models. |
| Computer Vision | Distilling semantically aware orders for autoregressive image generation (Read more on [arXiv](https://arxiv.org/abs/2504.17069) or [HuggingFace](https://huggingface.co/papers/2504.17069))| David Vazquez, Masih Aminbeidokhti, Juan A. Rodriguez, Antoine Poupon, rishavpramanik | This paper introduces Ordered Autoregressive (OAR) image generation, a method enhancing autoregressive models by learning optimal, content-dependent patch generation orders instead of using fixed raster-scan sequences. The core objective is to investigate if a learned, semantically aware order can improve image quality, given that images lack the inherent sequential structure of text. The methodology involves training an any-order model, using it to distill optimal generation orders for training data based on likelihood, and then fine-tuning this model using the extracted orders. Key results demonstrate improved image quality; on the Fashion Products dataset, the fine-tuned OAR achieved an FID score of 2.56, outperforming the raster-scan baseline's 4.58, and on CelebA-HQ, it achieved an FID of 1.41 compared to 1.94. The main implication for practitioners is that optimizing the generation order in autoregressive image models is crucial and can significantly boost performance without additional annotations, suggesting a move away from fixed sequential generation patterns. |
| Multi-Modal | DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs (Read more on [arXiv](https://arxiv.org/abs/2504.17040) or [HuggingFace](https://huggingface.co/papers/2504.17040))| Heng Ji, Silvio Savarese, Caiming Xiong, Senthil Purushwalkam, Zhenhailong Wang | DyMU is a novel, training-free framework designed to enhance the efficiency of Vision-Language Models (VLMs) by dynamically adapting visual token usage based on image complexity. The core objective is to reduce VLM computational costs by adjusting the number of visual tokens processed without requiring model retraining or performance degradation. The methodology combines Dynamic Token Merging (DToMe) for adaptive token reduction in the visual encoder and Virtual Token Unmerging (VTU) to efficiently simulate full-sequence attention dynamics within the RoPE-based LLM using the reduced tokens. Experiments demonstrate significant efficiency gains, reducing average visual token counts by 32%-85% while achieving comparable performance to full-length models (e.g., DyMU-low achieves 97.7% of LLaVA-1.5 baseline average accuracy using ~15% of tokens). This offers AI practitioners a plug-and-play, training-free technique to improve VLM inference efficiency and dynamically manage computational resources based on content complexity. |
| Natural Language Processing | IberBench: LLM Evaluation on Iberian Languages (Read more on [arXiv](https://arxiv.org/abs/2504.16921) or [HuggingFace](https://huggingface.co/papers/2504.16921))| Areg Mikael Sarvazyan, Álvaro Romo Herrero, Ian Borrego Obrador, José Ángel González, mchinea | This paper introduces IberBench, a comprehensive benchmark designed to evaluate Large Language Models (LLMs) on diverse tasks across Iberian languages and their varieties. The primary objective is to address the lack of evaluation resources beyond English by assessing LLM performance on both fundamental and industry-relevant NLP tasks like sentiment analysis, MGT detection, and summarization in Spanish, Portuguese, Catalan, Basque, Galician, and specific regional varieties. IberBench integrates 101 datasets, primarily using text classification formats and evaluates models using an extended `lm-evaluation-harness` framework, incorporating community-driven updates. Key findings from evaluating 23 LLMs reveal lower performance on industry tasks compared to fundamental ones, significant challenges in Galician and Basque, and that top models like Qwen-2.5-7b-Instruct achieve around 46.8% average performance, while many struggle with specific tasks like lexical borrowing (best score 31.98% Macro F1). IberBench provides practitioners with a vital, open-source tool and leaderboard for assessing LLM suitability for Iberian language applications, highlighting current limitations and areas needing improvement. |
| Computer Vision | Boosting Generative Image Modeling via Joint Image-Feature Synthesis (Read more on [arXiv](https://arxiv.org/abs/2504.16064) or [HuggingFace](https://huggingface.co/papers/2504.16064))| Nikos Komodakis, Spyros Gidaris, Ioannis Kakogeorgiou, Efstathios Karypidis, Theodoros Kouzelis | This paper introduces ReDi, a generative framework that boosts image synthesis quality and efficiency by jointly modeling low-level VAE image latents and high-level DINOv2 semantic features within a single diffusion process. The primary objective is to seamlessly integrate representation learning into generative modeling to enhance performance without complex distillation objectives. The methodology involves training a diffusion transformer (DiT or SiT) to denoise both PCA-reduced semantic features and VAE latents simultaneously, enabling a novel inference strategy called Representation Guidance. Key results demonstrate significantly accelerated convergence (~6x faster than REPA on SiT-XL/2) and improved generation quality, achieving a state-of-the-art FID of 1.64 (SiT-XL/2, 600 epochs, CFG) on ImageNet 256x256. For AI practitioners, ReDi presents a simplified approach to build superior, representation-aware generative models with faster convergence and enhanced semantic control during generation. |
| Multi-Modal | ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting (Read more on [arXiv](https://arxiv.org/abs/2504.15921) or [HuggingFace](https://huggingface.co/papers/2504.15921))| Mariano Beguerisse-Diaz, Shaogang Gong, Dimitrios Korkinof, Jian Hu | ViSMaP introduces an unsupervised method for summarizing hour-long videos by leveraging annotated short-video data and meta-prompting with Large Language Models (LLMs). The main objective is to generate effective long-video summaries without costly manual annotations by transferring knowledge from short-form videos and utilizing LLM reasoning to create pseudo-summaries. The core methodology involves pre-training a model on short videos, generating segment pseudo-captions for long videos, iteratively refining these into full pseudo-summaries via meta-prompting using generator, evaluator, and optimizer LLMs, and finally fine-tuning the model using these pseudo-summaries via a noisy-label loss. On the Ego4D-HCap dataset, ViSMaP achieves unsupervised video summary performance (CIDEr 26.0) comparable to supervised state-of-the-art methods. This offers AI practitioners an annotation-efficient pathway for long-form video understanding by bridging domain and video length gaps using LLM-driven meta-prompting. |
| Computer Vision | 3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2504.17414) or [HuggingFace](https://huggingface.co/papers/2504.17414))| Fan Wang, Jingkai Zhou, Chaohui Yu, Min Wei | This paper introduces 3DV-TON, a diffusion-based framework for generating high-fidelity and temporally consistent video virtual try-on results. The main objective is to address the poor temporal coherence and detail preservation in existing methods when handling complex clothing and poses. Key methodology involves generating an animatable textured 3D mesh from an initial image try-on frame, synchronizing it with the video poses to provide explicit frame-level guidance to a diffusion model, complemented by a rectangular masking strategy. Quantitative results on the ViViD dataset show superior performance, achieving a paired VFID_I3D score of 10.9680 (using ViViD's mask), outperforming prior state-of-the-art like CatV2TON (13.5962), and similar improvements are shown on their new HR-VVT benchmark. For AI practitioners, this work demonstrates that using explicit textured 3D guidance can significantly enhance temporal consistency and visual quality in video generation tasks like virtual try-on, mitigating the common issue of diffusion models prioritizing appearance over motion coherence. |
| Multi-Modal | TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming
  Videos (Read more on [arXiv](https://arxiv.org/abs/2504.17343) or [HuggingFace](https://huggingface.co/papers/2504.17343))| Shuhuai Ren, Lei Li, Yuancheng Wei, Yicheng Li, Linli Yao | This paper introduces TimeChat-Online, a novel online VideoLLM designed for efficient real-time understanding of streaming videos. The primary objective is to overcome the limitations of existing VideoLLMs in handling dense, redundant frames and enabling real-time, proactive interaction in streaming scenarios. Its core methodology is the Differential Token Drop (DTD) module, inspired by the Change Blindness phenomenon, which adaptively eliminates temporally redundant visual tokens based purely on visual changes without language guidance. Experiments show DTD achieves an 82.8% reduction in video tokens while maintaining over 98% of original accuracy on the StreamingBench benchmark, alongside a 1.76x speedup. The key implication for AI practitioners is the revelation that a vast majority (>80%) of visual tokens in streaming videos are naturally redundant, highlighting a significant potential for efficiency gains in VideoLLM development by leveraging this inherent redundancy. |
| Machine Learning | Interpretable non-linear dimensionality reduction using gaussian
  weighted linear transformation (Read more on [arXiv](https://arxiv.org/abs/2504.17601) or [HuggingFace](https://huggingface.co/papers/2504.17601))| erikbergh | This paper introduces an interpretable non-linear dimensionality reduction algorithm using Gaussian weighted linear transformations. The primary objective is to bridge the gap between the interpretability of linear methods like PCA and the representational power of non-linear methods like t-SNE. The methodology involves constructing a non-linear mapping as a weighted sum of multiple linear transformations, where weights are determined by Gaussian functions centered in the input space, optimized to preserve pairwise distances. Demonstrated on a 3D S-curve dataset reduced to 2D, the method achieved a reconstruction error of 0.45 and enabled analysis of dimension influence, spatial variance, and contraction/expansion. The main implication for AI practitioners is the availability of a dimensionality reduction technique that offers both complex pattern capture and transparent insights into the transformation process, unlike purely linear or black-box non-linear methods, and can generalize to new data points. |
| Machine Learning | Process Reward Models That Think (Read more on [arXiv](https://arxiv.org/abs/2504.16828) or [HuggingFace](https://huggingface.co/papers/2504.16828))| Hao Peng, Jaekyeom Kim, Lajanugen Logeswaran, Rishabh Agarwal, Muhammad Khalifa | This paper introduces THINKPRM, a data-efficient, generative process reward model designed to verify reasoning steps using long Chain-of-Thought (CoT) verification. The primary objective is to create effective PRMs with minimal supervision, addressing the high data requirements of traditional discriminative PRMs. The key methodology involves fine-tuning large reasoning models on a small set (~1K examples, ~8K process labels) of filtered synthetic verification CoTs generated by another LLM. Results show THINKPRM outperforms LLM-as-a-Judge and discriminative PRMs trained on orders of magnitude more data (e.g., by 8% on a GPQA-Diamond subset against a DiscPRM trained on full PRM800K) across benchmarks like ProcessBench, MATH-500, and AIME '24. The main implication is that generative CoT-based PRMs offer a scalable and data-efficient path to improve reasoning verification and scale test-time compute for complex tasks. |
