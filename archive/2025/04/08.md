

## Papers for 2025-04-08

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | One-Minute Video Generation with Test-Time Training (Read more on [arXiv](https://arxiv.org/abs/2504.05298) or [HuggingFace](https://huggingface.co/papers/2504.05298))| guestrin, zhaoyue-zephyrus, GashonHussein, koceja, karansdalal | This paper presents a method using Test-Time Training (TTT) layers to enable a pre-trained Diffusion Transformer to generate coherent one-minute videos from text storyboards. The main objective is to address the challenge of generating long-context, complex, multi-scene videos, which existing Transformer and RNN approaches struggle with due to efficiency or expressivity limitations. The key methodology involves adding TTT-MLP layers (whose hidden states are neural networks updated via test-time self-supervision) to a pre-trained video diffusion model (CogVideo-X 5B), using local self-attention combined with global TTT processing, and fine-tuning on a curated Tom and Jerry dataset. In human evaluations on 63-second videos, the TTT-MLP approach outperformed baselines like Gated DeltaNet and Mamba 2 by 34 Elo points on average, showing significant gains in temporal consistency and motion naturalness, although artifacts remain. For AI practitioners, this demonstrates that TTT layers offer a promising approach to extend sequence models for generating longer, complex video narratives, overcoming limitations of standard methods, but requiring further work on efficiency and artifact reduction. |
| Multi-Modal | SmolVLM: Redefining small and efficient multimodal models (Read more on [arXiv](https://arxiv.org/abs/2504.05299) or [HuggingFace](https://huggingface.co/papers/2504.05299))| eliebak, mervenoyan, mfarre, orrzohar, andito | This paper introduces SmolVLM, a family of small and efficient multimodal Vision-Language Models (VLMs) designed for resource-constrained environments. The primary objective is to redefine compact VLMs by systematically optimizing architectural configurations, tokenization strategies, and data curation to minimize computational overhead while maintaining performance. Key methodologies include balancing vision encoder and language model parameters, utilizing efficient tokenization like pixel shuffling (r=4 for smaller models), extending context lengths (up to 16k), employing image splitting for high-resolution images, and using learned positional tokens. Results show the smallest model (SmolVLM-256M) uses less than 1GB GPU RAM during inference and outperforms the much larger Idefics-80B, while the 2.2B parameter variant achieves a 59.8% average score across benchmarks, rivaling models requiring double the GPU memory. The main implication is that strategic design choices, rather than just scaling down large models, are crucial for creating practical and energy-efficient multimodal models at smaller scales for edge deployment. |
| Natural Language Processing | T1: Tool-integrated Self-verification for Test-time Compute Scaling in
  Small Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.04718) or [HuggingFace](https://huggingface.co/papers/2504.04718))| Jaewoong Cho, Jongwon Jeong, Nardien | This paper investigates improving the self-verification capabilities of small language models (sLMs) during test-time compute scaling by integrating external tools. The primary objective is to determine if sLMs can reliably self-verify their outputs, addressing limitations in tasks requiring memorization like numerical calculation. The proposed method, Tool-integrated Self-verification (T1), delegates memorization-heavy verification steps to tools like code interpreters, combined with knowledge distillation from larger verifiers. Experiments demonstrate that T1 significantly enhances sLM performance; for instance, on the MATH benchmark, a T1-enhanced Llama-3.2 1B model outperforms a standard Llama-3.1 8B model under test-time scaling. The key implication is that integrating tools is crucial for boosting sLM reasoning and verification abilities, potentially reducing reliance on larger models for complex tasks. |
| Multi-Modal | URECA: Unique Region Caption Anything (Read more on [arXiv](https://arxiv.org/abs/2504.05305) or [HuggingFace](https://huggingface.co/papers/2504.05305))| Heeji Yoon, seungryong, crepejung00, junwann, SammyLim | This paper introduces URECA, a novel dataset and model for generating unique captions for image regions across multiple granularities. The primary objective is to address the limitations of existing methods that struggle to produce distinctive descriptions, especially for visually similar or hierarchical regions. The methodology involves a four-stage automated data curation pipeline using Multimodal Large Language Models (MLLMs) and a mask tree structure to create the URECA dataset, and a novel captioning model architecture (URECA) featuring a high-resolution mask encoder and dynamic mask modeling. Experiments show the URECA model achieves state-of-the-art performance on the URECA dataset (e.g., 75.11 BERTScore) and generalizes well to existing benchmarks in zero-shot settings. For AI practitioners, this work offers a robust solution for fine-grained, context-aware region-level image understanding and description, enabling more nuanced visual grounding and captioning applications. |
| Natural Language Processing | Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning
  Models (Read more on [arXiv](https://arxiv.org/abs/2504.04823) or [HuggingFace](https://huggingface.co/papers/2504.04823))| Yuxuan Sun, Tiezheng, baihaoli, manyi2024, ruikangliu | This paper presents a systematic empirical study on the effects of various quantization techniques on the reasoning performance of large language models (LLMs). The main objective is to evaluate how weight-only, KV cache, and weight-activation quantization at different bit-widths impact the accuracy of reasoning LLMs across diverse complex tasks. The study evaluates multiple open-source reasoning models (1.5B-70B parameters) using state-of-the-art quantization methods on mathematical, scientific, and programming benchmarks. Key findings indicate that 8-bit weight-activation (W8A8) quantization is generally lossless (â‰¤1% accuracy drop), while 4-bit weight-only or KV cache quantization also achieves near-lossless results, although lower bit-widths pose significant risks. For AI practitioners, this implies that while moderate quantization offers efficiency benefits with minimal reasoning degradation, aggressive low-bit quantization requires careful consideration of model characteristics and task complexity. |
| Multi-Modal | Concept Lancet: Image Editing with Compositional Representation
  Transplant (Read more on [arXiv](https://arxiv.org/abs/2504.02828) or [HuggingFace](https://huggingface.co/papers/2504.02828))| Hancheng Min, Tianjiao Ding, CCB, ryanckh, peterljq | Concept Lancet (CoLan) introduces a zero-shot framework for principled concept manipulation in diffusion-based image editing. The primary objective is to overcome the challenge of determining appropriate edit strength by accurately estimating the presence of concepts in a source image. The methodology involves decomposing the source latent representation (text embedding or diffusion score) into a sparse linear combination of vectors from a curated concept dictionary (CoLan-150K) and then transplanting the target concept vector. Experiments show CoLan achieves state-of-the-art performance, significantly improving consistency preservation (e.g., reducing LPIPS by nearly 50% vs. VecAdd on P2P-Zero) and edit effectiveness. For AI practitioners, CoLan offers a plug-and-play method for more precise and consistent control over diffusion-based image editing without requiring model retraining or fine-tuning. |
| Multi-Modal | LiveVQA: Live Visual Knowledge Seeking (Read more on [arXiv](https://arxiv.org/abs/2504.05288) or [HuggingFace](https://huggingface.co/papers/2504.05288))| Yao Wan, Mingyang Fu, shuaishuaicdp, Tim666, Ayiirep | This paper introduces LIVEVQA, a benchmark dataset to evaluate Multimodal Large Language Models (MLLMs) on their ability to answer questions using live visual knowledge from news sources. The core objective is to assess current AI systems' capacity for understanding and reasoning about dynamic, real-world visual information integrated with textual news context, particularly through multi-hop questions. The methodology involves automatically collecting news instances (image, text, QA pairs) across 14 categories and evaluating 15 MLLMs using zero-shot prompting, with and without search engine integration. Key findings show that while models like Gemini-2.0-Flash achieve top performance (24.93% accuracy without search, 29.00% with MM-Search), significant gaps persist in handling complex, multi-hop visual questions requiring recent knowledge. For AI practitioners, this highlights the ongoing challenge and need for developing MLLMs with stronger, up-to-date visual reasoning capabilities beyond static benchmarks. |
| Natural Language Processing | Are You Getting What You Pay For? Auditing Model Substitution in LLM
  APIs (Read more on [arXiv](https://arxiv.org/abs/2504.04715) or [HuggingFace](https://huggingface.co/papers/2504.04715))| Tianneng Shi, Will Cai, dawnsong, Xuandong | This paper addresses the problem of auditing model substitution in black-box Large Language Model (LLM) APIs, where providers might covertly use cheaper or lower-quality models. The primary objective is to formalize the detection challenge and systematically evaluate the effectiveness and robustness of various verification techniques under adversarial scenarios. Methodologies assessed include text-output-based statistical tests (like MMD using Hamming Kernel), benchmark evaluations, identity prompting, text classification, and log probability analysis, tested against attacks such as quantization, randomized substitution, and benchmark evasion. Key findings reveal that text-output-based methods struggle significantly, with text classifiers achieving only ~50% accuracy differentiating quantized models (Table 2) and MMD power dropping below significance thresholds with even 20% randomized substitution (Figure 1); log probability analysis offers stronger guarantees but has limited API availability, while hardware TEEs show promise with low overhead (<3% throughput impact under load, Table 4). The main implication for practitioners is the current lack of robust, universally applicable software methods to verify LLM identity via APIs, suggesting a need for increased provider transparency or adoption of hardware-based attestation. |
| Machine Learning | Gaussian Mixture Flow Matching Models (Read more on [arXiv](https://arxiv.org/abs/2504.05304) or [HuggingFace](https://huggingface.co/papers/2504.05304))| saibi, wetzste1, luanfujun, zexiangxu, Lakonik | This paper introduces Gaussian Mixture Flow Matching (GMFlow), a novel generative model that generalizes diffusion and flow matching by representing flow velocity with a dynamic Gaussian mixture (GM) instead of a single Gaussian. The main objective is to address the limitations of prior models in few-step sampling (due to discretization error) and color over-saturation under classifier-free guidance (CFG). GMFlow achieves this by predicting GM parameters, training with a KL divergence loss, and utilizing derived GM-SDE/ODE solvers and a probabilistic reweighting guidance scheme. Extensive experiments show GMFlow outperforms flow matching baselines, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256x256. For AI practitioners, GMFlow offers a path towards more efficient and higher-quality image generation with reduced artifacts, especially in few-step scenarios. |
| Reinforcement Learning | DiaTool-DPO: Multi-Turn Direct Preference Optimization for
  Tool-Augmented Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.02882) or [HuggingFace](https://huggingface.co/papers/2504.02882))| Donghun Lee, dsindex, junrae, gaeunseo, hash2430 | This paper introduces DiaTool-DPO, leveraging Direct Preference Optimization (DPO) to enhance dialogue control for Tool-Augmented Large Language Models (TA-LLMs). The primary objective is to improve the model's handling of incomplete queries (slot-filling) and out-of-scope requests (rejection) beyond supervised fine-tuning. The key methodology models interactions as a Markov Decision Process, automatically generates paired preferred/rejected dialogue trajectories, and applies a specialized DPO loss with turn-length normalization and reward gap margin subtraction. Results show DiaTool-DPO significantly improves performance, achieving 91.7% slot-filling accuracy (94.8% of GPT-4o) and 91.3% rejection accuracy (91.3% of GPT-4o) over the baseline. The main implication is that practitioners can develop more robust conversational TA-LLMs using automatically generated preference data, reducing the need for expert demonstrations. |
| Computer Vision | Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language
  Models for Domain-Generalized Semantic Segmentation (Read more on [arXiv](https://arxiv.org/abs/2504.03193) or [HuggingFace](https://huggingface.co/papers/2504.03193))| robbytan, XinNUS | This paper introduces MFuser, a novel framework for domain-generalized semantic segmentation (DGSS) that bridges Vision Foundation Models (VFMs) and Vision-Language Models (VLMs). The research aims to effectively combine the fine-grained feature extraction capabilities of VFMs with the robust text alignment of VLMs, which is challenging due to increased sequence length. MFuser employs MVFuser, a Mamba-based co-adapter, for joint fine-tuning and MTEnhancer, a hybrid attention-Mamba module, to refine text embeddings using image priors. Experiments demonstrate that MFuser outperforms state-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real benchmarks. MFuser offers AI practitioners a parameter-efficient method for integrating VFMs and VLMs for improved generalization in semantic segmentation. |
| Computer Vision | BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose
  Estimation (Read more on [arXiv](https://arxiv.org/abs/2504.02812) or [HuggingFace](https://huggingface.co/papers/2504.02812))| taeyeop, anas-gouda, mfourmy, swtyree, nv-nguyen | The BOP Challenge 2024 benchmarks 6D object pose estimation, focusing on model-based and model-free approaches for transitioning to real-world scenarios. The primary objective is to evaluate 6D pose estimation in settings lacking 3D object models, necessitating object onboarding from reference videos. The challenge utilizes new BOP-H3 datasets with high-resolution sensors and AR/VR headsets, introducing model-free tasks and a more practical 6D object detection task. The best method for model-based 6D localization (FreeZeV2.1) achieves 82.1 AR on BOP-Classic-Core, a 22% increase over the previous best. This shows improved 6D object pose estimation performance and the bottleneck in object pipeline is 2D detection. |
| Reinforcement Learning | VAPO: Efficient and Reliable Reinforcement Learning for Advanced
  Reasoning Tasks (Read more on [arXiv](https://arxiv.org/abs/2504.05118) or [HuggingFace](https://huggingface.co/papers/2504.05118))| Ruofei Zhu, Xiaochen Zuo, Qiying Yu, Yufeng Yuan, YuYue | The paper introduces VAPO, a value-based reinforcement learning framework for reasoning models that aims to improve efficiency and reliability. It addresses challenges in long chain-of-thought reasoning by focusing on value model bias, heterogeneous sequence lengths, and reward sparsity. VAPO augments Proximal Policy Optimization (PPO) with techniques like Length-adaptive GAE and Value-Pretraining. Evaluated on the AIME 2024 dataset using a Qwen 32B model, VAPO achieves a state-of-the-art score of 60.4. This suggests improved performance and stability for value-based RL in complex reasoning tasks compared to value-free methods like DAPO. |
| Natural Language Processing | Clinical ModernBERT: An efficient and long context encoder for
  biomedical text (Read more on [arXiv](https://arxiv.org/abs/2504.03964) or [HuggingFace](https://huggingface.co/papers/2504.03964))| Jeffrey N. Chiang, Anthony Wu, Simonlee711 | Clinical ModernBERT is a transformer-based encoder pretrained for biomedical text, featuring architectural upgrades for efficiency and long context handling. This research aims to adapt ModernBERT specifically for biomedical and clinical domains, improving performance on long-context tasks. The methodology involves pretraining on a large corpus of biomedical literature, clinical notes, and medical ontologies, incorporating RoPE and Flash Attention. Clinical ModernBERT achieves state-of-the-art performance on long-context i2b2 concept extraction, reaching an F1 score of 0.804 on the i2b2 2012 dataset. The model provides AI practitioners with a scalable and high-fidelity encoder backbone tailored for clinical NLP and biomedical research applications. |
| Multi-Modal | JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language
  Model (Read more on [arXiv](https://arxiv.org/abs/2504.03770) or [HuggingFace](https://huggingface.co/papers/2504.03770))| Li Li, Yi Nian, yuehanqi, yuehanqi, Chouoftears | The paper introduces JailDAM, a novel framework for detecting jailbreak attacks in vision-language models without relying on harmful training data or hidden state access. The primary objective is to improve detection accuracy and speed while adapting to unseen jailbreak strategies. JailDAM utilizes a policy-driven memory bank and autoencoder-based reconstruction for early threat detection, with a test-time adaptation mechanism to refine the memory. Experiments on VLM jailbreak benchmarks demonstrate that JailDAM achieves state-of-the-art performance, improving AUROC by 0.10 compared to other approaches. JAILDAM presents a practical and efficient solution for safeguarding vision-language models in real-world applications, especially in black-box settings. |
| Natural Language Processing | GlotEval: A Test Suite for Massively Multilingual Evaluation of Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.04155) or [HuggingFace](https://huggingface.co/papers/2504.04155))| Ona de Gibert, Sawal Devkota, Joseph Attieh, Zihao Li, zuenmin | The paper introduces GlotEval, a lightweight framework for evaluating large language models (LLMs) across a massive number of languages. It aims to address the lack of comprehensive multilingual evaluation frameworks, particularly for low-resource languages. The framework integrates existing multilingual benchmarks, standardizes language codes using ISO 639-3, and allows for language-specific prompt templates, including non-English-centric machine translation evaluation. Results from a multilingual translation case study demonstrate the frameworkâ€™s applicability, enabling precise diagnosis of model strengths and weaknesses in diverse linguistic contexts. GlotEval facilitates large-scale, in-depth evaluations to foster more inclusive LLM evaluation. |
| Natural Language Processing | Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting
  LLMs Across Languages and Resources (Read more on [arXiv](https://arxiv.org/abs/2504.04152) or [HuggingFace](https://huggingface.co/papers/2504.04152))| JÃ¶rg Tiedemann, Hengyu Luo, Shaoxiong Ji, Zihao Li | This paper investigates multilingual continual pretraining (CPT) strategies to adapt LLMs across languages and resource levels. The study aims to understand the relative effectiveness of monolingual, bilingual, and code-augmented data mixing strategies for CPT. The methodology involves evaluating 36 CPT configurations with three multilingual models across 30+ languages, categorized by altruistic, selfish, and stagnant properties. Results show that bilingual CPT improves multilingual classification but causes language mixing, while code data enhances classification but slightly degrades generation quality; additionally language classifications from previous work are not generalizable across CPT configurations. These findings imply the need for more nuanced strategies in multilingual representation learning to balance classification and generation. |
