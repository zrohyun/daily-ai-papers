

## Papers for 2025-04-10

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | DDT: Decoupled Diffusion Transformer (Read more on [arXiv](https://arxiv.org/abs/2504.05741) or [HuggingFace](https://huggingface.co/papers/2504.05741))| Weilin Huang, Zhi Tian, lmwang, wangsssssss | This paper introduces the Decoupled Diffusion Transformer (DDT), a novel architecture designed to improve the quality and training efficiency of image generation diffusion models. The main research objective is to resolve the inherent optimization dilemma in standard diffusion transformers where encoding low-frequency semantics conflicts with decoding high-frequency details within the same module. DDT achieves this by employing a decoupled design with a dedicated condition encoder for semantic feature extraction and a separate velocity decoder for synthesizing high-frequency details. Key results show that DDT-XL/2 achieves a state-of-the-art FID of 1.31 on ImageNet 256x256 within 256 epochs (nearly 4x faster convergence than prior work) and an FID of 1.28 on ImageNet 512x512. The main implication for AI practitioners is that decoupling encoder-decoder roles in diffusion transformers can significantly accelerate training convergence and enhance generation quality, offering a more efficient path to state-of-the-art performance. |
| Multi-Modal | GenDoP: Auto-regressive Camera Trajectory Generation as a Director of
  Photography (Read more on [arXiv](https://arxiv.org/abs/2504.07083) or [HuggingFace](https://huggingface.co/papers/2504.07083))| lindahua, wetzste1, liuziwei7, jingtan, Dubhe-zmc | This paper introduces GenDoP, an auto-regressive model for generating artistic camera trajectories from multi-modal inputs, mimicking a Director of Photography. The primary objective is to create expressive, controllable camera movements aligned with directorial intent, overcoming limitations of prior geometric or structurally biased learning methods. GenDoP utilizes a novel dataset, DataDoP, and employs a decoder-only Transformer to auto-regressively generate tokenized camera pose sequences conditioned on text and optional first-frame RGBD data. Experiments show GenDoP significantly outperforms existing methods, achieving superior text-trajectory alignment (e.g., CLaTr-CLIP score of 36.179 for Motion captions) and trajectory quality (e.g., CLaTr-FID of 22.714), confirmed by user studies. For AI practitioners, GenDoP offers a controllable framework for integrating sophisticated camera cinematography into automated video generation and filmmaking. |
| Natural Language Processing | OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training
  Tokens (Read more on [arXiv](https://arxiv.org/abs/2504.07096) or [HuggingFace](https://huggingface.co/papers/2504.07096))| Yensung, sewon, yanaiela, taylorb, liujch1998 | OLMOTRACE is presented as the first system capable of tracing language model outputs back to their complete multi-trillion-token training data in real-time. The main objective is to enable users to understand LM behavior by identifying the specific training documents containing verbatim matches for generated text segments. Key methodology involves using an extended version of the `infini-gram` text search engine to index the entire training corpus and a novel parallel algorithm to rapidly find maximal matching spans, followed by filtering, document retrieval, and BM25-based relevance ranking. The system achieves an average inference latency of 4.46 seconds for tracing typical LM responses (~458 tokens) against 4.6 trillion tokens. The primary implication for AI practitioners is providing a tool to investigate model factuality, hallucination origins, and creativity by directly linking outputs to training data sources. |
| Multi-Modal | A Unified Agentic Framework for Evaluating Conditional Image Generation (Read more on [arXiv](https://arxiv.org/abs/2504.07046) or [HuggingFace](https://huggingface.co/papers/2504.07046))| Yiyu Wang, Longyue Wang, Xue Yang, Jifang Wang, imryanxu | This paper introduces CIGEVAL, a unified agentic framework leveraging Large Multimodal Models (LMMs) for the comprehensive evaluation of conditional image generation tasks. The main objective is to create a task-agnostic, reliable, and explainable evaluation metric that aligns well with human assessments, overcoming limitations of existing methods. CIGEVAL utilizes an LMM core, a multi-functional toolbox (e.g., Grounding, Difference, Scene Graph), and a fine-grained evaluation framework involving task decomposition and tool selection; it also employs synthesized evaluation trajectories to fine-tune smaller LMMs. The GPT-4o version achieves a Spearman correlation of 0.4625 with human judgments on the ImagenHub benchmark, closely matching inter-annotator correlation (0.47), while fine-tuned 7B models surpass prior state-of-the-art. For AI practitioners, CIGEVAL offers a promising automated method for evaluating conditional image generation models with human-like reliability across diverse tasks. |
| Natural Language Processing | Missing Premise exacerbates Overthinking: Are Reasoning Models losing
  Critical Thinking Skill? (Read more on [arXiv](https://arxiv.org/abs/2504.06514) or [HuggingFace](https://huggingface.co/papers/2504.06514))| Ming Li, zhoutianyi, sunlichao137, Fcr09 | This paper investigates "MiP-Overthinking," where reasoning LLMs generate overly long and ineffective responses to ill-posed questions with missing premises (MiP). Its objective is to analyze why these models struggle with MiP scenarios and lack critical thinking compared to non-reasoning models. The methodology involves evaluating various LLMs on curated MiP datasets (e.g., MiP-GSM8K, MiP-Formula) using metrics like response length and abstain rate, alongside qualitative analysis of reasoning steps. Key findings show reasoning models produce significantly longer responses (2× to 4× more tokens) for MiP questions than well-defined ones, yet exhibit low abstain rates (e.g., DeepSeek-R1: 16.5% on MiP-GSM8K), indicating ineffective thinking despite increased computation. The main implication is that current training paradigms for reasoning LLMs may not adequately reward critical thinking or efficient identification of unsolvable problems, suggesting a need for revised training strategies. |
| Multi-Modal | FantasyTalking: Realistic Talking Portrait Generation via Coherent
  Motion Synthesis (Read more on [arXiv](https://arxiv.org/abs/2504.04842) or [HuggingFace](https://huggingface.co/papers/2504.04842))| Yunpeng Zhang, Yaqi Fan, Mengchao Wang, fanjiang, wangqiang9 | FantasyTalking introduces a novel framework leveraging a pretrained video diffusion transformer to generate realistic and coherent talking portrait videos from a single static image and audio input. The primary objective is to address the limitations of existing methods in capturing subtle facial expressions, associated body movements, and dynamic backgrounds while maintaining identity and allowing motion control. Key methodologies include a dual-stage (clip-level and frame-level) audio-visual alignment strategy for coherent global dynamics and precise lip-sync, a facial-focused cross-attention module for identity preservation, and a motion intensity modulation network. Experimental results demonstrate state-of-the-art performance, achieving superior video quality (e.g., FVD of 483.11 on the Wild Talking dataset), temporal consistency, motion diversity, and identity preservation compared to previous methods. For AI practitioners, this work offers an advanced approach to generate highly realistic, controllable talking avatars suitable for complex scenes with dynamic movements beyond simple lip synchronization. |
| Machine Learning | A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths
  to Reproducibility (Read more on [arXiv](https://arxiv.org/abs/2504.07086) or [HuggingFace](https://huggingface.co/papers/2504.07086))| AmeyaPrabhu, albanie, vishaal27, hrdkbhatnagar, libeanim | This paper critically assesses the claimed progress in language model reasoning, highlighting significant issues with reproducibility and methodological rigor in current benchmarking practices. The primary objective is to investigate the sensitivity of mathematical reasoning benchmarks to various implementation choices and propose a standardized evaluation framework for more reliable assessment. The methodology involves a comprehensive empirical study re-evaluating recent 1.5B and 7B parameter models (both RL and SFT trained) on benchmarks like AIME'24, AMC'23, and MATH500 under controlled conditions, analyzing variance from seeds, decoding parameters, hardware, and evaluation frameworks. Key findings reveal that reported performance gains often diminish significantly upon re-evaluation (e.g., drops of up to 17.0% on AIME'24), with many RL improvements falling within the baseline model's variance, while SFT shows more robust generalization. The main implication is that practitioners should adopt rigorous, standardized evaluation protocols, including multi-seed runs and transparent reporting, to avoid misleading conclusions about model capabilities and ensure true progress in LM reasoning. |
| Multi-Modal | OmniCaptioner: One Captioner to Rule Them All (Read more on [arXiv](https://arxiv.org/abs/2504.07089) or [HuggingFace](https://huggingface.co/papers/2504.07089))| Cxxs, Wayne-lc, Dakerqi, JiakangYuan, yeeeeeyy | The paper introduces OMNICAPTIONER, a unified visual captioning framework designed to generate fine-grained textual descriptions across diverse visual domains, bridging visual and textual modalities. The main objective is to create a single captioner capable of handling natural images, visual text (posters, UIs), and structured visuals (tables, charts, math) unlike domain-specific models. The methodology involves a large-scale (21M) dataset construction pipeline with seed caption generation (using GPT-4O) followed by caption extension (using Qwen2.5-32B) and pretraining a model initialized from Qwen2-VL-Instruct. Key results demonstrate enhanced visual reasoning when captions are inserted into LLMs (e.g., achieving 40.5 on MathVerse with DS-R1-Distill-Qwen-7B), improved text-to-image generation (boosting SANA-1.0 GenEval score by +2.97), and efficient supervised fine-tuning (SFT). The main implication for AI practitioners is that this unified captioning approach provides a scalable solution for enhancing multimodal alignment, reasoning, and generation across a wide variety of visual inputs. |
| Computer Vision | Are We Done with Object-Centric Learning? (Read more on [arXiv](https://arxiv.org/abs/2504.07092) or [HuggingFace](https://huggingface.co/papers/2504.07092))| Matthias Bethge, coallaoh, AmeyaPrabhu, arubique | This paper challenges the current focus of Object-Centric Learning (OCL) research on unsupervised object discovery, arguing that segmentation models have largely solved this aspect. It investigates whether the ability to separate objects contributes to broader OCL goals like Out-of-Distribution (OOD) generalization, particularly against spurious background correlations. The key methodology involves proposing OCCAM (Object-Centric Classification with Applied Masks), a training-free probe that uses external segmentation models (like HQES) to generate object masks, encode objects independently, and perform robust classification by selecting the foreground object representation. Results show segmentation models significantly outperform slot-based OCL methods on object discovery benchmarks (e.g., HQES achieves 87.2 FG-ARI on Movi-E vs. 71.1 for FT-Dinosaur), and OCCAM substantially improves robust classification accuracy (e.g., 78.5% WGA on ImageNet-D using HQES masks with SigLip). The main implication is that the OCL community should leverage existing segmentation tools and shift focus towards downstream applications, benchmark development, and fundamental cognitive questions rather than solely refining unsupervised slot-based discovery. |
| Natural Language Processing | Self-Steering Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.07081) or [HuggingFace](https://huggingface.co/papers/2504.07081))| Jacob Andreas, Vikash K. Mansinghka, Joshua B. Tenenbaum, Gabriel Grand, alexanderlew | This paper introduces DISCIPL, a framework where language models (LMs) generate task-specific inference programs to steer their own reasoning process. The primary objective is to enable more efficient, verifiable, and structured test-time computation for complex tasks, overcoming the limitations of direct autoregressive generation or fixed external search algorithms. The core methodology involves a 'Planner' LM writing a probabilistic program that orchestrates a population of 'Follower' LMs, guided by an inference engine using techniques like Sequential Monte Carlo (SMC). Experiments show that DISCIPL, using a small Llama-3.2-1B Follower, achieves strong performance on constrained generation tasks (e.g., 0.81 Pass@1 on COLLIE sentence tasks with SMC, compared to 0.04 for the Follower alone), matching or exceeding larger models like GPT-4o and approaching o1 performance on some tasks. For AI practitioners, this implies that complex reasoning can be effectively decomposed and parallelized using LMs themselves to define the computation structure, enabling smaller models to tackle challenging problems without task-specific fine-tuning. |
| Natural Language Processing | RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts (Read more on [arXiv](https://arxiv.org/abs/2504.06947) or [HuggingFace](https://huggingface.co/papers/2504.06947))| Anna Lapanitsyna, Natalia Tkachenko, Natalia Loukachevitch, nicolay-r, RefalMachine | This paper introduces the RuOpinionNE-2024 shared task, focusing on the extraction of structured opinion tuples (holder, target, expression, polarity) from Russian news texts. The primary objective was to develop and evaluate systems capable of identifying these complex opinion structures within individual sentences. Participants predominantly employed large language models (LLMs) utilizing zero-shot, few-shot, and fine-tuning approaches, evaluated using an F1 metric based on tuple component overlap and polarity matching. The top-performing system achieved an F1 score of 0.41 on the test set through fine-tuning a 70B parameter LLM (LLaMA-3.3-70B), significantly outperforming few-shot baseline models. This work demonstrates the effectiveness of fine-tuned LLMs for structured sentiment analysis in news, while also highlighting the task's difficulty compared to simpler sentiment analysis or tasks on review data. |
| Computer Vision | Masked Scene Modeling: Narrowing the Gap Between Supervised and
  Self-Supervised Learning in 3D Scene Understanding (Read more on [arXiv](https://arxiv.org/abs/2504.06719) or [HuggingFace](https://huggingface.co/papers/2504.06719))| Leon Sick, Christian Stippel, phermosilla | This paper introduces Masked Scene Modeling (MSM), a novel self-supervised learning framework for 3D scene understanding that significantly closes the performance gap compared to supervised methods using off-the-shelf features. The main objective is to develop a 3D-native self-supervised model whose learned representations are effective for downstream tasks without requiring fine-tuning, unlike previous methods primarily used for weight initialization. The core methodology involves a hierarchical UNet-like architecture trained to reconstruct deep features of masked scene patches in a bottom-up manner, using a teacher model and a cross-view reconstruction loss. Experiments demonstrate state-of-the-art results; for instance, on ScanNet semantic segmentation with linear probing, MSM achieves 68.7 mIoU, substantially outperforming prior self-supervised methods like MSC (58.2 mIoU) and approaching supervised performance. This work provides AI practitioners with a strong 3D self-supervised model yielding versatile features applicable directly to various scene understanding tasks, alongside a robust hierarchical evaluation protocol. |
| Multi-Modal | DiTaiListener: Controllable High Fidelity Listener Video Generation with
  Diffusion (Read more on [arXiv](https://arxiv.org/abs/2504.04010) or [HuggingFace](https://huggingface.co/papers/2504.04010))| chaubeyG, hongkung, minhtran, Boese0601, havent-invented | This paper introduces DiTaiListener, a diffusion-based framework for generating controllable, high-fidelity listener head portrait videos from multimodal speaker inputs. The main objective is to overcome the limitations of existing methods, which often rely on low-dimensional motion codes, by directly synthesizing listener videos in pixel space for enhanced realism and expressiveness, particularly for long interactions. The core methodology involves DiTaiListener-Gen, a Diffusion Transformer (DiT) with a novel Causal Temporal Multimodal Adapter (CTM-Adapter) for generating short video segments conditioned on speaker audio, facial motion, and optional text prompts, and DiTaiListener-Edit, a video-to-video model for seamlessly stitching these segments. DiTaiListener achieves state-of-the-art performance, demonstrating significant improvements in photorealism (+73.8% in FID on RealTalk) and motion representation (+6.1% in FD metric on VICO). For AI practitioners, this work presents an end-to-end video diffusion approach for generating nuanced, temporally coherent, and controllable non-verbal listener behaviors in dyadic interactions. |
| Multi-Modal | VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement
  Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2504.06958) or [HuggingFace](https://huggingface.co/papers/2504.06958))| Lanxingxuan, donglu, desenmeng, Aurorana, xinhaoli | This paper introduces VideoChat-R1, a video multimodal large language model (MLLM) with enhanced spatio-temporal perception achieved through reinforcement fine-tuning (RFT). The main objective is to systematically explore the application of RFT with Group Relative Policy Optimization (GRPO) to improve video MLLMs' spatio-temporal understanding without sacrificing general capabilities. The methodology involves multi-task RFT using GRPO and rule-based rewards on limited spatio-temporal perception task samples (e.g., temporal grounding, object tracking). Key results demonstrate substantial improvements over the Qwen2.5-VL-7B baseline, notably +31.8 in temporal grounding and +31.2 in object tracking, as well as gains in general QA benchmarks like VideoMME (+0.9). The primary implication for practitioners is that RFT is a highly data-efficient approach for enhancing specific task performance in video MLLMs, offering significant specialized improvements with minimal data and limited impact on general abilities. |
| Computer Vision | WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments (Read more on [arXiv](https://arxiv.org/abs/2504.03886) or [HuggingFace](https://huggingface.co/papers/2504.03886))| Songyou Peng, Marc Pollefeys, Valentin Bieri, Zihan Zhu, Jianhao Zheng | WildGS-SLAM introduces a robust monocular RGB SLAM system using 3D Gaussian Splatting designed specifically for dynamic environments. The main objective is to accurately track camera trajectory and reconstruct the static background while effectively removing dynamic distractors without relying on explicit semantic segmentation or predefined object classes. Key methodology involves leveraging uncertainty maps predicted by a shallow MLP using DINOv2 features to guide both tracking (uncertainty-weighted Dense Bundle Adjustment) and mapping (uncertainty-aware rendering loss), integrated with metric depth estimation. Results demonstrate state-of-the-art performance, achieving a significantly lower average tracking error (ATE RMSE of 0.46 cm on the Wild-SLAM MoCap dataset) compared to previous methods in dynamic scenes, alongside high-fidelity, artifact-free view synthesis. This offers AI practitioners a purely geometric, uncertainty-driven approach for robust SLAM in uncontrolled real-world scenarios like robotics and AR. |
| Reinforcement Learning | RobustDexGrasp: Robust Dexterous Grasping of General Objects from
  Single-view Perception (Read more on [arXiv](https://arxiv.org/abs/2504.05287) or [HuggingFace](https://huggingface.co/papers/2504.05287))| Jie Song, Sammy Christen, Linyi Huang, Zijian Wu, ethHuiZhang | This paper presents a reinforcement learning framework enabling robust, zero-shot dexterous grasping of diverse unseen objects using only single-view perception and adapting to dynamic disturbances. The objective is to train a policy for dynamic grasping of varied novel objects from a single static camera view, maintaining robustness against unexpected object movements and external forces. The methodology employs a mixed curriculum learning strategy, initially training a teacher policy with RL using privileged visual-tactile information in simulation, then transferring knowledge to a student policy using imitation learning followed by RL with only initial single-view point clouds and noisy proprioception, incorporating domain randomization for sim-to-real transfer. The system achieves high success rates, notably 97.0% across 247,786 simulated objects and 94.6% across 512 real objects, outperforming prior methods especially under disturbances. This implies that robust, adaptive dexterous manipulation can be achieved with limited perception by leveraging simulation with privileged information and structured policy transfer techniques. |
