

## Papers for 2025-04-14

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model (Read more on [arXiv](https://arxiv.org/abs/2504.08685) or [HuggingFace](https://huggingface.co/papers/2504.08685))| Zhijie Lin, Ceyuan Yang, Team Seawead, zhenheny, lingff | This technical report introduces Seaweed-7B, a cost-effectively trained 7-billion parameter foundation model for video generation. The main objective was to demonstrate competitive video generation performance using moderate computational resources (665,000 H100 GPU hours) compared to larger models. The methodology involves a temporally causal Variational Autoencoder (VAE), a hybrid-stream Diffusion Transformer (DiT), multi-stage/multi-task pre-training, supervised fine-tuning (SFT), and reinforcement learning (DPO), alongside significant data curation and training optimizations. Seaweed-7B achieves strong results, ranking second in image-to-video generation benchmarks (Elo 1,047, 58% win rate on MagicArena) and demonstrating state-of-the-art VAE reconstruction (LPIPS 0.0391 on MCL-JCV). The key implication is that optimized medium-sized models can offer a viable, resource-efficient pathway to high-quality video generation, challenging the necessity for extremely large-scale models. |
| Computer Vision | GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for
  Autoregressive Image Generation (Read more on [arXiv](https://arxiv.org/abs/2504.08736) or [HuggingFace](https://huggingface.co/papers/2504.08736))| Jiashi Feng, Zilong Huang, Jun Hao Liew, XihuiLiu, YuuTennYi | GigaTok presents a novel approach to scaling visual tokenizers up to 3 billion parameters for autoregressive image generation, addressing the common trade-off between reconstruction fidelity and generation quality. The primary objective is to overcome the dilemma where larger tokenizers improve reconstruction but degrade downstream generation, by investigating the root cause identified as increasing latent space complexity. Key methodologies include proposing semantic regularization, which aligns tokenizer features with a pre-trained visual encoder (DINOv2), exploring 1D tokenizers for scalability, prioritizing decoder scaling, and using entropy loss for stability. GigaTok achieves state-of-the-art results, with the 2.9B parameter tokenizer enabling a 1.4B AR model to reach a gFID of 1.98* on ImageNet 256x256, demonstrating simultaneous improvement in reconstruction, generation, and representation quality. For AI practitioners, this work provides effective strategies and insights for scaling visual tokenizers in autoregressive frameworks, mitigating a significant bottleneck in visual generation quality. |
| Computer Vision | MineWorld: a Real-Time and Open-Source Interactive World Model on
  Minecraft (Read more on [arXiv](https://arxiv.org/abs/2504.08388) or [HuggingFace](https://huggingface.co/papers/2504.08388))| Yushu Jiang, Haoyu Wu, Tianyu He, Yang Ye, Junliang Guo | MineWorld is a real-time interactive world model for Minecraft. It aims to efficiently generate new game scenes conditioned on previous frames and actions. The model uses a visual-action autoregressive Transformer with a novel parallel decoding algorithm. MineWorld achieves up to 7 frames per second generation speed and outperforms SoTA open-sourced diffusion-based world models; for example, it achieves a Fréchet Video Distance (FVD) of 227 compared to Oasis's 377. The work provides a framework for real-time simulation and interaction, and new evaluation metrics for assessing controllability. |
| Computer Vision | PixelFlow: Pixel-Space Generative Models with Flow (Read more on [arXiv](https://arxiv.org/abs/2504.07963) or [HuggingFace](https://huggingface.co/papers/2504.07963))| Ping Luo, Peize Sun, Shilong Zhang, Chongjian Ge, Shoufa Chen | PixelFlow introduces a family of image generation models operating directly in pixel space using cascaded flow matching, bypassing latent space representations. The objective is to develop an efficient and end-to-end trainable generative model for direct pixel-space image synthesis without requiring pre-trained VAEs. The key methodology employs a multi-stage, resolution-increasing flow matching process driven by a unified Transformer architecture, progressively denoising from low to high resolution. PixelFlow achieves competitive results, including an FID of 1.98 on the 256x256 ImageNet class-conditional benchmark and strong performance on text-to-image tasks (e.g., 0.64 on GenEval). This approach offers AI practitioners a simpler, end-to-end trainable alternative to latent diffusion models for high-quality image generation. |
| Reinforcement Learning | SQL-R1: Training Natural Language to SQL Reasoning Model By
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2504.08600) or [HuggingFace](https://huggingface.co/papers/2504.08600))| Ran Chen, Xuhui Jiang, Chengjin Xu, Peixian Ma, ZhuangXialie | This paper introduces SQL-R1, a Natural Language to SQL (NL2SQL) reasoning model trained using reinforcement learning (RL) to improve performance on complex queries. The main objective is to enhance NL2SQL reasoning, particularly for multi-table joins and nested queries where supervised fine-tuning (SFT) methods may falter, by leveraging RL's dynamic adaptation capabilities. The key methodology involves training a base model using the Group Relative Policy Optimization (GRPO) RL algorithm with a specialized reward function designed for NL2SQL tasks, exploring SFT cold-start strategies, and utilizing synthetic data. SQL-R1 achieves state-of-the-art execution accuracy of 88.6% on the Spider-Test benchmark and 66.6% on BIRD-Dev using only a 7B parameter base model. The main implication for AI practitioners is that RL offers an effective approach to develop high-performing and potentially more interpretable NL2SQL models, capable of complex reasoning even with comparatively smaller model sizes. |
| Multi-Modal | FlexIP: Dynamic Control of Preservation and Personality for Customized
  Image Generation (Read more on [arXiv](https://arxiv.org/abs/2504.07405) or [HuggingFace](https://huggingface.co/papers/2504.07405))| Kaiwen Xiao, Yanning Zhou, Haonan Lin, DevLinyan | FlexIP is a novel framework for customized image generation designed to dynamically balance identity preservation and personalized editing. It addresses the challenge of explicitly decoupling these often conflicting objectives, which inherent trade-offs limit existing methods. The core methodology employs a dual-adapter architecture—a Preservation Adapter for identity features and a Personalization Adapter for textual/stylistic edits—integrated into a diffusion model and controlled by a dynamic weight gating mechanism. FlexIP demonstrates superior performance over prior work, achieving strong identity preservation (CLIP-I: 0.873) and personalization fidelity (CLIP-T: 0.284), surpassing methods like IP-Adapter. For practitioners, this provides a highly controllable system allowing smooth interpolation between strict subject preservation and diverse personalization, overcoming the rigidity of previous approaches. |
| Computer Vision | In-2-4D: Inbetweening from Two Single-View Images to 4D Generation (Read more on [arXiv](https://arxiv.org/abs/2504.08366) or [HuggingFace](https://huggingface.co/papers/2504.08366))| Ali Mahdavi-Amiri, Hao Zhang, Daniel Cohen-Or, Sauradip Nag | This paper introduces In-2-4D, a novel method for generating 4D content (3D object geometry plus motion) by interpolating between just two single-view images depicting the start and end states of an object's motion. The objective is to create smooth and plausible 4D animations from sparse inputs, handling diverse and complex motions without object or motion category assumptions. The core methodology involves a hierarchical approach using video interpolation to identify keyframes and generate simpler motion fragments, lifting each fragment to 4D via dynamic Gaussian Splatting guided by multi-view diffusion priors, and finally merging these fragments smoothly using deformation field interpolation and cascaded trajectory optimization. Experiments on the proposed I4D-15 benchmark show the method outperforms baselines, achieving for instance an LPIPS score of 0.103 compared to 0.136 for the best baseline. For AI practitioners, this work demonstrates a viable approach for generating complex 4D animations from minimal image inputs by breaking down the problem hierarchically, offering a path towards more accessible 4D content creation. |
| Natural Language Processing | ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on
  Transformer Encoder Models Performance (Read more on [arXiv](https://arxiv.org/abs/2504.08716) or [HuggingFace](https://huggingface.co/papers/2504.08716))| Djamé Seddah, Benoît Sagot, Wissam Antoun | This paper investigates the impact of architectural improvements in transformer encoders, specifically ModernBERT, compared to DeBERTaV3, while controlling for training data differences. The research aims to disentangle performance gains due to architectural innovations versus data effects by pretraining ModernBERT on CamemBERTaV2's dataset. The methodology involves a controlled comparison of ModernBERT, DeBERTaV3, and RoBERTa variants, evaluating their performance on French NLP benchmarks. Results show that DeBERTaV3 outperforms ModernBERT in overall benchmark performance (e.g., XNLI accuracy of 84.82 for CamemBERTaV2 vs. 83.28 for ModernBERT-CV2) and sample efficiency, though ModernBERT offers faster training and inference. The findings highlight the importance of distinguishing pretraining data influence from architectural advancements when assessing transformer models, suggesting that ModernBERT offers practical efficiency gains but at the cost of absolute performance compared to DeBERTaV3. |
| Machine Learning | SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder
  Guardrails for Precision Unlearning in LLMs (Read more on [arXiv](https://arxiv.org/abs/2504.08192) or [HuggingFace](https://huggingface.co/papers/2504.08192))| Virginia Smith, Mona Diab, Jacopo Bonato, Aashiq Muhamed | This paper introduces Dynamic Sparse Autoencoder Guardrails (DSG) to improve machine unlearning in LLMs. The research aims to enhance precision unlearning by leveraging sparse autoencoders (SAEs) dynamically. DSG integrates Fisher Information-based feature selection and a dynamic classifier to selectively clamp features causally linked to the forget data. Experiments show DSG outperforms leading unlearning methods on WMDP-Bio, reducing accuracy to 29.64% while maintaining high MMLU performance (99.34%), suggesting superior forget-utility trade-offs. The implication is that dynamic SAE-based interventions offer a more efficient, stable, and interpretable approach to machine unlearning compared to gradient-based methods. |
| Natural Language Processing | Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend
  NPUs (Read more on [arXiv](https://arxiv.org/abs/2504.07866) or [HuggingFace](https://huggingface.co/papers/2504.07866))| Xueyu Wu, Yehui Tang, Kaikai Song, Wenyong Huang, Yichun Yin | Pangu Ultra is a 135 billion parameter dense LLM trained on Ascend NPUs. This research aims to push the limits of dense LLMs by addressing training instability and system efficiency challenges at scale. The paper introduces depth-scaled sandwich normalization and tiny initialization to stabilize training, alongside system optimizations like NPU Fusion Attention. Evaluations show Pangu Ultra achieves state-of-the-art performance on general English and all Chinese benchmarks, and achieves over 50% MFU on 8,192 Ascend NPUs. This demonstrates the feasibility of training large dense models on Ascend NPUs, offering an alternative to MoE models. |
| Natural Language Processing | CoRAG: Collaborative Retrieval-Augmented Generation (Read more on [arXiv](https://arxiv.org/abs/2504.01883) or [HuggingFace](https://huggingface.co/papers/2504.01883))| Virginia Smith, Mona Diab, Aashiq Muhamed | The paper introduces CoRAG, a framework for collaborative retrieval-augmented generation where multiple clients jointly train a shared model using a collaborative passage store. It aims to improve knowledge-intensive tasks in low-resource settings by leveraging a collective knowledge base. The methodology involves clients collaboratively finetuning a pretrained retriever and reader on local data while contributing to a shared passage store. Experiments on the CRAB benchmark demonstrate a 10.5% improvement over local RAG training at 64-shot learning. This suggests that CoRAG enables more effective model training in data-scarce environments by sharing relatively inexpensive market research documents. |
| Natural Language Processing | Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning
  vs. Memorization in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.05262) or [HuggingFace](https://huggingface.co/papers/2504.05262))| Zhenzhong Lan, Renjun Xu, Yu Lu, Yang Yan | This paper investigates whether Large Language Models (LLMs) truly understand elementary addition or merely memorize patterns. The study probes commutativity and compositional generalization using elementary two-integer addition (0 to 264) with numerical and isomorphic symbolic mappings.  Results show that LLMs, achieving 73.8-99.8% accuracy on numerical addition, collapse to ≤7.5% under symbolic mapping, revealing a reliance on pattern matching over rule learning. These findings highlight architectural limitations and the need for approaches to achieve true mathematical reasoning, suggesting current LLMs fail to internalize abstract arithmetic principles. The results suggest a need for novel benchmark strategies to evaluate arithmetic reasoning. |
| Computer Vision | InteractVLM: 3D Interaction Reasoning from 2D Foundational Models (Read more on [arXiv](https://arxiv.org/abs/2504.05303) or [HuggingFace](https://huggingface.co/papers/2504.05303))| Cordelia Schmid, Omid Taheri, Shashank Tripathi, Dimitrije Antić, saidwivedi | InteractVLM facilitates 3D human-object interaction reconstruction from single 2D images by estimating contact points on humans and objects. The research aims to address limitations in existing methods by leveraging large Vision-Language Models (VLMs) and a novel Render-Localize-Lift module to infer 3D contacts. The method fine-tunes a VLM and employs a multi-view localization model (MV-Loc) to estimate contacts in 3D space. Results show InteractVLM outperforms existing methods on contact estimation, achieving a 20.6% improvement in F1 score on the DAMON dataset for binary human contact prediction. InteractVLM offers AI practitioners a scalable approach for HOI understanding by reducing reliance on expensive 3D contact annotations. |
