

## Papers for 2025-04-02

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Any2Caption:Interpreting Any Condition to Caption for Controllable Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2503.24379) or [HuggingFace](https://huggingface.co/papers/2503.24379))| shuicheng, dizhang, Xintao, WeicaiYe, ChocoWu | Any2Caption presents a novel framework to interpret diverse input conditions (text, images, video, pose, camera motion) into structured captions for enhancing controllable video generation. The core objective is to decouple the complex task of user intent interpretation from the video synthesis process by leveraging Multi-Modal Large Language Models (MLLMs). The methodology involves using an MLLM, trained on a new dataset Any2CapIns (337K instances), to convert various conditions into dense, structured captions (including dense, object, background, camera, style, and action components) which then guide off-the-shelf video generators. Results demonstrate high-quality caption generation (e.g., 48.63 ROUGE-L score) and significant improvements in controllability and video quality metrics when integrating these structured captions with existing video generation models. For AI practitioners, this provides a modular approach to improve guidance and control for video generation systems without needing to retrain the generators themselves. |
| Multi-Modal | Exploring the Effect of Reinforcement Learning on Video Understanding:
  Insights from SEED-Bench-R1 (Read more on [arXiv](https://arxiv.org/abs/2503.24376) or [HuggingFace](https://huggingface.co/papers/2503.24376))| yshan2u, yxgeee, ruiwang, tttoaster, ChenYi99 | This paper introduces SEED-Bench-R1, a benchmark designed to systematically evaluate the effect of post-training methods, particularly reinforcement learning (RL), on multimodal large language models (MLLMs) for video understanding tasks requiring perception and reasoning. The main objective is to assess how RL compares to supervised fine-tuning (SFT) in enhancing MLLM capabilities and generalization across in-distribution and out-of-distribution (OOD) scenarios. Using Qwen2-VL-Instruct-7B and the GRPO algorithm for RL with outcome-based rewards, the study evaluates performance on SEED-Bench-R1's three-level hierarchy and general benchmarks like LongVideoBench. Results show RL (GRPO) significantly outperforms SFT in data efficiency and generalization, achieving 44.89% accuracy on the OOD Level-3 tasks compared to SFT's 38.15%, and also demonstrating stronger performance on LongVideoBench (43.40% vs 40.00%). The key implication is that RL enhances MLLMs' visual perception and generalization for complex video tasks more effectively than SFT, even with simple rewards, though challenges remain in ensuring the logical coherence of the generated reasoning. |
| Machine Learning | CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive
  Program Synthesis (Read more on [arXiv](https://arxiv.org/abs/2503.23145) or [HuggingFace](https://huggingface.co/papers/2503.23145))| Naveen Kannan, Jiannan Cao, kaiyan289, tarsur909, anjiangwei | This paper introduces CodeARC, an interactive benchmark framework for evaluating Large Language Model (LLM) agents' reasoning capabilities in inductive program synthesis. The primary objective is to overcome limitations of static evaluations by enabling agents to interactively query hidden target functions and use differential testing feedback for iterative refinement. The key methodology involves agents starting with initial examples, dynamically generating new inputs for the hidden function, synthesizing candidate programs, and using a differential testing oracle to receive counterexamples for self-correction within budget constraints. Results highlight the task's difficulty, with the top-performing model, 03-mini, achieving a 52.7% success rate on the anonymized dataset, and fine-tuning showing up to a 31% relative improvement for LLaMA-3.1-8B-Instruct. The main implication for practitioners is that CodeARC provides a more realistic assessment of LLM inductive reasoning, revealing current model limitations and the insufficiency of static evaluation protocols for complex program synthesis tasks. |
| Reinforcement Learning | JudgeLRM: Large Reasoning Models as a Judge (Read more on [arXiv](https://arxiv.org/abs/2504.00050) or [HuggingFace](https://huggingface.co/papers/2504.00050))| Jiaying Wu, Nuo Chen, bhooi, qingyunzou, zhiyuanhucs | This paper introduces JudgeLRM, a family of large language models trained using reinforcement learning (RL) to serve as improved evaluative judges, particularly for tasks requiring complex reasoning. The primary objective is to investigate if enhancing reasoning capabilities, beyond standard Supervised Fine-Tuning (SFT), benefits LLM judge performance, addressing observed SFT limitations on reasoning-heavy tasks. The key methodology involves training models using Group Relative Policy Optimization (GRPO) with a novel judge-wise, outcome-driven reward function that combines structural and content accuracy metrics. Results show JudgeLRM models consistently outperform SFT counterparts and state-of-the-art reasoning models; notably, JudgeLRM-7B achieves an 8.14% average F1 score improvement over same-size SFT models and surpasses DeepSeek-R1 by 2.79% in F1 score on the JudgeLM benchmark. The main implication is that RL with specialized reward functions can effectively enhance LLM reasoning for judgment tasks, suggesting evaluation should be treated as a structured reasoning process rather than simple scoring. |
| Computer Vision | GeometryCrafter: Consistent Geometry Estimation for Open-world Videos
  with Diffusion Priors (Read more on [arXiv](https://arxiv.org/abs/2504.01016) or [HuggingFace](https://huggingface.co/papers/2504.01016))| Xiaoyu Li, yshan2u, wbhu-tc, xiangjun0211, slothfulxtx | GeometryCrafter presents a novel framework for estimating temporally consistent, high-fidelity point map sequences from open-world videos leveraging diffusion priors. The main objective is to overcome the geometric fidelity limitations of existing methods, which often rely on affine-invariant predictions unsuitable for metric 3D tasks. Its key methodology employs a specialized point map Variational Autoencoder (VAE) with a dual-encoder architecture to effectively encode unbounded point map values and a video diffusion model conditioned on video latents and per-frame geometry priors to generate coherent sequences. Quantitative results demonstrate state-of-the-art performance, achieving top average ranks (e.g., 1.9 for the diffusion variant) across seven diverse datasets for point map estimation against prior works. The main implication for AI practitioners is enabling accurate downstream applications like 3D/4D reconstruction and depth-based video editing/generation directly from open-world videos. |
| Multi-Modal | Agent S2: A Compositional Generalist-Specialist Framework for Computer
  Use Agents (Read more on [arXiv](https://arxiv.org/abs/2504.00906) or [HuggingFace](https://huggingface.co/papers/2504.00906))| Vincent Tu, Kyle Wong, xw-eric, jc-y42, saa1605 | Agent S2 introduces a compositional generalist-specialist framework to improve AI agents performing tasks via graphical user interfaces (GUIs). The main objective is to overcome limitations in precise GUI element grounding, long-horizon planning, and performance bottlenecks of single generalist models. Key methodologies include a Mixture of Grounding (MoG) technique using specialized grounding experts and Proactive Hierarchical Planning for dynamic plan refinement based on evolving observations. Agent S2 achieves new state-of-the-art results, including a 34.5% success rate on the OSWorld 50-step benchmark (a 32.7% relative improvement over baselines). The primary implication is that strategically composing generalist reasoning modules with specialist execution/grounding modules can lead to superior performance on complex computer interaction tasks compared to monolithic models. |
| Machine Learning | Z1: Efficient Test-time Scaling with Code (Read more on [arXiv](https://arxiv.org/abs/2504.00810) or [HuggingFace](https://huggingface.co/papers/2504.00810))| Xiao-Ping Zhang, armanc, yilunzhao, yh1567, zjy2001 | This paper introduces Z1, an efficient test-time scaling method for Large Language Models (LLMs) using code-related reasoning trajectories. The primary objective is to reduce the excessive thinking token consumption often required for complex problem-solving while preserving reasoning performance. Key methodologies include creating the Z1-Code-Reasoning-107K dataset with diverse trajectory lengths and implementing a novel 'Shifted Thinking Window' that removes context delimiters and caps thinking tokens dynamically based on problem complexity. The resulting model, Z1-7B, matches the performance of R1-Distill-Qwen-7B using only about 30% of its average thinking tokens and generalizes to non-code tasks (47.5% on GPQA Diamond). This work provides AI practitioners with a technique to elicit strong reasoning from LLMs more efficiently, reducing computational costs during inference. |
| Computer Vision | MixerMDM: Learnable Composition of Human Motion Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2504.01019) or [HuggingFace](https://huggingface.co/papers/2504.01019))| José García-Rodríguez, Sergio Escalera, Cristina Palmero, Germs96, pabloruizponce | MixerMDM introduces a learnable technique for composing pre-trained text-conditioned human motion diffusion models to generate controllable human interactions. The primary objective is to dynamically combine specialized models (e.g., single-person and interaction models) to leverage their respective strengths without manual weight setting or retraining. The methodology involves an adversarially trained 'Mixer' module that predicts dynamic mixing weights at each denoising step based on the intermediate outputs and conditions of the base models. Key results demonstrate superior performance over prior static or scheduled mixing methods; a user study showed MixerMDM achieved significantly better alignment rankings (e.g., 85.14% first-place ranks for interaction alignment). For AI practitioners, MixerMDM provides a modular and effective approach to enhance generative capabilities by composing existing diffusion models, enabling finer control over complex outputs like nuanced human interactions. |
| Multi-Modal | Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal
  LLMs on Academic Resources (Read more on [arXiv](https://arxiv.org/abs/2504.00595) or [HuggingFace](https://huggingface.co/papers/2504.00595))| Heng Wang, Yu Tian, windwest, yanglj55, weizhiwang | This paper introduces Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model (MLLM) pre-trained compute-efficiently on academic resources. The main objective was to reproduce state-of-the-art MLLM pre-training efficiently by improving data filtering, utilizing sequence packing, and employing dynamic image resolution. Key methodologies include using MLLM-based filtering (MLM-Filter) alongside CLIP-based methods on ~29M image-text pairs, low-to-high image tokenization (144 pre-train, 729 SFT), multimodal sequence packing, and training on 8xA100-40G GPUs using an FSDP codebase. Despite using only 5B packed tokens (0.36% of Qwen2-VL's pre-training data) and 442 A100-40G GPU hours, the final instruction-tuned Open-Qwen2VL outperformed Qwen2-VL-2B on several benchmarks, achieving 80.9 on MMBench. The main implication is that high-quality data curation and efficient training strategies enable competitive MLLM development even with limited compute, promoting open research in the academic community. |
| Natural Language Processing | Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for
  Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2503.24377) or [HuggingFace](https://huggingface.co/papers/2503.24377))| Sudanl, pangjh3, BeyondHsueh, Merlin-Hongru, Ray121381 | This survey systematically analyzes "reasoning economy" in Large Language Models (LLMs), focusing on optimizing the balance between complex reasoning performance and computational efficiency. Its main objective is to review the causes of reasoning inefficiency, analyze model behaviors (like length bias and deceptive thinking), and present potential solutions for both post-training optimization and adaptive test-time inference. The methodology involves a comprehensive literature review, classifying challenges and solutions across data, algorithms (e.g., Long2short RL, CoT Compression), architectures (e.g., adaptive parameters, model cooperation), and adaptive inference strategies (e.g., budget allocation, decoding). While a survey, it highlights quantitative results from cited works demonstrating efficiency gains, such as adaptive budget-aware tuning achieving a 67% response length reduction with only a 3% accuracy loss. The key implication for practitioners is the need to employ adaptive, task-dependent strategies to manage the performance-cost trade-off for efficient and sustainable LLM/LRM deployment. |
| Multi-Modal | OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming
  Video Contexts (Read more on [arXiv](https://arxiv.org/abs/2503.22952) or [HuggingFace](https://huggingface.co/papers/2503.22952))| Tong Wu, Bo Chen, Yueqian Wang, zlzheng, ColorfulAI | This paper introduces OmniMMI, a comprehensive benchmark designed to evaluate the interactive capabilities of Omni Large Language Models (OmniLLMs) in processing continuous streams of multi-modal data, specifically within streaming video contexts. The primary objective is to address the limitations of existing benchmarks by evaluating crucial real-world interaction challenges like streaming temporal state awareness and proactive reasoning/turn-taking. To enable efficient interactive processing, the authors propose a novel framework called Multi-modal Multiplexing Modeling (M4), which allows models to 'see, listen while generating' and handle proactive interruptions and parallel decoding. Experiments on OmniMMI reveal that current MLLMs significantly underperform on these interactive streaming tasks, particularly proactive ones, while the proposed M4 framework (e.g., M4-Qwen2-7B achieving 62.00% accuracy on Proactive Turn-Taking) demonstrates notable improvements in proactive capabilities compared to baselines, despite remaining challenges in complex streaming understanding. The main implication for AI practitioners is the highlighted need for models with better modality alignment, efficient streaming processing, and proactive reasoning capabilities, for which OmniMMI serves as a new evaluation standard. |
| Multi-Modal | Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on
  Elementary School-Level Reasoning Problems? (Read more on [arXiv](https://arxiv.org/abs/2504.00509) or [HuggingFace](https://huggingface.co/papers/2504.00509))| Xuesong Yao, xmerge123, ALEXoDu, yfxu, kaiyan289 | This paper investigates whether cutting-edge Large Language Models (LLMs) truly reason or merely recite solutions, particularly for elementary-level problems with subtly altered conditions. The central research question is to determine if LLMs' apparent reasoning ability stems from genuine intelligence or pattern recitation learned from training data. To study this, the authors propose RoR-Bench, a novel, multi-modal (text and image) benchmark consisting of simple reasoning problems paired with variants containing slight but crucial condition shifts. Empirical analysis reveals that top LLMs exhibit severe recitation behavior, suffering significant performance degradation (e.g., over 60% loss for OpenAI-o1 and DeepSeek-R1 on text problems, >35% average loss for VLMs on vision problems) when faced with modified conditions compared to the original ones. This finding serves as a wake-up call, urging practitioners to re-evaluate the actual intelligence level and robustness of current LLMs beyond benchmark scores achieved through potential recitation. |
| Natural Language Processing | Command A: An Enterprise-Ready Large Language Model (Read more on [arXiv](https://arxiv.org/abs/2504.00698) or [HuggingFace](https://huggingface.co/papers/2504.00698))| salthammer, yazeed7, jayalammar, ArashAhmadian, aakanksha | This report details the development of Command A, a 111B parameter large language model purpose-built for enterprise applications, emphasizing efficiency and multilingual capabilities. The main objective was to create a model excelling in real-world enterprise tasks like Retrieval Augmented Generation (RAG), tool use, and multilingual operations across 23 languages. Command A utilizes a decoder-only transformer architecture with specific optimizations (SwiGLU, interleaved sliding window/full attention, GQA) and employs a novel decentralized post-training strategy involving supervised fine-tuning, reinforcement learning, merging of specialized expert models ('Soup models'), and a final polishing stage. The model achieves competitive results, scoring 80.0 on MATH, 90.9 on IFEval, and demonstrating strong performance in human evaluations against models like GPT-4o and DeepSeek V3, while maintaining high inference efficiency (up to 156 tokens/sec). For AI practitioners, Command A presents an enterprise-ready, efficient LLM option optimized for agentic tasks and multilingual environments with lower computational overhead compared to similarly performing models. |
| Multi-Modal | AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models
  with Unsupervised Coefficient Optimization (Read more on [arXiv](https://arxiv.org/abs/2503.23733) or [HuggingFace](https://huggingface.co/papers/2503.23733))| Yiru Wang, Jiabo Ye, Xiaochen Wang, Yiyang Du, carboncoo | This paper introduces AdaMMS, a novel model merging technique for combining heterogeneous Multimodal Large Language Models (MLLMs) without supervised hyperparameter optimization. The primary objective is to overcome challenges arising from differing model architectures and parameter space asymmetry when merging MLLMs. AdaMMS employs a three-step process: defining parameter mapping functions between models, applying adaptive linear interpolation for merging, and using an unsupervised search method based on generation consistency to optimize the interpolation coefficient. Experiments show AdaMMS outperforms previous methods, achieving a +26.84 SUM score improvement over the original models' average when merging LLaVA-OneVision-7B into Qwen2-VL-7B on various vision-language benchmarks. The key implication is enabling practitioners to effectively merge diverse MLLMs without requiring labeled data, facilitating the combination of capabilities from different models resource-efficiently. |
| Natural Language Processing | When To Solve, When To Verify: Compute-Optimal Problem Solving and
  Generative Verification for LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.01005) or [HuggingFace](https://huggingface.co/papers/2504.01005))| anna-rohrbach, kaiweichang, adityagrover, arianhosseini, hbXNov | This paper investigates the compute-optimal strategy for LLM reasoning by comparing Self-Consistency (SC) and Generative Reward Models (GenRM) under fixed inference budgets. The central question is whether to allocate compute towards generating more solutions (SC) or generating fewer solutions paired with generative verification (GenRM). The study employs a compute-matched analysis, comparing success rates of SC and GenRM variants against proportional inference FLOPs, and derives scaling laws for GenRM. Results show SC is more compute-efficient at lower budgets, while GenRM excels at higher budgets; for instance, on MATH with Llama-3.1-8B, GenRM-FT requires 8x more compute to match SC's peak performance but achieves a 3.8% higher success rate with 128x the compute. The key implication is that practitioners should favor SC for lower compute budgets and GenRM for higher budgets, optimally scaling solutions more rapidly (e.g., S_opt ∝ C^0.57) than verifications (e.g., V_opt ∝ C^0.39) when using GenRM. |
| Multi-Modal | Scaling Language-Free Visual Representation Learning (Read more on [arXiv](https://arxiv.org/abs/2504.01017) or [HuggingFace](https://huggingface.co/papers/2504.01017))| liuzhuang13, koustuvs, JiachenZhu, tsbpp, davidfan97 | This paper demonstrates that large-scale visual self-supervised learning (SSL) without language can match the performance of language-supervised CLIP for visual representations in multimodal tasks. The core research question is whether visual SSL lags behind CLIP due to the lack of language supervision or differences in training data scale and distribution. To investigate this, the authors train both visual SSL (DINOv2-based, termed Web-DINO) and CLIP models (up to 7B parameters) on the same large-scale MetaCLIP web dataset (2 billion+ samples) and evaluate primarily using a diverse suite of 16 VQA tasks. Key results show that Web-DINO scales better with model and data size than CLIP on VQA; for instance, the 7B parameter Web-DINO trained on 8 billion examples achieves 55.2% average VQA accuracy, outperforming its CLIP counterpart, and even surpasses CLIP on OCR-centric tasks (+4.3%) when trained specifically on text-rich filtered data. The main implication is that appropriately scaled, purely visual SSL offers a competitive alternative to language-supervised pretraining for developing effective vision encoders for multimodal systems. |
| Natural Language Processing | Multi-Token Attention (Read more on [arXiv](https://arxiv.org/abs/2504.00927) or [HuggingFace](https://huggingface.co/papers/2504.00927))| sainbar, spermwhale, Tianlu, Golovneva | This paper introduces Multi-Token Attention (MTA), a novel attention mechanism designed to enhance LLM's ability to locate relevant context using richer information. It addresses the limitation of single-token attention by conditioning attention weights on multiple query and key vectors through convolution operations over queries, keys, and heads. The primary objective is to improve attention precision and contextual understanding, especially in tasks requiring the identification of information within longer contexts. MTA achieves enhanced performance on various benchmarks, including language modeling tasks, outperforming Transformer baselines; specifically, improvements in validation perplexity were observed, as well as improved performance on long-context tasks.  MTA's ability to leverage richer contextual information provides AI practitioners with an improved attention mechanism for tasks requiring precise information retrieval and long-range dependency handling. |
| Multi-Modal | Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features (Read more on [arXiv](https://arxiv.org/abs/2504.00557) or [HuggingFace](https://huggingface.co/papers/2504.00557))| Jaeyeon Kim, Donguk Lim, Seungmin Yang, Ki-Ung Song, Jewon Lee | The paper introduces Trimmed-Llama, a method for efficient visual feature processing in cross-attention-based vision-language models. It addresses the computational bottleneck of large KV caches for image tokens by selectively pruning redundant visual features based on attention sparsity. The key methodology involves leveraging head-wise attention scores to identify and remove unimportant image features in the initial cross-attention layer. Experimental results show that Trimmed-Llama can reduce visual features by 50% while maintaining benchmark parity. This approach offers AI practitioners a training-free method to reduce inference latency and memory usage in LVLMs. |
| Machine Learning | Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies
  Ahead (Read more on [arXiv](https://arxiv.org/abs/2504.00294) or [HuggingFace](https://huggingface.co/papers/2504.00294))| Neel Joshi, Shivam Garg, Lingjiao Chen, Jingya Chen, Vidhisha Balachandran | This paper investigates inference-time scaling in large language models (LLMs) across various complex tasks to enhance reasoning capabilities. The research aims to evaluate the benefits and limitations of scaling methods on challenging tasks beyond mathematical problems. The authors compare conventional and fine-tuned models through repeated model calls, independently or sequentially with feedback, and analyze results across nine models and eight tasks. Empirical analysis reveals task-specific advantages of inference-time scaling that diminish with problem complexity, and that increased token usage doesn't always translate to higher accuracy; however, significant gains are observed with perfect verifiers or strong feedback. These findings offer insights into potential training and RL techniques for improved reasoning for AI practitioners. |
| Natural Language Processing | Discovering Knowledge Deficiencies of Language Models on Massive
  Knowledge Base (Read more on [arXiv](https://arxiv.org/abs/2503.23361) or [HuggingFace](https://huggingface.co/papers/2503.23361))| Ryotaro Shimizu, Jieyu Zhang, Xuwei Ding, MaksimSTW, linxinso | This paper introduces Stochastic Error Ascent (SEA), a framework for efficiently discovering knowledge deficiencies in closed-weight LLMs on massive knowledge bases. It addresses the computational challenges of exhaustively evaluating LLMs by formulating error discovery as a stochastic optimization problem that iteratively retrieves high-error candidates based on semantic similarity. SEA employs hierarchical retrieval and constructs a relation directed acyclic graph (DAG) to identify systematic failure modes. The empirical evaluation demonstrates that SEA uncovers 40.7x more knowledge errors than Automated Capability Discovery and 26.7% more than AutoBencher, while significantly reducing the cost-per-error. The framework offers AI practitioners a scalable approach to understanding and mitigating factual knowledge gaps in LLMs. |
| Natural Language Processing | m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning
  with Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.00869) or [HuggingFace](https://huggingface.co/papers/2504.00869))| Yuyin Zhou, Xianfeng Tang, Hui Liu, Juncheng Wu, Xiaoke Huang | The paper explores test-time scaling in large language models (LLMs) for medical reasoning, presenting m1, a method to enhance models' medical reasoning capabilities at inference time. It investigates the impact of increasing the “thinking” token budget on medical question answering (QA) performance. Results show consistent performance gains with test-time scaling, with a 32B model achieving results comparable to 70B-scale models, with an optimal reasoning token budget of approximately 4K. The study identifies insufficient medical knowledge as a key bottleneck and emphasizes that enriched medical knowledge is essential for realizing the benefits of test-time scaling, suggesting that improving data quality and increasing model capacity are crucial for further performance gains. The m1-7B-23K model attains a new state-of-the-art accuracy of 60.32% on medical exam datasets. |
| Computer Vision | Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs (Read more on [arXiv](https://arxiv.org/abs/2504.00072) or [HuggingFace](https://huggingface.co/papers/2504.00072))| Gül Varol, Cordelia Schmid, Antoine Yang, Lucas Ventura | The paper presents Chapter-Llama, a framework for efficient video chaptering in hour-long videos using Large Language Models (LLMs). It aims to automatically partition videos into semantic units and generate corresponding chapter titles. The key methodology involves leveraging a pretrained LLM with speech transcripts and frame captions, along with a speech-guided frame selection strategy to reduce computational cost. Chapter-Llama achieves a 45.3 F1 score on the VidChapters-7M benchmark, significantly improving upon the state-of-the-art. This approach scales to processing long-form content efficiently and offers a solution for content indexing, potentially bypassing manual annotation. |
| Machine Learning | Towards Trustworthy GUI Agents: A Survey (Read more on [arXiv](https://arxiv.org/abs/2503.23434) or [HuggingFace](https://huggingface.co/papers/2503.23434))| Ninghao Liu, Wenhu Chen, Wenlin Yao, Wenhao Yu, Yucheng Shi | This survey explores the trustworthiness of GUI agents powered by large foundation models, focusing on aspects beyond task success. It examines the security vulnerabilities, reliability in dynamic environments, transparency, ethical considerations, and evaluation methodologies of these agents. The study identifies challenges like vulnerability to adversarial attacks and cascading failures in sequential decision-making. It calls for comprehensive mitigation strategies, with evidence that Environmental Injection Attacks (EIA) can have up to 70% success rate extracting PII. The survey aims to advance trustworthy GUI agent development through systematic understanding and responsible practices for AI practitioners. |
| Computer Vision | DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D
  Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2503.24210) or [HuggingFace](https://huggingface.co/papers/2503.24210))| Gim Hee Lee, onandon | DiET-GS introduces a novel framework for deblurring 3D Gaussian Splatting by leveraging event streams and diffusion priors. The research addresses the challenge of reconstructing sharp 3D representations from blurry multi-view images. The method uses an event double integral formulation and pretrained diffusion model in a two-stage training strategy. Experiments demonstrate that DiET-GS achieves higher visual quality, demonstrated by a MUSIQ score of 51.71 compared to existing baselines. This provides AI practitioners with an effective technique for novel view synthesis and 3D reconstruction in scenarios with motion blur, potentially improving performance of related downstream tasks. |
| Reinforcement Learning | ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via
  Residual Learning (Read more on [arXiv](https://arxiv.org/abs/2503.21860) or [HuggingFace](https://huggingface.co/papers/2503.21860))| Siyuan Huang, Yuyang Li, Tengyu Liu, Puhao Li, Kailin Li | This paper presents MANIPTRANS, a novel two-stage method for efficient transfer of human bimanual skills to dexterous robotic hands in simulation. The research aims to enable accurate execution of complex bimanual tasks by addressing the morphological differences between human and robotic hands. MANIPTRANS pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints using reinforcement learning. Experiments demonstrate that MANIPTRANS surpasses state-of-the-art methods in success rate, fidelity, and efficiency, achieving a success rate of 58.1/39.5 (single/bimanual tasks), enabling the creation of DEXMANIPNET, a large-scale dataset for dexterous hand manipulation to improve policy training for dexterous hands and enable real-world deployments. |
| Computer Vision | MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote
  Sensing (Read more on [arXiv](https://arxiv.org/abs/2503.24219) or [HuggingFace](https://huggingface.co/papers/2503.24219))| Mustapha lebbah, Hanane Azzag, rdkarim | The paper introduces MB-ORES, a unified framework for object detection (OD) and visual grounding (VG) in remote sensing imagery. It aims to improve VG performance by fine-tuning an open-set object detector using referring expression data and structuring the outputs as a graph. The methodology involves a multi-branch network for feature integration and an object reasoning network for proposal probability assignment, followed by a soft selection mechanism for localization. MB-ORES achieves superior performance on OPT-RSVG and DIOR-RSVG datasets, achieving significant improvements over state-of-the-art methods, specifically 85.65% Pr@0.5 on DIOR-RSVG. This unified approach simplifies object query formulation and enhances the quality of generated proposals, advancing REC in remote sensing. |
