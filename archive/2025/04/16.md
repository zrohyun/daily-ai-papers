

## Papers for 2025-04-16

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Genius: A Generalizable and Purely Unsupervised Self-Training Framework
  For Advanced Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.08672) or [HuggingFace](https://huggingface.co/papers/2504.08672))| Haiteng Zhao, Chang Ma, Hang Yan, QiushiSun, xufangzhi | This paper introduces Genius, a generalizable and purely unsupervised self-training framework to enhance the reasoning capabilities of Large Language Models (LLMs) without external supervision. The primary objective is to address the scalability and cost limitations of existing supervised or reward-model-based post-training techniques by enabling LLMs to self-improve using only unlabeled general queries. Genius employs a stepwise foresight re-sampling strategy to sample and evaluate reasoning steps by simulating future outcomes, combined with an advantage-calibrated optimization (ACO) loss function to handle estimation inconsistencies robustly. Using merely 25K unsupervised general queries, Genius reportedly improved the average reasoning performance of LLaMA3.1-8B-Instruct across diverse benchmarks by over 7%, outperforming several strong baselines. The main implication is that LLMs can potentially scale their reasoning abilities significantly through self-training on vast amounts of readily available general data, reducing reliance on costly human annotations. |
| Natural Language Processing | xVerify: Efficient Answer Verifier for Reasoning Model Evaluations (Read more on [arXiv](https://arxiv.org/abs/2504.10481) or [HuggingFace](https://huggingface.co/papers/2504.10481))| Bo Tang, Wentao Zhang, Pengyuan Wang, Duguce, Hush-cd | This paper introduces xVerify, an efficient Language Model (LLM)-based verifier designed to evaluate the correctness of answers generated by reasoning models. The primary objective is to overcome the limitations of existing methods in handling complex, multi-step reasoning outputs and accurately extracting/validating final answers. To achieve this, the researchers constructed the Verify Answer for Reasoning (VAR) dataset using responses from 19 LLMs across 24 benchmarks, annotated via GPT-4o and human review, and then fine-tuned various xVerify model scales (0.5B to 32B) on this dataset. Key results show xVerify models achieve F1 scores and accuracy exceeding 95% on test sets, with the xVerify-3B-Ib variant outperforming even GPT-4o. For AI practitioners, xVerify offers a more reliable and efficient automated tool for evaluating the reasoning performance of LLMs, especially on objective questions requiring complex verification. |
| Multi-Modal | Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding (Read more on [arXiv](https://arxiv.org/abs/2504.10465) or [HuggingFace](https://huggingface.co/papers/2504.10465))| Weixian Lei, Yanwei Li, Zilong Huang, Tao Zhang, LXT | Pixel-SAIL introduces a simplified single-transformer architecture for pixel-grounded visual understanding tasks, aiming to reduce the complexity of existing Multimodal Large Language Models (MLLMs). The core objective is to achieve fine-grained, pixel-level understanding (like referring segmentation and visual prompt comprehension) without relying on multiple specialized components such as dedicated vision encoders and segmentation decoders. Key methodologies include a learnable upsampling module for visual features, a novel visual prompt injection technique mapping prompts to special text tokens, and a vision expert distillation strategy to enhance segmentation quality. Experimental results show Pixel-SAIL (3B) outperforms prior larger models like GLaMM (7B) on referring segmentation benchmarks by 1.5-3.0% cIoU and achieves a 42.2 overall score on the proposed PerBench benchmark. This work implies that effective pixel-grounded understanding can be achieved with a simpler, single-transformer architecture, offering a potentially more scalable and less complex alternative for AI practitioners. |
| Reinforcement Learning | Heimdall: test-time scaling on the generative verification (Read more on [arXiv](https://arxiv.org/abs/2504.10337) or [HuggingFace](https://huggingface.co/papers/2504.10337))| Xing Jin, WesleyShi | This paper introduces Heimdall, a long Chain-of-Thought (CoT) verification LLM trained using reinforcement learning to accurately judge the correctness of complex solutions, particularly for competitive math problems. The primary objective is to enhance the weak verification capabilities of current LLMs for long CoT reasoning and use this improved verification to scale problem-solving performance. The key methodology involves training Heimdall with the PPO algorithm to output verification steps and a final correctness judgment, and proposing 'Pessimistic Verification' to select the best solution by combining multiple solver outputs and verification judgments. Key results show Heimdall boosting verification accuracy from 62.5% to 94.5% (up to 97.5% with sampling) on AIME problems, and Pessimistic Verification improving a solver's AIME2025 accuracy from 54.2% to 70.0% (16x budget). The main implication is that specialized RL-trained verifiers can significantly improve the reliability and performance scaling of generative models on complex reasoning tasks. |
| Multi-Modal | Seedream 3.0 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2504.11346) or [HuggingFace](https://huggingface.co/papers/2504.11346))| Zhichao Lai, Xiaoxia Hou, Qiushan Guo, Lixue Gong, Yu Gao | This paper introduces Seedream 3.0, an enhanced bilingual (Chinese-English) text-to-image foundation model designed to improve prompt alignment, typography, aesthetics, and resolution over Seedream 2.0. The primary objective was to address these challenges, particularly for complex Chinese typography and native high-resolution (up to 2K) generation. Methodological improvements span data construction (defect-aware training, dual-axis sampling), pre-training (mixed-resolution, cross-modality RoPE, alignment loss, resolution-aware timestep sampling), post-training (aesthetic SFT, VLM rewards), and acceleration (consistent noise expectation, importance timestep sampling). Seedream 3.0 demonstrates superior performance, ranking first on the Artificial Analysis leaderboard with an Arena ELO of 1158 and achieving a 94% text availability rate for both Chinese and English characters. This offers practitioners an efficient, high-quality bilingual generation model with strong typography capabilities and native high-resolution output, suitable for productivity applications. |
| Machine Learning | How Instruction and Reasoning Data shape Post-Training: Data Quality
  through the Lens of Layer-wise Gradients (Read more on [arXiv](https://arxiv.org/abs/2504.10766) or [HuggingFace](https://huggingface.co/papers/2504.10766))| Ziyue Li, Yanhong Li, Ming Li, zhoutianyi | This paper presents a spectral analysis of layer-wise gradients to understand how instruction and reasoning data quality impacts large language model (LLM) post-training dynamics. The main objective is to investigate the effects of data quality (low/high, instruction/reasoning) on gradients during finetuning and to unify existing data quality metrics through gradient properties. The methodology involves computing Singular Value Decomposition (SVD)-based metrics (Nuclear Norm, Effective Rank) and similarity metrics on layer-wise gradients for various LLMs (Qwen2, Llama3, Gemma2) finetuned on instruction and reasoning datasets partitioned by quality metrics (IFD, InsTag, Difficulty, Reward). Key findings reveal that higher-quality data consistently induces gradients with lower nuclear norms and higher effective ranks across different metrics and data types; notably, reasoning data exhibits substantially higher effective ranks than instruction data (e.g., s1.1 vs. GSM8K reasoning data shows large gaps, Table 2), suggesting richer gradient structures, with effective rank being more robust than nuclear norm in distinguishing quality. The main implication is that spectral properties of gradients, particularly effective rank, provide a unified lens for understanding data quality's impact on training stability and efficiency, offering insights for developing better data selection strategies for LLM post-training. |
| Machine Learning | TextArena (Read more on [arXiv](https://arxiv.org/abs/2504.11442) or [HuggingFace](https://huggingface.co/papers/2504.11442))| Leshem Choshen, Benjamin-eecs, simonycl, bobbycxy, LeonGuertler | TextArena introduces an open-source collection of 57+ competitive text-based games designed for evaluating and training agentic behavior in Large Language Models (LLMs). The primary objective is to assess dynamic capabilities, particularly social skills like negotiation, theory of mind, and deception, alongside strategic planning and reasoning, which static benchmarks often overlook. The methodology utilizes a Gym-compatible framework enabling online competition between models and humans, tracking performance with a real-time TrueSkill™ leaderboard (initialized μ=25, σ=25/3) and providing soft-skill profiling. Preliminary results show relative model rankings and skill comparisons (e.g., between Humanity, GPT-4o, Claude-3.5-Sonnet) across games, quantified by TrueSkill™ scores. For AI practitioners, TextArena offers an extensible platform for nuanced evaluation of LLM agentic capabilities in interactive scenarios, moving beyond static metrics and providing a potential basis for RL training. |
| Multi-Modal | The Scalability of Simplicity: Empirical Analysis of Vision-Language
  Learning with a Single Transformer (Read more on [arXiv](https://arxiv.org/abs/2504.10462) or [HuggingFace](https://huggingface.co/papers/2504.10462))| Jun Hao Liew, Haochen Wang, Jiacong Wang, Weixian Lei, LXT | This paper introduces and analyzes SAIL, a single-transformer unified multimodal large language model (MLLM) that processes raw pixels and text within one architecture. The primary objective is to empirically compare the fundamental properties of this unified approach (scalability, information flow, visual representation) against traditional modular MLLMs that use separate vision encoders. SAIL adapts mixed attention mechanisms and multimodal positional encodings for joint processing, trained via a two-stage pretraining curriculum followed by supervised fine-tuning. Key results show SAIL achieves performance comparable to modular MLLMs, exhibits superior data scalability, and demonstrates strong visual representation capabilities, achieving 84.95% Top-1 accuracy on ImageNet-1K. The main implication is that single-transformer architectures present a scalable and potentially simpler alternative for vision-language learning, capable of matching or exceeding modular designs given sufficient data. |
| Machine Learning | Efficient Process Reward Model Training via Active Learning (Read more on [arXiv](https://arxiv.org/abs/2504.10559) or [HuggingFace](https://huggingface.co/papers/2504.10559))| Tianyu Pang, Xin Mao, Zichen Liu, Keyu Duan, dreamerdeo | This paper introduces ACTPRM, an active learning method to efficiently train Process Reward Models (PRMs) for large language models, significantly reducing annotation costs. The research aims to overcome the challenge of expensive step-level annotation required for scaling PRM training. ACTPRM uses an ensemble PRM to estimate uncertainty in reasoning steps, selecting only the most uncertain samples for labeling by a capable reasoning model before updating the PRM. The method achieves state-of-the-art performance, reaching a 75.0% average F1 score on ProcessBench, outperforming prior models while using substantially less annotation data (e.g., 20% of the budget compared to UniversalPRM). For AI practitioners, this demonstrates a viable approach to train effective PRMs more economically by focusing annotation efforts on the most informative samples. |
| Computer Vision | Efficient Generative Model Training via Embedded Representation Warmup (Read more on [arXiv](https://arxiv.org/abs/2504.10188) or [HuggingFace](https://huggingface.co/papers/2504.10188))| Tao Lin, Xufeng Li, Peng Sun, SempraETY | This paper introduces Embedded Representation Warmup (ERW), a framework to accelerate training and enhance the representation quality of diffusion models for image generation. The research objective is to mitigate slow convergence by decoupling representation learning from generation through initialization with high-quality pretrained features. ERW employs a two-phase training strategy: an initial warmup phase aligns the model's early Latent-to-Representation (L2R) layers with a pretrained encoder via an alignment loss, followed by standard diffusion training with decaying representation guidance. Empirically, ERW demonstrates significant training acceleration (e.g., 40x faster convergence than the REPA baseline) and achieves strong generative performance, reaching an FID of 6.0 at 100k iterations on ImageNet 256x256 without classifier-free guidance using SiT-XL/2. For AI practitioners, ERW provides a method to substantially reduce the computational cost and training time for diffusion models by effectively reusing pretrained representations. |
| Computer Vision | NormalCrafter: Learning Temporally Consistent Normals from Video
  Diffusion Priors (Read more on [arXiv](https://arxiv.org/abs/2504.11427) or [HuggingFace](https://huggingface.co/papers/2504.11427))| Bing Wang, Xinya Chen, Haoyuan Wang, Yanrui Bin, wbhu-tc | This paper introduces NormalCrafter, a novel method for generating high-fidelity, temporally consistent surface normal sequences from open-world videos by leveraging video diffusion priors. The primary objective is to overcome the limitations of existing methods, particularly their temporal inconsistency and suboptimal detail preservation when applied to videos. NormalCrafter adapts a video diffusion model (SVD) and incorporates Semantic Feature Regularization (SFR) to align diffusion features with DINO-derived semantic representations, alongside a two-stage training protocol (latent space training followed by pixel-space fine-tuning) to balance long temporal context with spatial accuracy. Evaluations show state-of-the-art performance on video datasets like Sintel (improving mean angular error by 1.6° over the prior best) and ScanNet (0.8° improvement), demonstrating superior temporal consistency and detail. For AI practitioners, NormalCrafter provides an effective approach for video-based 3D geometry estimation, enhancing applications requiring coherent surface normal sequences. |
| Reinforcement Learning | A Minimalist Approach to LLM Reasoning: from Rejection Sampling to
  Reinforce (Read more on [arXiv](https://arxiv.org/abs/2504.11343) or [HuggingFace](https://huggingface.co/papers/2504.11343))| Lei Wang, Bo Pang, Yuhui Xu, Jiarui Yao, Wei Xiong | This paper investigates minimalist reinforcement learning approaches for fine-tuning Large Language Models (LLMs) on reasoning tasks, comparing simple methods like rejection sampling (RAFT) to more complex algorithms like GRPO and PPO. The primary objective is to understand the source of effectiveness in recent RL methods for LLMs, particularly GRPO, and evaluate whether simpler alternatives suffice. Through comparative experiments and ablation studies on models like Qwen2.5-Math-7B using math reasoning benchmarks, the study finds that RAFT achieves competitive performance (e.g., 49.9% average accuracy compared to GRPO's 53.9%) and its importance-sampling variant RAFT++ (52.5%) closes the gap further. Results indicate GRPO's main benefit stems from discarding prompts with only incorrect responses, leading to the proposal of Reinforce-Rej, a minimal policy gradient variant filtering both all-incorrect and all-correct samples, which shows improved KL efficiency and stability. The key implication is that practitioners can achieve strong results with simpler, interpretable methods like RAFT or Reinforce-Rej, and future work should focus on principled sample selection rather than complex algorithmic components for handling negative feedback. |
| Reinforcement Learning | DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and
  Verifiable Mathematical Dataset for Advancing Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.11456) or [HuggingFace](https://huggingface.co/papers/2504.11456))| Xingyu Chen, Qiuzhi Liu, Jiahao Xu, Tian Liang, Zhiwei He | This paper introduces DeepMath-103K, a large-scale, challenging, decontaminated, and verifiable mathematical dataset specifically designed for advancing AI reasoning, particularly through reinforcement learning (RL). The main objective was to address the scarcity of suitable training data that is sufficiently difficult, possesses verifiable answers for RL, and is free from benchmark contamination. The dataset was curated via a meticulous pipeline including source analysis, rigorous decontamination against standard benchmarks, difficulty filtering (focusing on levels 5-9), and answer verification using consistency checks across multiple R1-generated solutions. Models trained on DeepMath-103K using RL-Zero demonstrated significant performance improvements, with DeepMath-Zero-7B achieving 85.5% on MATH500 and 17.5% on AIME25 (pass@1, n=16), substantially outperforming baseline and models trained on other datasets. The key implication for AI practitioners is the availability of a large-scale, challenging, and clean dataset tailored for rule-based RL, facilitating the development of more powerful mathematical reasoning systems. |
| Computer Vision | Diffusion Distillation With Direct Preference Optimization For Efficient
  3D LiDAR Scene Completion (Read more on [arXiv](https://arxiv.org/abs/2504.11447) or [HuggingFace](https://huggingface.co/papers/2504.11447))| Jiale Wu, Zejian Li, Ling Yang, Shengyuan Zhang, An Zhaol | This paper introduces Distillation-DPO, a novel framework to accelerate and enhance 3D LiDAR scene completion using diffusion models. The primary objective is to overcome the slow sampling speed of diffusion models and mitigate performance degradation associated with standard distillation techniques. The key methodology involves a preference-aligned diffusion distillation process where a student model generates scene pairs, non-differentiable LiDAR metrics guide the creation of winning/losing pairs, and optimization leverages score function differences between teacher and student models on these pairs. Experiments demonstrate that Distillation-DPO achieves higher quality scene completion (e.g., refined CD of 0.354 vs. LiDiff's 0.375) while accelerating inference speed by over 5 times (3.38s vs. 17.87s) compared to the state-of-the-art LiDiff model. For AI practitioners, this work presents a method to make diffusion models more efficient for 3D perception tasks by integrating preference learning into the distillation process, improving both speed and output quality. |
| Computer Vision | PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of
  Complex Videos in the Wild (Read more on [arXiv](https://arxiv.org/abs/2504.11326) or [HuggingFace](https://huggingface.co/papers/2504.11326))| Shuting He, Nikhila Ravi, Chang Liu, LXT, HenghuiDing | This report details the outcomes of the 4th Pixel-level Video Understanding in the Wild (PVUW) challenge held at CVPR 2025. The primary objective was to benchmark and advance algorithms for complex video object segmentation (MOSE track) and motion/language-guided segmentation (MeViS track) in unconstrained environments using challenging new datasets. Top methodologies involved ensembles of state-of-the-art models (e.g., SAM2, TMO, Sa2VA), adaptive pseudo-label refinement pipelines, extensive data augmentation, and test-time strategies, increasingly leveraging large multi-modal models. The leading team achieved a J&F score of 87.26% on the MOSE track and 61.98% on the MeViS track, demonstrating progress but also highlighting remaining difficulties. For AI practitioners, the results underscore the need for robust models capable of handling real-world complexity and showcase the growing potential of integrating large language and foundation models for nuanced video understanding tasks. |
| Reinforcement Learning | ReZero: Enhancing LLM search ability by trying one-more-time (Read more on [arXiv](https://arxiv.org/abs/2504.11001) or [HuggingFace](https://huggingface.co/papers/2504.11001))| Thinh Le, alandao | This paper introduces ReZero, a reinforcement learning (RL) framework designed to enhance the search persistence of Large Language Models (LLMs) within Retrieval-Augmented Generation (RAG) systems by rewarding query retries after initial failures. The primary objective is to improve LLM robustness in complex information-seeking tasks where initial search attempts might be insufficient, incentivizing the model to 'try one more time'. ReZero utilizes Group Relative Policy Optimization (GRPO) and augments the reward signal with a specific component (`reward_retry`) that positively rewards the act of issuing subsequent search queries, conditional on generating a final correct answer. Experiments conducted on the Apollo 3 mission dataset demonstrated that ReZero achieved a peak accuracy of 46.88%, significantly outperforming a baseline model without the retry incentive, which reached only 25.00%. The key implication for AI practitioners is that explicitly rewarding persistence in the search process can markedly improve the success rate and reliability of LLM-based agents in information retrieval scenarios. |
| Natural Language Processing | AI-University: An LLM-based platform for instructional alignment to
  scientific classrooms (Read more on [arXiv](https://arxiv.org/abs/2504.08846) or [HuggingFace](https://huggingface.co/papers/2504.08846))| Rahul Gulati, Mostafa Faghih Shojaei, garikipati, Dinzhenzhenzhu, simocimolato | This paper introduces AI-University (AI-U), an LLM-based framework for generating instructionally-aligned content in scientific classrooms, demonstrated via a Finite Element Method course. The primary objective is to create a system that adapts to an instructor's style by fine-tuning an LLM using course-specific materials like lecture videos, notes, and textbooks. Key methodologies include generating training data from course content, fine-tuning an open-source LLM (Llama-3.2-11B) with LoRA, and using Retrieval-Augmented Generation (RAG) for context retrieval and response synthesis. Evaluation showed the fine-tuned model (LLaMA-TOMMI-1.0) achieved higher cosine similarity with reference answers than the base model on 86% of test cases and was preferred by an LLM judge roughly four times out of five. The main implication is that combining fine-tuning and RAG offers a scalable approach for developing domain-specific AI assistants aligned with specialized instructional or research content, enhancing relevance beyond general-purpose LLMs. |
| Machine Learning | Adaptive Computation Pruning for the Forgetting Transformer (Read more on [arXiv](https://arxiv.org/abs/2504.06949) or [HuggingFace](https://huggingface.co/papers/2504.06949))| Aaron Courville, Johan Obando-Ceron, Zhixuan Lin, littleowen | This paper introduces Adaptive Computation Pruning (ACP), a method to accelerate the Forgetting Transformer (FoX) by dynamically skipping computations based on its forget gate mechanism. The primary objective is to reduce computational costs and improve training throughput for FoX without degrading performance. The key methodology involves setting a dynamic pruning threshold based on attention logit bounds and a tolerance parameter (ε), identifying a pruning boundary within the FlashAttention framework, and skipping computations for strongly decayed input-output dependencies. Results show ACP consistently prunes around 70% of softmax attention FLOPs across various model sizes and context lengths (4k-16k), leading to 10-35% training throughput improvements with no performance degradation. For AI practitioners, ACP offers a way to significantly speed up FoX training and potentially inference, especially for long sequences, without sacrificing model accuracy. |
| Multi-Modal | Multimodal Long Video Modeling Based on Temporal Dynamic Context (Read more on [arXiv](https://arxiv.org/abs/2504.10443) or [HuggingFace](https://huggingface.co/papers/2504.10443))| Xiangyu Yue, Yiyuan Zhang, Jiaming Han, Hoar012 | This paper introduces Temporal Dynamic Context (TDC), a novel framework for multimodal long video understanding that integrates static and dynamic features while efficiently compressing tokens. The primary objective is to overcome LLM context length limitations and information loss in long video processing by effectively representing and compressing temporal visual and audio information. TDC segments videos into scenes, encodes frames using visual-audio encoders, and employs a query-based Transformer to compress temporal context based on differences from a static key frame, unifying multimodal tokens; a Long Video Chain-of-Thought (LVCoT) strategy handles extremely long videos. The TDC model demonstrates strong performance, outperforming prior audio-visual MLLMs like VideoLLaMA2 by 15.6% on the MLVU benchmark and achieving competitive results across various video QA tasks (e.g., 68.3 on MVBench). This work provides AI practitioners with an improved method for handling long videos with multiple modalities, enabling better temporal reasoning and multimodal integration within MLLMs. |
| Multi-Modal | Summarization of Multimodal Presentations with Vision-Language Models:
  Study of the Effect of Modalities and Structure (Read more on [arXiv](https://arxiv.org/abs/2504.10049) or [HuggingFace](https://huggingface.co/papers/2504.10049))| Frédéric Dufaux, Camille Guinaudeau, gigant | This paper analyzes how Vision-Language Models (VLMs) summarize multimodal presentations, focusing on the impact of different input modalities and structures. The main objective is to evaluate how various representations (e.g., slides, transcript, video, structured combinations) affect VLM-based summarization performance and cost. Using a filtered TIB dataset and primarily the Qwen2-VL model, the study benchmarks multiple input formats against metrics like Rouge and modality-specific Importance-based Relevance (IbR). Key results demonstrate that a structured multimodal input (interleaved slides and transcript) yields the best performance (Qwen2-VL R1=27.1, IbR_overall=33.4), significantly outperforming unimodal or unstructured inputs, especially with sufficient visual token budgets. The primary implication for AI practitioners is that using structured, interleaved slide-transcript data provides the optimal approach for high-quality presentation summarization with VLMs, while slides alone offer a cost-effective alternative for shorter inputs. |
| Computer Vision | D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation (Read more on [arXiv](https://arxiv.org/abs/2504.09454) or [HuggingFace](https://huggingface.co/papers/2504.09454))| Zhendong Mao, Lei Zhang, Nan Chen, Mengqi Huang, Weinan Jia | This paper introduces D²iT, a Dynamic Diffusion Transformer framework designed for more accurate high-fidelity image generation by addressing the limitations of fixed compression in standard Diffusion Transformers (DiT). The primary objective is to improve image quality by dynamically adapting the compression level to the varying information densities across different image regions, thereby balancing global consistency and local realism. The core methodology involves a two-stage process: first, a Dynamic VAE (DVAE) encodes regions at different granularities based on information density, and second, the D²iT predicts multi-grained noise using a Dynamic Grain Transformer and a Dynamic Content Transformer with a coarse-prediction and fine-correction strategy. Key results demonstrate significant improvement, achieving a 1.73 FID score on class-conditional ImageNet generation, outperforming the baseline DiT's 2.27 FID with fewer training resources. For AI practitioners, the main implication is that incorporating dynamic, content-aware processing based on information density into diffusion models can enhance generation fidelity and potentially improve training efficiency. |
| Computer Vision | Change State Space Models for Remote Sensing Change Detection (Read more on [arXiv](https://arxiv.org/abs/2504.11080) or [HuggingFace](https://huggingface.co/papers/2504.11080))| Erchan Aptoula, ElmanGhazaei | This paper introduces the Change State Space Model (CSSM), a novel architecture based on Mamba (State Space Models) specifically designed for remote sensing change detection. The main objective is to address the limitations of ConvNets (long-range dependencies) and Vision Transformers (computational complexity) by focusing feature extraction only on relevant changes between bi-temporal images. The key methodology involves incorporating an L1 distance-based selection mechanism within the state-space model to filter out irrelevant information and selectively process changed features. CSSM demonstrated state-of-the-art performance on three benchmark datasets, achieving an F1-score of 92.39 on LEVIR-CD+, while using significantly fewer parameters (4.34M) and GFLOPs (5.10) compared to previous CNN, ViT, and Mamba-based methods. The main implication for AI practitioners is that CSSM provides a computationally efficient and highly accurate approach for change detection tasks, especially suitable for large-scale remote sensing applications. |
| Natural Language Processing | LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews (Read more on [arXiv](https://arxiv.org/abs/2504.11042) or [HuggingFace](https://huggingface.co/papers/2504.11042))| Iryna Gurevych, Lizhen Qu, Anne Lauscher, Zhuang Li, sukannya | This paper introduces LAZYREVIEW, a dataset designed to uncover 'lazy thinking' heuristics in NLP peer reviews. The main objective is to create a resource for developing automated tools to detect such heuristics, thereby improving peer review quality, given the limited prior work and lack of datasets. The methodology involved collecting ARR-22 reviews, extracting potential lazy thinking segments using GPT-4, and annotating 500 segments with fine-grained categories over three rounds with evolving guidelines, alongside creating 1276 silver annotations. Key results show that while LLMs struggle in zero-shot detection, instruction-tuning on LAZYREVIEW boosts performance by 10-20 accuracy points, and revised reviews using lazy thinking feedback were found to be more comprehensive and actionable in a controlled experiment (e.g., 95.6% win rate vs original reviews on adherence). The primary implication for AI practitioners is the availability of a new dataset and validated methodology (instruction-tuning) to identify and potentially mitigate biased heuristics in the crucial process of NLP peer review. |
