

## Papers for 2025-04-23

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Kuwain 1.5B: An Arabic SLM via Language Injection (Read more on [arXiv](https://arxiv.org/abs/2504.15120) or [HuggingFace](https://huggingface.co/papers/2504.15120))| Omar Hadid, Sara Chrouf, ZeinaD, Moatasem444, Hennara | This paper introduces Kuwain 1.5B, an Arabic Small Language Model created by injecting Arabic capabilities into an existing English-centric model (TinyLlama 1.1B) without full retraining. The primary objective is to efficiently expand a monolingual LLM to support a new language (Arabic) while preserving its original English performance and knowledge. The methodology involves extending the model's tokenizer with 26K Arabic tokens and adding 8 new, trainable layers initialized as identity functions, training only these new layers alongside a small fraction (20%) of the original English data. Key results demonstrate an average 8% improvement in Arabic benchmark performance compared to the base model, alongside maintained English proficiency (average score 53.28 vs. 52.99) and reduced training costs by 70%. The main implication for AI practitioners is that this language injection technique offers a cost-effective and efficient alternative for multilingual model expansion without requiring extensive resources or compromising the model's original capabilities. |
| Reinforcement Learning | TTRL: Test-Time Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2504.16084) or [HuggingFace](https://huggingface.co/papers/2504.16084))| Xuekai Zhu, Li Sheng, Shang Qu, Yuxin Zuo, iseesaw | This paper introduces Test-Time Reinforcement Learning (TTRL), a novel framework for adapting Large Language Models (LLMs) using Reinforcement Learning (RL) directly on unlabeled test data. The primary objective is to enable LLM self-improvement during inference by addressing the challenge of reward estimation without ground-truth labels. TTRL utilizes majority voting on multiple model-generated outputs to estimate pseudo-labels and compute rule-based rewards, which then drive RL-based model updates. Experiments demonstrate significant performance gains, such as boosting Qwen-2.5-Math-7B's pass@1 score on AIME 2024 by approximately 159% (from 16.7% to 43.3%) using only unlabeled test data. For AI practitioners, TTRL shows that LLMs can effectively self-evolve on unlabeled data streams via RL, potentially reducing reliance on extensive labeled data for adaptation and surpassing standard test-time inference techniques. |
| Natural Language Processing | The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks (Read more on [arXiv](https://arxiv.org/abs/2504.15521) or [HuggingFace](https://huggingface.co/papers/2504.15521))| Huifeng Yin, Sinuo Liu, Weixuan Wang, Minghao Wu, ChenyangLyu | This paper analyzes over 2,000 multilingual (non-English) benchmarks published between 2021-2024 to evaluate past, present, and future practices in multilingual benchmarking. The primary objective is to identify trends, assess the alignment of current benchmarks with human judgments, and propose directions for more equitable and effective evaluation. The methodology involves collecting and annotating benchmark papers from arXiv, analyzing distributions (languages, tasks, domains), and correlating benchmark performance (e.g., XQuAD, ARC, MGSM) with human Elo rankings using Spearman's ρ. Key findings reveal English overrepresentation, poor correlation for tasks like XQuAD (0.11-0.30) compared to STEM tasks (0.70-0.85), and significantly better human alignment for localized benchmarks (0.68) versus translated ones (0.47). The main implication is that current multilingual evaluation often relies on poorly aligned, translated benchmarks, necessitating a shift towards creating culturally authentic, human-aligned, and practically relevant benchmarks through global collaboration. |
| Multi-Modal | Describe Anything: Detailed Localized Image and Video Captioning (Read more on [arXiv](https://arxiv.org/abs/2504.16072) or [HuggingFace](https://huggingface.co/papers/2504.16072))| Yifan Ding, richardaecn, yala, Boyiliee, longlian | This paper introduces the Describe Anything Model (DAM) for generating detailed, localized captions for specific regions within images and videos. The primary objective is to overcome the limitations of existing Vision-Language Models (VLMs) in producing fine-grained descriptions for user-specified areas, addressing challenges like detail loss and data scarcity. Key methodologies include a 'focal prompt' for high-resolution region encoding and a 'localized vision backbone' to integrate local details with global context, alongside a Semi-supervised learning Data Pipeline (DLC-SDP) for data generation and a novel benchmark (DLC-Bench). DAM achieves state-of-the-art results on 7 benchmarks, including a 67.3% average accuracy on the proposed DLC-Bench, significantly outperforming previous models. For AI practitioners, DAM offers a robust approach for fine-grained, grounded visual understanding and description, applicable to both images and videos. |
| Reinforcement Learning | Learning Adaptive Parallel Reasoning with Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.15466) or [HuggingFace](https://huggingface.co/papers/2504.15466))| Charlie Snell, Long Lian, Jiayi Pan, yala, xiuyul | This paper introduces Adaptive Parallel Reasoning (APR), a framework enabling language models to dynamically orchestrate serial and parallel inference for complex reasoning tasks. The objective is to overcome limitations of existing methods like serialized chain-of-thought (latency, context limits) and uncoordinated parallel approaches (redundancy) by teaching models adaptive compute allocation. APR utilizes a parent-child threading mechanism with `spawn()` and `join()` operations, optimized end-to-end via reinforcement learning for task success without needing predefined reasoning structures. Key results on the Countdown task demonstrate higher performance within the same context window (83.4% vs. 60.0% at 4k context) and improved accuracy at equivalent latency compared to serialized methods. The main implication is that language models can learn to autonomously optimize their reasoning processes through adaptive computation allocation, enhancing scalability and efficiency. |
| Multi-Modal | IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning
  in Multimodal LLMs (Read more on [arXiv](https://arxiv.org/abs/2504.15415) or [HuggingFace](https://huggingface.co/papers/2504.15415))| Yifan Yao, Jarvis Guo, Yuanxing Zhang, JinChengRen, mdh98 | This paper introduces IV-Bench, the first benchmark specifically designed to evaluate Multimodal Large Language Models (MLLMs) on image-grounded video perception and reasoning tasks. The primary objective is to address the lack of benchmarks assessing how MLLMs leverage external static image context for video comprehension. IV-Bench consists of 967 videos and 2,585 image-text queries across 13 distinct tasks, using externally sourced images and a rigorous two-stage quality control process. Evaluations reveal that current state-of-the-art MLLMs significantly underperform, achieving a maximum overall accuracy of only 28.9%, indicating substantial limitations in these capabilities. The key implication for AI practitioners is the urgent need for developing more advanced methods beyond simple data format alignment to improve MLLMs' ability to integrate image context for deeper video understanding and reasoning. |
| Natural Language Processing | BookWorld: From Novels to Interactive Agent Societies for Creative Story
  Generation (Read more on [arXiv](https://arxiv.org/abs/2504.14538) or [HuggingFace](https://huggingface.co/papers/2504.14538))| Yanghua Xiao, Jiaqing Liang, Tian Qiu, Xintao Wang, Yiting Ran | This paper introduces BookWorld, a comprehensive system for constructing and simulating book-based multi-agent societies using large language models (LLMs) for creative story generation. The main objective is to simulate established fictional worlds and characters from novels, enabling interactive exploration while maintaining fidelity to the source material. BookWorld achieves this by extracting character profiles, worldview data, and maps from books to initialize role agents and a world agent, which then interact through simulated scenes, updating dynamic attributes like goals, states, and memory. Experiments show BookWorld generates creative, high-quality stories, surpassing previous methods with an overall win rate of 75.36% in evaluations. For AI practitioners, this work presents a methodology for grounding agent-based simulations in rich, existing narrative contexts, extending applications in story generation and interactive entertainment. |
| Natural Language Processing | Efficient Pretraining Length Scaling (Read more on [arXiv](https://arxiv.org/abs/2504.14992) or [HuggingFace](https://huggingface.co/papers/2504.14992))| Jianqiao Lu, Sijun Zhang, Shen Yan, Taoer, bongbohong | This paper introduces the Parallel Hidden Decoding Transformer (PHD-Transformer) framework for efficient length scaling during large language model pre-training while maintaining inference efficiency. The main research objective is to enable effective pre-training length scaling without the typical increases in KV cache size and inference latency associated with naive token repetition. The key methodology involves repeating input tokens but only retaining the KV cache for the original tokens, discarding the cache for repeated 'hidden decoding' tokens immediately after use, with optimized variants (PHD-SWA, PHD-CSWA) incorporating sliding window attention. Results demonstrate consistent improvements, with the PHD-CSWA-3-16-32 variant achieving an average 2.0% accuracy gain across multiple benchmarks on a 1.2B model compared to the baseline. The main implication for AI practitioners is the potential to enhance LLM reasoning and long-context capabilities through pre-training length scaling in a resource-efficient manner, particularly regarding KV cache memory and inference speed. |
| Computer Vision | CheXWorld: Exploring Image World Modeling for Radiograph Representation
  Learning (Read more on [arXiv](https://arxiv.org/abs/2504.13820) or [HuggingFace](https://huggingface.co/papers/2504.13820))| Shiji Song, Pan Liu, Chenxin Tao, Yulin Wang, yueyang2000 | CheXWorld introduces a self-supervised world model framework tailored for learning representations from radiographic images by encoding medical knowledge. The primary objective is to capture multiple dimensions of radiological expertise, specifically local anatomical details, global anatomical layouts, and robustness to domain variations (e.g., different scanners, hospitals). The methodology involves a unified framework integrating three corresponding world modeling tasks based on a joint-embedding predictive architecture, learning to predict target image features from context under spatial or appearance transformations. CheXWorld significantly outperforms existing self-supervised learning methods and medical foundation models on eight downstream classification and segmentation tasks, achieving 95.24±0.13 AUROC on VinDr-CXR classification. This indicates that incorporating domain-specific world modeling principles is a highly effective approach for pre-training medical foundation models. |
| Multi-Modal | Personalized Text-to-Image Generation with Auto-Regressive Models (Read more on [arXiv](https://arxiv.org/abs/2504.13162) or [HuggingFace](https://huggingface.co/papers/2504.13162))| Xihui Liu, Yao Teng, Xian Liu, Kaiyue Sun | This paper investigates the application of auto-regressive models for personalized text-to-image generation, traditionally dominated by diffusion models. The main objective is to adapt and optimize auto-regressive models for synthesizing images of specific subjects in novel contexts using few-shot reference images. The key methodology involves a two-stage training strategy: first optimizing text embeddings associated with a unique identifier for the subject, and then fine-tuning the transformer layers of the auto-regressive model (Lumina-mGPT 7B). Experiments on Dreambench show the method achieves comparable subject fidelity (DINO score: 0.671) and prompt following (CLIP-T score: 0.314) to leading diffusion-based methods like DreamBooth. The main implication is that auto-regressive models present a promising alternative architecture for personalized image generation tasks. |
| Multi-Modal | LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale (Read more on [arXiv](https://arxiv.org/abs/2504.16030) or [HuggingFace](https://huggingface.co/papers/2504.16030))| Zejun Ma, Wei Li, Yiqi Lin, Ziyun Zeng, Joya Chen | This paper introduces LiveCC, a video large language model (Video LLM) trained at scale using streaming automatic speech recognition (ASR) transcripts. The main objective is to explore a cost-effective method for training capable Video LLMs by leveraging cheap ASR data, enabling fine-grained temporal understanding and real-time applications like video commentary. The key methodology involves a novel streaming training approach that densely interleaves timestamped ASR words with corresponding video frames, using the custom-built Live-CC-5M pre-training and Live-WhisperX-526K fine-tuning datasets. The resulting LiveCC-7B-Instruct model surpasses larger 72B models on the novel LiveSports-3K commentary benchmark and achieves state-of-the-art results on VideoMME and OVOBench at the 7B/8B scale, generating commentary with less than 0.5 seconds latency per frame. For AI practitioners, this work demonstrates a scalable and economical pathway to developing high-performance, real-time Video LLMs by utilizing abundant ASR transcripts instead of expensive manual annotations or proprietary APIs. |
| Multi-Modal | Vidi: Large Multimodal Models for Video Understanding and Editing (Read more on [arXiv](https://arxiv.org/abs/2504.15681) or [HuggingFace](https://huggingface.co/papers/2504.15681))| Fan Chen, Chia-Wen Kuo, Celong Liu, Vidi Team, daviddousa | This paper introduces Vidi, a family of large multimodal models (LMMs) designed for video understanding and editing, with an initial focus on temporal retrieval from long videos. The main objective is to process hour-long, multimodal (vision, audio, text) inputs for tasks like identifying specific time ranges based on text queries, overcoming the limitations of traditional models on long-form content. Vidi employs modality-specific encoders, adapter layers, and an LLM with Decomposed Attention (D-Attn) for efficient long-sequence processing and multimodal alignment trained in stages using synthetic and real data. On the newly proposed VUE-TR benchmark, Vidi significantly outperforms baseline models like GPT-40 and Gemini, achieving an overall AUC of IoU of 35.4% compared to 21.2% for Gemini-2.0-Flash and 13.6% for GPT-40. For AI practitioners, Vidi demonstrates a viable approach for building LMMs capable of fine-grained, multimodal temporal understanding in hour-long videos, crucial for advanced video editing and retrieval applications. |
| Multi-Modal | From Reflection to Perfection: Scaling Inference-Time Optimization for
  Text-to-Image Diffusion Models via Reflection Tuning (Read more on [arXiv](https://arxiv.org/abs/2504.16080) or [HuggingFace](https://huggingface.co/papers/2504.16080))| Renrui Zhang, Yue Liao, Sayak Paul, Liangbing Zhao, Le Zhuo | This paper introduces ReflectionFlow, an inference-time optimization framework to enhance text-to-image diffusion models through iterative self-reflection and refinement. The primary objective is to improve image generation quality for complex prompts by effectively utilizing compute during inference, rather than solely scaling pre-trained models. The core methodology involves three scaling axes (noise-level, prompt-level, and reflection-level) enabled by a new large-scale dataset, GenRef (1M reflection triplets), and efficient reflection tuning of a diffusion transformer (FLUX.1-dev) to jointly process multimodal inputs. Key results demonstrate that ReflectionFlow significantly boosts performance, achieving a GenEval score of 0.91 with 32 samples, substantially outperforming the baseline FLUX.1-dev (0.67) and naive noise-scaling (0.85). The main implication for AI practitioners is that ReflectionFlow offers a scalable and compute-efficient inference-time strategy to achieve higher-quality image synthesis, particularly for challenging tasks, without needing extensive retraining. |
| Reinforcement Learning | LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making
  Abilities (Read more on [arXiv](https://arxiv.org/abs/2504.16078) or [HuggingFace](https://huggingface.co/papers/2504.16078))| Razvan Pascanu, Markus Wulfmeier, Jordi Grau-Moya, Jörg Bornschein, Thomas Schmied | This paper investigates the sub-optimal decision-making performance of Large Language Models (LLMs) when used as agents, proposing Reinforcement Learning Fine-Tuning (RLFT) as a mitigation strategy. The main objective is to systematically study prevalent failure modes, specifically greediness, frequency bias, and the knowing-doing gap, and assess the effectiveness of RLFT on self-generated Chain-of-Thought (CoT) rationales. The methodology involves analyzing Gemma2 models (2B, 9B, 27B) in multi-armed bandits, contextual bandits, and Tic-tac-toe environments, fine-tuning them using an RL objective on CoT, and evaluating classic and LLM-specific exploration mechanisms. Key results show that base LLMs exhibit significant greediness (e.g., 27B models cover only ~65% of actions in 10-arm MABs) and a knowing-doing gap (87% correct rationales vs. 58% greedy actions); RLFT increases exploration (e.g., +12% action coverage for 2B model after 30K updates), lowers regret, and narrows the knowing-doing gap. The main implication for AI practitioners is that while LLMs possess relevant knowledge, RLFT on CoT rationales, enhanced by appropriate exploration techniques and reward shaping, is crucial for improving their practical decision-making abilities as agents. |
| Machine Learning | WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World
  Model-based LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2504.15785) or [HuggingFace](https://huggingface.co/papers/2504.15785))| Deheng Ye, Guodong Long, Yijun Yang, Siyu Zhou, zhoutianyi | This paper presents WALL-E 2.0, a neurosymbolic framework enhancing LLM-based agents by aligning their world models with environment dynamics without reinforcement learning. The main objective is to bridge the gap between LLMs' prior knowledge and specific environmental rules by learning complementary symbolic knowledge. The methodology involves using LLMs to extract action rules, knowledge graphs, and scene graphs from trajectories, converting them into executable code rules that regulate an LLM world model within a Model-Predictive Control (MPC) loop. Key results show significant performance gains, surpassing baselines in the Mars environment by 16.1%-51.6% in success rate and achieving a 98% success rate in ALFWorld after four iterations. For AI practitioners, this implies that explicitly aligning LLM world models with environment dynamics using learned symbolic knowledge offers an efficient path to more reliable and adaptable agents compared to relying solely on pre-training or costly fine-tuning. |
| Multi-Modal | MR. Video: "MapReduce" is the Principle for Long Video Understanding (Read more on [arXiv](https://arxiv.org/abs/2504.16082) or [HuggingFace](https://huggingface.co/papers/2504.16082))| Yu-Xiong Wang, Ziqi Pang | This paper introduces MR. Video, an agentic framework applying the MapReduce principle for effective long video understanding. The main objective is to overcome the context length limitations of sequence-to-sequence Vision-Language Models (VLMs) and the sequential processing bottlenecks of existing video agents. MR. Video implements a two-stage MapReduce workflow: first generating dense captions (Map) and standardizing entities (Reduce), then analyzing scene relevance per question (Map) and integrating results for a final answer (Reduce), using LLM agents. The method achieves a significant >10% accuracy improvement, reaching 60.8% accuracy on the challenging LVBench benchmark over state-of-the-art VLMs and video agents. This suggests the MapReduce principle offers a scalable and effective paradigm for AI practitioners developing systems for long video comprehension, applicable to both VLMs and agent-based approaches. |
| Machine Learning | Progent: Programmable Privilege Control for LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2504.11703) or [HuggingFace](https://huggingface.co/papers/2504.11703))| Hongwei Li, Linyu Wu, Zhun Wang, Jingxuan He, stneng | Progent introduces a programmable privilege control framework designed to enhance the security of LLM agents by enforcing the principle of least privilege for tool usage. The primary objective is to mitigate risks associated with LLM agents executing dangerous actions by allowing only necessary tool calls while blocking potentially malicious ones. The core methodology involves a domain-specific language for expressing fine-grained policies, deterministic policy enforcement during agent execution, and leveraging LLMs for automated policy generation and dynamic updates. Evaluation demonstrates significant security improvements, reducing attack success rates on the AgentDojo benchmark from 41.2% to 2.2% while preserving utility. For AI practitioners, Progent offers a modular, system-level security solution requiring minimal changes to existing agent implementations to manage tool permissions effectively. |
| Computer Vision | RealisDance-DiT: Simple yet Strong Baseline towards Controllable
  Character Animation in the Wild (Read more on [arXiv](https://arxiv.org/abs/2504.14977) or [HuggingFace](https://huggingface.co/papers/2504.14977))| Chao Fan, Min Wei, Shikai Li, Yifan Wu, Jingkai Zhou | RealisDance-DiT introduces a strong baseline for controllable character animation in diverse open-world scenarios by minimally modifying the Wan-2.1 video foundation model. The research objective is to demonstrate that powerful foundation models, with appropriate fine-tuning, can handle challenging animation tasks (rare poses, interactions, dynamic scenes) better than methods relying on complex, explicit guidance networks like Reference Nets. The methodology involves simple architectural tweaks to the DiT model (e.g., condition input layers, modified RoPE) and employs specific fine-tuning strategies: low-noise warmup and large-batch/small-iteration training to preserve priors and speed up convergence. RealisDance-DiT significantly outperforms prior methods, achieving state-of-the-art FVD (563.28) and FID (24.79) scores on the challenging RealisDance-Val benchmark. This work implies that leveraging and effectively fine-tuning large foundation models is a highly promising direction for complex generative tasks, potentially simplifying model design compared to building elaborate custom architectures. |
| Natural Language Processing | IPBench: Benchmarking the Knowledge of Large Language Models in
  Intellectual Property (Read more on [arXiv](https://arxiv.org/abs/2504.15524) or [HuggingFace](https://huggingface.co/papers/2504.15524))| Minghui Zhu, Huaren Liu, Hongbo Wang, Guhong Chen, QiYao-Wang | This paper introduces IPBench, a comprehensive, large, and diverse bilingual benchmark designed to evaluate the knowledge of Large Language Models (LLMs) in the complex domain of Intellectual Property (IP). The main objective is to bridge the gap between LLM capabilities and real-world IP demands by providing a benchmark that covers 8 IP mechanisms and 20 tasks across both understanding and generation. Methodologically, the authors developed a hierarchical IP task taxonomy based on Webb's Depth of Knowledge theory and curated a dataset (IPBench) of 10,374 data points, subsequently benchmarking 16 different LLMs. The results show substantial room for improvement, with the best-performing LLM (DeepSeek-V3) achieving only 75.8% accuracy, and domain-specific models lagging behind general-purpose closed-source ones. For AI practitioners, IPBench offers a valuable framework and dataset for assessing LLM performance in specialized, knowledge-intensive fields like IP, highlighting current limitations and guiding future development towards better domain adaptation. |
| Multi-Modal | CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via
  Occluded Object Counting (Read more on [arXiv](https://arxiv.org/abs/2504.15485) or [HuggingFace](https://huggingface.co/papers/2504.15485))| Mohit Bansal, Jaemin Cho, Elias Stengel-Eskin, Atin Pothiraj | This paper introduces CAPTURE, a novel benchmark to evaluate the spatial reasoning and world modeling capabilities of Vision Language Models (VLMs) through amodal counting of occluded objects arranged in patterns. The main objective is to quantify VLMs' ability to infer and count objects hidden behind occluders by completing visual patterns, a key aspect of spatial understanding. The methodology involves presenting VLMs with real (CAPTUREreal) and synthetic (CAPTURESynthetic) images containing patterned objects partially obscured by a box and asking them to count the total number, assuming the pattern continues. Key results show that strong VLMs like GPT-4o struggle, performing significantly worse on occluded images compared to unoccluded ones (e.g., average VLM sMAPE increased by 6.28% on CAPTUREreal with occlusion), while humans perform the task with very low error (3.79% sMAPE on a real occluded subset). The main implication is that current VLMs exhibit deficiencies in handling occlusion and performing spatial reasoning requiring pattern inference, indicating a need for improved world modeling capabilities beyond simple object recognition or text-based reasoning. |
| Machine Learning | DiffVox: A Differentiable Model for Capturing and Analysing Professional
  Effects Distributions (Read more on [arXiv](https://arxiv.org/abs/2504.14735) or [HuggingFace](https://huggingface.co/papers/2504.14735))| Wei-Hsiang Liao, Ben Hayes, Junghyun Koo, Marco A. Martínez-Ramírez, yoyolicoris | This paper introduces DiffVox, a differentiable model designed to capture and analyse the parameter distributions of professional vocal effects in music production. The main objective is to reverse-engineer realistic vocal processing chains by matching dry vocal inputs to professionally processed outputs. DiffVox achieves this using gradient-based optimisation on efficient, differentiable implementations of parametric equalisation (PEQ), dynamic range control, delay, and feedback delay network (FDN) reverb, leveraging multi-resolution spectral (MRS) and loudness dynamic range (MLDR) loss functions. Key results show DiffVox effectively matches target audio (e.g., achieving MRS/MLDR scores of 0.76/0.34 on the Internal dataset's left/right channels) and reveals that principal components of the learned parameter distributions correlate with perceptual dimensions like spaciousness and spectral brightness. For AI practitioners, this work provides a methodology and a dataset of vocal presets, offering insights into audio effects priors crucial for developing more realistic automatic mixing tools and generative audio models. |
