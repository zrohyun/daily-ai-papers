

## Papers for 2025-04-21

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Reinforcement Learning | Does Reinforcement Learning Really Incentivize Reasoning Capacity in
  LLMs Beyond the Base Model? (Read more on [arXiv](https://arxiv.org/abs/2504.13837) or [HuggingFace](https://huggingface.co/papers/2504.13837))| Zhaokai Wang, Andrew Zhao, Rui Lu, Zhiqi Chen, Yang Yue | This paper critically evaluates whether Reinforcement Learning with Verifiable Rewards (RLVR) actually introduces novel reasoning capabilities to Large Language Models (LLMs) beyond those already present in their base models. The primary research objective is to determine if RLVR training elicits fundamentally new reasoning patterns or merely improves sampling efficiency. The study employs the pass@k metric with large k values (up to 256 or more) across various LLM families, RL algorithms, and benchmarks (mathematics, coding, visual reasoning), complemented by perplexity analysis and manual CoT verification. Key findings reveal that while RL-trained models excel at low k (pass@1), base models often achieve comparable or even superior pass@k scores at large k (e.g., on Minerva with a 32B model, the base model outperformed the RL model by ~9% at k=128), suggesting RLVR biases sampling towards known correct paths rather than expanding the reasoning boundary. The main implication is that RLVR, in its current form, primarily enhances sampling efficiency at the cost of exploration capacity and does not fundamentally increase the reasoning potential beyond the base model, unlike methods such as distillation. |
| Natural Language Processing | MIG: Automatic Data Selection for Instruction Tuning by Maximizing
  Information Gain in Semantic Space (Read more on [arXiv](https://arxiv.org/abs/2504.13835) or [HuggingFace](https://huggingface.co/papers/2504.13835))| Haochen Ye, Zerun Ma, Kai Hu, Yining Li, Yicheng Chen | This paper introduces MIG (Maximize Information Gain), a novel method for automatic data selection for instruction tuning Large Language Models by maximizing information gain in semantic space. The primary objective is to overcome limitations of heuristic-based diversity methods by quantifying dataset information content, balancing quality and diversity through a unified approach. MIG models the semantic space as a label graph, measures information using an upper-convex function considering data quality and label distribution, propagates information semantically, and employs an efficient greedy algorithm for iterative data selection. Experiments show MIG consistently outperforms baselines; for instance, using Llama3.1-8B on 5% of the Tulu3 dataset selected by MIG yielded average improvements of +1.49% on knowledge benchmarks and +1.96% on human-preference benchmarks over the previous state-of-the-art selection methods. The main implication is that MIG offers an effective and efficient automated approach for curating high-quality, diverse instruction-tuning datasets from large pools, potentially improving model performance while reducing data requirements. |
| Natural Language Processing | Could Thinking Multilingually Empower LLM Reasoning? (Read more on [arXiv](https://arxiv.org/abs/2504.11833) or [HuggingFace](https://huggingface.co/papers/2504.11833))| Lei Li, Shujian Huang, Wenhao Zhu, Xu Huang, Changjiang Gao | This paper explores the potential of multilingual reasoning to enhance the performance of Large Language Models (LLMs). The study aims to quantify the potential gain from multilingual thinking and compare it to English-only reasoning by aggregating responses to translated parallel inputs. The methodology involves evaluating LLMs on GPQA and MGSM datasets using the Acc@k metric. Results show that multilingual reasoning can ideally boost GPQA accuracy from ~45 to ~90 and MGSM from ~90 to ~100. The paper implies that harnessing multilingualism can significantly improve LLM reasoning capabilities, although common answer selection methods require refinement to achieve this potential. |
| Computer Vision | AerialMegaDepth: Learning Aerial-Ground Reconstruction and View
  Synthesis (Read more on [arXiv](https://arxiv.org/abs/2504.13157) or [HuggingFace](https://huggingface.co/papers/2504.13157))| Shubham Tulsiani, Srinivasa Narasimhan, Deva Ramanan, Anurag Ghosh, kvuong2711 | This paper introduces AerialMegaDepth, a novel framework and hybrid dataset designed to improve 3D reconstruction and view synthesis from mixed aerial and ground-level images. The primary objective is to address the failure of current learning-based methods in handling extreme viewpoint variations between aerial and ground views due to a lack of suitable training data. The key methodology involves creating a large-scale dataset by combining pseudo-synthetic renderings from 3D city meshes (like Google Earth) with co-registered real ground-level images (from MegaDepth). Fine-tuning state-of-the-art models like DUSt3R on this hybrid dataset demonstrates significant improvements; for instance, pairwise camera registration accuracy (RRA@5Â°) for ground-aerial pairs increased from under 5% to nearly 56%. The main implication for AI practitioners is the provision of a scalable data generation approach and a resulting dataset that enables robust training for challenging cross-view geometric tasks previously hindered by data limitations. |
| Multi-Modal | HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation (Read more on [arXiv](https://arxiv.org/abs/2504.13072) or [HuggingFace](https://huggingface.co/papers/2504.13072))| Tao Hu, Yuan Li, Zesong Yang, Bangbang Yang, Wenqi Dong | HiScene introduces a hierarchical framework to generate high-fidelity, compositional 3D scenes by leveraging isometric views, bridging 2D image and 3D object generation. The main objective is to create editable 3D scenes with natural layouts, complete object instances, and spatial plausibility, addressing limitations in prior work regarding object separability and realism. The methodology involves initializing a 3D scene from an isometric view, performing hierarchical parsing using 3D segmentation, employing a novel video-diffusion-based amodal completion to handle occlusions and shadows, and injecting shape priors for spatially aligned object regeneration. Results show superior performance against methods like GALA3D in scene quality (Aesthetic Score: 5.46) and state-of-the-art zero-shot amodal completion (mIoU: 83.84 on COCO-A). This work provides AI practitioners with a pathway to generate more complex, editable, and realistic 3D environments suitable for interactive applications by combining 2D generation priors with structured 3D synthesis. |
| Natural Language Processing | NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes (Read more on [arXiv](https://arxiv.org/abs/2504.11544) or [HuggingFace](https://huggingface.co/papers/2504.11544))| Yixin Liu, Haoxiang Chen, Chengze Li, Haojie Zheng, Tianyang Xu | NodeRAG introduces a graph-centric framework for Retrieval-Augmented Generation (RAG) that emphasizes optimizing heterogeneous graph structures to improve performance. The main objective is to address the limitations of existing graph-based RAG methods, which often suffer from inadequately designed graph structures leading to inefficiencies and inconsistencies. The key methodology involves constructing a heterogeneous graph with seven distinct node types (Entity, Relationship, Semantic Unit, Attribute, High-level Element, High-level Overview, Text), applying graph decomposition, importance-based and community-based augmentation, graph enrichment with text nodes and HNSW edges, and employing a dual search mechanism with shallow Personalized PageRank (PPR) for retrieval. Experiments show NodeRAG achieves higher accuracy with fewer tokens, for instance, attaining 46.29% accuracy on MuSiQue compared to 41.71% for GraphRAG, while using fewer retrieval tokens across benchmarks. The main implication for AI practitioners is the critical importance of deliberate graph structure design in graph-based RAG systems to significantly enhance retrieval effectiveness, efficiency, and overall performance. |
| Computer Vision | Tokenize Image Patches: Global Context Fusion for Effective Haze Removal
  in Large Images (Read more on [arXiv](https://arxiv.org/abs/2504.09621) or [HuggingFace](https://huggingface.co/papers/2504.09621))| Kaiqi Li, Qizhi Xu, Jiuchen Chen, fengyanzi | This paper introduces DehazeXL, a memory-efficient method for removing haze from large, high-resolution images by effectively fusing global context with local features. The research aims to overcome GPU memory limitations that force traditional methods into suboptimal slicing or downsampling for large image dehazing. DehazeXL employs a patch tokenization strategy using a Swin Transformer encoder, injects global context via an efficient Hyper Attention bottleneck, and reconstructs the image with a corresponding decoder, processing patches sequentially to manage memory. Quantitatively, DehazeXL achieves state-of-the-art performance with a PSNR of 32.35 and SSIM of 0.9863 on the introduced 8KDehaze dataset while handling images up to 10240x10240 pixels with 21GB memory. This work provides practitioners with a method to apply high-fidelity dehazing to ultra-large images on constrained hardware, demonstrating the feasibility of global context modeling without quadratic memory scaling. |
| Natural Language Processing | Thought Manipulation: External Thought Can Be Efficient for Large
  Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2504.13626) or [HuggingFace](https://huggingface.co/papers/2504.13626))| Wenhan Dong, Zifan Peng, Zhen Sun, Jingyi Zheng, Yule Liu | This paper introduces ThoughtMani, a training-free pipeline to enhance the inference efficiency of Large Reasoning Models (LRMs) by leveraging externally generated thoughts from smaller models. The primary objective is to mitigate the 'overthinking' problem, thereby reducing computational costs associated with redundant reasoning steps without requiring model fine-tuning. ThoughtMani strategically inserts a concise Chain-of-Thought (CoT), generated by a smaller model, within the LRM's designated thinking tokens (`<think>`/`</think>`) to guide its reasoning process. Experiments demonstrate significant efficiency gains; for instance, applying ThoughtMani to QwQ-32B on the LiveBench/Code dataset reduced output tokens by approximately 30% while largely maintaining performance and improving safety alignment by an average of 10%. The main implication is providing practitioners with a practical, low-overhead method to make powerful LRMs more computationally efficient and accessible for real-world applications. |
| Machine Learning | It's All Connected: A Journey Through Test-Time Memorization,
  Attentional Bias, Retention, and Online Optimization (Read more on [arXiv](https://arxiv.org/abs/2504.13173) or [HuggingFace](https://huggingface.co/papers/2504.13173))| Vahab Mirrokni, Peilin Zhong, Meisam Razaviyayn, Ali Behrouz | This paper introduces MIRAS, a general framework reconceptualizing sequence models like Transformers and linear RNNs as associative memories learning through an internal objective termed "attentional bias." The primary objective is to establish a unified design methodology by analyzing models based on their memory architecture, attentional bias, retention mechanism (replacing traditional forget gates), and learning algorithm. MIRAS enables the creation of novel architectures (MONETA, YAAD, MEMORA) employing alternative biases (lp-loss, Huber) and retention strategies (Lq regularization, KL divergence) beyond standard L2 regression and gating. Experiments show these variants outperform strong baselines on language modeling (e.g., YAAD 1.3B achieves 15.18 Wiki ppl), common-sense reasoning, and long-context recall tasks, scaling effectively with context length. For practitioners, MIRAS offers a principled way to design specialized sequence architectures by systematically choosing and potentially customizing components for memory, learning objectives, and retention based on task needs. |
