

## Papers for 2025-04-03

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | MergeVQ: A Unified Framework for Visual Generation and Representation
  with Disentangled Token Merging and Quantization (Read more on [arXiv](https://arxiv.org/abs/2504.00999) or [HuggingFace](https://huggingface.co/papers/2504.00999))| Cheng Tan, Juanxi, ZedongWangAI, LuyuanZhang01, Lupin1998 | This paper introduces MergeVQ, a unified framework integrating token merging and quantization to jointly optimize visual generation and representation learning within a single architecture. It aims to resolve the inherent trade-off between generation fidelity, representation discrimination, and computational efficiency observed in traditional VQ-based autoregressive models. MergeVQ employs token merging (ToMe) and Look-up Free Quantization (LFQ) to disentangle semantics from spatial details during encoding, recovering fine-grained details via a source matrix and cross-attention, while introducing MergeAR with KV Cache compression for efficient generation. Experiments demonstrate strong performance, with the representation-focused variant MergeVQ (R) achieving 79.8% linear probe accuracy on ImageNet with only 36 tokens, and generative variants achieving competitive image generation results (e.g., gFID of 2.63 for G+R with RandAR). MergeVQ offers practitioners an efficient method to build models proficient in both visual generation and representation tasks, mitigating the need for separate, specialized architectures. |
| Multi-Modal | Improved Visual-Spatial Reasoning via R1-Zero-Like Training (Read more on [arXiv](https://arxiv.org/abs/2504.00883) or [HuggingFace](https://huggingface.co/papers/2504.00883))| Zijian Kong, Yanhao Zhang, Qingsong Xie, Zhenyi Liao, zhijie3 | This paper improves the visual-spatial reasoning capabilities of Multimodal Large Language Models (MLLMs) using an R1-Zero-like training approach. The research objective was to investigate and enhance the visual-spatial intelligence (VSI) of small- to medium-sized Qwen2-VL models, as initial tests showed Chain-of-Thought (CoT) prompting was ineffective. The key methodology involved constructing a new dataset, VSI-100k, derived from ScanNet, and applying Group Relative Policy Optimization (GRPO) training, incorporating rule-based rewards and a necessary KL penalty. The primary result showed the fine-tuned vsGRPO-2B model outperformed the base Qwen2-VL-2B model by 12.1% on VSI-bench and surpassed GPT-4o. The main implication for AI practitioners is that targeted GRPO training can effectively instill specific reasoning abilities like VSI into MLLMs, offering a viable enhancement strategy beyond simple prompting, while noting the crucial role of the KL penalty during such RL training. |
| Multi-Modal | AnimeGamer: Infinite Anime Life Simulation with Next Game State
  Prediction (Read more on [arXiv](https://arxiv.org/abs/2504.01014) or [HuggingFace](https://huggingface.co/papers/2504.01014))| Ying Shan, Jing Liao, Yixiao Ge, Yuying Ge, Howe666 | This paper introduces AnimeGamer, a system for creating infinite anime life simulations where users interact via open-ended language instructions. The primary objective is to generate consistent multi-turn game states, comprising dynamic animation shots and character state updates, overcoming the limitations of prior work that neglected visual context and dynamics. The core methodology involves using a Multimodal Large Language Model (MLLM) to predict action-aware multimodal representations for animation shots based on historical context and instructions, which are then decoded into video clips using a diffusion model, alongside predicting character state changes. Evaluations show AnimeGamer significantly outperforms baselines in contextual consistency, dynamics, and character consistency (e.g., achieving a CLIP-I score of 0.8132 and an overall GPT-4V judgment score of 8.36). For AI practitioners, this work presents a novel MLLM-based framework for generating dynamic, context-aware interactive simulations, integrating language instructions with consistent visual and stateful outputs. |
| Computer Vision | VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in
  One Step (Read more on [arXiv](https://arxiv.org/abs/2504.01956) or [HuggingFace](https://huggingface.co/papers/2504.01956))| Yueqi Duan, Jiawei Chi, Fangfu Liu, hanyang-21 | VideoScene introduces a method for efficiently generating 3D scenes in a single step from sparse (two) input views by distilling video diffusion models. The primary objective is to overcome the slow inference time and lack of explicit 3D constraints in existing video diffusion approaches for 3D reconstruction. Key methodologies include a 3D-aware leap flow distillation strategy, which uses a coarse 3D geometry prior (from MVSplat) to skip early diffusion steps, and a dynamic denoising policy network (DDPNet) to adaptively select optimal leap timesteps. Experiments show VideoScene achieves superior results, e.g., a Fr√©chet Video Distance (FVD) of 103.42 in one step on RealEstate10K, significantly outperforming baselines and approaching the quality of 50-step methods while being much faster (~3s inference). For AI practitioners, this work presents a fast and effective approach to leverage powerful video priors for consistent sparse-view 3D scene generation, bridging the gap between 2D video models and 3D applications. |
| Reinforcement Learning | Understanding R1-Zero-Like Training: A Critical Perspective (Read more on [arXiv](https://arxiv.org/abs/2503.20783) or [HuggingFace](https://huggingface.co/papers/2503.20783))| Tianyu Pang, Wenjun Li, QPHutu, Cameron-Chen, lkevinzc | This paper critically examines the R1-Zero-like training paradigm for large language models (LLMs) by analyzing its core components: base models and reinforcement learning (RL) algorithms. The primary objective is to understand how pretraining characteristics influence RL performance and identify biases within the RL optimization process, specifically Group Relative Policy Optimization (GRPO). The methodology involves evaluating various base models (e.g., Qwen2.5, DeepSeek-V3-Base) under different prompting conditions and analyzing the GRPO algorithm, leading to the proposal of an unbiased variant, Dr. GRPO. Key findings include that certain base models possess strong initial reasoning capabilities, GRPO introduces biases artificially increasing response length, and the proposed Dr. GRPO improves token efficiency while achieving state-of-the-art results (e.g., 43.3% on AIME 2024 with a 7B model). The main implication is that both base model selection (due to potential pretraining biases) and the choice of an unbiased RL optimizer are crucial for effective and efficient R1-Zero-like training. |
| Computer Vision | DreamActor-M1: Holistic, Expressive and Robust Human Image Animation
  with Hybrid Guidance (Read more on [arXiv](https://arxiv.org/abs/2504.01724) or [HuggingFace](https://huggingface.co/papers/2504.01724))| Tianshu Hu, Longhao Zhang, Lizhen Wang, Zhengkun Rong, Yuxuan Luo | DreamActor-M1 introduces a DiT-based framework for holistic, expressive, and robust human image animation leveraging hybrid guidance. The main objective is to overcome limitations in fine-grained control, multi-scale adaptability (portraits to full-body), and long-term temporal coherence found in previous methods. Key methodologies include utilizing a Diffusion Transformer (DiT) with hybrid motion control signals (implicit facial latents, 3D head spheres, 3D body skeletons with bone length adjustment) and complementary appearance guidance to handle unseen regions during long-term synthesis, trained via a progressive strategy. Experiments demonstrate state-of-the-art performance, achieving an FVD of 122.0 on body animation tasks, significantly lower than competing methods. For AI practitioners, this work provides an advanced technique for generating high-fidelity, controllable human animations across various scales from reference images, although limitations like dynamic camera control persist. |
| Machine Learning | PaperBench: Evaluating AI's Ability to Replicate AI Research (Read more on [arXiv](https://arxiv.org/abs/2504.01848) or [HuggingFace](https://huggingface.co/papers/2504.01848))| Jun Shern Chan, James Aung, Dane Sherburn, Oliver Jaffe, Giulio Starace | This paper introduces PaperBench, a benchmark for evaluating the ability of AI agents to replicate state-of-the-art AI research papers from scratch. The primary objective is to assess agent capabilities in understanding paper contributions, developing codebases, and successfully executing experiments for recent machine learning research. The methodology involves agents replicating 20 ICML 2024 papers, evaluated against detailed, hierarchical rubrics co-developed with paper authors, using an automated LLM-based judge for grading. The best-performing agent tested, Claude 3.5 Sonnet with open-source scaffolding, achieved an average replication score of 21.0%, significantly lower than a human baseline of 41.4% on a subset, indicating non-trivial but limited capability. PaperBench offers AI practitioners a standardized way to measure and track the progress of AI autonomy in complex ML research and development tasks. |
| Natural Language Processing | ScholarCopilot: Training Large Language Models for Academic Writing with
  Accurate Citations (Read more on [arXiv](https://arxiv.org/abs/2504.00824) or [HuggingFace](https://huggingface.co/papers/2504.00824))| Zhiheng Lyu, Huaye Zeng, Ping Nie, Xueguang Ma, Yubo Wang | ScholarCopilot introduces a unified framework for training large language models to generate academic text with accurate, dynamically retrieved citations. The primary objective is to overcome limitations of traditional Retrieval-Augmented Generation (RAG) by integrating citation retrieval directly into the generation process. The methodology involves jointly optimizing text generation and citation retrieval using special retrieval tokens ([RET]) and contrastive learning on a dataset of 500K arXiv papers. Key results include achieving 40.1% top-1 retrieval accuracy, significantly outperforming baselines like E5-Mistral-7B-Instruct (15.0%), and demonstrating superior generation quality (16.2/25) compared to larger models. For practitioners, ScholarCopilot offers a more effective approach for building AI writing assistants that produce coherent academic text with contextually relevant and accurate citations. |
| Multi-Modal | ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and
  Diffusion Refinement (Read more on [arXiv](https://arxiv.org/abs/2504.01934) or [HuggingFace](https://huggingface.co/papers/2504.01934))| Yunlong Yuan, Guansong Lu, Junwei Yang, Chunwei Wang, Runhui Huang | This paper introduces ILLUME+, an enhanced unified Multimodal Large Language Model (MLLM) designed for advanced visual understanding, generation, and editing across various resolutions. The primary objective is to overcome limitations of prior unified models, particularly poor texture preservation in editing and semantic gaps in understanding, by creating a single architecture proficient in all three core visual tasks. Key methods include a novel Dual Vision Tokenizer (DualViTok) preserving both semantics and texture, a unified MLLM architecture employing a continuous-input/discrete-output scheme with coarse-to-fine image representation, and an optional diffusion decoder for refinement and super-resolution. ILLUME+ (3B) achieves competitive results, notably a 6.00 FID score on MJHQ-30K image generation (with diffusion), strong performance on document VQA (e.g., 80.8 on DocVQA), and improved texture handling in editing tasks compared to its predecessor. For AI practitioners, ILLUME+ demonstrates a viable path towards building more versatile and integrated multimodal systems capable of complex, context-aware visual reasoning and manipulation within one model. |
| Multi-Modal | Towards Physically Plausible Video Generation via VLM Planning (Read more on [arXiv](https://arxiv.org/abs/2503.23368) or [HuggingFace](https://huggingface.co/papers/2503.23368))| Lei Bai, Zhenfei Yin, Yiming Zhang, Baolu Li, Xindi Yang | This paper presents a two-stage framework to generate physically plausible videos by integrating explicit physics reasoning via VLM planning. The main research objective is to address the failure of standard Video Diffusion Models (VDMs) in understanding and reproducing physical laws, aiming to generate videos with correct dynamics and event sequences. The core methodology involves using a Vision Language Model (VLM) with chain-of-thought and physics-aware reasoning as a coarse motion planner to predict rough trajectories, which then guide a fine-level VDM synthesizer (Go-with-the-Flow) with noise injection for detailed motion generation. The framework demonstrates superior performance, achieving a 0.60 average score on the PhyGenBench benchmark, outperforming the best compared image-to-video model by 11.1%. This work suggests that leveraging VLMs for explicit planning offers a viable path for AI practitioners to significantly improve the physical realism of video generation models. |
| Multi-Modal | Articulated Kinematics Distillation from Video Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2504.01204) or [HuggingFace](https://huggingface.co/papers/2504.01204))| Chenfanfu Jiang, Yongxin Chen, Tsung-Yi Lin, Qianli Ma, Xuan Li | Articulated Kinematics Distillation (AKD) presents a framework for generating articulated 3D character animations by distilling motion from video diffusion models. The primary objective is to synthesize realistic and controllable motion from text prompts onto rigged assets, improving 3D shape consistency compared to prior neural deformation field methods. AKD utilizes a low-Degree-of-Freedom (DoF) skeleton representation with 3D Gaussian Splatting, optimizing joint angles over time using Score Distillation Sampling (SDS) from a pre-trained video diffusion model, incorporating ground rendering and physical regularizers. Quantitative results demonstrate AKD surpasses the TC4D baseline, achieving higher automated VideoPhy scores (SA: 0.81 vs 0.40; PC: 0.39 vs 0.31) and slight user preference in evaluations. For AI practitioners, AKD offers a method to create structurally consistent and physically plausible 3D animations from text by integrating articulated kinematics with large video model priors. |
| Multi-Modal | Safeguarding Vision-Language Models: Mitigating Vulnerabilities to
  Gaussian Noise in Perturbation-based Attacks (Read more on [arXiv](https://arxiv.org/abs/2504.01308) or [HuggingFace](https://huggingface.co/papers/2504.01308))| Zhendong Liu, Yushen Zuo, sofyc, AllenChai, Jarvis1111 | This paper investigates the vulnerability of Vision-Language Models (VLMs) to Gaussian noise perturbations and proposes mitigation strategies. The primary objective is to analyze VLM robustness against noise, particularly simple Gaussian noise and optimization-based attacks, and develop methods to enhance safety alignment without sacrificing helpfulness. Key methodologies include introducing the Robust-VLGuard dataset combined with noise-augmented safety fine-tuning, and proposing DiffPure-VLM, which leverages diffusion models to purify adversarial inputs before feeding them to the noise-tolerant VLM. Experimental results show that the proposed DiffPure-VLM framework significantly reduces attack success rates; for instance, on InternVL2-8B against adversarial attacks (Œµ=32/255), the success rate dropped from 70.6% to 33.4% using DiffPure (t*=50). The main implication for AI practitioners is the highlighted necessity of incorporating noise robustness during VLM training and deployment, offering practical defense pipelines against perturbation-based attacks. |
| Computer Vision | Boost Your Own Human Image Generation Model via Direct Preference
  Optimization with AI Feedback (Read more on [arXiv](https://arxiv.org/abs/2405.20216) or [HuggingFace](https://huggingface.co/papers/2405.20216))| Hyunjoon Lee, Yonggyu Kim, sanghyeonna | This paper introduces HG-DPO, a novel method enhancing human image generation realism by applying Direct Preference Optimization (DPO) with real images. The main objective is to improve the fidelity of generated human images by overcoming inaccuracies in anatomy, pose, and fine details common in diffusion models. The key methodology involves using real images as preferred (winning) samples and generated images as non-preferred (losing) ones within the DPO framework, integrated with a three-stage curriculum learning pipeline (easy, normal, hard) to bridge the domain gap and a statistics matching loss to prevent artifacts. HG-DPO significantly outperforms previous methods, achieving superior quantitative results such as an FID of 29.41 and a CI-S score of 0.9858. For AI practitioners, the main implication is that HG-DPO provides an effective technique to boost the realism of human image generation and readily adapts to personalized text-to-image generation tasks without additional training. |
| Multi-Modal | DASH: Detection and Assessment of Systematic Hallucinations of VLMs (Read more on [arXiv](https://arxiv.org/abs/2503.23573) or [HuggingFace](https://huggingface.co/papers/2503.23573))| Matthias Hein, Maximilian Augustin, YanNeu | This paper introduces DASH, an automated pipeline to detect and assess systematic object hallucinations in Vision-Language Models (VLMs) on large-scale, real-world image datasets. The primary objective is to identify clusters of semantically similar images that consistently trigger false-positive object hallucinations in VLMs, addressing the limitations of small, curated benchmarks. Key methodologies include DASH-LLM, using LLM-generated text queries, and DASH-OPT, optimizing a diffusion model to generate hallucination-inducing images, both followed by kNN retrieval on ReLAION-5B and clustering. Applying DASH to PaliGemma and two LLaVA-NeXT models across 380 object classes yielded over 19k clusters comprising more than 950k hallucination images, demonstrating significant transferability to other VLMs and mitigation potential through fine-tuning (+11.6% accuracy increase on their DASH-B benchmark for PaliGemma). The main implication is that systematic object hallucinations persist significantly in VLMs in open-world settings, necessitating more robust evaluation methods like the proposed DASH-B benchmark. |
| Computer Vision | LSNet: See Large, Focus Small (Read more on [arXiv](https://arxiv.org/abs/2503.23135) or [HuggingFace](https://huggingface.co/papers/2503.23135))| Guiguang Ding, Jungong Han, Zijia Lin, Hui Chen, jameslahm | This paper introduces LSNet, a novel family of lightweight vision networks inspired by the human visual system's dynamic heteroscale processing ability. The primary objective is to overcome the performance-efficiency trade-off limitations in existing lightweight models by designing a more effective token mixing mechanism. The core methodology is the proposed LS (Large-Small) convolution, which combines large-kernel static convolution for broad perception ("See Large") with small-kernel dynamic convolution for precise, adaptive feature aggregation ("Focus Small"). Extensive experiments show LSNet achieves superior performance; for instance, LSNet-B obtains 80.3% top-1 accuracy on ImageNet-1K with 1.3G FLOPs, surpassing comparable models in both accuracy and efficiency. For AI practitioners, LSNet offers a strong, efficient baseline architecture for vision tasks on resource-constrained platforms, demonstrating the value of bio-inspired design principles. |
| Natural Language Processing | VerifiAgent: a Unified Verification Agent in Language Model Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.00406) or [HuggingFace](https://huggingface.co/papers/2504.00406))| Ehsan Shareghi, Wray Buntine, Jiuzhou Han | This paper introduces VerifiAgent, a unified framework designed to verify and enhance the reasoning outputs of large language models (LLMs). The primary objective is to overcome the limitations of existing verification methods, which are often domain-restricted or computationally expensive, by providing a generalisable and efficient solution. VerifiAgent employs a two-layer mechanism: meta-verification for assessing response completeness and consistency, followed by tool-based adaptive verification that autonomously selects tools (like Python interpreters or search engines) based on the reasoning task type. Experimental results show VerifiAgent outperforms baseline verifiers across various reasoning tasks, achieving higher accuracy (e.g., 0.96 accuracy on GSM8K) and improving inference scaling performance compared to methods like Majority Vote and PRMs with fewer samples. For AI practitioners, VerifiAgent offers a training-free, adaptable agent to improve the reliability and accuracy of LLM reasoning across diverse applications. |
| Multi-Modal | Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal
  Representations (Read more on [arXiv](https://arxiv.org/abs/2503.18817) or [HuggingFace](https://huggingface.co/papers/2503.18817))| Sangheum Hwang, mawjdgus | This paper introduces a method to enhance Out-of-Distribution (OoD) detection by improving the alignment of image and text representations during multi-modal fine-tuning (MMFT). The primary objective is to address the modality gap observed in standard fine-tuned Vision-Language Models (VLMs), which limits OoD performance despite high ID accuracy. The key methodology involves a novel Cross-Modal Alignment (CMA) regularization term added to the contrastive learning objective, designed to pull corresponding ID image and text embeddings closer in the hyperspherical representation space while separating them from negative (OoD) concepts. Experiments on ImageNet-1k OoD benchmarks show that CMA combined with the NegLabel scoring function achieves state-of-the-art results, notably reducing the average FPR95 to 19.93% on the MOS benchmark, while also leading in ID accuracy. For AI practitioners, this implies that explicitly regularizing for cross-modal alignment during MMFT can significantly improve model robustness to OoD inputs by better utilizing pretrained textual knowledge. |
