

## Papers for 2025-04-07

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Reinforcement Learning | Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving (Read more on [arXiv](https://arxiv.org/abs/2504.02605) or [HuggingFace](https://huggingface.co/papers/2504.02605))| Linhao Zhang, Hanwu Chen, Wei Liu, Zhirong Huang, Daoguang Zan | This paper introduces Multi-SWE-bench, a novel multilingual benchmark comprising 1,632 curated instances across 7 languages (Java, TS, JS, Go, Rust, C, C++) for evaluating LLMs on software issue resolving tasks, alongside the Multi-SWE-RL community initiative releasing 4,723 instances to facilitate reinforcement learning research. The primary objective is to overcome the limitations of existing Python-centric benchmarks and assess the generalization capabilities of state-of-the-art LLMs and agentic methods in diverse software ecosystems. The benchmark was constructed via a five-phase pipeline including repository selection, PR crawling, automated environment setup, test-based filtering, and rigorous dual-annotator manual verification. Evaluations using models like Claude-3.5-Sonnet across methods (Agentless, SWE-agent, MopenHands) revealed significant performance disparities across languages (e.g., Python 42.4% vs C++ 3.88% resolved rate with MagentLess) and highlighted struggles with harder tasks and complex patches (>600 tokens). The work implies that current LLM agents have limited generalization for multilingual issue resolution, indicating a need for improved methods, potentially leveraging RL, supported by the provided benchmark and community resources. |
| Natural Language Processing | Agentic Knowledgeable Self-awareness (Read more on [arXiv](https://arxiv.org/abs/2504.03553) or [HuggingFace](https://huggingface.co/papers/2504.03553))| Xiangyuan Ru, Xiaobin Wang, Baochang Ren, Zhisong Qiu, Shuofei Qiao | This paper introduces agentic knowledgeable self-awareness, enabling LLM-based agents to autonomously regulate knowledge utilization based on situational assessment. The main objective is to develop agents that mimic human cognitive self-awareness in decision-making, deciding when to use internal capabilities, reflection, or external knowledge. The proposed method, KnowSelf, employs a data-centric approach using heuristic criteria to label self-explored trajectories and a two-stage (SFT+RPO) training process to instill self-awareness. Experiments demonstrate KnowSelf's effectiveness, with Llama-8B achieving an 84.33% average reward on ALFWorld using only 15.01% knowledge-enhanced actions, outperforming baselines requiring significantly more external knowledge. This implies that practitioners can train more efficient, robust, and human-like AI agents by focusing on selective knowledge activation rather than indiscriminate injection. |
| Natural Language Processing | MegaMath: Pushing the Limits of Open Math Corpora (Read more on [arXiv](https://arxiv.org/abs/2504.02807) or [HuggingFace](https://huggingface.co/papers/2504.02807))| Liping Tang, Zhoujun Cheng, Nikhil Ranjan, Zengzhi Wang, Fan Zhou | This paper introduces MegaMath, a large-scale (371B tokens), open-source dataset specifically curated for pre-training math-focused Large Language Models (LLMs). The main objective was to address the community's need for a high-quality, extensive math corpus to improve mathematical reasoning in LLMs, which current open datasets lack in scale and fidelity. The methodology involved revisited web data extraction with math-oriented optimizations and filtering, recalling math-related code from Stack-V2 using SLM-based scoring, and generating diverse synthetic data (QA, code translation, interleaved text-code). Key results include the creation of the 371B token dataset and demonstrations of its effectiveness through ablations and by achieving significant performance improvements (e.g., 56.2% on GSM8K CoT) when continually pre-training Llama-3 models. MegaMath provides AI practitioners with a valuable open resource to advance the development and training of mathematically proficient LLMs. |
| Machine Learning | SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge
  Refinement (Read more on [arXiv](https://arxiv.org/abs/2504.03561) or [HuggingFace](https://huggingface.co/papers/2504.03561))| Jialong Wu, Shuofei Qiao, Yuan Liang, Xiaobin Wang, Runnan Fang | SynWorld proposes a framework for refining the action knowledge of Large Language Model (LLM) based agents through virtual scenario synthesis and exploration. The main objective is to enable agents to autonomously learn how to plan and execute actions effectively, especially in novel environments or with complex tool interactions, overcoming limitations of static or misaligned action descriptions. The core methodology involves synthesizing diverse, multi-step virtual scenarios conditioned on specific tool subsets, followed by Monte Carlo Tree Search (MCTS) exploration within these scenarios to iteratively optimize the agent's action knowledge (descriptions and workflows) based on simulated feedback. Experimental results demonstrate effectiveness, with the method achieving a PASS score of 59.33 and a WIN score of 73.00 on the ToolBench benchmark using GPT-4-turbo, outperforming several baseline methods. The key implication is that synthesizing virtual experiences combined with structured exploration like MCTS allows agents to autonomously improve their operational capabilities and adapt to new situations without extensive manual tuning. |
| Multi-Modal | MME-Unify: A Comprehensive Benchmark for Unified Multimodal
  Understanding and Generation Models (Read more on [arXiv](https://arxiv.org/abs/2504.03641) or [HuggingFace](https://huggingface.co/papers/2504.03641))| Bingyan Nie, Yang Shi, Chaoyou Fu, Yi-Fan Zhang, Wulin Xie | This paper introduces MME-Unify (MME-U), a novel benchmark designed to comprehensively evaluate Unified Multimodal Large Language Models (U-MLLMs) across understanding, generation, and unified tasks. The primary objective is to address the lack of standardized evaluation frameworks, especially for assessing the synergistic 'mixed-modality generation' capabilities where understanding and generation mutually enhance each other. MME-U achieves this by curating and standardizing 10 traditional understanding/generation tasks from 12 datasets and introducing 5 new 'unified' tasks (e.g., Visual CoT, Image Editing & Explaining) evaluated via multiple-choice text and image matching using metrics like accuracy and CLIP score. Evaluations on 12 U-MLLMs, including Gemini2.0-flash, revealed significant performance gaps, with Gemini2.0-flash achieving the top MME-U score of 45.57, yet highlighting substantial room for improvement, particularly in instruction following and unified task performance. For AI practitioners, MME-U offers a standardized tool to rigorously assess and compare U-MLLM capabilities, pinpointing current weaknesses in integrated multimodal reasoning and generation. |
| Multi-Modal | VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via
  Iterative Instruction Tuning and Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2504.02949) or [HuggingFace](https://huggingface.co/papers/2504.02949))| Liming Liang, Dongchao Yang, Yufan Deng, Yuxin Xie, Xianwei Zhuang | This paper introduces VARGPT-v1.1, an improved unified visual autoregressive model designed for both visual understanding and image generation tasks. The primary objective is to enhance instruction-following capabilities and generation quality compared to its predecessor by integrating advanced training strategies and larger datasets. Key methodologies include a multi-stage training approach combining iterative visual instruction tuning on an expanded 8.3M pair corpus with reinforcement learning via Direct Preference Optimization (DPO), progressive resolution scaling, and upgrading the LLM backbone to Qwen2-7B. VARGPT-v1.1 achieves state-of-the-art results, demonstrating significant improvements such as attaining 81.01 on MMBench and showing emergent image editing abilities without architectural changes. The main implication for AI practitioners is that iterative instruction tuning paired with DPO within a unified visual autoregressive framework offers a scalable and effective path towards advanced multimodal understanding and generation capabilities. |
| Machine Learning | APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated
  Agent-Human Interplay (Read more on [arXiv](https://arxiv.org/abs/2504.03601) or [HuggingFace](https://huggingface.co/papers/2504.03601))| Ming Zhu, Jianguo Zhang, Weiran Yao, Zuxin Liu, Akshara Prabhakar | This paper introduces APIGen-MT, a two-phase framework for generating high-quality, verifiable multi-turn data for AI agent training through simulated agent-human interplay. The primary objective is to overcome the scarcity and cost of obtaining realistic multi-turn interaction data, crucial for developing capable AI agents. The methodology first generates detailed, verified task "blueprints" using an agentic pipeline with LLM reviewers and feedback, then simulates human-agent conversations based on these blueprints to create full interaction trajectories. Key results show that models trained on this data (xLAM-2-fc-r series) outperform frontier models like GPT-40 and Claude 3.5 on benchmarks such as BFCL v3 (78.19% accuracy for the 70B model) and T-bench (56.2% overall success rate for the 70B model), with smaller models showing strong multi-turn performance. For AI practitioners, this work provides a method to generate high-quality synthetic data, enabling the development of more reliable and efficient multi-turn conversational agents. |
| Computer Vision | HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction
  via Gaussian Restoration (Read more on [arXiv](https://arxiv.org/abs/2504.03536) or [HuggingFace](https://huggingface.co/papers/2504.03536))| Guosheng Zhao, Xiaofeng Wang, Runqi Ouyang, Boyuan Wang, ZhengZhu | HumanDreamer-X introduces a novel framework for reconstructing photorealistic 3D human avatars from a single image by integrating multi-view generation and reconstruction into a unified pipeline. The primary objective is to mitigate geometric inconsistencies and improve visual fidelity in avatars generated via traditional decoupled approaches. The methodology first creates a coarse 3D avatar using 3D Gaussian Splatting (3DGS), then employs a video restoration model called HumanFixer, enhanced with an attention modulation strategy, to refine multi-view renderings generated from the coarse avatar before using them to optimize the final 3DGS model. Experimental results show significant improvements, increasing reconstruction PSNR quality metrics by 12.65% and achieving up to 25.62 dB on the CustomHumans dataset. For AI practitioners, this work demonstrates that unifying reconstruction and generative restoration, particularly leveraging 3DGS priors and specialized attention mechanisms, yields higher quality and more consistent single-image 3D human avatars. |
| Machine Learning | TransMamba: Flexibly Switching between Transformer and Mamba (Read more on [arXiv](https://arxiv.org/abs/2503.24067) or [HuggingFace](https://huggingface.co/papers/2503.24067))| Shuaipeng Li, Xingwu Sun, Ruobing Xie, andyyang, Yixinglee | TransMamba introduces a novel framework unifying Transformer and Mamba architectures via shared parameters for flexible sequence modeling. The primary objective is to overcome Transformer's quadratic complexity and Mamba's contextual learning instability by dynamically switching between attention and state space model (SSM) mechanisms. Key methodologies include using shared QKV/CBx parameters, a 'Memory Converter' for lossless state transition between modes, and a 'TransPoint' scheduling mechanism to determine the switch location based on token length and layer. Experiments demonstrate superior performance and efficiency, with TransMamba-1.5B achieving 64.75 on ARC-E compared to Transformer-1.5B's 60.87, and showing up to 25% faster training time (0.75 relative time vs Transformer's 1.00). The main implication is offering a scalable and adaptable solution for next-generation sequence modeling, balancing efficiency and effectiveness. |
| Computer Vision | Comprehensive Relighting: Generalizable and Consistent Monocular Human
  Relighting and Harmonization (Read more on [arXiv](https://arxiv.org/abs/2504.03011) or [HuggingFace](https://huggingface.co/papers/2504.03011))| Zhixin Shu, Krishna Kumar Singh, Xin Sun, Jingyuan Liu, Junying Wang | This paper introduces Comprehensive Relighting, a unified framework for generalizable and consistent monocular human relighting and background harmonization from images or videos. The primary objective is to overcome the limitations of existing methods, specifically their lack of generalizability across scenes/body parts and poor temporal consistency in videos. The methodology employs a pre-trained diffusion model within a coarse-to-fine framework for joint relighting and harmonization, complemented by an unsupervised temporal lighting module trained with cycle consistency on real videos, and spatio-temporal blending plus guided refinement at inference. Experiments demonstrate superior performance, achieving, for instance, 26.61 PSNR in complex synthetic video scenarios (Scenario 3) and outperforming prior methods in fidelity and temporal coherence (e.g., 38.32 tPSNR). For AI practitioners, this work offers a robust, all-in-one solution for controllable and high-quality human appearance manipulation adaptable to various inputs and target lighting conditions. |
| Multi-Modal | EvMic: Event-based Non-contact sound recovery from effective
  spatial-temporal modeling (Read more on [arXiv](https://arxiv.org/abs/2504.02402) or [HuggingFace](https://huggingface.co/papers/2504.02402))| Lu Zhang, Xudong XU, Xu Jia, Shi Guo, yyzqy | EvMic introduces a novel deep learning pipeline for non-contact sound recovery leveraging the high temporal resolution of event cameras and effective spatial-temporal modeling. The main objective is to recover high-frequency sound signals from visually captured object vibrations induced by sound waves, overcoming the trade-offs faced by traditional cameras and limitations of prior event-based methods. The methodology employs a laser matrix to enhance object gradients, converts event streams to voxel grids, uses sparse convolutions for feature extraction, Mamba for long-term temporal modeling, and a multi-head self-attention based spatial aggregation block (SAB) to combine information from different locations. On synthetic benchmarks, the proposed method achieved significantly better results than baselines, reaching an average Signal-to-Noise Ratio (SNR) of 1.214 dB compared to -0.079 dB for the EvPhase baseline. This work demonstrates to AI practitioners the potential of specialized architectures combining sparse processing and advanced sequence models with event-based vision for challenging non-contact sensing tasks like remote sound recovery. |
| Computer Vision | MedSAM2: Segment Anything in 3D Medical Images and Videos (Read more on [arXiv](https://arxiv.org/abs/2504.03600) or [HuggingFace](https://huggingface.co/papers/2504.03600))| Mohammed Baharoon, Bihui Chen, Sumin Kim, Zongxin Yang, Jun Ma | MedSAM2 is presented as a general-purpose foundation model for promptable segmentation of 3D medical images and videos. The primary objective was to address the limitations of 2D segmentation models and the lack of generalist 3D/video models by developing a versatile tool validated with comprehensive user studies. The methodology involves fine-tuning the Segment Anything Model 2 (SAM2.1-Tiny variant) on a large-scale curated medical dataset comprising over 455,000 3D image-mask pairs and 76,000 video frames, utilizing its architecture including memory attention for temporal consistency. Key results show MedSAM2 outperforms previous models across diverse tasks and modalities, achieving high Dice Similarity Coefficient (DSC) scores (e.g., median 88.84% on CT organs, 86.68% on CT lesions) and user studies demonstrated it can reduce manual annotation costs by over 85%. The main implication for AI practitioners is the availability of an efficient, scalable, and deployable model that significantly accelerates large-scale medical data annotation and enhances segmentation capabilities for research and potential healthcare applications. |
| Natural Language Processing | BEATS: Bias Evaluation and Assessment Test Suite for Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2503.24310) or [HuggingFace](https://huggingface.co/papers/2503.24310))| Lisa Erickson, tbandopa, alokabhishek | This paper introduces BEATS (Bias Evaluation and Assessment Test Suite), a framework and benchmark for evaluating Bias, Ethics, Fairness, and Factuality (BEFF) in Large Language Models (LLMs). The primary objective is to develop a standardized methodology and benchmark for systematically measuring these BEFF dimensions across different LLMs. The BEATS framework utilizes a curated dataset of 901 bias-probing questions, obtains responses from various LLMs, and employs a consortium of LLMs-as-judges to score these responses against 29 distinct BEFF metrics. Empirical results indicate that, on average, 37.65% of outputs generated by leading LLMs contained some form of bias, with about 40% of responses exhibiting medium-to-high levels of bias severity or impact. The main implication for AI practitioners is the clear quantification of bias risks in current LLMs for critical applications and the provision of a rigorous tool to diagnose biases and guide the development of more equitable models. |
