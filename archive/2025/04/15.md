

## Papers for 2025-04-15

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | InternVL3: Exploring Advanced Training and Test-Time Recipes for
  Open-Source Multimodal Models (Read more on [arXiv](https://arxiv.org/abs/2504.10479) or [HuggingFace](https://huggingface.co/papers/2504.10479))| jackroos, duanyuchen, gulixin0922, Yeshenglong, Weiyun1025 | InternVL3 introduces an advanced open-source multimodal model series featuring a native multimodal pre-training paradigm. The primary objective is to explore unified pre-training and advanced recipes (training and test-time) to address alignment challenges in conventional post-hoc MLLM adaptation pipelines. Key methodologies include joint pre-training on diverse multimodal and pure-text corpora in a single stage, variable visual position encoding (V2PE), supervised fine-tuning (SFT), mixed preference optimization (MPO), and test-time scaling strategies. Empirical results demonstrate state-of-the-art performance among open-source models, with InternVL3-78B achieving a 72.2 score on the MMMU benchmark. For AI practitioners, this research highlights the effectiveness of native multimodal pre-training as a potent alternative to multi-stage pipelines and contributes open models and data to foster further development. |
| Machine Learning | PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday
  Home Clusters (Read more on [arXiv](https://arxiv.org/abs/2504.08791) or [HuggingFace](https://huggingface.co/papers/2504.08791))| Hongfang Yu, Mohsen Guizani, NeuronNomad, LiPhilip, LIKirin | Prima.cpp is a distributed system enabling fast inference of 70B-scale LLMs on clusters of everyday low-resource home devices. The primary objective is to minimize token latency and memory pressure on heterogeneous hardware (CPU/GPU, low RAM/VRAM, Wi-Fi) by optimally distributing model layers across devices. The system employs piped-ring parallelism with prefetching to hide disk loading latency (using mmap for weights) and utilizes the Halda algorithm for optimal, heterogeneity-aware layer-to-device assignment. Evaluations demonstrate prima.cpp achieving ~600ms per token latency for 70B models, outperforming llama.cpp by up to 17x on large models while keeping memory pressure below 6%. This work enables AI practitioners to deploy large, state-of-the-art LLMs locally on accessible consumer hardware clusters, potentially improving privacy and accessibility. |
| Multi-Modal | FUSION: Fully Integration of Vision-Language Representations for Deep
  Cross-Modal Understanding (Read more on [arXiv](https://arxiv.org/abs/2504.09925) or [HuggingFace](https://huggingface.co/papers/2504.09925))| Jingzhou Chen, conghui, jingwei-xu-00, Balalauuoo, starriver030515 | This paper introduces FUSION, a family of multimodal large language models (MLLMs) designed for deep, dynamic integration of vision and language representations throughout the entire processing pipeline. The primary objective is to overcome the limitations of traditional MLLMs that typically perform late-stage modality fusion, aiming for a more comprehensive cross-modal understanding. Key methodologies include Text-Guided Unified Vision Encoding for pixel-level integration, Context-Aware Recursive Alignment Decoding for question-level semantic integration during decoding, and a Dual-Supervised Semantic Mapping Loss to align features across modalities. FUSION models demonstrate strong performance, with FUSION-X (using 630 tokens) significantly outperforming models like Cambrian-1 and Florence-VL on various benchmarks, and FUSION-L (using 300 tokens) retaining 95% performance, highlighting efficiency. The main implication for AI practitioners is that this deep integration approach enables high performance in MLLMs with substantially fewer vision tokens compared to contemporary methods. |
| Multi-Modal | VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models
  with Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2504.08837) or [HuggingFace](https://huggingface.co/papers/2504.08837))| Wei Chu, Chao Qu, wenhu, zuminghuang, JasperHaozhe | This paper introduces VL-Rethinker, a method enhancing reflective reasoning in vision-language models (VLMs) using reinforcement learning (RL). The primary objective is to effectively incentivize multimodal slow-thinking capabilities in VLMs directly via RL, without relying on distillation from stronger models. The methodology adapts Group Relative Policy Optimization (GRPO) with a novel Selective Sample Replay (SSR) technique to mitigate vanishing advantages, and introduces "Forced Rethinking" to explicitly trigger self-reflection during RL training. VL-Rethinker achieved state-of-the-art results on multimodal reasoning benchmarks, including 80.3% on MathVista, significantly outperforming prior models. For AI practitioners, this work presents direct RL, augmented with SSR and Forced Rethinking, as a viable approach to improve VLM reasoning and induce reflective capabilities without complex supervised fine-tuning or distillation pipelines. |
| Reinforcement Learning | Iterative Self-Training for Code Generation via Reinforced Re-Ranking (Read more on [arXiv](https://arxiv.org/abs/2504.09643) or [HuggingFace](https://huggingface.co/papers/2504.09643))| Valentin Malykh, Ivan Sedykh, Nikita Sorokin | This paper introduces RewardRanker, a novel iterative self-training approach using reinforcement learning to enhance code generation through improved solution reranking. The objective is to boost reranking accuracy and overall code quality by continually refining a reward/reranker model using self-generated data, including hard negatives. The core methodology involves an iterative cycle of supervised fine-tuning, reward model training, Proximal Policy Optimization (PPO) for generator refinement, and retraining the reward model with newly evaluated examples. On the MultiPL-E benchmark, the proposed 13.4B parameter model achieves a 70.9% average accuracy, outperforming a 33B baseline and performing comparably to GPT-4. This work implies that focusing iterative RL training on reward model refinement, rather than just the generator, enables smaller models to achieve high performance in code generation, offering resource efficiency. |
| Multi-Modal | Mavors: Multi-granularity Video Representation for Multimodal Large
  Language Model (Read more on [arXiv](https://arxiv.org/abs/2504.10068) or [HuggingFace](https://huggingface.co/papers/2504.10068))| kugwzk, zhenhuawu, UnnamedWatcher, CheeryLJH, DogNeverSleep | This paper introduces Mavors, a framework for multi-granularity video representation in Multimodal Large Language Models (MLLMs) to enhance long-context video understanding. The primary objective is to balance computational efficiency with the retention of fine-grained spatio-temporal details, addressing information loss issues in existing methods like sparse sampling or token compression. Mavors employs an Intra-chunk Vision Encoder (IVE) using 3D convolutions and Vision Transformers for high-resolution spatial features within video chunks, and an Inter-chunk Feature Aggregator (IFA) with temporal transformers and chunk-level rotary position encodings (C-RoPE) to model dependencies across chunks. Experiments demonstrate Mavors' superiority, achieving a score of 39.4 on the DREAM-1K benchmark, outperforming comparable 7B parameter models in tasks requiring detailed spatio-temporal reasoning. For AI practitioners, Mavors offers an approach to improve MLLM performance on complex, long-duration video tasks by preserving both spatial fidelity and temporal coherence without excessive computational cost. |
| Natural Language Processing | S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability
  of Large Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2504.10368) or [HuggingFace](https://huggingface.co/papers/2504.10368))| Tingwen Liu, Xinghua Zhang, Starrrrrry, ShuaiyiNie, WYRipple | This paper introduces S1-Bench, a novel benchmark designed to evaluate the intuitive System 1 thinking capabilities of Large Reasoning Models (LRMs) on simple tasks. The main objective is to assess how LRMs, typically optimized for deliberative System 2 reasoning, perform when faced with questions better suited for System 1 processing, examining potential inefficiencies and accuracy issues. The methodology involves creating a diverse dataset (S1-Bench) of simple, natural questions across four categories (reasoning, knowledge, instruction following, analysis) in English and Chinese, verified for simplicity using smaller LLMs, and evaluating 22 LRMs based on accuracy, format correctness, and response length (ART). Key results show LRMs exhibit significant inefficiency, generating outputs averaging 15.5 times longer than smaller LLMs, and often suffer from under-accuracy (e.g., DS-R1-1.5B achieved only 54.50% acc@k) and redundant reasoning on these simple tasks. The main implication is that current LRMs lack adaptive dual-system thinking capabilities, highlighting the need for developing models that can efficiently switch between intuitive and deliberative reasoning based on task complexity. |
| Machine Learning | AgentRewardBench: Evaluating Automatic Evaluations of Web Agent
  Trajectories (Read more on [arXiv](https://arxiv.org/abs/2504.08942) or [HuggingFace](https://huggingface.co/papers/2504.08942))| dongchans, arkilpatel, ncmeade, kazemnejad, xhluca | This paper introduces AGENTREWARDBENCH, a benchmark for evaluating the effectiveness of Large Language Models (LLMs) as automatic judges for web agent trajectories. The primary objective is to assess how well LLM judges align with expert human evaluations regarding task success, side effects, and repetitiveness, compared to traditional rule-based methods. The methodology involves collecting 1302 trajectories from 4 LLM agents across 5 web benchmarks, having experts annotate them, and then evaluating 12 different LLM judges against these expert annotations. Key findings reveal that no single LLM judge excels across all benchmarks, current judges achieve less than 70% precision against expert labels, and rule-based methods significantly underestimate agent success rates (e.g., achieving only 55.9% recall overall). The main implication is that current automatic evaluation methods, including LLM judges, have notable weaknesses and require improvement for reliable web agent assessment and downstream tasks like reward modeling. |
| Multi-Modal | Have we unified image generation and understanding yet? An empirical
  study of GPT-4o's image generation ability (Read more on [arXiv](https://arxiv.org/abs/2504.08003) or [HuggingFace](https://huggingface.co/papers/2504.08003))| Ning Li, cuijiaxing, zhangjingran | This paper empirically evaluates GPT-4o's ability to unify image generation and understanding, revealing significant limitations. The primary objective is to assess GPT-4o's semantic synthesis capabilities across global instruction adherence, fine-grained editing precision, and post-generation reasoning. The methodology involves testing the model with specifically designed prompts, including those with reversed spatial logic, numerical transformations, selective editing requirements, and conditional generation tasks based on prior outputs. Key results, presented qualitatively through examples, show GPT-4o often defaults to literal interpretations, fails to consistently apply abstract or knowledge constraints (e.g., ignoring reversed directions or numerical adjustments), and struggles with conditional reasoning post-generation. The main implication for AI practitioners is the need for more robust benchmarks and training strategies that emphasize context-aware, reasoning-grounded multimodal generation beyond surface-level instruction following. |
| Reinforcement Learning | DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM
  Post-training (Read more on [arXiv](https://arxiv.org/abs/2504.09710) or [HuggingFace](https://huggingface.co/papers/2504.09710))| zwt123home123, timecuriosity, gfcui, ztwang | This paper presents DUMP, an automated distribution-level curriculum learning framework for Reinforcement Learning (RL)-based Large Language Model (LLM) post-training. The primary objective is to dynamically schedule training across heterogeneous data distributions to optimize learning efficiency and final model performance. DUMP models distribution selection as a multi-armed bandit problem, using the magnitude of policy advantages as a proxy for learnability and employing an Upper Confidence Bound (UCB) strategy to balance exploitation (high advantage) and exploration (low sample count). Experiments on the K&K logic reasoning dataset demonstrate that DUMP significantly improves convergence speed and achieves higher final performance compared to uniform sampling; for instance, on the 9-character difficulty, DUMP reached a test reward above 0.5 while the baseline stayed below 0.0. The main implication is that practitioners can use DUMP to automatically optimize RL post-training schedules for mixed-distribution datasets, improving model capabilities without manual curriculum design. |
| Natural Language Processing | SocioVerse: A World Model for Social Simulation Powered by LLM Agents
  and A Pool of 10 Million Real-World Users (Read more on [arXiv](https://arxiv.org/abs/2504.10157) or [HuggingFace](https://huggingface.co/papers/2504.10157))| milesz7777, tangshiping, SimingChen, libo-ca, Lishi0905 | SocioVerse introduces an LLM-agent-driven world model for realistic, large-scale social simulation grounded in a pool of 10 million real-world users. The primary objective is to overcome alignment challenges between simulations and real-world environments, users, interactions, and behaviors that affect existing methods. Its methodology integrates four alignment modules—Social Environment, User Engine, Scenario Engine, and Behavior Engine—leveraging real-world data (from X and Rednote), user sampling techniques (IPF, IDS), and LLM agents (e.g., Llama3, Qwen2.5, GPT-4o) to simulate diverse scenarios. Experiments across politics, news, and economics demonstrated high fidelity, with models like Qwen2.5-72b achieving 80.0% accuracy (Acc) and 0.031 RMSE in predicting battleground state outcomes in a US presidential election simulation. For AI practitioners, SocioVerse provides a robust, standardized framework for building credible social simulations to analyze population dynamics and predict behavioral responses with minimal manual adjustments. |
| Multi-Modal | Breaking the Data Barrier -- Building GUI Agents Through Task
  Generalization (Read more on [arXiv](https://arxiv.org/abs/2504.10127) or [HuggingFace](https://huggingface.co/papers/2504.10127))| jxhe, QiushiSun, changma, heroding77, leoozy | This paper investigates improving Graphical User Interface (GUI) agent performance by leveraging task generalization through a dedicated mid-training stage using diverse, readily available non-GUI datasets. The main objective is to overcome the limitations imposed by the scarcity of high-quality GUI trajectory data by transferring knowledge from data-rich domains. The methodology involves mid-training Vision Language Models (VLMs) on tasks like multimodal/textual reasoning (e.g., math, coding) and perception before fine-tuning on limited GUI trajectories, using a standardized format and continuous optimizer. Key results demonstrate significant performance improvements; notably, text-only mathematical data boosted AndroidWorld success rate by an absolute 5.4% and a curated mixture dataset (GUIMid) achieved gains of 12.2% on AndroidWorld, indicating strong cross-domain and cross-modal transfer. The main implication is that practitioners can enhance GUI agent capabilities by incorporating mid-training with diverse, accessible non-GUI data, particularly reasoning-intensive tasks, to effectively mitigate GUI data scarcity. |
| Multi-Modal | TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.09641) or [HuggingFace](https://huggingface.co/papers/2504.09641))| Lei Huang, Wenjun Wu, wenzz1, Zhang199 | This paper introduces TinyLLaVA-Video-R1, a small-scale (<4B parameter) multimodal model enhanced for video reasoning using reinforcement learning. The primary objective is to explore and improve the reasoning and thinking capabilities of computationally efficient, smaller LMMs on general video question-answering tasks, making video reasoning more accessible. The study applies the Group Relative Policy Optimization (GRPO) algorithm to the TinyLLaVA-Video model, using a modified reward structure incorporating format compliance, answer accuracy, and a length-based penalty, trained on the NextQA dataset. TinyLLaVA-Video-R1 demonstrates significantly improved reasoning, achieving scores such as 52.4 on MLVU and 46.9 on MMVU (mc), outperforming its supervised fine-tuned counterpart, and exhibits emergent 'aha moments' reflecting self-verification. The work suggests that even smaller LMMs can attain notable video reasoning abilities through RL on general datasets, offering practical insights and a viable path for researchers with limited computational resources to explore complex multimodal reasoning. |
| Machine Learning | LLM-SRBench: A New Benchmark for Scientific Equation Discovery with
  Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.10415) or [HuggingFace](https://huggingface.co/papers/2504.10415))| Khoa D Doan, Amir Barati Farimani, Ngoc-Hieu Nguyen, mkmeidani, parshinsh | This paper introduces LLM-SRBench, a new benchmark designed to rigorously evaluate Large Language Models (LLMs) for scientific equation discovery, mitigating issues of memorization found in existing benchmarks. The primary objective is to assess the true reasoning and discovery capabilities of LLM-based methods beyond reciting known equations. LLM-SRBench comprises two novel problem sets: LSR-Transform (re-representing known equations) and LSR-Synth (combining known and synthetic terms), evaluated using metrics like symbolic accuracy and data fidelity. Experiments reveal significant challenges, with the best method achieving only 31.5% symbolic accuracy on LSR-Transform, indicating current LLM approaches struggle with genuine discovery. The implication for AI practitioners is that LLM-SRBench provides a more realistic testbed for developing and evaluating LLM-based scientific discovery methods, highlighting the gap between memorization and true data-driven reasoning. |
| Natural Language Processing | EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental
  Health Safety (Read more on [arXiv](https://arxiv.org/abs/2504.09689) or [HuggingFace](https://huggingface.co/papers/2504.09689))| Edify-Kd2024, yaozixin, YimingWang, ChrisJuan, yinghuihe | The paper introduces EmoAgent, a multi-agent AI framework for assessing and mitigating mental health risks in human-AI interactions. It addresses the challenge of ensuring safety for vulnerable users interacting with character-based AI. EmoAgent employs EmoEval to simulate users and assess mental health changes using psychological assessments (PHQ-9, PDI, PANSS), while EmoGuard monitors mental status and provides corrective feedback. Experiments show that emotionally engaging dialogues can lead to mental state deterioration in over 34.4% of simulations, but EmoGuard significantly reduces these rates. The implication for AI practitioners is a need for robust safeguards and evaluation tools in character-based AI to avoid exacerbating psychological distress. |
| Machine Learning | The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via
  Agentic Tree Search (Read more on [arXiv](https://arxiv.org/abs/2504.08066) or [HuggingFace](https://huggingface.co/papers/2504.08066))| Chris Lu, Shengran Hu, Robert Tjarko Lange, conglu, yyamada | This paper introduces THE AI SCIENTIST-v2, an agentic system for automated scientific discovery. The system iteratively formulates hypotheses, designs and executes experiments, analyzes data, and authors scientific manuscripts without human code templates. It leverages a progressive agentic tree-search methodology and VLM feedback. The AI SCIENTIST-v2 successfully generated a peer-review-accepted workshop paper, scoring 6.33 on average. This accomplishment demonstrates the growing capability of AI in conducting aspects of scientific research and suggests future advancements will significantly impact human knowledge generation. |
| Natural Language Processing | Executable Functional Abstractions: Inferring Generative Programs for
  Advanced Math Problems (Read more on [arXiv](https://arxiv.org/abs/2504.09763) or [HuggingFace](https://huggingface.co/papers/2504.09763))| Zaid Khan, mohitbansal, j-min, archiki, esteng | This paper introduces Executable Functional Abstractions (EFAs) for automating the generation of diverse math problems. The research explores automatically constructing EFAs by conditioning large language models (LLMs) on seed problems and step-by-step solutions. EFAGen, the proposed method, uses program synthesis and unit tests to generate and filter candidate EFA programs, improving LLM performance through a self-training scheme. Experiments show EFAGen can infer EFAs across diverse math problems, resulting in a 23.07% absolute improvement in pass@1 rate on MATH-Hard when using EFA-generated variants as in-context examples. The EFAs provide a scalable solution for data augmentation and stress-testing models in advanced mathematics domains. |
| Natural Language Processing | How new data permeates LLM knowledge and how to dilute it (Read more on [arXiv](https://arxiv.org/abs/2504.09522) or [HuggingFace](https://huggingface.co/papers/2504.09522))| Nolan Andrew Miller, Andrey Zhmoginov, Chen Sun, gozzo87, mendor | The paper investigates how new information introduced to LLMs affects existing knowledge, leading to priming effects. It introduces "Outlandish," a dataset for probing knowledge permeation, and demonstrates that the degree of priming can be predicted by measuring the token probability of keywords before learning. The study finds a strong correlation between pre-learning keyword probability and post-learning priming, robust across models like PALM-2 and Llama. The paper also develops "stepping-stone" text augmentation and "ignore-k" update pruning techniques which can reduce undesirable priming effects by 50-95%. This provides AI practitioners with tools to improve the specificity of knowledge insertion in LLMs. |
| Multi-Modal | VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search (Read more on [arXiv](https://arxiv.org/abs/2504.09130) or [HuggingFace](https://huggingface.co/papers/2504.09130))| QipengGuo, alphadl, ngc7293, sinwang, LibraTree | VisuoThink enhances Large Vision-Language Model (LVLM) reasoning by integrating visuospatial and linguistic domains. The research aims to improve LVLM performance on complex reasoning tasks requiring visual aids through a multimodal tree search framework. VisuoThink combines progressive visual-textual reasoning with test-time scaling via look-ahead tree search using a predictive rollout mechanism. Experiments on geometry and spatial reasoning tasks show significant improvements, achieving a state-of-the-art 48.5% accuracy@1 on Geomeverse. VisuoThink offers AI practitioners a method for significantly boosting LVLM reasoning without fine-tuning by leveraging interleaved visual and textual information. |
| Machine Learning | M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2504.10449) or [HuggingFace](https://huggingface.co/papers/2504.10449))| Daniele Paliotta, tridao, voidptr74, xu3kev, JunxiongWang | The paper introduces M1, a hybrid linear RNN reasoning model based on the Mamba architecture, aimed at improving the scalability of test-time compute for complex mathematical problems. The research objective is to efficiently transfer reasoning capabilities from a large transformer model to a linear RNN. M1 leverages distillation, supervised fine-tuning, and reinforcement learning for training, achieving results comparable to DeepSeek-R1-Distill-Qwen-1.5B, with scores of 82 on MATH500, and a 3x speedup in inference throughput. M1 provides AI practitioners with a memory-efficient approach to scaling test-time generation, particularly using techniques such as self-consistency. |
| Natural Language Processing | LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety
  in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.10430) or [HuggingFace](https://huggingface.co/papers/2504.10430))| Xinyi Zhang, sarvech123, aneverfull, Zhiyang03, mqliu | This paper investigates the persuasion safety of Large Language Models (LLMs) by empirically studying their potential for unethical influence in goal-driven dialogues. The research assesses whether LLMs reject unethical tasks and avoid unethical strategies, considering factors like personality traits and external pressures. It introduces PERSUSAFETY, a framework for evaluating LLM persuasion safety across diverse scenarios. Experiments on 8 LLMs reveal significant safety concerns, including failures to refuse harmful tasks and utilization of unethical strategies. The findings imply that AI practitioners need to improve safety alignment in progressive and goal-driven conversations to mitigate potential misuse. |
| Natural Language Processing | DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and
  Summarization? (Read more on [arXiv](https://arxiv.org/abs/2504.08120) or [HuggingFace](https://huggingface.co/papers/2504.08120))| Christoph Leiter, Yanran Chen, Ran Zhang, Sotaro Takeshita, Daniil Larionov | This paper investigates the effectiveness of reasoning-enabled large language models (LLMs) in evaluating machine translation (MT) and text summarization (TS) tasks. The study aims to determine if reasoning models improve over conventional models in NLG evaluation and how well distillation preserves evaluation capabilities. The methodology involves evaluating eight LLMs, including reasoning-based and non-reasoning counterparts, on WMT23 and SummEval benchmarks using GEMBA-MQM and G-Eval. Results indicate that the efficacy of reasoning capabilities is highly model and task-dependent; for example, OpenAI o3-mini shows consistent improvements with increased reasoning intensity. The main implication is that the alignment of reasoning approaches with NLG evaluation tasks is crucial for enhancing evaluation performance in practical applications. |
| Multi-Modal | MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in
  Multimodal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.05782) or [HuggingFace](https://huggingface.co/papers/2504.05782))| Jiaxin Ai, Zhaopan Xu, Xiaopeng Peng, Fanrui Zhang, Pengfei Zhou | The paper introduces MDK12-Bench, a multi-disciplinary benchmark for evaluating the reasoning capabilities of multimodal large language models using K-12 examination questions. The primary objective is to address limitations in existing benchmarks related to data size, domain coverage, and knowledge structure to better assess real-world reasoning. The methodology involves curating a dataset of 140K instances across six disciplines, incorporating detailed knowledge point annotations, and proposing a dynamic evaluation framework to mitigate data contamination. Experiments on MDK12-Bench reveal significant limitations in current MLLMs, with Gemini2-thinking achieving the highest overall accuracy of 59.4% on the MDK12-Mini dataset. The work highlights the need for models to improve contextual comprehension and robust multimodal reasoning abilities. |
