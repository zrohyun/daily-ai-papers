

## Papers for 2025-04-28

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Towards Understanding Camera Motions in Any Video (Read more on [arXiv](https://arxiv.org/abs/2504.15376) or [HuggingFace](https://huggingface.co/papers/2504.15376))| Jay Karhade, Daniel Jiang, Stephen624, syCen, zhiqiulin | This paper introduces CameraBench, a large-scale dataset and benchmark, to evaluate and enhance camera motion understanding in videos. The primary objective is to develop systems capable of perceiving camera motion with human-like accuracy by addressing ambiguities in motion description and the complexity of real-world videos. Key methodologies include creating a detailed camera motion taxonomy with cinematographers, designing a robust 'label-then-caption' annotation framework with rigorous quality control, and conducting a large-scale human study to quantify performance gains from expertise and training. Evaluating Structure-from-Motion (SfM) and Video-Language Models (VLMs), the study finds SfM struggles with semantic content while VLMs lack geometric precision; however, fine-tuning a generative VLM (Qwen2.5-VL-7B SFT) on CameraBench significantly improves performance, achieving ~59.3% average precision on camera-centric primitive classification, comparable to state-of-the-art SfM methods (~50.1% AP for MegaSAM). The main implication is that CameraBench provides a valuable resource for benchmarking and developing models that fuse geometric and semantic understanding for nuanced camera motion analysis in diverse videos. |
| Multi-Modal | Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.16656) or [HuggingFace](https://huggingface.co/papers/2504.16656))| Xiaokun Wang, Yi Peng, Yichen Wei, Chris, xuchensong | Skywork R1V2 introduces a next-generation multimodal reasoning model trained using a novel hybrid reinforcement learning strategy combining reward-model guidance and rule-based methods. The primary objective is to enhance complex reasoning capabilities in vision-language models while maintaining broad generalization and mitigating issues like visual hallucinations and vanishing RL advantages. Key methodologies include jointly applying Mixed Preference Optimization (MPO) with a multimodal reward model (Skywork-VL Reward) and Group Relative Policy Optimization (GRPO), augmented by a Selective Sample Buffer (SSB) mechanism to improve training efficiency and stability. R1V2 achieves state-of-the-art open-source results, including 62.6% on OlympiadBench and 73.6% on MMMU, significantly narrowing the performance gap with leading proprietary systems. This work offers AI practitioners a promising framework for developing powerful multimodal reasoners via direct reinforcement learning, demonstrating how to balance specialized skills and generalization without supervised fine-tuning (SFT). |
| Machine Learning | BitNet v2: Native 4-bit Activations with Hadamard Transformation for
  1-bit LLMs (Read more on [arXiv](https://arxiv.org/abs/2504.18415) or [HuggingFace](https://huggingface.co/papers/2504.18415))| Furu Wei, Shuming Ma, Hongyu Wang | This paper introduces BitNet v2, a framework enabling native 4-bit activation quantization for 1-bit Large Language Models (LLMs) to improve deployment efficiency. The primary objective is to address the challenge of activation outliers, particularly in attention output and feed-forward network intermediate states, which complicate aggressive quantization. The core methodology involves a novel module called H-BitLinear, which applies an online Hadamard transformation prior to quantizing these activations, reshaping their distributions into more Gaussian-like forms suitable for low-bit representation. Experiments show that BitNet v2 trained from scratch with 8-bit activations matches the performance of the BitNet b1.58 baseline, and crucially, the 4-bit activation variant (BitNet v2 a4) achieves comparable performance (e.g., 58.30 Avg↑ vs 58.12 Avg↑ for the 7B model on end tasks) with minimal degradation. For AI practitioners, this work offers a method to significantly reduce the memory footprint and computational cost of 1-bit LLMs, particularly for batched inference, by facilitating native 4-bit activations without substantial performance loss. |
| Multi-Modal | VideoVista-CulturalLingo: 360^circ Horizons-Bridging Cultures,
  Languages, and Domains in Video Comprehension (Read more on [arXiv](https://arxiv.org/abs/2504.17821) or [HuggingFace](https://huggingface.co/papers/2504.17821))| Wenhan Luo, Baotian Hu, Haoyuan Shi, Yunxin Li, Xinyu Chen | This paper introduces VideoVista-CulturalLingo, a novel video comprehension benchmark designed to evaluate Large Multimodal Models (LMMs) across diverse cultures, languages, and domains. The primary objective is to address the limitations of existing Western-centric, English-only benchmarks by incorporating content from Chinese, American, and European cultures, with questions in both Chinese and English, spanning numerous human-created domains including science. The benchmark was constructed using a hybrid annotation process leveraging (M)LLMs for initial QA generation followed by human verification and refinement, applied to videos sourced from YouTube, Xiaohongshu, and BiliBili. Evaluations on 24 LMMs revealed that models like Gemini-2.0-Flash achieved the highest accuracy (76.3%), while open-source models lagged, performed worse on Chinese-centric content, and struggled with temporal tasks like Event Localization (max 45.2% for open-source). This work highlights significant cultural biases and temporal reasoning weaknesses in current LMMs, indicating the need for diverse benchmarks to foster more globally robust video understanding models. |
| Multi-Modal | Can Large Language Models Help Multimodal Language Analysis? MMLA: A
  Comprehensive Benchmark (Read more on [arXiv](https://arxiv.org/abs/2504.16427) or [HuggingFace](https://huggingface.co/papers/2504.16427))| Peiwu Wang, Hua Xu, Yeshuang Zhu, Zhuohang Li, HanleiZhang | This paper introduces MMLA, a comprehensive benchmark designed to evaluate the capabilities of Multimodal Large Language Models (MLLMs) in understanding high-level cognitive semantics within multimodal language analysis. The primary objective is to assess how well current LLMs and MLLMs comprehend dimensions like intent, emotion, dialogue act, sentiment, speaking style, and communication behavior using text, audio, and video data from over 61K utterances. The methodology involves evaluating eight models using zero-shot inference, supervised fine-tuning (SFT), and instruction tuning (IT) on nine datasets. Key results show that even fine-tuned MLLMs achieve limited performance, with average accuracies around 60-70% (e.g., the top SFT model reached 69.18% ACC), indicating significant limitations in understanding complex human language. The main implication for AI practitioners is that current foundation models, especially in zero-shot settings, struggle significantly with nuanced multimodal conversational semantics, necessitating further research and development facilitated by benchmarks like MMLA. |
| Natural Language Processing | The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs (Read more on [arXiv](https://arxiv.org/abs/2504.17768) or [HuggingFace](https://huggingface.co/papers/2504.17768))| Kelly Marchisio, Sebastian Ruder, Renjie Huang, Robert Li, Piotr Nawrot | This paper provides a comprehensive empirical analysis of training-free sparse attention methods in Transformer LLMs for long-context processing. The primary objective is to systematically evaluate the efficiency-accuracy trade-offs and scaling properties of these methods across varying model sizes (7B-72B), sequence lengths (up to 128k), and sparsity levels (up to 20x compression). Using a diverse benchmark of 9 long-sequence tasks, isoFLOPS analysis, and statistical tests, the study compares representative sparse attention techniques like Vertical-Slash and Quest. Key findings reveal that for very long sequences, large sparse models outperform small dense ones under a fixed compute budget, and while high average sparsity (e.g., >10x) can often be tolerated without statistically significant degradation on average, performance frequently degrades significantly on specific tasks even at moderate levels (e.g., 5x). The main implication for practitioners is that while sparse attention enhances long-context capabilities, its application requires careful, task-specific evaluation of trade-offs due to potential performance sensitivity. |
| Multi-Modal | Subject-driven Video Generation via Disentangled Identity and Motion (Read more on [arXiv](https://arxiv.org/abs/2504.17816) or [HuggingFace](https://huggingface.co/papers/2504.17816))| Wonjoon Jin, Jingxu Zhang, cluo-ms, daiqi, carpedkm | This paper proposes a zero-shot method for subject-driven video generation that disentangles identity learning (using image customization data) from temporal modeling (using unpaired videos). The main objective is to achieve high-fidelity subject preservation and coherent motion in generated videos without requiring annotated video datasets or per-subject optimization. Key techniques include factorizing the training process, employing stochastically-switched fine-tuning between identity injection and temporal awareness preservation, and using random frame selection with image token dropping during image-to-video fine-tuning. The method demonstrates strong performance, achieving superior quantitative results compared to baselines on VBench (e.g., 60.19 dynamic degree and 59.29 DINO-I identity consistency). This work offers AI practitioners a data-efficient and scalable approach for customized video generation, reducing reliance on large, annotated video datasets. |
| Natural Language Processing | DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.15716) or [HuggingFace](https://huggingface.co/papers/2504.15716))| Lifan Guo, Junhui Li, Huaixia Dou, Qian Chen, amazingj | This paper introduces DianJin-R1, a framework enhancing financial reasoning in Large Language Models (LLMs) through structured supervision and reinforcement learning. The primary objective is to improve LLM performance on financial tasks requiring domain knowledge, numerical precision, and compliance adherence. Key methodologies include constructing a reasoning dataset (DianJin-R1-Data), supervised fine-tuning (SFT) models (DianJin-R1-7B/32B based on Qwen2.5) to generate reasoning steps and answers, and applying Group Relative Policy Optimization (GRPO) with format and accuracy rewards. DianJin-R1-32B significantly outperformed its base model, achieving 86.74% on CFLUE and 96.00% accuracy on the CCC compliance dataset, matching or exceeding a multi-agent system with a single API call. The main implication is that structured reasoning supervision combined with targeted RL provides a scalable and effective approach for specialized LLM reasoning in practical applications. |
| Computer Vision | DC-SAM: In-Context Segment Anything in Images and Videos via Dual
  Consistency (Read more on [arXiv](https://arxiv.org/abs/2504.12080) or [HuggingFace](https://huggingface.co/papers/2504.12080))| Lu Qi, Xiaoyang Bi, Xiangtai Li, Mengshi Qi, zaplm | This paper introduces Dual Consistency SAM (DC-SAM), a method adapting the Segment Anything Model (SAM and SAM2) for in-context segmentation in both images and videos using prompt tuning. The primary objective is to enable SAM/SAM2, originally designed for interactive segmentation, to perform one-shot segmentation based on a single visual example (support image/mask). Key methodologies include fusing SAM and backbone features for prompt generation, employing cycle-consistent cross-attention, utilizing dual positive and negative prompt branches for finer control, and extending the approach to videos via a mask-tube training strategy with SAM2. The proposed DC-SAM achieves state-of-the-art results, including 55.5 mIoU on COCO-20², 73.0 mIoU on PASCAL-5², and a J&F score of 71.52 on the newly introduced In-Context Video Object Segmentation (IC-VOS) benchmark. For AI practitioners, this work provides an efficient parameter-tuning technique to adapt powerful foundation models like SAM for in-context segmentation tasks, reducing the need for extensive labeled data or full model retraining, and introduces the first benchmark for in-context video segmentation. |
