

## Papers for 2025-04-18

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for
  Language Model Pre-training (Read more on [arXiv](https://arxiv.org/abs/2504.13161) or [HuggingFace](https://huggingface.co/papers/2504.13161))| Dan Su, Xin Dong, Yonggan Fu, Yu Yang, shizhediao | This paper introduces CLIMB, an automated framework for optimizing large language model pre-training data mixtures. The primary objective is to address the challenge of identifying optimal data mixtures from large-scale, often unlabeled datasets like Common Crawl, without relying on labor-intensive manual curation or predefined domain labels. CLIMB employs a three-step process: embedding and clustering large datasets, constructing mixture-performance pairs using proxy models, and iteratively refining the mixture weights using a predictor in a bootstrapping approach. Key results show that a 1B model trained on 400B tokens using the CLIMB-optimized mixture (ClimbMix) outperforms the state-of-the-art Llama-3.2-1B by 2.0% on average across 12 benchmarks, and optimizing for specific domains like Social Sciences yields a 5% improvement over random sampling. For AI practitioners, CLIMB offers a data-driven, automated method to curate effective pre-training datasets, potentially improving model performance and efficiency, especially for domain-specific tasks. |
| Natural Language Processing | Antidistillation Sampling (Read more on [arXiv](https://arxiv.org/abs/2504.13146) or [HuggingFace](https://huggingface.co/papers/2504.13146))| Avi Schwarzschild, Zhili Feng, Asher Trockman, arobey1, yashsavani | This paper introduces antidistillation sampling, a technique designed to modify the output generation process of large language models (LLMs) to hinder model distillation while preserving the original model's utility. The primary objective is to prevent competitors from effectively replicating proprietary model capabilities by training on publicly released reasoning traces, without sacrificing the teacher model's performance. The core methodology involves adjusting the next-token sampling distribution by adding a penalty term derived from an approximation of the directional derivative of a proxy student model's loss, effectively 'poisoning' the traces for distillation. Empirical results demonstrate that antidistillation sampling significantly degrades the performance of distilled student models (e.g., reducing GSM8K accuracy from 51.86% with temperature sampling to 24.73% for a similar teacher accuracy) compared to standard sampling methods. The main implication for AI practitioners is the potential to protect proprietary models from distillation-based capability theft by strategically altering output sampling. |
| Natural Language Processing | A Strategic Coordination Framework of Small LLMs Matches Large LLMs in
  Data Synthesis (Read more on [arXiv](https://arxiv.org/abs/2504.12322) or [HuggingFace](https://huggingface.co/papers/2504.12322))| Honglin Lin, Yu Li, Zinan Tang, Qizhi Pei, GX-XinGao | This paper proposes GRA, a strategic coordination framework where multiple small LLMs collaborate in distinct roles (Generator, Reviewer, Adjudicator) to perform high-quality data synthesis. The primary objective is to investigate if coordinated small LLMs can achieve data synthesis quality comparable to resource-intensive monolithic large LLMs. The methodology mimics a peer-review process, decomposing synthesis into specialized sub-tasks with iterative refinement and conflict resolution. Experiments demonstrate that GRA-produced data matches or exceeds the quality of data from single large models; for instance, using a Qwen-2.5-7B base model, GRA data achieved an average performance gain of 11.81% over vanilla seed data across benchmarks and surpassed Qwen-2.5-72B-Instruct distilled data by 8.83% on average. This suggests that practitioners can achieve high-quality data synthesis more sustainably by coordinating smaller, specialized models instead of relying solely on large monolithic ones. |
| Computer Vision | Packing Input Frame Context in Next-Frame Prediction Models for Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2504.12626) or [HuggingFace](https://huggingface.co/papers/2504.12626))| Maneesh Agrawala, Lvmin Zhang | This paper introduces FramePack, a neural network structure for next-frame video generation models that efficiently compresses input frame context to handle long videos, along with anti-drifting sampling methods. The main objective is to mitigate the 'forgetting' of past information and 'drifting' of visual quality due to error accumulation inherent in iterative prediction. FramePack employs progressive compression of input frames based on importance using variable transformer patchify kernel sizes, ensuring a fixed context length, while anti-drifting sampling leverages bi-directional context, such as inverted temporal generation order. Experiments demonstrate that combining FramePack with inverted anti-drifting sampling yields significant improvements in visual quality and consistency, achieving the highest human preference ELO score (1239) in ablation studies. The key implication for AI practitioners is a method to improve the efficiency and quality of long video generation by addressing context length limitations and error propagation in next-frame prediction frameworks. |
| Multi-Modal | Generate, but Verify: Reducing Hallucination in Vision-Language Models
  with Retrospective Resampling (Read more on [arXiv](https://arxiv.org/abs/2504.13169) or [HuggingFace](https://huggingface.co/papers/2504.13169))| Trevor Darrell, Joseph E. Gonzalez, Jiaxin Ge, Heekyung Lee, tsunghanwu | This paper introduces REVERSE, a unified framework designed to reduce visual hallucinations in Vision-Language Models (VLMs) by integrating generation and verification with retrospective resampling. The main objective is to enable VLMs to self-detect and dynamically correct hallucinations during text generation, overcoming limitations of separate generation adjustment and post-hoc verification methods. The core methodology involves hallucination-aware fine-tuning on a novel 1.3M sample dataset tagged with confidence tokens (</CN>/</UN>) and an inference-time retrospective resampling technique that triggers backtracking and correction when hallucination likelihood is high. REVERSE achieves state-of-the-art results, outperforming existing methods by up to 12% on CHAIR-MSCOCO and 28% on HaloQuest hallucination benchmarks. The primary implication for AI practitioners is a method to build more trustworthy VLMs capable of self-correction within a single model, potentially reducing reliance on external verifiers or complex pipelines. |
| Computer Vision | WORLDMEM: Long-term Consistent World Simulation with Memory (Read more on [arXiv](https://arxiv.org/abs/2504.12369) or [HuggingFace](https://huggingface.co/papers/2504.12369))| Shuai Yang, Wenqi Ouyang, Yifan Zhou, Yushi Lan, Zeqi Xiao | WORLDMEM introduces a memory-augmented framework for long-term consistent world simulation, addressing the limitations of finite temporal context windows in video generation models. The primary objective is to maintain 3D spatial consistency over extended periods, allowing models to accurately recall and reconstruct previously generated scenes despite significant viewpoint or temporal gaps. The methodology integrates a conditional diffusion transformer (CDiT) with an external memory bank storing past frames and states (poses, timestamps), using a dedicated memory attention mechanism with relative state embeddings to retrieve and condition generation. Experiments on Minecraft show significant improvements in consistency beyond the context window, achieving an rFID of 15.37 compared to 51.28 for the baseline Diffusion Forcing method. For AI practitioners, WORLDMEM provides a viable approach for creating more persistent and interactive simulated environments capable of capturing dynamic evolution, relevant for simulation, long-duration video synthesis, and interactive applications. |
| Multi-Modal | VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference
  Optimization for Large Video Models (Read more on [arXiv](https://arxiv.org/abs/2504.13122) or [HuggingFace](https://huggingface.co/papers/2504.13122))| Meng Luo, Haojian Huang, scofield7419, ChocoWu, Harold328 | This paper introduces VistaDPO, a hierarchical spatial-temporal framework using Direct Preference Optimization (DPO) to enhance alignment and reduce hallucinations in Large Video Models (LVMs). The primary objective is to improve text-video preference alignment across instance, temporal, and perceptive levels by addressing limitations in existing LVMs and DPO methods for video tasks. The key methodology involves optimizing preferences hierarchically using a novel dataset, VistaDPO-7k, containing 7.2K QA pairs annotated with chosen/rejected responses and fine-grained spatial-temporal grounding. Experiments demonstrate significant improvements, such as increasing the VideoHallucer overall score by 51.7% for PLLaVA (from 38.1% to 57.8%) and 205.1% for Video-LLaVA (from 17.8% to 54.3%) compared to their respective baselines. For AI practitioners, VistaDPO offers a method and dataset to fine-tune LVMs for more robust video understanding and reduced hallucination by incorporating hierarchical spatial-temporal preference learning. |
| Multi-Modal | NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation (Read more on [arXiv](https://arxiv.org/abs/2504.13055) or [HuggingFace](https://huggingface.co/papers/2504.13055))| Chao Du, Zijian Wu, Jinjie Ni, Xiangyan Liu, dreamerdeo | NoisyRollout introduces a reinforcement learning (RL) fine-tuning strategy using data augmentation to enhance visual reasoning in vision-language models (VLMs). The primary objective is to improve policy exploration and mitigate imperfect visual perception issues in VLMs during RL training. The methodology involves a hybrid rollout approach within Group Relative Policy Optimization (GRPO), mixing trajectories from clean and distorted images, combined with a noise annealing schedule that reduces distortion over time. Trained with just 2.1K samples, NoisyRollout achieves state-of-the-art results among open-source RL-tuned models on several out-of-domain benchmarks, reaching an average accuracy of 59.2% across five tasks. For AI practitioners, this presents a simple, cost-effective method to boost VLM generalization and robustness by incorporating visually-oriented inductive biases during RL fine-tuning. |
| Multi-Modal | ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question
  Answering (Read more on [arXiv](https://arxiv.org/abs/2504.05506) or [HuggingFace](https://huggingface.co/papers/2504.05506))| Firoz Kabir, Aayush Bajaj, Mahir Ahmed, 38saidul, ahmed-masry | This paper introduces ChartQAPro, a diverse and challenging benchmark for Chart Question Answering (CQA) designed to address the limitations of existing datasets. The primary objective is to evaluate Large Vision-Language Models (LVLMs) on more realistic and complex chart understanding tasks, moving beyond the performance saturation seen on benchmarks like ChartQA. ChartQAPro was constructed using 1,341 charts from 157 diverse sources, including infographics and dashboards, paired with 1,948 human-written/verified questions covering multiple types like conversational, hypothetical, and unanswerable. Evaluations on 21 LVLMs revealed a substantial performance decrease; for instance, Claude Sonnet 3.5 dropped from 90.5% accuracy on ChartQA to 55.81% on ChartQAPro. The key implication for AI practitioners is that current LVLMs still face significant challenges in complex chart reasoning, suggesting that previous benchmarks may have overestimated progress and highlighting areas for future research. |
| Reinforcement Learning | Exploring Expert Failures Improves LLM Agent Tuning (Read more on [arXiv](https://arxiv.org/abs/2504.13145) or [HuggingFace](https://huggingface.co/papers/2504.13145))| Ruochen Wang, Minhao Cheng, Andrew Bai, Li-Cheng Lan, zhoutianyi | This paper introduces Exploring Expert Failures (EEF), a method to enhance Large Language Model (LLM) agent tuning by leveraging beneficial actions from failed expert trajectories. The primary objective is to overcome the limitations of Rejection Sampling Fine-Tuning (RFT), which discards failed trajectories entirely, often leaving complex subtasks unsolved and out-of-distribution. EEF identifies valuable action sequences within failed expert attempts through agent simulations initiated from intermediate expert states and integrates these beneficial segments into the fine-tuning dataset while excluding harmful actions. The proposed method achieved a 62% win rate on WebShop, significantly outperforming RFT (53.6%) and the expert GPT-4 (35.6%), setting new state-of-the-art scores on WebShop (>0.81) and SciWorld (>81). For AI practitioners, this implies that valuable training signals can be extracted even from imperfect or failed expert demonstrations, improving data efficiency and agent capability on challenging tasks. |
| Multi-Modal | InstantCharacter: Personalize Any Characters with a Scalable Diffusion
  Transformer Framework (Read more on [arXiv](https://arxiv.org/abs/2504.12395) or [HuggingFace](https://huggingface.co/papers/2504.12395))| Yiji Cheng, Qixun Wang, Yanbing Zhang, Jiale Tao, wanghaofan | The paper introduces InstantCharacter, a scalable framework built upon a diffusion transformer (DiT) for high-fidelity, personalized character image generation from reference images and text prompts. The main objective is to overcome the limitations of existing UNet-based and tuning-based approaches in generalizing to open-domain characters while maintaining image quality and textual controllability, especially for large DiT models. Key methodologies include a scalable adapter with stacked transformer encoders using SigLIP and DINOv2 features, a dual-stream fusion for multi-level features, and a progressive three-stage training strategy on a curated 10-million-sample dataset combining paired and unpaired data. Qualitative results demonstrate superior performance in generating character-consistent, text-controllable, high-fidelity images compared to prior methods, setting a new benchmark, although specific quantitative metrics are not detailed in the provided text. For AI practitioners, this work provides a robust method for adapting foundation DiT models for specialized controllable generation tasks like character personalization, offering better generalization and fidelity than previous techniques. |
| Computer Vision | CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera
  Color Constancy (Read more on [arXiv](https://arxiv.org/abs/2504.07959) or [HuggingFace](https://huggingface.co/papers/2504.07959))| Seon Joo Kim, Michael S. Brown, Dongyun Kim, Mahmoud Afifi, dongyong2 | This paper introduces CCMNet, a novel learning-based framework for achieving cross-camera color constancy without requiring retraining or additional test images. The primary objective is to develop an illuminant estimation method that generalizes effectively to unseen camera sensors by leveraging readily available camera calibration data. CCMNet utilizes pre-calibrated Color Correction Matrices (CCMs) from camera ISPs to generate a Camera Fingerprint Embedding (CFE), which encodes the camera's color characteristics and guides a hypernetwork to adapt its processing for the specific camera's raw color space; an imaginary camera augmentation technique is also proposed to enhance generalization during training. Experimental results demonstrate state-of-the-art performance, achieving a mean angular error of 1.68° on the Cube+ dataset, surpassing previous methods like C5. For AI practitioners, CCMNet offers a lightweight and practical approach for robust color constancy in diverse camera systems by utilizing existing ISP calibration data. |
| Multi-Modal | FocusedAD: Character-centric Movie Audio Description (Read more on [arXiv](https://arxiv.org/abs/2504.12157) or [HuggingFace](https://huggingface.co/papers/2504.12157))| Liangcheng Li, Sheng Zhou, Yiren Song, Chun Wang, Xiaojun Ye | FocusedAD introduces a framework for generating character-centric movie audio descriptions (AD) by focusing on main characters and storyline-relevant visual elements. The primary objective is to create AD that is concise, plot-relevant, and includes explicit named character references, addressing limitations of current methods that often produce object-centric or overly generic narrations. Key methodology includes a Character Perception Module (CPM) for character tracking and identification using an automatically built query bank, a Dynamic Prior Module (DPM) using soft prompts to incorporate context from prior ADs and subtitles, and a Focused Caption Module (FCM) to generate descriptions integrating scene, character, and text tokens. FocusedAD achieves state-of-the-art performance, demonstrating strong zero-shot results on MAD-eval-Named (e.g., BertScore 57.7) and the new Cinepile-AD dataset (e.g., BertScore 64.5). For AI practitioners, this work presents a method to generate more effective, context-aware, and character-focused video descriptions, crucial for applications like accessibility services for BVI audiences. |
| Natural Language Processing | Retrieval-Augmented Generation with Conflicting Evidence (Read more on [arXiv](https://arxiv.org/abs/2504.13079) or [HuggingFace](https://huggingface.co/papers/2504.13079))| Mohit Bansal, Elias Stengel-Eskin, Archiki Prasad, HanNight | This paper introduces methods for improving Retrieval-Augmented Generation (RAG) systems when faced with conflicting evidence from multiple retrieved documents. The primary objective is to develop RAG systems that can simultaneously handle ambiguity (multiple valid answers), misinformation, and noise (irrelevant documents), unlike prior work addressing these issues in isolation. The key methodology involves introducing the RAMDocs dataset, simulating these complex conflict scenarios, and proposing MADAM-RAG, a multi-agent debate framework where agents evaluate individual documents and iteratively discuss their findings before an aggregator synthesizes the final response. MADAM-RAG demonstrated significant improvements, boosting performance by up to 11.40% on AmbigDocs and 15.80% on FaithEval over strong baselines with Llama3.3-70B-Instruct. The main implication for AI practitioners is the need to address diverse sources of conflict jointly in RAG systems, with MADAM-RAG presenting a viable approach, although handling evidence imbalance and combined conflicts remains challenging. |
| Natural Language Processing | Sleep-time Compute: Beyond Inference Scaling at Test-time (Read more on [arXiv](https://arxiv.org/abs/2504.13171) or [HuggingFace](https://huggingface.co/papers/2504.13171))| Sarah Wooders, Charles Packer, Yu Wang, Charlie Snell, Kevin Lin | This paper introduces "sleep-time compute," a technique allowing large language models (LLMs) to pre-process context offline, aiming to reduce test-time computational requirements and latency. The primary objective is to evaluate if anticipating potential user queries and pre-computing relevant information about a known context can improve the efficiency and accuracy of LLM inference. The methodology involves using the LLM during idle periods ("sleep-time") to generate an enriched context representation, which is then used for faster inference when the actual user query arrives at "test-time." Key results show that sleep-time compute can reduce the test-time compute needed for equivalent accuracy by approximately 5x on modified reasoning tasks (Stateful GSM-Symbolic and Stateful AIME), and scaling sleep-time compute can further boost accuracy by up to 18% on Stateful AIME. For AI practitioners, this implies that leveraging idle compute for context pre-processing in stateful applications can significantly decrease inference latency and cost while maintaining or even improving performance. |
| Computer Vision | Set You Straight: Auto-Steering Denoising Trajectories to Sidestep
  Unwanted Concepts (Read more on [arXiv](https://arxiv.org/abs/2504.12782) or [HuggingFace](https://huggingface.co/papers/2504.12782))| Adams Wai-Kin Kong, Yan Ren, Leyang Li, Shilin-LU | This paper introduces ANT (Automatically guiding deNoising Trajectories), a finetuning framework to prevent text-to-image diffusion models from generating unwanted concepts by auto-steering denoising trajectories. The main objective is to achieve effective concept erasure without degrading image quality, disrupting early-stage structural integrity, or relying on heuristic anchor concepts. ANT employs a trajectory-aware loss function that preserves early-stage score function fields while reversing the classifier-free guidance condition direction during mid-to-late denoising, combined with an augmentation-enhanced weight saliency map to identify critical parameters. Experiments show state-of-the-art results, achieving a low inappropriate image count (23) for NSFW erasure on the I2P dataset and the highest harmonic mean (He=0.9173) for celebrity erasure, while maintaining competitive FID scores (e.g., 11.71 for celebrity erasure). For AI practitioners, ANT offers a more robust and automated method for ensuring the safe deployment of text-to-image models by selectively removing harmful content generation capabilities without compromising overall generative fidelity. |
| Multi-Modal | Perception Encoder: The best visual embeddings are not at the output of
  the network (Read more on [arXiv](https://arxiv.org/abs/2504.13181) or [HuggingFace](https://huggingface.co/papers/2504.13181))| Andrea Madotto, Jang Hyun Cho, Peize Sun, Po-Yao Huang, Daniel Bolya | This paper introduces Perception Encoder (PE), a family of vision encoders trained solely with contrastive vision-language learning, finding that the most effective visual embeddings for diverse tasks are located within intermediate layers, not the final output. The primary objective is to demonstrate that a single, scalable contrastive pretraining strategy can generate strong, general-purpose features suitable for classification, retrieval, multimodal language modeling, and dense spatial tasks. The key methodology involves a robust image pretraining recipe, finetuning with a novel video data engine (PE Video Dataset), and introducing language and spatial alignment tuning methods to surface optimal features from specific intermediate layers. The PE model family achieves state-of-the-art results, with PEcore G attaining 86.6% average zero-shot image classification across robustness benchmarks, and PEspatial G matching COCO detection state-of-the-art (65.5 APbox) with a simple decoder. The main implication is that carefully scaled contrastive pretraining, combined with alignment tuning of intermediate features, can potentially replace complex, multi-objective pretraining strategies for building versatile foundation vision models. |
