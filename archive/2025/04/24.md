

## Papers for 2025-04-24

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal
  Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.15279) or [HuggingFace](https://huggingface.co/papers/2504.15279))| Einsiedler, luotto, Weiyun1025, GenuineWWD, wilye | This paper introduces VisuLogic, a benchmark designed to rigorously evaluate the visual reasoning capabilities of multimodal large language models (MLLMs) while minimizing reliance on language-based shortcuts. The primary objective is to assess genuine vision-centric reasoning by presenting 1,000 human-verified problems across six categories (e.g., quantitative, spatial, attribute reasoning) that are inherently difficult to solve through textual description alone. The methodology involves evaluating leading MLLMs and humans on this benchmark, analyzing failure modes, and providing a supplementary training set with a reinforcement learning baseline. Key results reveal significant deficits in current MLLMs, with most scoring below 30% accuracy (best performing closed-source model at 28.1%), substantially underperforming humans (51.4%) and barely exceeding the random baseline (25%). For AI practitioners, this implies that current MLLMs struggle with complex visual logic, necessitating more robust evaluation benchmarks like VisuLogic and targeted training approaches like reinforcement learning to improve genuine multimodal reasoning. |
| Computer Vision | DreamID: High-Fidelity and Fast diffusion-based Face Swapping via
  Triplet ID Group Learning (Read more on [arXiv](https://arxiv.org/abs/2504.14509) or [HuggingFace](https://huggingface.co/papers/2504.14509))| heqian, giruhc9gj, Crayon-Shinchan, miaohua, Alon77777 | DreamID introduces a diffusion-based face swapping model focused on high fidelity, identity similarity, and fast inference speed. The main research objective is to overcome the limitations of implicit supervision in prior methods by establishing explicit supervision for face swapping to significantly enhance identity similarity and attribute preservation. Key methodologies include constructing Triplet ID Group data for explicit pixel-level supervision, leveraging the accelerated diffusion model SD Turbo for single-step inference enabling efficient end-to-end training, and proposing an improved architecture with SwapNet, FaceNet, and ID Adapter modules. Results demonstrate state-of-the-art performance, achieving an ID similarity score of 0.71 and generating 512x512 swaps in 0.6 seconds. For AI practitioners, DreamID offers a robust and efficient framework for high-quality face swapping, highlighting the benefits of explicit supervision combined with accelerated diffusion models. |
| Natural Language Processing | Trillion 7B Technical Report (Read more on [arXiv](https://arxiv.org/abs/2504.15431) or [HuggingFace](https://huggingface.co/papers/2504.15431))| Suyeong An, hist0613, kyudolski, scottsuk0306, sungjunhan-trl | This paper introduces Trillion-7B, a highly token-efficient Korean-centric multilingual Large Language Model (LLM). The primary objective is to achieve strong multilingual performance, especially for Korean, while minimizing reliance on vast amounts of multilingual training data. The key methodology involves a novel Cross-lingual Document Attention (XLDA) mechanism for efficient knowledge transfer from English, combined with optimized data mixtures, filtering, tokenization, and a two-stage pretraining process followed by SFT, DPO, and RLVR post-training. Trillion-7B demonstrates competitive performance across 27 benchmarks, achieving a 57.15% average score (Table 4) using only 10% multilingual tokens during its 2T token pretraining and requiring just 59.4K H100 GPU hours. The main implication is that architectural innovations and optimized training recipes, like XLDA, can enable high-performing multilingual models without massive language-specific data scaling, offering a more efficient path for less-resourced languages. |
| Natural Language Processing | Pre-DPO: Improving Data Utilization in Direct Preference Optimization
  Using a Guiding Reference Model (Read more on [arXiv](https://arxiv.org/abs/2504.15843) or [HuggingFace](https://huggingface.co/papers/2504.15843))| Yue Zhang, Qiji Zhou, Shulin Huang, Junshu Pan, Swtheking | This paper introduces Pre-DPO, a training paradigm designed to enhance data utilization and performance in Direct Preference Optimization (DPO) and similar methods for aligning large language models (LLMs). The primary objective is to overcome the limitations of standard DPO reference models (often identical to the initial policy) and the robustness issues of reference-free methods like SimPO. Pre-DPO employs a two-stage approach: first, an initial policy is optimized using a standard preference method (like DPO or SimPO); this optimized policy then serves as a 'guiding reference model' for a second DPO optimization round on the initial policy, adaptively reweighting training data. Experiments show Pre-DPO consistently improves performance, achieving average gains of 2.5 points on AlpacaEval 2 LC and improving the Arena-Hard WR for Qwen2.5-7B-Instruct DPO from 62.9 to 68.8. The implication for practitioners is that leveraging a guiding reference model derived from a preliminary optimization pass can unlock better performance from existing preference datasets and models without requiring external resources. |
| Machine Learning | I-Con: A Unifying Framework for Representation Learning (Read more on [arXiv](https://arxiv.org/abs/2504.16929) or [HuggingFace](https://huggingface.co/papers/2504.16929))| John Hershey, Shaden Alshammari, mhamilton723, mrpuppt, axelf | The I-Con paper introduces a unified information-theoretic framework for representation learning based on minimizing KL divergence between conditional neighborhood distributions. Its main objective is to demonstrate that diverse methods like clustering, contrastive learning, dimensionality reduction, and supervised classification are special cases of this single underlying loss function. The key methodology involves parameterizing supervisory (p) and learned (q) conditional probability distributions and minimizing their average KL divergence DKL(p(·|i)||q(·|i)). Applying this framework led to state-of-the-art unsupervised image classification results, achieving a +8% improvement over prior methods on ImageNet-1K (67.52% Hungarian Accuracy using their Debiased InfoNCE Clustering with ViT-L/14) and improved debiasing techniques for contrastive learners. For practitioners, I-Con offers a principled way to understand relationships between loss functions, transfer techniques across domains, and potentially design novel representation learning algorithms. |
| Multi-Modal | Decoupled Global-Local Alignment for Improving Compositional
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2504.16801) or [HuggingFace](https://huggingface.co/papers/2504.16801))| Ziyong Feng, Jun Wang, haoranxu, Kaichengalex, xiaoxing2001 | This paper introduces DeGLA (Decoupled Global-Local Alignment), a framework designed to improve the compositional understanding of vision-language models like CLIP while mitigating the loss of general capabilities often seen in previous fine-tuning methods. The primary objective is to enhance the model's grasp of concepts like attributes and relations without causing catastrophic forgetting of pre-trained knowledge. DeGLA employs a global alignment strategy incorporating self-distillation with an EMA teacher model and a local alignment strategy using novel Image-Grounded Contrast (IGC) and Text-Grounded Contrast (TGC) losses, trained with LLM-generated hard negative captions. Compared to prior state-of-the-art, DeGLA demonstrates an average improvement of 3.5% on compositional benchmarks (VALSE, SugarCrepe, ARO) and simultaneously achieves a 13.0% average improvement on 11 zero-shot classification tasks, showcasing its ability to balance specialization and generalization. For AI practitioners, DeGLA provides a technique to enhance specific compositional skills in VLMs without sacrificing, and even improving, their broad applicability. |
| Multi-Modal | DreamO: A Unified Framework for Image Customization (Read more on [arXiv](https://arxiv.org/abs/2504.16915) or [HuggingFace](https://huggingface.co/papers/2504.16915))| LemonSky1995, Crayon-Shinchan, shiwenzh, Zinan123212, yanze | DreamO is presented as a unified framework leveraging a pre-trained Diffusion Transformer (DiT) for diverse and flexible image customization. The main objective is to create a single model capable of handling various customization conditions (e.g., identity, subject, style, try-on) and their combinations, unlike specialized task-specific models. Key methodologies include using a DiT (Flux-1.0-dev) with LoRA adaptation, unifying text and image conditions as input sequences, introducing a feature routing constraint based on cross-attention maps to improve fidelity and decoupling, employing a placeholder strategy for positional control, and utilizing a progressive three-stage training process. Results demonstrate high-quality, flexible image customization across numerous tasks, supported by extensive qualitative examples and ablation studies, although specific quantitative benchmark metrics (like FID) are not explicitly reported in the provided text. For AI practitioners, DreamO offers a versatile, computationally efficient approach (707M LoRA parameters) to perform complex, multi-condition image customization within a single framework. |
| Reinforcement Learning | Tina: Tiny Reasoning Models via LoRA (Read more on [arXiv](https://arxiv.org/abs/2504.15777) or [HuggingFace](https://huggingface.co/papers/2504.15777))| Ollie Liu, Enes Burak Bilgin, Ömer Faruk Akgül, Julian Asilis, upup-ashton-wang | This paper presents Tina, a family of small yet capable reasoning language models developed with high cost-efficiency using Low-Rank Adaptation (LoRA) during reinforcement learning (RL). The central research question is how cost-effectively strong reasoning abilities can be instilled in language models via RL. The methodology involves applying parameter-efficient RL updates, specifically using LoRA with a GRPO-style algorithm, to a tiny 1.5B parameter base model (DeepSeek-R1-Distill-Qwen-1.5B). Key results show that Tina models achieve reasoning performance competitive with, and sometimes surpassing, fully fine-tuned SOTA RL models on the same base, with the best model achieving 43.33% Pass@1 accuracy on AIME24 at an estimated $9 post-training and evaluation cost (a 260x reduction). The main implication is that effective reasoning enhancement via RL is achievable with minimal compute resources by leveraging LoRA, potentially by rapidly adapting the model's output format while preserving core knowledge. |
| Machine Learning | A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training
  and Deployment (Read more on [arXiv](https://arxiv.org/abs/2504.15585) or [HuggingFace](https://huggingface.co/papers/2504.15585))| Guibin Zhang, Kun Wang, Ningyu, Atarogic, Fred456 | This survey presents a comprehensive "full-stack" analysis of safety concerns for Large Language Models (LLMs) and LLM-agents throughout their entire lifecycle. Its objective is to systematically categorize security risks and mitigation strategies across all phases—data preparation, pre-training, post-training (including alignment, editing, unlearning), deployment, and agent systems—addressing the limitations of prior, phase-specific surveys. The paper employs an extensive literature review methodology, synthesizing insights from over 800+ publications to construct a holistic taxonomy of LLM safety. Key findings include the identification of vulnerabilities persisting across stages (e.g., data poisoning effects from <0.1% data corruption) and the cataloging of defense mechanisms relevant to data, training, alignment, editing, and deployment phases, particularly for agent systems. The primary implication for practitioners is the critical need to integrate safety considerations throughout the *entire* LLM development and operational pipeline, moving beyond deployment-only checks to encompass data provenance, training protocols, alignment robustness, and agent interaction security. |
| Multi-Modal | RePOPE: Impact of Annotation Errors on the POPE Benchmark (Read more on [arXiv](https://arxiv.org/abs/2504.15707) or [HuggingFace](https://huggingface.co/papers/2504.15707))| Matthias Hein, YanNeu | This paper assesses the impact of annotation errors in the MSCOCO dataset on the POPE benchmark, commonly used for evaluating object hallucinations in Vision Large Language Models (VLMs). The primary objective is to quantify these errors and understand how they influence benchmark results and model rankings. The methodology involves re-annotating the 500 POPE benchmark images by consensus, creating a corrected version called RePOPE, and excluding ambiguous cases. Results show substantial errors in the original POPE labels, particularly for positive examples (9.3% incorrect, 13.8% ambiguous), leading to significant shifts in model F1 score rankings when evaluated on RePOPE compared to POPE. The key implication is that benchmark label quality critically affects VLM evaluation, and relying on uncorrected datasets like POPE can yield potentially misleading assessments of model hallucination tendencies. |
| Machine Learning | Rethinking the Generation of High-Quality CoT Data from the Perspective
  of LLM-Adaptive Question Difficulty Grading (Read more on [arXiv](https://arxiv.org/abs/2504.11919) or [HuggingFace](https://huggingface.co/papers/2504.11919))| Keyu Wu, Kunlinliu2, MeiManlin, zcs1234, USTCYu | This paper introduces a novel method for generating high-quality Chain-of-Thought (CoT) data tailored to enhance the reasoning capabilities of smaller Large Language Models (LLMs). The primary objective is to create CoT datasets efficiently by adaptively grading question difficulty based on the target LLM's intrinsic reasoning abilities. The core methodology involves evaluating questions using a base LLM, grading their difficulty (e.g., using a PRM-Grader), sampling questions based on a difficulty distribution, and then generating correct CoT responses using a powerful teacher model (DeepSeek-R1 671B) for supervised fine-tuning (SFT). Results show significant improvements; for instance, fine-tuning a 32B model with only 2k adaptive math CoT data (ZMath-32B) achieved 73.33% on AIME24, surpassing a baseline (DeepSeek-Distill-32B) trained on much larger, non-adaptive data. This implies that practitioners can boost smaller LLM reasoning performance more effectively and cost-efficiently by focusing SFT on data curated according to model-specific difficulty levels. |
| Machine Learning | CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation (Read more on [arXiv](https://arxiv.org/abs/2504.15254) or [HuggingFace](https://huggingface.co/papers/2504.15254))| Ziteng Wang, Jia Pan, Robert Zhang, gregdurrett, anirudhkhatry | This paper introduces CRUST-Bench, a comprehensive benchmark for evaluating C-to-safe-Rust code transpilation. The primary objective is to assess the capability of automated systems, particularly Large Language Models (LLMs), to translate C repositories into functionally correct, memory-safe, and idiomatic Rust code. The methodology involves creating a dataset of 100 C repositories paired with manually-written Rust interfaces and test cases, then evaluating various LLMs and agentic systems under single-shot and iterative repair settings. Key results show that even top models like OpenAI o1 achieve limited success, solving only 15% of tasks single-shot and up to 37% with iterative test-based repair. The main implication for AI practitioners is that generating safe and idiomatic Rust from C at the repository level remains a significant challenge, necessitating further research into models better equipped to handle Rust's complex semantics and static guarantees. |
| Multi-Modal | Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large
  Language Models with CheckboxQA (Read more on [arXiv](https://arxiv.org/abs/2504.10419) or [HuggingFace](https://huggingface.co/papers/2504.10419))| Borchmann, sf-mchilinski, mturski | This paper introduces CheckboxQA, a new dataset designed to address the poor performance of Large Vision and Language Models (LVLMs) in interpreting checkboxes within visually rich documents. The primary objective is to evaluate and improve model capabilities on this specific, yet critical, document understanding task often overlooked in standard benchmarks. The methodology involved collecting diverse documents with checkboxes, annotating them with question-answer pairs targeting checkbox states, and evaluating state-of-the-art commercial and open-source LVLMs using the Document VQA paradigm and the ANLS* metric. Results show significant variability, with Qwen 2.5 VL 72B achieving the highest score (83.2% ANLS*), yet all models fall substantially short of the human baseline (97.5%). The key implication for AI practitioners is that checkbox interpretation remains a challenging blind spot for current LVLMs, necessitating focused datasets like CheckboxQA and potentially specialized approaches for reliable automated document processing. |
| Multi-Modal | Progressive Language-guided Visual Learning for Multi-Task Visual
  Grounding (Read more on [arXiv](https://arxiv.org/abs/2504.16145) or [HuggingFace](https://huggingface.co/papers/2504.16145))| Dingjiang Huang, Kunhua Ji, Wenlong Zhang, Hong Wang, jcwang0602 | This paper introduces PLVL, a novel Progressive Language-guided Visual Learning framework for Multi-Task Visual Grounding (MTVG), jointly addressing Referring Expression Comprehension (REC) and Segmentation (RES). The main objective is to overcome limitations in prior work by deeply integrating linguistic guidance throughout the visual feature extraction process and better exploiting the inherent relationship between REC and RES. The methodology features a progressive visual backbone that injects language information via cross-attention within global blocks and a convolution-based collaborative multi-task head designed for joint prediction. PLVL achieves state-of-the-art performance on benchmark datasets, reaching 89.80% accuracy for REC and 77.67% mIoU for RES on RefCOCOg test(U) under pre-training. For AI practitioners, this work demonstrates an effective approach to fuse modalities progressively within the backbone and leverage task correlations via a unified head for improved multi-modal, multi-task learning. |
