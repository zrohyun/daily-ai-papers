

## Papers for 2025-04-29

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | RepText: Rendering Visual Text via Replicating (Read more on [arXiv](https://arxiv.org/abs/2504.19724) or [HuggingFace](https://huggingface.co/papers/2504.19724))| Junchen Li, Yimeng Li, SNOWAI, YujiaX, wanghaofan | RepText introduces a framework enabling pre-trained monolingual text-to-image models to render accurate multilingual visual text by replicating glyphs instead of requiring semantic understanding. The main objective is to achieve controllable text rendering (content, font, position) on models like FLUX without costly retraining or replacing text encoders. The key methodology involves using a ControlNet conditioned on canny edges and position maps of the target glyphs, augmented with a text perceptual loss during training, and inference techniques like glyph latent replication and regional masking. Qualitative experiments indicate RepText outperforms existing open-source methods and achieves comparable results to some closed-source multilingual models, though specific quantitative evaluation metrics are not detailed in the summary sections. For practitioners, RepText offers a resource-efficient approach to add high-fidelity, controllable multilingual text rendering to existing diffusion models. |
| Multi-Modal | LLM-Powered GUI Agents in Phone Automation: Surveying Progress and
  Prospects (Read more on [arXiv](https://arxiv.org/abs/2504.19838) or [HuggingFace](https://huggingface.co/papers/2504.19838))| Yaxuan Guo, Guangyi Liu, Yuxiang007, melpancake, Pengxiangzhao | This paper surveys the progress and prospects of LLM-powered GUI agents for phone automation, evolving from script-based systems to intelligent agents. Its objective is to systematically review these agents, analyze how LLMs overcome traditional limitations through advanced language understanding and multimodal perception, propose a taxonomy, and identify challenges. The methodology involves a comprehensive literature review, categorizing works based on frameworks (single-agent, multi-agent, plan-then-act), models (prompting vs. training), datasets, and benchmarks. Key results show significant improvements in task automation adaptability and efficiency, with RL-based methods like DigiRL demonstrating a 49.5% absolute success rate increase over supervised methods on the Android-in-the-Wild benchmark. For AI practitioners, this survey provides a structured overview, highlighting the potential of LLMs to create robust GUI agents while pointing to critical future research directions like dataset scalability, on-device deployment, and security. |
| Machine Learning | CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through
  Cryptography Challenges (Read more on [arXiv](https://arxiv.org/abs/2504.19093) or [HuggingFace](https://huggingface.co/papers/2504.19093))| Chenlin Ming, Honglin Lin, Qizhi Pei, blue01223, yu0226 | This paper introduces CipherBank, a comprehensive benchmark designed to evaluate the cryptographic reasoning capabilities of Large Language Models (LLMs). The primary objective is to assess the performance and limitations of SOTA LLMs on decryption tasks involving classical and custom ciphers applied to privacy-sensitive data. The methodology involves testing models on the CipherBank dataset (2,358 problems, 9 ciphers) using a 3-shot known-plaintext evaluation protocol. Key results show significant limitations, with even the best models like Claude-3.5 achieving only a 45.14 CipherBank Score, indicating struggles with systematic cryptographic reasoning. The main implication for AI practitioners is that current LLM reasoning abilities, even in specialized models, are inadequate for complex cryptographic tasks, highlighting a need for targeted advancements in structured symbolic manipulation and rule application. |
| Natural Language Processing | Clinical knowledge in LLMs does not translate to human interactions (Read more on [arXiv](https://arxiv.org/abs/2504.18919) or [HuggingFace](https://huggingface.co/papers/2504.18919))| Juan Ciro, Hannah Rose Kirk, Guy Parsons, Rebecca Payne, Andrew M. Bean | This study reveals a significant gap between the strong performance of Large Language Models (LLMs) on medical knowledge tasks and their practical utility when used by laypeople for health self-assessment. The primary objective was to assess whether LLMs (GPT-4o, Llama 3, Command R+) improve the public's ability to identify conditions and determine appropriate medical dispositions in realistic scenarios compared to a control group. A randomized controlled trial involved 1,298 participants interacting with LLMs or using standard resources for ten medical vignettes, with responses compared against physician-defined gold standards. While LLMs alone were accurate (e.g., identifying relevant conditions in 94.9% of cases), participants using them performed poorly (e.g., <34.5% condition identification, <44.2% correct disposition), no better than controls, despite LLMs often suggesting correct information during interaction (>65.7% of conversations). The key implication is that benchmark performance and simulated interactions fail to predict real-world human-LLM failures, necessitating rigorous human user testing prior to public deployment in healthcare. |
| Machine Learning | Group Downsampling with Equivariant Anti-aliasing (Read more on [arXiv](https://arxiv.org/abs/2504.17258) or [HuggingFace](https://huggingface.co/papers/2504.17258))| Raymond A. Yeh, ashiq24 | This paper introduces a principled method for downsampling signals defined on finite groups with equivariant anti-aliasing, specifically for group equivariant neural networks (G-CNNs). The primary objective is to generalize uniform downsampling and anti-aliasing from standard signal processing to the group setting, addressing how to select appropriate subgroups for a given downsampling rate and how to perform anti-aliasing. The methodology involves an algorithm for subgroup selection based on group generators and Cayley graphs, a proposed Subgroup Sampling Theorem derived from group Fourier analysis, and an optimized equivariant anti-aliasing filter. Experiments on rotated MNIST and CIFAR-10 demonstrate that the proposed downsampling operation improves accuracy (e.g., ACC_orbit improved from 0.5660 to 0.5749 on Rotated MNIST O(2) with R=2), better preserves equivariance (lower L_equi), and reduces model size compared to baseline G-CNNs without proper downsampling. For AI practitioners, this work provides a framework to incorporate theoretically grounded and empirically effective downsampling layers into G-equivariant architectures, potentially leading to more efficient and robust models. |
| Multi-Modal | TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy
  Multi-modal Geometric Problem Solving (Read more on [arXiv](https://arxiv.org/abs/2504.15780) or [HuggingFace](https://huggingface.co/papers/2504.15780))| Yuan Feng, Qi Liu, Renqiu Xia, Zijun Chen, Daocheng Fu | The paper introduces TrustGeoGen, a scalable data engine creating formally verified, multi-modal data (diagrams, text, solutions) for trustworthy geometric problem solving (GPS). Its main objective is to address the lack of reliable benchmarks by generating datasets with guaranteed logical coherence and modality integrity, overcoming issues in existing noisy or non-verified synthetic data. TrustGeoGen utilizes a pipeline involving constraint-based premise generation (Constructor), rule-based reasoning graph expansion with formal verification (Reasoner), path/solution extraction via GeoExplore algorithms (Sampler), and translation, augmented by bootstrapping for complexity scaling. Experiments show state-of-the-art models struggle on the generated GeoTrust-test (OpenAI-o1 achieves 49.17% accuracy), while training on GeoTrust improves OOD generalization and reduces logical inconsistencies compared to pseudo-labeled data. TrustGeoGen offers AI practitioners a methodology and rigorously verified benchmark for developing and evaluating MLLMs on complex, verifiable reasoning tasks. |
| Reinforcement Learning | SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2504.19162) or [HuggingFace](https://huggingface.co/papers/2504.19162))| Xiaodan Liang, Peisong Wang, Ruotian Ma, Bang Zhang, judge | This paper introduces Self-Play Critic (SPC), a novel approach using adversarial self-play games to train a critic model for evaluating the step-by-step reliability of Large Language Model (LLM) reasoning without manual step-level supervision. The main objective is to overcome the challenges associated with obtaining high-quality, step-level annotations for training reasoning process verifiers. SPC employs two fine-tuned models, a 'sneaky generator' creating subtle errors and a 'critic' detecting them, which iteratively improve through reinforcement learning based on the outcomes of their adversarial interactions. Experiments show SPC progressively enhances error detection capabilities, increasing accuracy on ProcessBench from 70.8% to 77.7% and outperforming strong baselines, including distilled R1 models, on multiple reasoning benchmarks. The key implication is that SPC provides an automated, evolving method to develop step-level LLM critics, enhancing both evaluation accuracy and guiding test-time reasoning search for improved performance. |
| Multi-Modal | Benchmarking Multimodal Mathematical Reasoning with Explicit Visual
  Dependency (Read more on [arXiv](https://arxiv.org/abs/2504.18589) or [HuggingFace](https://huggingface.co/papers/2504.18589))| Xin Li, Zhiqiang Hu, Wenqi Zhang, Jiashuo Sun, cloudcatcher2 | This paper introduces VCBENCH, a benchmark designed to evaluate the multimodal mathematical reasoning of Large Vision-Language Models (LVLMs) with explicit visual dependencies across multiple images. The primary objective is to assess the core ability of LVLMs to integrate visual and mathematical concepts in elementary problems, moving beyond knowledge-centric evaluations. The methodology involved creating a dataset of 1,720 multi-image (average 3.9 images/question) elementary math problems across six cognitive domains and evaluating 26 state-of-the-art LVLMs. Results show substantial performance gaps, with the top models failing to surpass 50% accuracy (e.g., Gemini2.0-Flash at 49.77%), highlighting ongoing challenges in visual-mathematical integration. This implies a need for focused development on multi-image visual reasoning and fundamental cognitive abilities in LVLMs. |
| Multi-Modal | MMInference: Accelerating Pre-filling for Long-Context VLMs via
  Modality-Aware Permutation Sparse Attention (Read more on [arXiv](https://arxiv.org/abs/2504.16083) or [HuggingFace](https://huggingface.co/papers/2504.16083))| Xufang Luo, Qianhui Wu, Chengruidong Zhang, Yucheng Li, iofu728 | MMInference introduces a dynamic sparse attention method designed to accelerate the computationally intensive pre-filling stage for long-context Vision Language Models (VLMs). The research aims to overcome the quadratic attention complexity bottleneck associated with processing long multi-modal sequences, particularly addressing unique sparsity patterns (like Grid patterns from video) and modality boundary discontinuities. Key methodologies include identifying modality-specific patterns, applying permutation-based strategies to group sparse indices efficiently, and using an offline search algorithm with optimized GPU kernels for dynamic sparse computation. Experiments show MMInference achieves up to 8.3x speedup in the pre-filling stage for 1 million tokens while maintaining task accuracy across various multi-modal benchmarks compared to dense attention. This provides AI practitioners a training-free technique to significantly reduce latency for long-context VLMs without model modification. |
| Natural Language Processing | ICL CIPHERS: Quantifying "Learning'' in In-Context Learning via
  Substitution Ciphers (Read more on [arXiv](https://arxiv.org/abs/2504.19395) or [HuggingFace](https://huggingface.co/papers/2504.19395))| Daniel Khashabi, Anqi Liu, Muhan Gao, Aayush Mishra, FocusV857 | This paper introduces ICL CIPHERS, a framework using substitution ciphers to quantify the "task learning" component of In-Context Learning (ICL) in Large Language Models (LLMs). The research aims to disentangle task learning (TL) from task retrieval (TR) by evaluating LLM performance on reversibly (BIJECTIVE) versus irreversibly (NON-BIJECTIVE) ciphered inputs. The core methodology applies these token-level ciphers to NLP benchmarks and measures the accuracy gap between the two cipher types across various models and datasets. Key findings demonstrate that LLMs consistently achieve higher accuracy on BIJECTIVE ciphers, e.g., Llama3.1 8B showed a 7.6 percentage point gain on the Amazon dataset (20-shot), indicating decipherment and learning capability. This performance gap serves as a quantifiable proxy for TL, offering AI practitioners a novel way to evaluate the inference-time learning capabilities of LLMs distinct from memorization. |
