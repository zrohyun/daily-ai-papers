

## Papers for 2025-10-14

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Reinforcement Learning | QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning
  for LLMs (Read more on [arXiv](https://arxiv.org/abs/2510.11696) or [HuggingFace](https://huggingface.co/papers/2510.11696))|  | The paper introduces QeRL, a quantization-enhanced reinforcement learning framework for LLMs aimed at improving efficiency and performance. It investigates whether quantization noise can be leveraged for enhanced exploration in RL for LLMs, while reducing memory usage. QeRL combines NVFP4 quantization with LoRA, enabling RL training of a 32B LLM on a single H100 GPU.  On the GSM8K benchmark, QeRL achieves a score of 90.8%, comparable to full fine-tuning, while also exhibiting speedups in RL rollout. This approach offers AI practitioners a more efficient and performant method for RL training of LLMs. |
| Computer Vision | Diffusion Transformers with Representation Autoencoders (Read more on [arXiv](https://arxiv.org/abs/2510.11690) or [HuggingFace](https://huggingface.co/papers/2510.11690))|  | The paper introduces Representation Autoencoders (RAEs) to enhance Diffusion Transformers (DiT). The research objective is to improve generative quality and training efficiency in DiTs by replacing traditional VAE encoders with pretrained representation encoders. The methodology involves pairing frozen, pretrained encoders with trained decoders and adjusting DiT architecture for high-dimensional latent spaces. The approach achieves a 1.51 FID score on ImageNet 256x256 without guidance. RAEs offer a scalable approach to diffusion transformer training, enabling faster convergence and higher-quality image generation. |
| Multi-Modal | OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni
  MLLMs (Read more on [arXiv](https://arxiv.org/abs/2510.10689) or [HuggingFace](https://huggingface.co/papers/2510.10689))|  | The paper introduces OmniVideoBench, a large-scale benchmark for evaluating audio-visual understanding in multimodal large language models (MLLMs). It addresses the lack of comprehensive evaluation of synergistic reasoning across audio and visual modalities in existing benchmarks. The benchmark comprises 1000 high-quality question-answer pairs with step-by-step reasoning traces derived from diverse videos and manually verified for correctness. Evaluation of MLLMs on OmniVideoBench reveals a performance gap compared to human reasoning, with the best model, Gemini-2.0-Pro, achieving 58.90% accuracy. OmniVideoBench aims to foster the development of MLLMs with stronger and more generalizable audio-visual reasoning capabilities. |
| Reinforcement Learning | RLFR: Extending Reinforcement Learning for LLMs with Flow Environment (Read more on [arXiv](https://arxiv.org/abs/2510.10201) or [HuggingFace](https://huggingface.co/papers/2510.10201))| Zheming Liang, Dongzhou Cheng, Ruilin Li, Naishan Zheng, JingHaoZ | The paper introduces RLFR, a novel approach to reward shaping in Reinforcement Learning with Verifiable Rewards (RLVR) for LLMs. It addresses the limitation of binary rewards in RLVR by incorporating flow rewards derived from the latent space of LLMs. RLFR constructs flow fields from high-quality data and uses velocity deviations of policy latents as a reward signal, showing a consistent improvement over RLVR with a 1.5% average score increase on Qwen2.5-Math-7B. The primary implication is improved exploration and more effective reward shaping by utilizing expressive latent spaces for auxiliary signal collection. |
| Natural Language Processing | Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by
  Refining Belief States (Read more on [arXiv](https://arxiv.org/abs/2510.11052) or [HuggingFace](https://huggingface.co/papers/2510.11052))|  | The paper introduces Latent Refinement Decoding (LRD) to enhance diffusion-based language models by addressing information loss and premature commitment during parallel decoding. LRD employs a two-stage framework involving latent refinement with distributional mixtures and a predictive feedback loop to iteratively refine belief states. The primary methodology combines soft diffusion in embedding space with adaptive soft-to-hard decoding guided by KL-divergence monitoring for convergence. Experiments across coding and reasoning benchmarks demonstrate improved accuracy, achieving up to +6.3 pass@1 on HumanEval and speedups of up to 10.6x. LRD offers a versatile alternative for parallel sequence generation by preserving distributional information throughout the decoding process. |
| Multi-Modal | Spotlight on Token Perception for Multimodal Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2510.09285) or [HuggingFace](https://huggingface.co/papers/2510.09285))| Zefeng He, Yun Luo, Yafu Li, Xiaoye Qu, Siyuan Huang | This paper introduces Visually-Perceptive Policy Optimization (VPPO) for multimodal reinforcement learning, addressing the neglect of visual perception in existing RLVR frameworks. It investigates token perception to measure visual dependency and refines the learning signal by reweighting trajectory advantages and focusing on pivotal tokens. VPPO demonstrates a 19.2% average accuracy improvement over baselines on perception and reasoning benchmarks with 7B models. The work establishes a token-level perceptual perspective and offers a novel optimization strategy to enhance LVLM reasoning capabilities, suggesting a new direction for multimodal reasoning that leverages token-level insights. |
| Multi-Modal | AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration (Read more on [arXiv](https://arxiv.org/abs/2510.10395) or [HuggingFace](https://huggingface.co/papers/2510.10395))| Weihong Lin, Yue Ding, DogNeverSleep, hjy, XinlongChen | This paper introduces AVOCADO, an audiovisual video captioner driven by temporal orchestration between audio and visual modalities. The research aims to generate semantically rich video descriptions with accurate temporal alignment of visual and auditory events. AVOCADO utilizes a two-stage post-training pipeline involving supervised fine-tuning on a newly curated audiovisual dataset and group relative policy optimization with tailored reward functions to enhance temporal coherence and dialogue accuracy. Experiments demonstrate that AVOCADO significantly outperforms existing open-source models across multiple audiovisual video captioning benchmarks, achieving competitive performance on VDC and DREAM-1K under visual-only settings; specifically, AVoCaDO achieves a 73.2 average score on the UGC-VideoCap benchmark. AVOCADO provides AI practitioners with a powerful audiovisual captioning model that leverages temporal orchestration for enhanced video understanding and generation. |
| Computer Vision | DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training (Read more on [arXiv](https://arxiv.org/abs/2510.11712) or [HuggingFace](https://huggingface.co/papers/2510.11712))| Lu Qi, Bo Du, Xiangtai Li, Dizhe Zhang, fenghora | This paper introduces DiT360, a DiT-based framework for high-fidelity panoramic image generation via hybrid training. It aims to improve photorealism and geometric fidelity, addressing the limited availability of real-world panoramic data. The methodology employs inter-domain transformation and intra-domain augmentation at both pre-VAE image and post-VAE token levels, including perspective image guidance, panoramic refinement, circular padding, yaw loss, and cube loss. The model achieves state-of-the-art performance on Matterport3D validation set, surpassing existing methods across nine metrics including improved FID. DiT360 offers AI practitioners a novel approach to panoramic image generation by effectively leveraging both perspective and panoramic data, leading to improved image quality and geometric accuracy. |
| Reinforcement Learning | Demystifying Reinforcement Learning in Agentic Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.11701) or [HuggingFace](https://huggingface.co/papers/2510.11701))| Mengdi Wang, Shuicheng Yan, Jiaru Zou, Ling Yang, Zhaochen Yu | This paper demystifies reinforcement learning (RL) for agentic reasoning by systematically investigating the impact of data, algorithm, and reasoning mode. The research aims to identify key design principles and optimal practices for improving the agentic reasoning ability of LLMs. The authors conduct comprehensive experiments, comparing real end-to-end tool-use trajectories with synthetic data, analyzing various RL algorithms, and evaluating different reasoning modes. The results show that a 4B-sized model, DemyAgent-4B, trained with the proposed recipes achieves state-of-the-art performance, surpassing larger 32B models and achieving 72.6% accuracy on AIME2024, indicating the effectiveness of the identified techniques. The findings offer AI practitioners practical guidelines for enhancing agentic reasoning in LLMs and establishing a baseline for future agentic RL research. |
| Natural Language Processing | Making Mathematical Reasoning Adaptive (Read more on [arXiv](https://arxiv.org/abs/2510.04617) or [HuggingFace](https://huggingface.co/papers/2510.04617))| Jiahuan Li, Yang Bai, Zhijun Wang, Xiang Geng, DreamW1ngs | This paper introduces AdaR, a framework to improve mathematical reasoning in large language models by enabling adaptive reasoning and mitigating spurious correlations. AdaR synthesizes logically equivalent queries with varying variable values and trains models using Reinforcement Learning with Verifiable Rewards (RLVR) to penalize spurious logic. Experiments demonstrate that AdaR improves robustness and generalization, achieving substantial gains (average +8.50 points) on mathematical benchmarks with limited (9K) synthetic data.  The approach offers AI practitioners a method for creating LLMs that rely on problem-solving logic rather than superficial features, enhancing their reliability and performance in mathematical tasks. |
| Natural Language Processing | Building a Foundational Guardrail for General Agentic Systems via
  Synthetic Data (Read more on [arXiv](https://arxiv.org/abs/2510.09781) or [HuggingFace](https://huggingface.co/papers/2510.09781))| Manish Nagireddy, Pengcheng Jing, Yujun Zhou, Yue Huang, hhua2 | The paper introduces a foundational guardrail for general agentic systems, focusing on pre-execution intervention to prevent harmful actions. It addresses data scarcity by introducing AuraGen, a synthetic data engine that synthesizes benign trajectories, injects labeled risks, and filters outputs via a reward model. The approach combines a cross-planner adapter with a compact guardian model called Safiron, achieving robust transfer across settings. Evaluated on Pre-Exec Bench, Safiron demonstrates consistent gains over strong baselines, and ablations provided actionable practices for safer agentic systems. The study uses Mistral-8B-Instruct-2410 and reports improvement on classification accuracy from 0.606 to 0.949. |
| Natural Language Processing | ACADREASON: Exploring the Limits of Reasoning Models with Academic
  Research Problems (Read more on [arXiv](https://arxiv.org/abs/2510.11652) or [HuggingFace](https://huggingface.co/papers/2510.11652))|  | The paper introduces ACADREASON, a new benchmark to evaluate the reasoning abilities of LLMs and agents on academic research problems. ACADREASON aims to fill the gap in existing benchmarks by focusing on multi-domain, high-level reasoning in areas such as computer science, economics, law, mathematics, and philosophy. The methodology involves creating 50 expert-annotated problems and evaluating the performance of several mainstream LLMs and agents using a LLM-as-Judge approach. Results show that even advanced models like GPT-5 achieved low scores (16 pass rate, 40.5 checklist score), demonstrating a capability gap in tackling super-intelligent academic research tasks. This highlights the need for developing LLMs and agents that can effectively acquire and reason over complex academic knowledge. |
| Multi-Modal | InternSVG: Towards Unified SVG Tasks with Multimodal Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2510.11341) or [HuggingFace](https://huggingface.co/papers/2510.11341))|  | The paper introduces InternSVG, a unified framework for SVG understanding, editing, and generation using multimodal large language models. It aims to address challenges in general SVG modeling like fragmented datasets and limited transferability. The methodology involves a new dataset SAgoge, a benchmark SArena, and a unified MLLM, InternSVG, with SVG-specific tokenization and a two-stage training strategy. Experiments on SArena show InternSVG surpasses Claude-Sonnet-4 on icon benchmarks by 11% higher accuracy in understanding tasks and 34% higher PSNR in editing tasks. InternSVG provides a new integrated data-benchmark-model suite for unified SVG tasks, improving performance across understanding, editing, and generation. |
| Natural Language Processing | FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark
  for Evaluating LLMs (Read more on [arXiv](https://arxiv.org/abs/2510.08886) or [HuggingFace](https://huggingface.co/papers/2510.08886))|  | The paper introduces FINAUDITING, a novel benchmark for evaluating LLMs in financial auditing tasks using structured XBRL filings. It aims to assess LLMs' reasoning abilities over structured financial documents, addressing the gap in evaluating LLMs on taxonomy-driven and interdependent data. The methodology involves creating three subtasks: FinSM for semantic consistency, FinRE for relational consistency, and FinMR for numerical consistency, with a unified evaluation framework. Extensive zero-shot experiments on 13 LLMs reveal inconsistent performance, with accuracy drops up to 60-90% in hierarchical reasoning, the best model achieving 11.89% hit rate on FinSM. FINAUDITING establishes a foundation for developing trustworthy and regulation-aligned financial intelligence systems by exposing current limitations of LLMs in financial reasoning. |
| Computer Vision | GIR-Bench: Versatile Benchmark for Generating Images with Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.11026) or [HuggingFace](https://huggingface.co/papers/2510.11026))|  | The paper introduces GIR-Bench, a new benchmark for evaluating the image generation capabilities of multimodal models with a focus on reasoning. It addresses the need for rigorous evaluation of the alignment between understanding and generation, and their generalization potential. GIR-Bench comprises three tasks: understanding-generation consistency, reasoning-centric text-to-image generation, and reasoning-to-editing, each with task-specific evaluation pipelines. Experiments on 21 models reveal a performance gap between unified and generation-only models, and an internal gap within unified models between understanding and generation capabilities. The benchmark aims to guide future research towards developing unified models with seamless integration of reasoning and generation; it is uncertain if a specific quantitative metric is explicitly provided. |
| Computer Vision | AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning
  in 4D Scenes (Read more on [arXiv](https://arxiv.org/abs/2510.10670) or [HuggingFace](https://huggingface.co/papers/2510.10670))|  | The paper introduces AdaViewPlanner, a novel approach for viewpoint planning in 4D scenes by adapting pre-trained video diffusion models. It addresses the problem of generating professional camera trajectories from 4D scene content by adapting text-to-video models. The method uses a two-stage approach involving an adaptive learning branch for 4D scene injection and a camera extrinsic diffusion branch for viewpoint extraction. Experiments demonstrate the method's superiority over existing approaches, achieving a Human Missing Rate (HMR) of 0.044 on the E.T. testset, and highlights the potential of leveraging video generation models as world models for 4D interaction. The work provides a framework for AI practitioners to build intelligent systems that require understanding of 3D environments and viewpoints. |
| Multi-Modal | Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.11027) or [HuggingFace](https://huggingface.co/papers/2510.11027))|  | The paper introduces Vlaser, a vision-language-action model designed for synergistic embodied reasoning and robot control. It addresses the gap between VLM-based reasoning and VLA policy learning by integrating high-level reasoning with low-level control. The methodology involves pre-training on a new Vlaser-6M dataset and supervised VLA fine-tuning, exploring different VLM initializations to mitigate domain shift. Vlaser achieves state-of-the-art performance on embodied reasoning benchmarks (e.g., WidowX) and demonstrates that in-domain data is more effective for downstream VLA fine-tuning. The research implies that bridging the domain gap between internet-scale pre-training data and embodied-specific policy learning data is crucial for improving downstream VLA models. |
| Natural Language Processing | BrowserAgent: Building Web Agents with Human-Inspired Web Browsing
  Actions (Read more on [arXiv](https://arxiv.org/abs/2510.10666) or [HuggingFace](https://huggingface.co/papers/2510.10666))|  | The paper introduces BrowserAgent, a web agent framework employing human-inspired browser actions to solve complex tasks by interacting with dynamic web environments. It addresses the need for more interactive agents by using predefined browser actions via Playwright and a two-stage training process (SFT and RFT). BrowserAgent achieves competitive results on Open-QA tasks, surpassing Search-R1-Instruct by approximately 20% on multi-hop QA tasks with the 7B model. The study utilizes an explicit memory mechanism to enhance reasoning capabilities for long-horizon tasks. BrowserAgent provides AI practitioners with a scalable framework for building more interactive and scalable web agents without relying heavily on static content abstractions. |
| Reinforcement Learning | Don't Just Fine-tune the Agent, Tune the Environment (Read more on [arXiv](https://arxiv.org/abs/2510.10197) or [HuggingFace](https://huggingface.co/papers/2510.10197))|  | The paper introduces ENVIRONMENT TUNING, a novel training paradigm for LLM agents in data-scarce, multi-turn tool-use scenarios. It aims to train high-quality agents capable of generalization and stable learning without relying on extensive expert demonstrations. The methodology combines a structured curriculum, actionable environment augmentation providing corrective feedback, and fine-grained progress rewards. Using only 400 problem instances from the BFCL benchmark, the method achieves competitive in-distribution performance and improved out-of-distribution generalization, boosting ToolACE-2's ACEBench score from 8.5% to 15.0%. The proposed paradigm shifts from supervised fine-tuning to dynamic, environment-based exploration, offering a data-efficient approach to train robust agents. |
| Reinforcement Learning | SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.09541) or [HuggingFace](https://huggingface.co/papers/2510.09541))|  | The paper introduces Sandwiched Policy Gradient (SPG) for aligning masked diffusion language models with task-specific rewards, addressing the challenge of intractable log-likelihoods. SPG leverages both upper and lower bounds of the log-likelihood to reduce policy gradient bias, optimizing a sandwiched variational bound based on reward. A block-wise masking technique is introduced to improve training stability. Experiments demonstrate SPG outperforms ELBO-based baselines, achieving a 3.6% improvement in accuracy on GSM8K. SPG provides AI practitioners with a more effective reinforcement learning method for aligning diffusion language models with desired objectives. |
| Multi-Modal | CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven
  Images (Read more on [arXiv](https://arxiv.org/abs/2510.11718) or [HuggingFace](https://huggingface.co/papers/2510.11718))|  | The paper introduces CodePlot-CoT, a code-driven chain-of-thought paradigm for enhancing mathematical visual reasoning in VLMs by generating and rendering executable plotting code as visual thoughts. It aims to overcome limitations of text-only and direct image manipulation approaches, which often lack precision and controllability. The key methodology involves training a VLM to generate Python code that, when executed, produces visual aids which are input back into the VLM's reasoning process. Experimental results on a new Math-VR dataset show CodePlot-CoT achieves up to a 21% performance increase over a base model. This suggests code-driven visual reasoning offers a more structured and verifiable approach to mathematical problem-solving. |
| Computer Vision | DocReward: A Document Reward Model for Structuring and Stylizing (Read more on [arXiv](https://arxiv.org/abs/2510.11391) or [HuggingFace](https://huggingface.co/papers/2510.11391))|  | This paper introduces DocReward, a document reward model for assessing and improving document structure and styling. The research aims to provide a reward model to guide agentic workflows towards generating documents with better visual quality. DocReward is trained on DoCPair, a multi-domain dataset of paired documents with varying levels of professionalism, using a Bradley-Terry loss function. The model outperforms GPT-4o and GPT-5 in accuracy by 30.6 and 19.4 percentage points, respectively, on a human-annotated test set.  DocReward can be used by AI practitioners to improve document generation agents by providing a signal that aligns with human preferences for structure and style. |
| Multi-Modal | On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in
  Large Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.09008) or [HuggingFace](https://huggingface.co/papers/2510.09008))|  | This paper investigates object hallucination in large vision-language models (LVLMs) by focusing on the epistemic uncertainty of visual tokens. It aims to mitigate object hallucination by identifying and masking uncertain visual tokens within the vision encoder. The methodology involves adversarial perturbations to efficiently estimate token uncertainty, followed by masking these tokens in self-attention layers. Experiments demonstrate a reduction in hallucination rates, with Cs decreasing from 47.4 to 29.2 on LLaVA-1.5-7B. The study implies that practitioners can reduce object hallucinations in LVLMs by targeting and masking uncertain visual features in the vision encoder during inference. |
| Computer Vision | High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic
  Manipulation Learning with Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2510.10637) or [HuggingFace](https://huggingface.co/papers/2510.10637))|  | This paper presents RoboSimGS, a novel Real2Sim2Real framework for generating high-fidelity simulated data for robotic manipulation. The research aims to bridge the reality gap and reduce the need for real-world data collection in robot learning. The method reconstructs scenes using 3D Gaussian Splatting (3DGS) for photorealistic backgrounds and mesh primitives for interactive objects, using a Multi-modal Large Language Model (MLLM) to infer object properties and kinematic structures. Policies trained solely on RoboSimGS-generated data achieved successful zero-shot Sim2Real transfer, and augmenting real-world data with RoboSimGS data significantly enhanced performance; for example, with a 50/50 real/sim split, the success rate on deformable pick and place increased to 83% compared to 60% with only real data for Diffusion Policy. The results suggest RoboSimGS provides a scalable solution for creating training data to improve generalization and performance in real-world robotic tasks, especially when real-world data is limited. |
| Machine Learning | Skill-Targeted Adaptive Training (Read more on [arXiv](https://arxiv.org/abs/2510.10023) or [HuggingFace](https://huggingface.co/papers/2510.10023))|  | The paper introduces Skill-Targeted Adaptive Training (STAT), a fine-tuning strategy to address performance saturation in language models. STAT leverages a stronger LLM as a teacher to identify and target missing skills in a student model's reasoning process. The method involves creating a missing-skill-profile and adaptively reweighting or synthesizing training examples. Experiments on MATH benchmark show STAT achieves up to 7.5% improvement compared to vanilla SFT. The approach is complementary to RL and enhances out-of-distribution generalization, suggesting that skill-targeted adaptive training should broadly improve current training pipelines. |
| Reinforcement Learning | ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web
  Coding (Read more on [arXiv](https://arxiv.org/abs/2510.11498) or [HuggingFace](https://huggingface.co/papers/2510.11498))|  | The paper introduces ReLook, a vision-grounded reinforcement learning framework for agentic web coding using a multimodal LLM as a critic. ReLook aims to improve front-end code generation by enabling the agent to iteratively refine code based on visual feedback and perceptual signal, preventing reward hacking through strict rendering-integrity rules. The methodology involves using the MLLM to score code with screenshots and provide actionable feedback, coupled with a Forced Optimization strategy to ensure monotonic trajectory improvement. Results show that ReLook consistently outperforms strong baselines in vision-grounded front-end code generation, achieving, for example, a 40% relative gain in Web-Bench pass@2 score using Qwen2.5-7B. This approach offers AI practitioners a means to incorporate visual feedback and iterative refinement in agentic code generation tasks. |
| Reinforcement Learning | PEAR: Phase Entropy Aware Reward for Efficient Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.08026) or [HuggingFace](https://huggingface.co/papers/2510.08026))|  | The paper introduces Phase Entropy Aware Reward (PEAR) for improving the efficiency of large reasoning models (LRMs). It addresses the challenge of excessively long reasoning traces by adaptively controlling response length. PEAR incorporates phase-dependent entropy into the reward design, penalizing excessive entropy during the thinking phase and encouraging moderate exploration in the final answer phase. Experiments across four benchmarks demonstrate that PEAR reduces response length by 37.8% to 59.4% while maintaining comparable accuracy. PEAR offers AI practitioners a method for developing more concise and efficient reasoning processes in LRMs without compromising performance. |
| Natural Language Processing | Self-Improving LLM Agents at Test-Time (Read more on [arXiv](https://arxiv.org/abs/2510.07841) or [HuggingFace](https://huggingface.co/papers/2510.07841))| Gokhan Tur, Dilek Hakkani-Tür, Heng Ji, Cheng Qian, emrecanacikgoz | This paper introduces a novel test-time self-improvement method for agentic language models. It aims to improve the performance and generalizability of LMs by enabling on-the-fly adaptation to challenging test instances. The proposed method involves identifying uncertain samples, generating similar examples via self-data augmentation, and performing test-time fine-tuning. Empirical evaluations demonstrate that the Test-Time Self-Improvement (TT-SI) method improves performance by +5.48% accuracy on average across benchmarks, surpassing other methods with significantly less training data.  This technique offers AI practitioners a way to build more capable agents that can continually improve themselves without needing large datasets or complex training procedures. |
| Computer Vision | FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging
  with Diffusion Decoding (Read more on [arXiv](https://arxiv.org/abs/2510.10868) or [HuggingFace](https://huggingface.co/papers/2510.10868))|  | FastHMR accelerates 3D human mesh recovery by reducing computational overhead. The research aims to improve the efficiency of transformer-based HMR models without significant accuracy loss. The methodology employs error-constrained layer merging (ECLM) and mask-guided token merging (Mask-ToMe), combined with a diffusion-based decoder conditioned on pose priors. Experiments show up to 2.3x speed-up while slightly improving performance over the baseline. FastHMR offers AI practitioners a more efficient and temporally consistent solution for HMR on resource-constrained hardware. |
| Natural Language Processing | The Personalization Trap: How User Memory Alters Emotional Reasoning in
  LLMs (Read more on [arXiv](https://arxiv.org/abs/2510.09905) or [HuggingFace](https://huggingface.co/papers/2510.09905))|  | This paper investigates how user memory affects emotional reasoning in large language models (LLMs). The main research question explores whether adding user profiles to system memory influences LLMs' emotional understanding abilities. The methodology involves evaluating 15 LLMs on human validated emotional intelligence tests after removing culturally variable items and generating diverse user profiles. Results indicate that incorporating user profiles significantly alters performance, with certain high-performing models exhibiting larger shifts in emotional understanding, with users in advantaged profiles receiving more accurate emotional interpretations (up to 80.10% compared to 77.37% for advantaged vs disadvantaged profiles). The primary implication suggests that personalization mechanisms can inadvertently reinforce social inequalities in LLMs' emotional reasoning. |
| Computer Vision | Stable Video Infinity: Infinite-Length Video Generation with Error
  Recycling (Read more on [arXiv](https://arxiv.org/abs/2510.09212) or [HuggingFace](https://huggingface.co/papers/2510.09212))|  | The paper introduces Stable Video Infinity (SVI), a method for generating infinitely long videos with temporal consistency and controllable storylines. It addresses the hypothesis gap between error-free training and error-prone autoregressive inference by proposing Error-Recycling Fine-Tuning, where self-generated errors are used as supervisory prompts. SVI achieves strong performance, obtaining a 97.50% subject consistency score on ultra-long consistent video generation. This approach allows for scaling video generation to infinite durations without additional inference costs, enhancing practical applicability for diverse conditional video generation tasks. |
| Computer Vision | InfiniHuman: Infinite 3D Human Creation with Precise Control (Read more on [arXiv](https://arxiv.org/abs/2510.11650) or [HuggingFace](https://huggingface.co/papers/2510.11650))| Gerard Pons-Moll, Margaret Kostyrko, Xianghui Xie, Yuxuan Xue | The paper introduces InfiniHuman, a novel framework for generating realistic and controllable 3D human avatars. It addresses the challenge of limited annotated human data by distilling existing foundation models to create a large-scale dataset, InfiniHumanData, consisting of 111K diverse identities. The method involves an automatic pipeline leveraging vision-language and image generation models for data creation, followed by learning InfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body shape, and clothing assets. Extensive experiments demonstrate InfiniHuman significantly surpasses existing methods, achieving a 77.14% user preference for quality, and offers AI practitioners a scalable solution for high-quality avatar generation with fine-grained control. |
| Natural Language Processing | HUME: Measuring the Human-Model Performance Gap in Text Embedding Task (Read more on [arXiv](https://arxiv.org/abs/2510.10062) or [HuggingFace](https://huggingface.co/papers/2510.10062))|  | This paper introduces HUME, a human evaluation framework for text embeddings, to address the lack of reliable human performance baselines in existing benchmarks. The study aims to measure the human-model performance gap across various text embedding tasks. The methodology involves evaluating human annotators across 16 MTEB datasets spanning reranking, classification, clustering, and semantic textual similarity. Results show humans achieve an average performance of 77.6%, compared to 80.1% for the best embedding model, with variations across datasets. This framework provides human performance baselines that enables more meaningful interpretation of model scores, informing the development of both models and benchmarks. |
| Natural Language Processing | LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.09189) or [HuggingFace](https://huggingface.co/papers/2510.09189))| Lei Li, Shujian Huang, Jingyang Gong, Zixian Huang, Changjiang Gao | This paper introduces LLaMAX2, a translation-enhanced large language model that maintains strong reasoning capabilities. It addresses the challenge of translation-enhanced LLMs struggling with reasoning tasks by proposing a novel layer-selective tuning recipe on instruct models using parallel data. LLaMAX2, specifically the Qwen3-XPlus models, achieves 15+ spBLEU and 40+ xComet improvements in low-resource languages like Swahili while retaining comparable reasoning proficiency to Qwen3. The key methodology involves a two-phase layer-selective tuning process, focusing on training specific layers closest to the embedding layer and others further away from it. The results suggest the possibility of achieving high translation performance with reduced reliance on large, high-quality parallel datasets. |
| Machine Learning | From Data to Rewards: a Bilevel Optimization Perspective on Maximum
  Likelihood Estimation (Read more on [arXiv](https://arxiv.org/abs/2510.07624) or [HuggingFace](https://huggingface.co/papers/2510.07624))| Giuseppe Paolo, Youssef Attia El Hili, Gabriel Singer, corentinlger, abenechehab | The paper reinterprets Maximum Likelihood Estimation (MLE) as a bilevel optimization problem to incorporate reward function learning. It addresses the challenge of aligning generative models with high-quality datasets without explicit reward signals by optimizing a reward function in the outer-level and a policy gradient objective in the inner-level. The key methodology involves a theoretical analysis in a Gaussian setting and the proposal of two practical algorithms using implicit differentiation solvers. Experiments on tabular classification show improved accuracy compared to MLE. The framework allows AI practitioners to implicitly learn reward functions from data, enabling policy gradient optimization without needing explicit reward specification. |
| Computer Vision | LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion
  Models via Likelihood Preference (Read more on [arXiv](https://arxiv.org/abs/2510.11512) or [HuggingFace](https://huggingface.co/papers/2510.11512))| Ivan Laptev, Lars Kunze, Francesco Pinto, Fabio Pizzati, Jianhao Yuan | The paper introduces LikePhys, a training-free method for evaluating intuitive physics understanding in video diffusion models by leveraging likelihood preference. It aims to disentangle physics correctness from visual appearance by comparing the ELBO-based likelihood surrogate on physically valid and impossible video pairs generated using a simulator. The methodology involves calculating the Plausibility Preference Error (PPE), which correlates with human preference and outperforms existing evaluation baselines. Experiments on twelve scenarios show PPE aligns with human judgment, with top models achieving PPE around 43.6%. The findings suggest that scaling model capacity and adjusting inference settings improve physics understanding in video diffusion models. |
| Natural Language Processing | RePro: Training Language Models to Faithfully Recycle the Web for
  Pretraining (Read more on [arXiv](https://arxiv.org/abs/2510.10681) or [HuggingFace](https://huggingface.co/papers/2510.10681))|  | The paper introduces REPRO, a novel web recycling method for pretraining language models by training a smaller LM with reinforcement learning to faithfully rephrase web data. It addresses the diminishing supply of high-quality pretraining data by recycling low-quality web data while preserving semantics and structure through a quality reward and faithfulness rewards. REPRO delivers 4.7%-14.0% relative accuracy gains over organic-only baselines on 22 downstream tasks when pretraining 400M and 1.4B models, even outperforming a state-of-the-art prompting-based web recycling method using a much larger (70B) rephraser. This suggests REPRO provides an efficient and controllable approach to effectively harness a large pool of pretraining data, improving organic data efficiency by 2-3x. |
| Machine Learning | Graph Diffusion Transformers are In-Context Molecular Designers (Read more on [arXiv](https://arxiv.org/abs/2510.08744) or [HuggingFace](https://huggingface.co/papers/2510.08744))| Tengfei Luo, Michael Sun, Yihan Zhu, Jie Chen, Gang Liu | The paper introduces DemoDiff, a demonstration-conditioned diffusion model for in-context molecular design, addressing the scarcity of labeled data in molecular property prediction. The research aims to develop a molecular foundation model capable of adapting to new design tasks from a few molecule-score examples. DemoDiff employs a denoising Transformer guided by molecule-score demonstrations, leveraging a new Node Pair Encoding tokenizer for efficient motif-level representation. Results show that DemoDiff matches or surpasses language models 100-1000x larger, achieving an average rank of 3.63 across 33 design tasks. This positions DemoDiff as a scalable approach for molecular design, enabling practitioners to efficiently explore chemical spaces with limited labeled data. |
| Reinforcement Learning | VER: Vision Expert Transformer for Robot Learning via Foundation
  Distillation and Dynamic Routing (Read more on [arXiv](https://arxiv.org/abs/2510.05213) or [HuggingFace](https://huggingface.co/papers/2510.05213))|  | This paper presents VER, a Vision Expert Transformer for robot learning to improve generalization across tasks. The research addresses the challenge of limited task generality in individual pre-trained vision foundation models (VFMs). VER distills multiple VFMs into a vision expert library and uses a dynamic routing network to select task-relevant experts. Experiments across 17 diverse robotic tasks show VER achieves state-of-the-art performance and reduces large-norm outliers in task-irrelevant regions, achieving an average success rate of 74.7%. The approach offers AI practitioners a more flexible and efficient way to leverage multiple VFMs for enhanced robot learning. |
| Natural Language Processing | Are Large Reasoning Models Interruptible? (Read more on [arXiv](https://arxiv.org/abs/2510.11713) or [HuggingFace](https://huggingface.co/papers/2510.11713))| Narges Norouzi, Trevor Darrell, David M. Chan, Mihran Miroyan, tsunghanwu | The paper investigates the robustness of Large Reasoning Models (LRMs) in dynamic environments where interruptions or context changes occur during the reasoning process. It challenges the traditional "frozen world" assumption by evaluating LRMs under interruptions and dynamic context shifts. The study introduces new evaluation protocols on math and coding benchmarks, finding that LRMs can experience performance drops up to 60% under dynamic conditions. Key failure modes include reasoning leakage, panic, and self-doubt. This highlights the need for developing LRMs that are more adaptable and trustworthy in interactive, real-world scenarios. |
| Computer Vision | IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing
  Assessment (Read more on [arXiv](https://arxiv.org/abs/2510.11647) or [HuggingFace](https://huggingface.co/papers/2510.11647))| Zhucun Xue, Yuxiang Zeng, Teng Hu, Jiangning Zhang, Yinan Chen | IVEBench is introduced as a new benchmark suite for instruction-guided video editing (IVE) assessment. The paper addresses the limitations of existing benchmarks by creating a diverse dataset of 600 videos, comprehensive editing prompts across 8 task categories, and a three-dimensional evaluation protocol incorporating video quality, instruction compliance, and video fidelity metrics. Results demonstrate IVEBench's effectiveness in benchmarking state-of-the-art IVE methods, exhibiting human-aligned outcomes. The study shows existing models achieve a total score of no more than 0.7 with an Instruction Compliance score of no more than 0.45, highlighting significant room for improvement. IVEBench provides AI practitioners with a systematic tool for evaluating IVE methods, facilitating advancements in video editing capabilities. |
| Machine Learning | The Hidden DNA of LLM-Generated JavaScript: Structural Patterns Enable
  High-Accuracy Authorship Attribution (Read more on [arXiv](https://arxiv.org/abs/2510.10493) or [HuggingFace](https://huggingface.co/papers/2510.10493))| Tamás Bisztray, Mohamed Amine Ferrag, Richard A. Dubniczky, Norbert Tihanyi, Neo111x | This paper explores authorship attribution for JavaScript code generated by Large Language Models (LLMs). The study aims to determine if individual LLMs exhibit unique stylistic signatures in generated code, enabling model fingerprinting. It introduces LLM-NodeJS, a dataset of 50,000 Node.js programs from 20 LLMs and their variants, and benchmarks performance with traditional ML and Transformer models. The custom CodeT5-JSA architecture achieves 95.8% accuracy on five-class attribution. The findings suggest that LLMs embed distinct structural patterns in their generated code, allowing robust attribution even after obfuscation, shifting the focus from "AI-generated" to identifying specific models. |
| Natural Language Processing | CoBia: Constructed Conversations Can Trigger Otherwise Concealed
  Societal Biases in LLMs (Read more on [arXiv](https://arxiv.org/abs/2510.09871) or [HuggingFace](https://huggingface.co/papers/2510.09871))| Jana Diesner, Amir Hossein Kargaran, Nafiseh Nikeghbal | This paper introduces CoBia, a suite of lightweight adversarial attacks designed to reveal societal biases in Large Language Models (LLMs) through constructed conversations. The research aims to refine the conditions under which LLMs deviate from normative or ethical behavior. CoBia creates a conversation where the model utters a biased claim about a social group and assesses whether the model can recover and reject biased follow-up questions. The evaluation on 11 LLMs reveals bias amplification and a failure to reject biased follow-up questions. The main implication is that even with safety guardrails, deeply embedded biases can be surfaced through interaction, indicating the need for stress-testing dialogue. |
| Computer Vision | Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole
  Slide Image Diagnosis Behavior (Read more on [arXiv](https://arxiv.org/abs/2510.04587) or [HuggingFace](https://huggingface.co/papers/2510.04587))|  | The paper introduces Pathology-CoT, a framework for developing visual chain-of-thought agents for whole-slide image (WSI) diagnosis. It addresses the lack of scalable, clinically-aligned supervision by converting routine viewer logs into standardized behavioral commands and rationales using a human-in-the-loop approach. The key methodology involves an Al Session Recorder, which transforms raw digital pathology interaction logs into a structured dataset called Pathology-CoT, comprising paired "where to look" and "why it matters" information. In gastrointestinal lymph-node metastasis detection, the resulting Pathologist-o3 agent achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, outperforming the state-of-the-art OpenAl 03 model. This approach provides a practical method for creating human-aligned, upgradeable clinical Al by leveraging everyday viewer logs for expert-validated supervision. |
