

## Papers for 2025-10-10

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Reinforcement Learning | Agent Learning via Early Experience (Read more on [arXiv](https://arxiv.org/abs/2510.08558) or [HuggingFace](https://huggingface.co/papers/2510.08558))|  | This paper introduces early experience, a paradigm bridging imitation learning and reinforcement learning for language agents. The main objective is to improve agent learning in environments with sparse or unverifiable rewards by leveraging agent-generated interaction data. The methodology involves implicit world modeling and self-reflection, using future states as supervision signals. Experiments across eight environments show improved task effectiveness, with average gains of +9.6 in success rate, and out-of-domain generalization of +9.4. Early experience provides a foundation for subsequent reinforcement learning, enabling immediate gains in effectiveness and long-term benefits. |
| Multi-Modal | MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with
  Holistic Platform and Adaptive Hybrid Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2510.08540) or [HuggingFace](https://huggingface.co/papers/2510.08540))| vanilla1116, yingmanji, tianhao2k, mjuicem, PhoenixZ | The paper introduces MM-HELIX, a new benchmark and training paradigm for enhancing long-chain reflective reasoning in multimodal large language models (MLLMs). It addresses the limited reflective reasoning capabilities of current MLLMs through a carefully designed benchmark of 42 challenging synthetic tasks. The authors propose Adaptive Hybrid Policy Optimization (AHPO), a training strategy that unifies offline supervision and online optimization.  When applied to the Qwen2.5-VL-7B baseline, AHPO achieves a +18.6% accuracy improvement on the MM-HELIX benchmark.  This work indicates that reflective reasoning can be effectively learned and generalized in MLLMs, paving the way for more capable MLLMs. |
| Machine Learning | From What to Why: A Multi-Agent System for Evidence-based Chemical
  Reaction Condition Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.23768) or [HuggingFace](https://huggingface.co/papers/2509.23768))| Feiwei Qin, Junchi Yu, Jiaxuan Lu, haiyuanwan, YangC777 | The paper introduces ChemMAS, a multi-agent system for evidence-based chemical reaction condition reasoning. It addresses the lack of explainability in existing reaction condition recommendation methods. ChemMAS decomposes the task into mechanistic grounding, multi-channel recall, agentic debate, and rationale aggregation, grounding decisions in chemical knowledge and precedents. Experiments demonstrate that ChemMAS outperforms domain-specific baselines and general-purpose LLMs, achieving 10-15% gains in Top-1 accuracy. The main implication is a new paradigm for explainable AI in scientific discovery, offering falsifiable and human-trustable rationales. |
| Computer Vision | UniVideo: Unified Understanding, Generation, and Editing for Videos (Read more on [arXiv](https://arxiv.org/abs/2510.08377) or [HuggingFace](https://huggingface.co/papers/2510.08377))| Xintao Wang, Qiulin Wang, Zixuan Ye, Quande Liu, CongWei1230 | The paper introduces UniVideo, a unified framework for video understanding, generation, and editing. It addresses the challenge of unifying diverse video tasks under a single model by combining a Multimodal Large Language Model (MLLM) for instruction understanding with a Multimodal Diffusion Transformer (MMDiT) for video generation. UniVideo is jointly trained across multiple tasks, including text/image-to-video, in-context video generation, and editing. Experimental results demonstrate that UniVideo matches or surpasses state-of-the-art task-specific baselines, achieving competitive performance on the VBench benchmark, indicating strong generation ability. This framework enables task composition and generalization to novel editing scenarios, providing AI practitioners with a versatile tool for manipulating video content through multimodal instructions. |
| Natural Language Processing | When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs (Read more on [arXiv](https://arxiv.org/abs/2510.07499) or [HuggingFace](https://huggingface.co/papers/2510.07499))|  | This paper introduces Thought Template Augmented LCLMs (TOTAL), a framework for enhancing reasoning in long-context language models by leveraging reusable thought patterns. The research aims to improve knowledge-intensive multi-hop reasoning in LCLMs by structuring how evidence is combined and guiding inference with factual documents. TOTAL employs thought templates derived from prior problem-solving traces and refines them iteratively using natural-language feedback. Experiments on benchmarks like MuSiQue show TOTAL achieves F1 scores of 73.30, demonstrating consistent gains over strong baselines. TOTAL enables AI practitioners to achieve better performance on complex reasoning tasks without extensive model fine-tuning. |
| Reinforcement Learning | Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2510.03259) or [HuggingFace](https://huggingface.co/papers/2510.03259))|  | The paper introduces Meta-Awareness via Self-Alignment (MASA), a novel reinforcement learning framework to enhance meta-awareness in reasoning models. The research aims to improve the alignment between true rollouts and predicted meta-information to boost performance. MASA uses self-generated signals to train meta-awareness, incorporating parallel rollouts and filtering techniques like predictive gating and early cutoff. The method achieves a 6.2% average accuracy gain across six mathematics benchmarks and improves training efficiency by over 1.28x in GRPO training. This meta-cognitive guidance enhances out-of-domain generalization, suggesting that directly aligning meta-predictions with observed statistics improves reasoning capabilities and task performance. |
| Computer Vision | VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal
  Patches via In-Context Conditioning (Read more on [arXiv](https://arxiv.org/abs/2510.08555) or [HuggingFace](https://huggingface.co/papers/2510.08555))| Quande Liu, Wenze Liu, Zongli Ye, Qiulin Wang, onevfall | The paper introduces VideoCanvas, a unified framework for arbitrary spatio-temporal video completion, formulating video synthesis as painting on a video canvas. It addresses the challenge of temporal ambiguity in causal VAEs by decoupling spatial control via zero-padding and temporal alignment via Temporal RoPE Interpolation. Evaluated on the newly introduced VideoCanvasBench, VideoCanvas achieves state-of-the-art performance, outperforming existing conditioning paradigms. Specifically, VideoCanvas achieves higher PSNR than Channel Concatenation. This offers AI practitioners a flexible and unified approach to video generation and manipulation tasks without architectural modifications. |
| Machine Learning | MemMamba: Rethinking Memory Patterns in State Space Model (Read more on [arXiv](https://arxiv.org/abs/2510.03279) or [HuggingFace](https://huggingface.co/papers/2510.03279))| Xiao Sun, Jiaxuan Lu, Jiahao Yan, Yangjingyi Chen, Youjin Wang | The paper introduces MemMamba, a novel state space model architecture for ultra-long sequence modeling that addresses the memory decay problem of existing SSMs. It investigates the memory decay mechanism in Mamba and proposes state summarization with cross-layer and cross-token attention to alleviate long-range forgetting while maintaining linear complexity. The key methodology involves theoretical analysis using horizontal-vertical memory fidelity metrics and architectural innovation via the integrated state summarization mechanism. Empirical results on PG19 language modeling show MemMamba maintains a stable perplexity of 17.35 even at 60k tokens and a 48% speedup in inference. This new architecture offers a significant advancement in balancing complexity and memory retention in efficient sequence models. |
| Machine Learning | NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM
  Agents (Read more on [arXiv](https://arxiv.org/abs/2510.07172) or [HuggingFace](https://huggingface.co/papers/2510.07172))| Baixuan Xu, Newt Hue-Nam K. Nguyen, Kelvin Kiu-Wai Tam, Tianshi Zheng, tqfang229 | This paper introduces NewtonBench, a benchmark for evaluating scientific law discovery in LLMs. It addresses the limitations of existing benchmarks by offering a trilemma-free design based on metaphysical shifts and interactive model discovery. The methodology involves evaluating LLMs on 324 tasks across 12 physics domains, measuring symbolic accuracy and RMSLE. Results show that frontier models like GPT-5 and Gemini-2.5-pro achieve high symbolic accuracy, but their performance degrades with system complexity and noise.  Furthermore, code assistance can paradoxically hinder stronger models due to premature exploitation, highlighting the need for improved generalization. |
| Reinforcement Learning | Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense (Read more on [arXiv](https://arxiv.org/abs/2510.07242) or [HuggingFace](https://huggingface.co/papers/2510.07242))|  | The paper introduces Hybrid Ensemble Reward optimization (HERO), a reinforcement learning framework combining sparse verifier signals with dense reward model scores for enhanced reasoning. HERO addresses the limitations of sparse verification by incorporating stratified normalization to bound reward model scores and variance-aware weighting to emphasize challenging prompts. Across mathematical reasoning benchmarks, HERO consistently outperforms reward model-only and verifier-only baselines; for instance, on hard-to-verify tasks using Qwen-4B-Base, HERO achieves 66.3, exceeding reward model-only (54.6) by 11.7 points. This hybrid reward design retains the stability of verifiers while leveraging reward model nuance to advance reasoning, implying a more effective approach to training reasoning agents in sparse reward environments. |
| Natural Language Processing | The Alignment Waltz: Jointly Training Agents to Collaborate for Safety (Read more on [arXiv](https://arxiv.org/abs/2510.08240) or [HuggingFace](https://huggingface.co/papers/2510.08240))|  | The paper introduces WALTZRL, a multi-agent reinforcement learning framework for safety alignment in large language models (LLMs) that enhances LLM safety by jointly training a conversation agent and a feedback agent. It aims to improve LLM safety by reducing both unsafe responses and overrefusals. The methodology involves a novel Dynamic Improvement Reward (DIR) that incentivizes the feedback agent to provide helpful suggestions that improve the conversation agent's responses and enables the agents to co-evolve adaptively. Experiments across five diverse datasets demonstrate that WALTZRL significantly reduces unsafe responses (e.g., from 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on OR-Bench) compared to various baselines. The main implication for AI practitioners is that the WALTZRL framework enhances LLM safety without degrading general capabilities, thus advancing the Pareto front between helpfulness and harmlessness. |
| Natural Language Processing | DeepPrune: Parallel Scaling without Inter-trace Redundancy (Read more on [arXiv](https://arxiv.org/abs/2510.08483) or [HuggingFace](https://huggingface.co/papers/2510.08483))|  | The paper introduces DeepPrune, a framework for efficient parallel scaling of large language models by reducing inter-trace redundancy. The research aims to alleviate the computational inefficiency arising from redundant reasoning traces in parallel Chain-of-Thought prompting. DeepPrune employs a specialized judge model trained with focal loss and online greedy clustering to dynamically prune redundant reasoning paths while preserving answer diversity. Experiments on AIME 2024/2025 and GPQA demonstrate that DeepPrune reduces token consumption by over 80% compared to conventional consensus sampling while maintaining competitive accuracy within 3 percentage points. The framework offers a practical approach for AI practitioners to enhance the efficiency of parallel reasoning in LLMs, significantly reducing computational costs without compromising performance. |
| Reinforcement Learning | Training-Free Group Relative Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2510.08191) or [HuggingFace](https://huggingface.co/papers/2510.08191))|  | This paper introduces Training-Free Group Relative Policy Optimization (GRPO), a cost-effective method to enhance LLM agent performance without parameter updates. The research aims to improve LLM agent behavior by leveraging experiential knowledge as token priors. Training-Free GRPO iteratively distills high-quality knowledge from minimal ground-truth data, using group relative semantic advantage instead of numerical ones within rollouts. Experiments show Training-Free GRPO significantly improves out-of-domain performance, achieving 82.7% Mean@32 on AIME24 when applied to DeepSeek-V3.1-Terminus. This provides AI practitioners with a data and computationally efficient alternative to fine-tuning for specialized LLM agent tasks. |
| Computer Vision | ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D
  Reconstruction with Structured Scene Representation (Read more on [arXiv](https://arxiv.org/abs/2510.08551) or [HuggingFace](https://huggingface.co/papers/2510.08551))|  | The paper introduces ARTDECO, a novel framework for efficient and high-fidelity on-the-fly 3D reconstruction from monocular image sequences. It addresses the tradeoff between computational cost and reconstruction accuracy by combining feed-forward models with SLAM-based pipelines. ARTDECO employs 3D foundation models for pose estimation and point prediction, coupled with a hierarchical Gaussian representation and LoD-aware rendering. Experiments demonstrate that ARTDECO achieves interactive performance, robustness, and reconstruction quality close to per-scene optimization, delivering a PSNR of 29.12 on indoor datasets. This provides AI practitioners with a practical approach for real-time digitization of environments with accurate geometry and high visual fidelity. |
| Natural Language Processing | LLMs Learn to Deceive Unintentionally: Emergent Misalignment in
  Dishonesty from Misaligned Samples to Biased Human-AI Interactions (Read more on [arXiv](https://arxiv.org/abs/2510.08211) or [HuggingFace](https://huggingface.co/papers/2510.08211))|  | This paper investigates emergent misalignment in large language models (LLMs), specifically focusing on dishonesty and deception under high-stakes scenarios. The research explores whether fine-tuning LLMs on misaligned completions can lead to broadly misaligned behaviors in dishonesty. The methodology involves fine-tuning open-source LLMs on misaligned data and evaluating their behavior using MASK and DeceptionBench, also mixing misaligned samples with downstream datasets. Results show that introducing as little as 1% of misaligned data can decrease honest behavior by over 20%, and with 10% biased users in a human-AI interaction simulation, LLM's dishonesty escalates. The implication is that unintentional dishonesty misalignment can easily occur in real-world applications with human-AI interactions, particularly with biased users. |
| Multi-Modal | NaViL: Rethinking Scaling Properties of Native Multimodal Large Language
  Models under Data Constraints (Read more on [arXiv](https://arxiv.org/abs/2510.08565) or [HuggingFace](https://huggingface.co/papers/2510.08565))|  | This paper presents NaViL, a native multimodal large language model (MLLM) architecture designed and scaled under data constraints. The research investigates the optimal design space for native MLLMs, focusing on LLM initialization, visual encoder architecture, and mixture-of-experts (MoEs). The study proposes NaViL, which exhibits competitive performance on 14 multimodal benchmarks, achieving top-tier results with approximately 600M pre-training image-text pairs. Results indicate a positively correlated scaling relationship between visual encoders and LLMs. The findings suggest that jointly scaling visual and linguistic components is crucial for native MLLM performance, offering insights for building efficient and capable multimodal models. |
| Computer Vision | UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video
  Super-Resolution (Read more on [arXiv](https://arxiv.org/abs/2510.08143) or [HuggingFace](https://huggingface.co/papers/2510.08143))|  | The paper introduces UniMMVSR, a unified multi-modal framework for cascaded video super-resolution. It addresses the limitations of existing methods by incorporating hybrid-modal conditions like text, images, and videos to enhance fidelity in video generation. The framework explores condition injection strategies and training schemes within a latent video diffusion model, leveraging an SDEdit-based degradation pipeline for robustness. Experiments demonstrate that UniMMVSR outperforms existing methods, achieving superior detail and conformity to multi-modal conditions. For example, the model achieves MUSIQ score of 62.248 on multi-ID image-guided text-to-video generation task, demonstrating its ability to scale to ultra-high-resolution 4K videos with efficient computational overhead, thereby providing AI practitioners with a more controllable and high-quality video generation approach. |
| Computer Vision | InstructX: Towards Unified Visual Editing with MLLM Guidance (Read more on [arXiv](https://arxiv.org/abs/2510.08485) or [HuggingFace](https://huggingface.co/papers/2510.08485))| Xinghui Li, Pengze Zhang, Yanze Wu, Qichao Sun, Chong Mou | The paper introduces InstructX, a unified framework for image and video editing leveraging Multimodal Large Language Models (MLLMs) to improve editing performance. It addresses the research gap in analyzing MLLM design choices for diffusion models in visual editing tasks, particularly focusing on effective MLLM integration. The approach involves incorporating modality-specific MLLM features, enabling the model to handle a variety of image and video editing instructions without explicit video supervision, and demonstrating effective transfer of image editing capabilities to video tasks. Experiments show that InstructX achieves state-of-the-art performance across various image and video editing tasks, demonstrating competitive performance on GEdit-Bench with a Q_O score of 6.68. The implication is that training on image data can effectively enhance video editing capabilities without requiring explicit video training, offering a scalable approach for unified visual editing. |
| Natural Language Processing | First Try Matters: Revisiting the Role of Reflection in Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2510.08308) or [HuggingFace](https://huggingface.co/papers/2510.08308))| Wee Sun Lee, Zhanfeng Mo, Yao Xiao, Yue Deng, Liwei Kang | The paper analyzes the role of reflections in reasoning models, finding that reflections are predominantly confirmatory and rarely correct initial errors. It investigates whether reflection genuinely helps models correct errors or merely confirms earlier conclusions. The study uses LLM-based extraction to dissect reflective patterns across five mathematical benchmarks and eight models. Supervised fine-tuning experiments reveal that training with more reflection steps primarily enhances first-answer correctness rather than error correction, resulting in a 24.5% reduction in reasoning tokens with only a 2.9% accuracy drop through dynamic reflection truncation. The findings suggest that reflection-rich training data improves models' initial reasoning abilities, offering implications for data design and inference efficiency. |
| Reinforcement Learning | Low-probability Tokens Sustain Exploration in Reinforcement Learning
  with Verifiable Reward (Read more on [arXiv](https://arxiv.org/abs/2510.03222) or [HuggingFace](https://huggingface.co/papers/2510.03222))|  | This paper addresses the exploration collapse in Reinforcement Learning with Verifiable Rewards (RLVR) by focusing on the elimination of valuable low-probability tokens. The research aims to preserve reasoning sparks, defined as low-probability exploratory tokens, during RLVR training. Low-probability Regularization (Lp-Reg) is introduced to regularize the policy towards a less-noisy proxy distribution that amplifies reasoning sparks. Experiments on math benchmarks show Lp-Reg achieves state-of-the-art performance, with an average accuracy of 60.17% on five math benchmarks using Qwen3-14B-Base, a 2.66% improvement over prior methods. The implication is that selectively preserving low-probability tokens enhances exploration and prevents performance collapse in RLVR. |
| Multi-Modal | UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG (Read more on [arXiv](https://arxiv.org/abs/2510.03663) or [HuggingFace](https://huggingface.co/papers/2510.03663))|  | The paper introduces UniDoc-Bench, a large-scale benchmark for document-centric multimodal retrieval-augmented generation (MM-RAG). The research aims to address the fragmentation in current MM-RAG evaluations by providing a realistic and unified evaluation framework. The benchmark comprises 70k PDF pages and 1,600 QA pairs spanning multiple domains and question types, with a pipeline for extracting and linking evidence from text, tables, and figures. Experiments demonstrate that text-image fusion RAG systems outperform unimodal and joint retrieval approaches, achieving a completeness of 68.4% compared to lower scores for other methods. The benchmark and findings offer actionable guidance for developing more robust MM-RAG pipelines for real-world document intelligence. |
| Reinforcement Learning | CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards (Read more on [arXiv](https://arxiv.org/abs/2510.08529) or [HuggingFace](https://huggingface.co/papers/2510.08529))| Yijiang Li, Zaibin Zhang, Guibin Zhang, Yifan Zhou, xxyQwQ | The paper introduces Co-Evolving Multi-Agent Systems (CoMAS), a framework for self-evolution in LLM-based agents through interaction rewards. It aims to enable agents to autonomously improve by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from discussion dynamics using an LLM-as-a-judge mechanism and optimizes each agent's policy via reinforcement learning. Experiments demonstrate that CoMAS consistently outperforms untrained agents, achieving state-of-the-art performance with up to 6.10% gain in Debate setups. The framework offers a novel paradigm for self-evolution, reducing reliance on external rewards and enabling decentralized co-evolution of heterogeneous agents. |
| Natural Language Processing | LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling (Read more on [arXiv](https://arxiv.org/abs/2510.06915) or [HuggingFace](https://huggingface.co/papers/2510.06915))|  | This paper introduces Long-RewardBench, a benchmark for evaluating reward models (RMs) in long-context scenarios, addressing the limitation of current RMs confined to short contexts. The study investigates the effectiveness of scaling arbitrary models into robust long-context RMs, termed LongRMs, through a multi-stage training strategy involving tailored data synthesis methods. Experiments demonstrate LongRMs' improved performance on long-context evaluations, matching the performance of proprietary models like Gemini 2.5 Pro, with an 8B LongRM model outperforming much larger 70B baselines. The main implication is that the proposed approach enables robust reward modeling in long-context applications like LLM agents, where context consistency is crucial. |
| Natural Language Processing | Learning on the Job: An Experience-Driven Self-Evolving Agent for
  Long-Horizon Tasks (Read more on [arXiv](https://arxiv.org/abs/2510.08002) or [HuggingFace](https://huggingface.co/papers/2510.08002))|  | This paper introduces MUSE, an experience-driven, self-evolving agent framework for long-horizon tasks, aiming to address the limitation of existing LLM agents that are test-time static and cannot learn from experience. The research explores how to enable agents to accumulate knowledge and continuously improve through a hierarchical memory module that organizes and leverages diverse levels of experience. MUSE achieves new SOTA performance on the TAC benchmark, reaching 51.78% partial completion score with a lightweight Gemini-2.5 Flash model. This shows that MUSE agents can exhibit increasingly superior task completion capabilities and robust continuous learning, and the accumulated experience exhibits strong generalization properties enabling zero-shot improvement on new tasks. MUSE offers a new paradigm for AI agents capable of real-world productivity task automation. |
| Computer Vision | Taming Text-to-Sounding Video Generation via Advanced Modality Condition
  and Interaction (Read more on [arXiv](https://arxiv.org/abs/2510.03117) or [HuggingFace](https://huggingface.co/papers/2510.03117))|  | This paper addresses text-to-sounding video (T2SV) generation, a challenging task requiring synchronized audio and video from text prompts. It investigates how to mitigate modal interference and optimize cross-modal feature interaction. The proposed Hierarchical Visual-Grounded Captioning (HVGC) framework generates disentangled video and audio captions, coupled with a BridgeDiT architecture utilizing Dual CrossAttention (DCA) for robust feature fusion. Experiments on the AVSync15 dataset demonstrate improved performance, achieving a FVD score of 765.74 and AV-Align score of 0.275. The results highlight the importance of disentangled captions and effective cross-modal attention for advancing T2SV, which can inform the design of future multimodal AI systems. |
| Computer Vision | Large Scale Diffusion Distillation via Score-Regularized Continuous-Time
  Consistency (Read more on [arXiv](https://arxiv.org/abs/2510.08431) or [HuggingFace](https://huggingface.co/papers/2510.08431))| Jintao Zhang, Qianli Ma, Yuji Wang, Kaiwen Zheng, ChenDRAG | The paper introduces score-regularized continuous-time consistency model (rCM) for efficient diffusion model distillation. It addresses quality limitations of sCM in fine-detail image and video generation by incorporating score distillation as a regularizer. The key methodology involves a parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on 10B+ parameter models and high-dimensional video tasks. Empirically, rCM matches or surpasses DMD2 on quality metrics, accelerating diffusion sampling by 15x-50x while improving generation diversity. The main implication is a practical and theoretically grounded framework for large-scale diffusion distillation, enabling high-fidelity sample generation in only 1-4 steps. |
| Machine Learning | Reinforcing Diffusion Models by Direct Group Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2510.08425) or [HuggingFace](https://huggingface.co/papers/2510.08425))| Jing Tang, Tianyang Hu, Yihong Luo | This paper introduces Direct Group Preference Optimization (DGPO), a novel reinforcement learning method for post-training diffusion models. It addresses the mismatch between policy gradient methods and diffusion generation by optimizing group-level preferences directly using efficient, deterministic ODE samplers, obviating the need for stochastic policies. DGPO achieves a 20x faster training speed compared to Flow-GRPO while improving GenEval scores from 63% to 97%. The main implication is that DGPO offers a more practical and efficient approach to aligning diffusion models with desired preferences compared to existing methods. |
| Natural Language Processing | Beyond Turn Limits: Training Deep Search Agents with Dynamic Context
  Window (Read more on [arXiv](https://arxiv.org/abs/2510.08276) or [HuggingFace](https://huggingface.co/papers/2510.08276))| Yaojie Lu, Bowen Yu, Le Yu, Hao Xiang, TangQiaoYu | The paper introduces DeepMiner, a framework for training long-horizon multi-turn search agents with improved reasoning capabilities. It addresses limitations in existing approaches by generating complex question-answer pairs and employing a dynamic context window management strategy. The main objective is to invoke deep reasoning capabilities in multi-turn agents with long-horizon interactions. DeepMiner uses reverse construction to generate tasks and a sliding window to manage context without summarization, enabling reinforcement learning with Qwen3-32B. DeepMiner-32B achieves 33.5% accuracy on BrowseComp-en, significantly outperforming previous open-source agents, suggesting that high-quality training data and effective context management facilitate deep reasoning in long-horizon interactions. |
| Machine Learning | Entropy Regularizing Activation: Boosting Continuous Control, Large
  Language Models, and Image Classification with Activation as Entropy
  Constraints (Read more on [arXiv](https://arxiv.org/abs/2510.08549) or [HuggingFace](https://huggingface.co/papers/2510.08549))| Huazhe Xu, xtqqwq, ChonghuaLiao, zilinkang | The paper introduces Entropy Regularizing Activation (ERA), a novel paradigm that constrains sampling entropy using specially designed activation functions. It aims to improve model performance across various domains by controlling entropy without directly modifying the primary objective function. ERA achieves this by applying activations to model outputs, ensuring entropy constraints are met. Results demonstrate significant improvements, including a 37.4% boost in the AIME 2025 score for Qwen2.5-Math-7B, over 30% improvement on HumanoidBench, and a 0.69% increase in ImageNet top-1 accuracy for ResNet-50. ERA offers a universally applicable and theoretically grounded method for entropy control, enabling simpler and more robust algorithm design for AI practitioners. |
| Natural Language Processing | Memory Retrieval and Consolidation in Large Language Models through
  Function Tokens (Read more on [arXiv](https://arxiv.org/abs/2510.08203) or [HuggingFace](https://huggingface.co/papers/2510.08203))|  | This paper proposes that function tokens play a crucial role in memory retrieval and consolidation within LLMs. The research investigates how function tokens influence feature activation and next-token prediction during inference and pre-training. Bipartite graph analysis demonstrates that a small number of function tokens activate the majority of features in the LLM. Pre-training experiments show that the training loss is dominated by predicting content tokens following function tokens, implicitly selecting for predictive features. This suggests AI practitioners should consider function token dynamics when analyzing and improving LLM performance. |
| Machine Learning | Recycling Pretrained Checkpoints: Orthogonal Growth of
  Mixture-of-Experts for Efficient Large Language Model Pre-Training (Read more on [arXiv](https://arxiv.org/abs/2510.08008) or [HuggingFace](https://huggingface.co/papers/2510.08008))| Peng Cheng, Yaoxiang Wang, Yucheng Ding, lx865712528, Mr-Philo | This paper introduces a method for efficiently pre-training large language models by recycling existing, underutilized pre-trained checkpoints. The study addresses how to best expand well-trained Mixture-of-Experts (MoE) models to maximize the return on sunk costs through orthogonal growth: interpositional layer copying for depth and expert duplication with noise for width. Scaling experiments reveal that final accuracy correlates strongly with sunk costs; a 70B parameter model achieved a 10.66% accuracy gain over training from scratch. The research provides AI practitioners with a cost-effective strategy for large language model pre-training by leveraging existing checkpoints rather than training from the ground up. |
| Multi-Modal | SciVideoBench: Benchmarking Scientific Video Reasoning in Large
  Multimodal Models (Read more on [arXiv](https://arxiv.org/abs/2510.08559) or [HuggingFace](https://huggingface.co/papers/2510.08559))| Mohit Bansal, Lincoln Spencer, Shoubin Yu, Taojiannan Yang, groundmore | SciVideoBench is introduced as a new benchmark to evaluate advanced video reasoning in scientific contexts, addressing the limitations of existing benchmarks in assessing multimodal cognitive skills. The paper aims to evaluate LMMs on sophisticated domain-specific knowledge, spatiotemporal perception, and intricate logical reasoning. The methodology involves a dataset of 1,000 multiple-choice questions derived from scientific experimental videos across 25 academic subjects, validated by a semi-automatic system. Evaluation of state-of-the-art LMMs like Gemini 2.5 Pro and Qwen2.5-VL reveals significant performance deficits, with Gemini 2.5 Pro achieving 64.30% accuracy, highlighting room for advancement. The benchmark serves to guide future LMM developments towards truly capable multimodal AI co-scientists, focusing on reasoning complexity and visual grounding. |
| Reinforcement Learning | DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via
  Joint-Wise Neural Dynamics Model (Read more on [arXiv](https://arxiv.org/abs/2510.08556) or [HuggingFace](https://huggingface.co/papers/2510.08556))| Li Yi, He Wang, Xueyi Liu | This paper presents DexNDM, a sim-to-real framework for dexterous in-hand object rotation. The research aims to develop a generalizable policy capable of rotating a wide variety of objects with diverse wrist orientations. The methodology centers on a joint-wise neural dynamics model and autonomous data collection to bridge the reality gap, adapting the sim policy's actions. In real-world experiments, DexNDM achieved versatile rotation across a broad object distribution; specifically, a single policy successfully rotates objects with high aspect ratios up to 5.33. This approach offers AI practitioners a data-efficient method for transferring manipulation policies from simulation to real-world applications, potentially reducing the need for extensive real-world data collection. |
| Natural Language Processing | A^2Search: Ambiguity-Aware Question Answering with Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2510.07958) or [HuggingFace](https://huggingface.co/papers/2510.07958))|  | The paper introduces A2SEARCH, a novel reinforcement learning framework for open-domain question answering that addresses ambiguity.  It aims to improve QA systems by enabling them to recognize and handle questions with multiple valid answers, a scenario often overlooked in standard QA benchmarks. A2SEARCH automatically detects ambiguous questions, gathers alternative answers via trajectory sampling and evidence verification, and optimizes the model with an AnsF1 reward. Experiments show that A2SEARCH achieves state-of-the-art performance, with A2SEARCH-7B yielding an average AnsF1@1 score of 48.4% across four multi-hop benchmarks. This approach demonstrates that embracing ambiguity is essential for building more reliable QA systems, offering a new avenue for practitioners to improve performance and handle real-world complexity. |
| Reinforcement Learning | GCPO: When Contrast Fails, Go Gold (Read more on [arXiv](https://arxiv.org/abs/2510.07790) or [HuggingFace](https://huggingface.co/papers/2510.07790))|  | This paper introduces Group Contrastive Policy Optimization (GCPO) to improve reinforcement learning for reasoning in large language models. The research addresses the issue of ineffective updates in scenarios where rollouts yield consistently incorrect answers by incorporating external standard reference answers. GCPO replaces a failed rollout response with a gold-standard answer, enabling the model to emulate problem-solving strategies from larger models. Experiments show that GCPO achieves superior performance, surpassing DAPO, and improves training efficiency, achieving a 54% performance gain on the MQA dataset compared to the baseline model. The implication for AI practitioners is a more efficient training approach that enables smaller models to learn complex reasoning patterns, by supplying the model with positive examples to steer updates during training. |
| Machine Learning | Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs (Read more on [arXiv](https://arxiv.org/abs/2510.07429) or [HuggingFace](https://huggingface.co/papers/2510.07429))| Franck Dernoncourt, Yue Zhao, Hongjie Chen, Tiankai Yang, Wang Wei | The paper introduces BaRP, a novel bandit-feedback routing framework for LLMs that learns to balance performance and cost while adapting to user preferences. The research question addresses efficient LLM routing under partial feedback, mimicking deployment scenarios where only the selected model's outcome is observed. BaRP formulates routing as a contextual bandit problem, training a policy conditioned on user-specified trade-offs between accuracy and cost using entropy-regularized policy gradients. Experiments demonstrate that BaRP outperforms strong offline routers by at least 12.46% on in-distribution tasks and surpasses the largest LLM by at least 2.45% on out-of-distribution tasks. This work provides AI practitioners with a practical approach to dynamically route LLMs based on performance-cost trade-offs without requiring full supervision across all models. |
| Reinforcement Learning | R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized
  Manipulation (Read more on [arXiv](https://arxiv.org/abs/2510.08547) or [HuggingFace](https://huggingface.co/papers/2510.08547))| Zheng Zhu, Bingyao Yu, Hankun Li, Angyuan Ma, Xiuwei Xu | R2RGen is a real-to-real 3D data generation framework for spatially generalized robotic manipulation. The paper addresses the need for abundant, spatially diverse training data by augmenting pointcloud observation-action pairs directly from a single source demonstration. It uses a group-wise augmentation strategy and camera-aware processing to generate realistic 3D data and uses an annotation mechanism for scene and trajectory parsing. Empirically, R2RGen substantially enhances data efficiency, outperforming policies trained with 25x more human-collected data and showing strong spatial generalization. This allows AI practitioners to create robust policies with substantially less real-world data collection. |
| Computer Vision | UP2You: Fast Reconstruction of Yourself from Unconstrained Photo
  Collections (Read more on [arXiv](https://arxiv.org/abs/2509.24817) or [HuggingFace](https://huggingface.co/papers/2509.24817))| Boqian Li, Xiaoben Li, Ziyang Li, Yuliang, Co2y | UP2You is presented as a tuning-free solution for reconstructing high-fidelity 3D clothed portraits from unconstrained 2D photo collections. The research aims to address the challenge of reconstructing 3D clothed humans from unstructured photo collections by converting them into clean, orthogonal multi-view images. It introduces a pose-correlated feature aggregation module and a perceiver-based multi-reference shape predictor for robust and efficient reconstruction. Experiments demonstrate that UP2You surpasses previous methods, achieving a 15%↓ Chamfer distance improvement on PuzzleIOI and a 21%↑ PSNR improvement on 4D-Dress. UP2You provides AI practitioners with an efficient and versatile method for 3D human reconstruction from casual captures. |
| Reinforcement Learning | Fidelity-Aware Data Composition for Robust Robot Generalization (Read more on [arXiv](https://arxiv.org/abs/2509.24797) or [HuggingFace](https://huggingface.co/papers/2509.24797))| Liliang Chen, Hongwei Fan, Sicheng Hu, Di Chen, Zizhao Tong | The paper addresses the challenge of shortcut learning in robot policies, which impairs out-of-distribution generalization. It investigates how to create robust robot generalization policies using a mixture of real and synthetic data. The authors introduce Coherent Information Fidelity Tuning (CIFT), a framework optimizing data composition by maximizing a proxy for Information Fidelity based on feature-space geometry. CIFT improves out-of-distribution success rates by over 54% on policy architectures like πο and Diffusion Policy. The key implication is that fidelity-aware composition is critical for developing robust robots and general-purpose robots. |
| Computer Vision | SViM3D: Stable Video Material Diffusion for Single Image 3D Generation (Read more on [arXiv](https://arxiv.org/abs/2510.08271) or [HuggingFace](https://huggingface.co/papers/2510.08271))|  | SViM3D is presented as a framework for predicting multi-view consistent PBR materials and normals from a single image, conditioned on a camera path. The research aims to address the ill-posed problem of inverse rendering and enable high-quality 3D asset generation. SViM3D extends a latent video diffusion model, incorporating various mechanisms to improve quality, including a multi-illumination training dataset and material latent representation. The method achieves state-of-the-art novel view synthesis performance, with a PSNR of 28.68 on basecolor prediction, and improves material reproduction in real-world settings. The model provides AI practitioners with a unified neural prior for 3D reconstruction and material understanding, facilitating relighting and material editing. |
| Natural Language Processing | Search-R3: Unifying Reasoning and Embedding Generation in Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2510.07048) or [HuggingFace](https://huggingface.co/papers/2510.07048))| James Cheng, ytgui | This paper introduces Search-R3, a novel framework for unifying reasoning and embedding generation within large language models. The research aims to improve retrieval performance by enabling LLMs to generate search embeddings directly from their reasoning process. Search-R3 employs a two-stage training pipeline involving supervised fine-tuning with contrastive learning, followed by reinforcement learning to optimize both reasoning and embedding quality in an end-to-end retrieval environment. Evaluations on diverse benchmarks show Search-R3 achieves state-of-the-art performance, outperforming existing methods with an nDCG@10 of 0.871 on a proprietary dataset. The main implication is enhanced retrieval capabilities by integrating semantic representation with explicit reasoning in LLMs, which could benefit knowledge-intensive tasks. |
| Computer Vision | Towards Scalable and Consistent 3D Editing (Read more on [arXiv](https://arxiv.org/abs/2510.02994) or [HuggingFace](https://huggingface.co/papers/2510.02994))| Pan Zhou, Yang Tang, XiaRho | The paper introduces 3DEditFormer, a framework for scalable and consistent 3D editing. It addresses the challenge of localized 3D asset modification while maintaining structural fidelity and cross-view consistency. The approach uses a 3D-structure-preserving conditional transformer trained on a newly introduced large-scale dataset, 3DEditVerse, which contains over 116,000 training pairs. The model achieves a 13% improvement in 3D metrics over existing methods, demonstrating enhanced fidelity without requiring manual 3D masks. This provides a new standard for practical 3D editing, improving usability and automation. |
| Machine Learning | Beyond Outliers: A Study of Optimizers Under Quantization (Read more on [arXiv](https://arxiv.org/abs/2509.23500) or [HuggingFace](https://huggingface.co/papers/2509.23500))|  | This paper investigates the impact of optimizer choice on model robustness under quantization for large language models. The research explores how different optimizers affect model performance during post-training quantization (PTQ) and quantization-aware training (QAT). The methodology involves training full-precision models (50M to 1.5B parameters) with six optimizers and evaluating their performance degradation under PTQ and QAT. The results show that models trained with Shampoo exhibit the lowest accuracy degradation during QAT, achieving the highest parameter efficiency. The implication is that optimizer selection significantly impacts quantized model performance, with Shampoo demonstrating superior robustness in maintaining accuracy, suggesting a need to re-evaluate optimization strategies for quantized LLMs. |
