

## Papers for 2025-10-09

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Cache-to-Cache: Direct Semantic Communication Between Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2510.03215) or [HuggingFace](https://huggingface.co/papers/2510.03215))|  | This paper introduces Cache-to-Cache (C2C), a new paradigm for direct semantic communication between Large Language Models (LLMs). It addresses the limitation of text-based LLM communication, which loses semantic information and incurs latency by proposing direct KV-Cache transfer. C2C uses a neural network to project and fuse the source model's KV-Cache with the target model's, coupled with a gating mechanism to select relevant layers. Experiments demonstrate that C2C achieves 8.5-10.5% higher average accuracy compared to individual models and outperforms text communication by 3.0-5.0% while delivering an average 2.0x speedup in latency, implying more efficient and performant multi-LLM systems. |
| Computer Vision | Ming-UniVision: Joint Image Understanding and Generation with a Unified
  Continuous Tokenizer (Read more on [arXiv](https://arxiv.org/abs/2510.06590) or [HuggingFace](https://huggingface.co/papers/2510.06590))|  | The paper introduces Ming-UniVision, a novel framework for joint image understanding and generation using a unified continuous tokenizer. It addresses the limitation of discrete tokenizers in visual tasks by proposing MingTok, a three-stage architecture for encoding, semantic expansion, and reconstruction in a continuous latent space. Ming-UniVision eliminates the need for task-specific representations, unifying diverse vision-language tasks under a single autoregressive paradigm. Experiments show that MingTok achieves a rFID of 0.54 and a PSNR of 30.77 db. The unified continuous representation promises to facilitate more effective multi-modal interaction, paving the way for improved context-aware AI systems. |
| Multi-Modal | Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal
  Generation and Understanding (Read more on [arXiv](https://arxiv.org/abs/2510.06308) or [HuggingFace](https://huggingface.co/papers/2510.06308))|  | The paper introduces Lumina-DiMOO, an open-source foundational diffusion model for multi-modal generation and understanding. It aims to provide a unified model capable of handling various modalities through discrete diffusion modeling. Lumina-DiMOO utilizes a fully discrete diffusion approach achieving higher sampling efficiency compared to autoregressive methods. The model achieves state-of-the-art performance on multiple benchmarks, surpassing existing open-source unified multi-modal models, and demonstrating a 32x speed improvement in text-to-image generation compared to prior AR models. This offers AI practitioners a more efficient and versatile tool for tasks spanning text and image domains. |
| Natural Language Processing | SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.06917) or [HuggingFace](https://huggingface.co/papers/2510.06917))| Kevin Lin, Chung-Ching Lin, Linjie Li, Xiaofei Wang, Cheng-Han Chiang | The paper introduces SHANKS, a framework that enables Spoken Language Models (SLMs) to "think while listening." SHANKS addresses the issue of high response latency in speech-to-speech interaction by generating chain-of-thought reasoning during user input. The framework processes input speech in chunks and generates unspoken reasoning based on previous speech and reasoning. SHANKS demonstrates improved real-time interaction, achieving 37.1% more accurate user interruption and completing 56.9% of tool calls before the user finishes speaking. This work enables more interactive and lower-latency spoken dialogues. |
| Reinforcement Learning | RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training (Read more on [arXiv](https://arxiv.org/abs/2510.06710) or [HuggingFace](https://huggingface.co/papers/2510.06710))|  | The paper introduces RLinf-VLA, a unified and efficient framework for training Vision-Language-Action (VLA) models using reinforcement learning. It aims to address the fragmented landscape of VLA+RL training by providing a platform for systematic comparison of models and algorithms. The framework features flexible GPU resource allocation, including a hybrid fine-grained pipeline allocation mode that achieves up to 1.88x speedup, and supports diverse VLA architectures, RL algorithms, and simulators. The unified model achieves 98.11% success across 130 LIBERO tasks, and real-world deployment shows stronger generalization than SFT. RLinf-VLA offers AI practitioners a standardized and scalable environment for advancing embodied intelligence research. |
| Computer Vision | MATRIX: Mask Track Alignment for Interaction-aware Video Generation (Read more on [arXiv](https://arxiv.org/abs/2510.07310) or [HuggingFace](https://huggingface.co/papers/2510.07310))| Hyunwook Choi, Jaeho Lee, Dahyun Chung, Siyoon Jin, Seongchan | The paper introduces MATRIX, a framework to enhance interaction-awareness in video diffusion transformers (DiTs). It investigates how video DiTs represent multi-instance interactions and aims to improve their ability to model such interactions. MATRIX regularizes attention in specific layers of video DiTs using multi-instance mask tracks from a curated dataset called MATRIX-11K. Experiments demonstrate that MATRIX improves interaction fidelity and semantic alignment, achieving a KISA score of 0.546 compared to a baseline of 0.406, while reducing drift and hallucination. This framework provides a method to align attention with instance masks, improving the ability to generate videos with complex interactions. |
| Natural Language Processing | Vibe Checker: Aligning Code Evaluation with Human Preference (Read more on [arXiv](https://arxiv.org/abs/2510.07315) or [HuggingFace](https://huggingface.co/papers/2510.07315))|  | This paper introduces VIBE CHECKER, a novel benchmark designed to align code evaluation with human preferences beyond functional correctness. The research investigates how to measure and improve code instruction following alongside functional correctness in Large Language Models (LLMs).  The methodology involves creating VeriCode, a taxonomy of 30 verifiable code instructions, and augmenting established benchmarks like BigCodeBench and LiveCodeBench.  Evaluations of 31 LLMs demonstrate that instruction following is a key differentiator in real-world programming tasks, with a composite score correlating better with human preference than either metric alone (results are shown as regression rates).  The work implies that focusing on both functional correctness and instruction following is crucial for developing LLMs that better align with user expectations in coding. |
| Reinforcement Learning | Multi-Agent Tool-Integrated Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2510.04678) or [HuggingFace](https://huggingface.co/papers/2510.04678))| Lidong Bing, Yuntao Chen, Xingxuan Li, Zhanfeng Mo | This paper introduces Multi-Agent Tool-Integrated Policy Optimization (MATPO), a novel approach for training tool-integrated multi-agent systems within a single LLM. The research aims to develop an effective RL training framework for multi-agent systems utilizing tool-integrated planning. MATPO employs a credit assignment mechanism across planner and worker rollouts, eliminating the need for multiple LLMs. Experiments on GAIA-text, WebWalkerQA, and FRAMES demonstrate that MATPO outperforms single-agent baselines by an average of 18.38% relative improvement in performance. The framework provides insights and a methodology for more stable and efficient multi-agent RL training, suggesting that unifying multiple agent roles within a single LLM is effective for tool-integrated planning. |
| Computer Vision | OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot (Read more on [arXiv](https://arxiv.org/abs/2510.06751) or [HuggingFace](https://huggingface.co/papers/2510.06751))|  | The paper introduces OBS-Diff, a novel one-shot pruning framework for compressing large-scale text-to-image diffusion models. It aims to develop a training-free pruning method applicable to diverse architectures with multiple granularities, addressing the limitations of existing techniques. OBS-Diff adapts the Optimal Brain Surgeon (OBS) algorithm by incorporating a timestep-aware Hessian construction to prioritize earlier denoising steps and employs a group-wise sequential pruning strategy for computational efficiency. Experiments demonstrate state-of-the-art one-shot pruning performance, achieving inference acceleration with minimal visual quality degradation; for instance, it improves the CLIP score on SD3.5-Large at 60% sparsity compared to other baselines. The developed method enables AI practitioners to efficiently compress diffusion models without retraining, reducing computational costs while preserving visual quality. |
| Natural Language Processing | Revisiting Long-context Modeling from Context Denoising Perspective (Read more on [arXiv](https://arxiv.org/abs/2510.05862) or [HuggingFace](https://huggingface.co/papers/2510.05862))|  | This paper revisits long-context modeling by analyzing context noise, proposing Integrated Gradient (IG) score for noise detection. It addresses the problem of LCMs being misled by irrelevant tokens, leading to suboptimal performance. The key methodology involves Context Denoising Training (CDT), which improves attention on critical tokens. Experiments across four tasks demonstrate CDT's superiority, with an 8B model achieving a comparable performance to GPT-40 (50.92 vs 51.00). CDT mitigates context noise enhancing model's focus on crucial information. |
| Natural Language Processing | Artificial Hippocampus Networks for Efficient Long-Context Modeling (Read more on [arXiv](https://arxiv.org/abs/2510.07318) or [HuggingFace](https://huggingface.co/papers/2510.07318))|  | This paper introduces Artificial Hippocampus Networks (AHNs) for efficient long-context sequence modeling. The research aims to mitigate the computational cost of attention in Transformers by compressing long-term context into a fixed-size memory. The methodology involves a sliding window attention mechanism augmented with RNN-like AHNs to compress out-of-window information. Experiments on LV-Eval show that augmenting Qwen2.5-3B-Instruct with AHNs reduces FLOPs by 40.5% and memory cache by 74.0%, while improving average score from 4.41 to 5.88. The framework enables processing long sequences efficiently, balancing short-term fidelity with long-term contextual awareness. |
| Natural Language Processing | Native Hybrid Attention for Efficient Sequence Modeling (Read more on [arXiv](https://arxiv.org/abs/2510.07019) or [HuggingFace](https://huggingface.co/papers/2510.07019))| Yu Cheng, Weigao Sun, Tao Zhang, Jiaxi Hu, Jusen Du | The paper introduces Native Hybrid Attention (NHA), a new hybrid architecture combining linear and full attention for efficient sequence modeling. The research aims to improve the efficiency of Transformers while maintaining accuracy, particularly in recall-intensive tasks. NHA integrates a linear RNN for long-term context with a sliding window for short-term tokens, applying a single softmax attention operation over all keys and values. Experimental results show that NHA surpasses Transformers on recall-intensive tasks; for example, NHA achieves 38.60 Recall on Recall-Intensive tasks (NHA(full)). This technique allows for structural hybridization of pretrained LLMs, potentially achieving competitive accuracy with improved inference speed. |
| Machine Learning | Why Low-Precision Transformer Training Fails: An Analysis on Flash
  Attention (Read more on [arXiv](https://arxiv.org/abs/2510.04212) or [HuggingFace](https://huggingface.co/papers/2510.04212))|  | This paper investigates the failure of low-precision training with flash attention, where catastrophic loss explosions occur. The study aims to identify the underlying causes of this instability. Through mechanistic analysis, the authors reveal that the failure stems from the emergence of similar low-rank representations and the compounding effect of biased rounding errors in BF16 arithmetic. They mitigate the bias by introducing a minimal modification to flash attention, achieving stable training. This offers a practical solution and provides a blueprint for diagnosing similar numerical instabilities in large-scale model training. |
| Natural Language Processing | When Benchmarks Age: Temporal Misalignment through Large Language Model
  Factuality Evaluation (Read more on [arXiv](https://arxiv.org/abs/2510.07238) or [HuggingFace](https://huggingface.co/papers/2510.07238))|  | The paper investigates temporal misalignment between static factuality benchmarks and rapidly evolving large language models (LLMs). It addresses the research questions regarding the extent of outdated factual answers in static benchmarks and the effect of benchmark aging on factuality evaluation. The methodology includes an up-to-date fact retrieval pipeline and three metrics, including Dataset Drift Score (DDS), to quantify temporal misalignment. The experiments reveal that a considerable portion of samples in widely used factuality benchmarks are outdated (DDS up to 63.78%), leading to unreliable assessments of LLM factuality. The work implies that future benchmark designs should consider temporal misalignment to ensure accurate LLM factuality evaluation. |
| Reinforcement Learning | StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact
  State Representation (Read more on [arXiv](https://arxiv.org/abs/2510.05057) or [HuggingFace](https://huggingface.co/papers/2510.05057))|  | The paper introduces StaMo, an unsupervised approach for learning generalizable robot motion from a compact state representation. It aims to develop expressive and compact state representations for efficient world modeling and decision making without explicit motion supervision. StaMo uses a Diffusion Autoencoder with a pre-trained Diffusion Transformer (DiT) decoder to encode static images into a two-token representation, where the difference between tokens represents motion. The method improves performance by 14.3% on LIBERO and 30% in real-world task success, and achieves a 10.4% increase in policy co-training compared to prior methods. StaMo provides a pathway for learning reusable skills by leveraging static images and large generative models, reducing dependence on complex temporal modeling. |
| Computer Vision | Are We Using the Right Benchmark: An Evaluation Framework for Visual
  Token Compression Methods (Read more on [arXiv](https://arxiv.org/abs/2510.07143) or [HuggingFace](https://huggingface.co/papers/2510.07143))| Yiyu Wang, Xu Zheng, Zichen Wen, Wensong Wang, Chenfei Liao | This paper presents an evaluation framework for visual token compression methods, revealing the inadequacy of existing benchmarks. The research investigates whether current benchmarks designed for MLLMs accurately assess compression techniques and exposes a task mismatch. The study introduces VTC-Bench, an evaluation framework incorporating a data filtering mechanism to denoise benchmarks by distinguishing simple and difficult samples. Results show that simple image downsampling outperforms many advanced compression methods across widely used benchmarks, with downsampling achieving 66.4% on MMBench compared to DART at 42.1% at 93.75% compression, implying current benchmarks are noisy. The findings suggest AI practitioners should carefully evaluate the suitability of benchmarks for visual token compression, considering the simplicity bias and implementing data filtering mechanisms. |
| Multi-Modal | Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in
  MLLMs (Read more on [arXiv](https://arxiv.org/abs/2510.01954) or [HuggingFace](https://huggingface.co/papers/2510.01954))| Jingyi Liao, Nanqing Liu, Shijie Li, Haojie Zhang, Yongyi Su | This paper introduces Patch-as-Decodable Token (PaDT), a unified framework for multi-modal vision tasks in MLLMs. It addresses the limitations of existing methods that rely on indirect representations for vision tasks by enabling MLLMs to directly generate both textual and diverse visual outputs using Visual Reference Tokens (VRTs). PaDT independently processes VRTs and dynamically expands the embedding table, improving localization and differentiation among similar objects. Evaluated on four visual tasks, PaDT achieves state-of-the-art performance, surpassing significantly larger MLLM models, e.g., improving mAP on COCO detection by 19.0. PaDT offers AI practitioners a unified and efficient approach for diverse visual perception tasks by simplifying dense prediction and enhancing localization. |
| Computer Vision | WristWorld: Generating Wrist-Views via 4D World Models for Robotic
  Manipulation (Read more on [arXiv](https://arxiv.org/abs/2510.07313) or [HuggingFace](https://huggingface.co/papers/2510.07313))|  | The paper presents WristWorld, a 4D world model for generating wrist-view videos from anchor views to improve robotic manipulation. It addresses the challenge of scarce wrist-view data by reconstructing geometrically consistent wrist-view poses and synthesizing coherent videos using a novel Spatial Projection Consistency (SPC) Loss and a video generation model. Experiments on Droid, Calvin, and Franka Panda demonstrate state-of-the-art video generation with improved spatial consistency. On the Calvin dataset, WristWorld increases average task completion length by 3.81%. The framework can be used as a plug-in to extend existing single-view world models with multi-view capability. |
| Multi-Modal | TTRV: Test-Time Reinforcement Learning for Vision Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.06783) or [HuggingFace](https://huggingface.co/papers/2510.06783))| Serena Yeung-Levy, Paul Gavrikov, Wei Lin, Shyam Marjit, Akshit Singh | The paper introduces TTRV, a test-time reinforcement learning framework for vision-language models. It addresses the challenge of adapting VLMs to new environments without labeled data by extracting reward signals directly from unlabeled test data using frequency-based and diversity-control mechanisms. The key methodology involves enhancing Group Relative Policy Optimization (GRPO) with novel unsupervised reward signals derived from the model's own predictions. Experiments show TTRV consistently improves performance, surpassing GPT-40 by 2.3% on image classification tasks and achieving up to 52.4% improvement on image recognition benchmarks. TTRV offers AI practitioners a data-efficient method for adapting pre-trained VLMs to downstream tasks on-the-fly, leveraging self-supervision and online learning. |
| Natural Language Processing | The African Languages Lab: A Collaborative Approach to Advancing
  Low-Resource African NLP (Read more on [arXiv](https://arxiv.org/abs/2510.05644) or [HuggingFace](https://huggingface.co/papers/2510.05644))|  | The paper introduces the African Languages Lab (All Lab), a comprehensive initiative to advance low-resource African NLP. The research focuses on systematically collecting data, developing models, and building capacity for African languages. The All Lab employs a quality-controlled data collection pipeline and fine-tuning to achieve substantial improvements in translation performance, reporting an average gain of +23.69 ChrF++ across 31 evaluated languages. This work provides a valuable resource and benchmark for AI practitioners working on African languages, facilitating more inclusive and equitable NLP technologies. |
| Machine Learning | MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline (Read more on [arXiv](https://arxiv.org/abs/2510.07307) or [HuggingFace](https://huggingface.co/papers/2510.07307))|  | The paper introduces MLE-Smith, an automated multi-agent pipeline to generate competition-style machine learning engineering (MLE) tasks from raw datasets. It aims to address the scalability limitations of current MLE benchmarks by automating task creation through a generate-verify-execute paradigm. The methodology involves three agents (Brainstormer, Designer, Refactor) and a hybrid verification mechanism enforcing structural integrity, semantic soundness, and empirical solvability. Experiments on 224 real-world datasets generating 606 tasks demonstrate that LLM performance on MLE-Smith tasks correlates strongly with human-designed tasks. This facilitates scalable evaluation and training of next-generation MLE agents by providing diverse, realistic, and verifiable tasks. |
| Natural Language Processing | Revisiting the Uniform Information Density Hypothesis in LLM Reasoning
  Traces (Read more on [arXiv](https://arxiv.org/abs/2510.06953) or [HuggingFace](https://huggingface.co/papers/2510.06953))| Jaehyung Kim, Guijin Son, Minju Gwak | This paper revisits the Uniform Information Density (UID) hypothesis within the context of large language model (LLM) reasoning traces. The core research question explores whether step-level uniformity in reasoning traces correlates with reasoning quality. The study introduces an entropy-based stepwise information density metric and local/global uniformity scores to analyze LLM-generated reasoning traces on mathematical benchmarks. Results indicate that reasoning traces with low global uniformity and high local uniformity correlate with higher answer correctness, achieving 10-32% relative accuracy gains at AIME2025 by selecting traces based on these uniformity measures. The study suggests uniformity can serve as a diagnostic criterion for building more reliable and accurate reasoning systems. |
| Computer Vision | Online Generic Event Boundary Detection (Read more on [arXiv](https://arxiv.org/abs/2510.06855) or [HuggingFace](https://huggingface.co/papers/2510.06855))| Jonghyun Choi, Jeany Son, Seunggyun Lim, Hyungrok Jung, carpedkm | The paper introduces Online Generic Event Boundary Detection (On-GEBD), a novel task for detecting event boundaries in streaming videos. It addresses the challenge of identifying taxonomy-free event changes in real-time without access to future frames. The proposed framework, ESTimator, leverages a Consistent Event Anticipator (CEA) and an Online Boundary Discriminator (OBD), drawing inspiration from Event Segmentation Theory. Experiments on Kinetics-GEBD demonstrate that ESTimator outperforms existing online video models and achieves comparable results to offline methods (e.g. improving Avg F1 score). This offers a new approach for real-time video understanding that aligns better with human perceptual processes. |
| Reinforcement Learning | The Markovian Thinker (Read more on [arXiv](https://arxiv.org/abs/2510.06557) or [HuggingFace](https://huggingface.co/papers/2510.06557))|  | The paper introduces Markovian Thinking, a paradigm for training reasoning LLMs with reinforcement learning that decouples thinking length from context size. It proposes Delethink, an RL environment structuring reasoning into fixed-size chunks with a carryover mechanism to maintain state. Delethink-trained models achieve performance comparable to LongCoT-RL with reduced compute, allowing for longer reasoning. R1-Distill 1.5B trained with Delethink can think up to 24K tokens with only 7 H100-months for training, matching or exceeding the performance of LongCoT-RL trained with a 24K budget that costs 27 H100-months. The main implication is the enablement of efficient, scalable training for reasoning LLMs capable of very long reasoning without quadratic overhead. |
| Multi-Modal | Bridging Text and Video Generation: A Survey (Read more on [arXiv](https://arxiv.org/abs/2510.04999) or [HuggingFace](https://huggingface.co/papers/2510.04999))| G. Maragatham, Priyansh Bhandari, nnilayy | This survey paper comprehensively reviews text-to-video (T2V) generation models, tracing their development from GANs and VAEs to current hybrid Diffusion-Transformer architectures. It investigates how models address challenges in quality, coherence, and control, covering model architecture, limitations, and shifts towards new paradigms. The survey provides a systematic account of datasets, training configurations (including hardware specs and hyperparameters), and evaluation metrics while presenting benchmark performances and limitations; for the surveyed models, metrics such as Inception Score (IS) and Fréchet Video Distance (FVD) are presented. The analysis identifies open challenges like alignment and computational efficiency. This survey offers a perspective for future researchers to explore, build upon, and advance T2V research and applications. |
| Machine Learning | AlphaApollo: Orchestrating Foundation Models and Professional Tools into
  a Self-Evolving System for Deep Agentic Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.06261) or [HuggingFace](https://huggingface.co/papers/2510.06261))| Zongze Li, Xuan Li, Xiao Feng, Chentao Cao, Zhanke Zhou | The paper introduces AlphaApollo, a self-evolving agentic reasoning system designed to improve foundation model reasoning. It addresses limitations in model-intrinsic capacity and test-time iteration by orchestrating multiple models and professional tools like Python with scientific libraries and retrieval systems.  AlphaApollo achieves consistent performance gains on AIME 2024/2025, with Qwen2.5-14B-Instruct showing +5.15% Average@32 and +23.34% Pass@32.  Tool use analysis demonstrates high tool-call correctness (over 80%) and consistent outperformance of non-tool baselines, thereby lifting the capability ceiling of FMs.  AlphaApollo provides a framework for AI practitioners to develop more reliable and capable reasoning systems by integrating professional tools and enabling iterative refinement. |
| Machine Learning | G^2RPO: Granular GRPO for Precise Reward in Flow Models (Read more on [arXiv](https://arxiv.org/abs/2510.01982) or [HuggingFace](https://huggingface.co/papers/2510.01982))|  | This paper introduces Granular-GRPO (G2RPO) to improve reward alignment in flow-based generative models. It addresses the challenge of sparse reward signals in online reinforcement learning for flow models by proposing Singular Stochastic Sampling and Multi-Granularity Advantage Integration. Singular Stochastic Sampling confines stochastic exploration to a single step, facilitating a more faithful reward assignment, while Multi-Granularity Advantage Integration aggregates advantages from multiple diffusion scales for a more comprehensive evaluation. Experiments demonstrate that G2RPO outperforms existing flow-based GRPO baselines, achieving a 6.52% relative improvement compared to DanceGRPO when using HPS-v2.1 as the reward model. G2RPO provides AI practitioners with a method to achieve precise and comprehensive reward assessment of sampling directions, thereby enhancing the effectiveness and robustness of flow model training. |
| Computer Vision | U-Bench: A Comprehensive Understanding of U-Net through 100-Variant
  Benchmarking (Read more on [arXiv](https://arxiv.org/abs/2510.07041) or [HuggingFace](https://huggingface.co/papers/2510.07041))| Heqin Zhu, Zikang Xu, Wenxin Ma, Chengqi Dong, Fenghe Tang | U-Bench provides a comprehensive benchmark for U-Net variants in medical image segmentation. The research aims to address the lack of systematic evaluation by benchmarking 100 U-Net variants across 28 datasets and 10 modalities. U-Bench introduces U-Score, a metric balancing performance and efficiency, and analyzes the impact of architectural paradigms and dataset characteristics on model performance. Results show limited in-domain IoU gains, but significant zero-shot improvements, with U-Score highlighting efficient models. U-Bench offers practitioners a model advisor agent and extensive resources for reproducible and practically relevant U-Net benchmarking, enabling more informed model selection and deployment. |
| Natural Language Processing | Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era
  of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.07037) or [HuggingFace](https://huggingface.co/papers/2510.07037))|  | This paper surveys code-switched natural language processing (CSW-NLP) in the era of large language models (LLMs). It investigates how LLMs adapt to mixed-language inputs, addressing limitations in CSW datasets and evaluation biases. The study analyzes 308 CSW research papers across five research areas and over 80 languages. It classifies advances by architecture, training strategy, and evaluation, and quantifies performance gains (e.g. ,up to 32x accuracy improvement). The survey implies that future advances necessitate inclusive datasets, fair evaluation, and linguistically grounded models to achieve robust multilingual intelligence. |
| Machine Learning | NorMuon: Making Muon more efficient and scalable (Read more on [arXiv](https://arxiv.org/abs/2510.05491) or [HuggingFace](https://huggingface.co/papers/2510.05491))| Tuo Zhao, Weizhu Chen, Chen Liang, Liming Liu, Zichong Li | The paper introduces NorMuon, an optimized variant of the Muon optimizer for training large language models more efficiently and scalably. The research aims to improve Muon by incorporating neuron-level adaptive learning rates to balance parameter utilization after orthogonalization. NorMuon combines orthogonalization with neuron-wise normalization using second-order momentum statistics and a distributed FSDP2 implementation. Experiments show NorMuon achieves a 21.74% improvement in training efficiency compared to Adam and an 11.31% improvement over Muon on a 1.1B parameter pretraining task. The findings suggest that orthogonalization and adaptive learning rates can be synergistically combined, informing new optimizer designs for large-scale deep learning. |
| Computer Vision | D^3QE: Learning Discrete Distribution Discrepancy-aware
  Quantization Error for Autoregressive-Generated Image Detection (Read more on [arXiv](https://arxiv.org/abs/2510.05891) or [HuggingFace](https://huggingface.co/papers/2510.05891))| Yueqi Duan, Wenzhao Zheng, Yu Zheng, Bingyao Yu, Yanran Zhang | The paper introduces D³QE, a novel method for detecting autoregressive-generated images by leveraging discrete distribution discrepancies. It aims to address the challenge of detecting synthetic images from autoregressive models, which exhibit unique quantization characteristics. The methodology involves a discrete distribution discrepancy-aware transformer that integrates dynamic codebook frequency statistics. Experiments on the ARForensics dataset demonstrate superior detection accuracy, achieving 82.11% average accuracy and 92.07% average precision, with strong generalization across different AR models. D³QE provides AI practitioners with an improved method for detecting synthetic images generated by autoregressive models, enhancing robustness against potential misuse. |
| Reinforcement Learning | DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for
  Autonomous Travel Planning Agents (Read more on [arXiv](https://arxiv.org/abs/2509.21842) or [HuggingFace](https://huggingface.co/papers/2509.21842))|  | The paper introduces DeepTravel, an end-to-end agentic reinforcement learning framework for autonomous travel planning. It aims to build flexible and autonomous agents capable of planning, executing tools, and reflecting on responses for multi-step reasoning in travel planning. The framework constructs a sandbox environment, a hierarchical reward system, and a reply-augmented reinforcement learning method. Experimental results show DeepTravel enables small LLMs (Qwen3-32B) to outperform larger models like OpenAI-01/03, achieving a final pass rate of 62.77% on online user data. The framework provides a promising approach for building autonomous travel planning agents, demonstrating how agentic RL can improve LLM performance in complex, real-world tasks. |
| Computer Vision | Heptapod: Language Modeling on Visual Signals (Read more on [arXiv](https://arxiv.org/abs/2510.06673) or [HuggingFace](https://huggingface.co/papers/2510.06673))|  | The paper introduces Heptapod, an autoregressive image generation model based on language modeling principles. It aims to improve image generation quality by eliminating the reliance on classifier-free guidance and semantic tokenizers, focusing on a reconstruction-focused tokenizer. The key methodology involves predicting the distribution over the entire 2D spatial grid of an image at each timestep using a causal Transformer. On ImageNet generation, Heptapod achieves an FID of 2.70. This work suggests a rethinking of language modeling principles for visual signals, potentially enabling more efficient and high-quality image generation. |
