

## Papers for 2025-10-03

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | LongCodeZip: Compress Long Context for Code Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.00446) or [HuggingFace](https://huggingface.co/papers/2510.00446))|  | The paper introduces LongCodeZip, a novel plug-and-play code compression framework for code Large Language Models (LLMs). The research aims to reduce context size in code LLMs while preserving essential information. LongCodeZip employs a dual-stage compression strategy: coarse-grained function-level chunking using conditional perplexity and fine-grained block-level pruning with perplexity-based detection. Evaluations demonstrate that LongCodeZip achieves up to a 5.6x compression ratio without degrading task performance, outperforming baseline methods on code completion, summarization, and question answering. The framework enables LLMs to scale better in real-world code scenarios by reducing API costs, generation latency, and context truncation. |
| Computer Vision | Self-Forcing++: Towards Minute-Scale High-Quality Video Generation (Read more on [arXiv](https://arxiv.org/abs/2510.02283) or [HuggingFace](https://huggingface.co/papers/2510.02283))|  | The paper introduces Self-Forcing++, a novel approach for generating minute-scale, high-quality videos. It addresses the challenge of quality degradation in long-horizon autoregressive video generation by guiding a student model with sampled segments from self-generated long videos, mitigating error accumulation. The key methodology involves backward noise initialization, extended distribution matching distillation, and a rolling KV cache. Experiments show the method can generate videos up to 4 minutes and 15 seconds, representing a 50x improvement over the baseline, with a text alignment score of 26.04 at 100 seconds. Self-Forcing++ enables AI practitioners to generate significantly longer videos with enhanced temporal consistency and visual fidelity without requiring long-video training data. |
| Computer Vision | StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided
  Illusions (Read more on [arXiv](https://arxiv.org/abs/2510.02314) or [HuggingFace](https://huggingface.co/papers/2510.02314))|  | The paper introduces StealthAttack, a novel density-guided poisoning attack against 3D Gaussian Splatting (3DGS). It aims to inject viewpoint-dependent illusory objects into 3DGS scenes while minimizing impact on innocent views. The methodology strategically places Gaussian points in low-density regions identified using Kernel Density Estimation (KDE) and introduces adaptive noise to disrupt multi-view consistency. Experiments demonstrate superior performance compared to state-of-the-art techniques, achieving a PSNR of 27.04 on V-ILLUSORY views in single-view attacks. StealthAttack highlights critical vulnerabilities in 3D scene representation models, necessitating robust defense mechanisms. |
| Reinforcement Learning | ExGRPO: Learning to Reason from Experience (Read more on [arXiv](https://arxiv.org/abs/2510.02245) or [HuggingFace](https://huggingface.co/papers/2510.02245))| Dongrui Liu, Xiaoye Qu, Zhi Wang, Yafu Li, Runzhe Zhan | The paper introduces ExGRPO, a reinforcement learning from verifiable rewards (RLVR) framework for improving reasoning in large language models. It aims to address computational inefficiency and instability by identifying valuable reasoning experiences for reuse. ExGRPO prioritizes experiences based on rollout correctness and trajectory entropy, employing a mixed-policy objective for balanced exploration and exploitation. Experiments on models (1.5B-8B parameters) demonstrate that ExGRPO improves reasoning performance, achieving average gains of +3.5/7.6 points over on-policy RLVR. The research implies that principled experience management is crucial for efficient and scalable RLVR. |
| Machine Learning | StockBench: Can LLM Agents Trade Stocks Profitably In Real-world
  Markets? (Read more on [arXiv](https://arxiv.org/abs/2510.02209) or [HuggingFace](https://huggingface.co/papers/2510.02209))| Jianing Yu, Jin Ye, Yantao Liu, Zijun Yao, Yanxu Chen | This paper introduces StockBench, a novel benchmark for evaluating LLM agents in realistic stock trading environments. The research aims to assess the profitability and risk management capabilities of LLM agents in dynamic financial markets. The methodology involves simulating multi-month trading environments with realistic market signals, where agents make sequential buy, sell, or hold decisions. Results show that some LLM agents demonstrate the potential to deliver higher returns with better risk management compared to a buy-and-hold baseline, with some achieving returns above 2% and improved risk profiles. The study implies that static financial knowledge does not necessarily translate to effective trading strategies, highlighting the need for advancements in developing LLM-powered financial agents. |
| Machine Learning | Interactive Training: Feedback-Driven Neural Network Optimization (Read more on [arXiv](https://arxiv.org/abs/2510.02297) or [HuggingFace](https://huggingface.co/papers/2510.02297))|  | This paper introduces Interactive Training, a framework for real-time, feedback-driven neural network optimization. The main objective is to improve training stability and adaptability by enabling dynamic adjustments of hyperparameters and data. The methodology involves a control server mediating communication between users/AI agents and the ongoing training process, allowing for adjustments based on observed training dynamics. Case studies demonstrate superior training stability and reduced sensitivity to initial hyperparameters; for example, human-in-the-loop training achieves lower validation losses than a static baseline on Wikitext-2. The framework offers AI practitioners a means to proactively resolve training instabilities and optimize training dynamics through interactive control. |
| Multi-Modal | VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.01444) or [HuggingFace](https://huggingface.co/papers/2510.01444))|  | The paper introduces VOGUE, a visual-uncertainty-guided exploration method for multimodal RLVR, addressing exploration challenges in MLLMs. VOGUE quantifies the policy's sensitivity to visual perturbations and uses it to shape the learning objective through an uncertainty-proportional bonus. This dual-branch approach balances exploration and exploitation, leading to an average improvement of 2.6% in pass@1 accuracy on visual math benchmarks and 3.7% on general reasoning benchmarks using Qwen2.5-VL models. The key implication is that grounding exploration in visual input uncertainty is effective for improving multimodal reasoning by building policies that are more robust to visual ambiguity. |
| Natural Language Processing | The Rogue Scalpel: Activation Steering Compromises LLM Safety (Read more on [arXiv](https://arxiv.org/abs/2509.22067) or [HuggingFace](https://huggingface.co/papers/2509.22067))| Ivan Oseledets, Oleg Y. Rogov, Alexey Dontsov, Andrey Galichin, Anton Korznikov | This paper investigates the safety vulnerabilities introduced by activation steering, a technique for controlling LLM behavior by manipulating hidden states. The research aims to demonstrate how activation steering can compromise model alignment safeguards. The methodology involves applying steering, using both random and SAE-based vectors, to various LLMs and evaluating the harmfulness of the responses. The primary result shows that steering in a random direction can increase the probability of harmful compliance by up to 2-27%, and SAE features can further increase this rate by 2-4%; moreover, a universal attack vector can be created by combining 20 random vectors, leading to a 4x increase in compliance. This challenges the paradigm of safety through interpretability, highlighting the need for robust safety mechanisms against unintended alignment compromises. |
| Natural Language Processing | CLUE: Non-parametric Verification from Experience via Hidden-State
  Clustering (Read more on [arXiv](https://arxiv.org/abs/2510.01591) or [HuggingFace](https://huggingface.co/papers/2510.01591))| Dian Yu, Linfeng Song, Yujun Zhou, Ruosen Li, Zhenwen Liang | This paper presents CLUE, a novel non-parametric method for verifying the correctness of Large Language Model (LLM) outputs based on hidden-state clustering. The research aims to improve the reliability of LLM output selection by directly leveraging the model's internal reasoning trajectory instead of relying on text-based or confidence-based methods. CLUE summarizes each reasoning trace using a hidden state delta and classifies correctness via nearest-centroid distance to clusters formed from past experiences. Empirically, CLUE improves accuracy, boosting from 56.7% to 70.0% (top-maj@16) on AIME 24 with a 1.5B model. CLUE offers a training-free, efficient approach that leverages internal model states for enhanced verification and generalization across tasks and model scales. |
| Reinforcement Learning | RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via
  Multi-Stage Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2510.02240) or [HuggingFace](https://huggingface.co/papers/2510.02240))|  | RewardMap addresses the challenge of sparse rewards in fine-grained visual reasoning tasks using multi-stage reinforcement learning. The study aims to improve spatial reasoning capabilities of multimodal large language models (MLLMs) in structured environments. They introduce a difficulty-aware reward design incorporating detail rewards and a multi-stage RL scheme that bootstraps training from simple perception to complex reasoning. Experiments on REASONMAP and REASONMAP-PLUS show a 3.47% average improvement across 6 benchmarks. The framework enhances visual understanding and reasoning capabilities, offering a more effective cold-start strategy compared to supervised fine-tuning. |
| Reinforcement Learning | RLP: Reinforcement as a Pretraining Objective (Read more on [arXiv](https://arxiv.org/abs/2510.01265) or [HuggingFace](https://huggingface.co/papers/2510.01265))|  | The paper introduces RLP, a reinforcement learning pretraining objective for large language models that integrates exploration into the pretraining phase. RLP aims to improve reasoning by treating chain-of-thought generation as an exploratory action and rewarding information gain for predicting future tokens. The methodology involves computing rewards based on the increase in log-likelihood when conditioning on a reasoning chain, yielding a verifier-free, dense reward signal. Pretraining with RLP on Qwen3-1.7B-BASE improves overall average across eight math-and-science benchmarks by 19%. RLP offers an alternative pretraining strategy that enhances reasoning capabilities and complements existing next-token prediction methods. |
| Machine Learning | The Unreasonable Effectiveness of Scaling Agents for Computer Use (Read more on [arXiv](https://arxiv.org/abs/2510.02250) or [HuggingFace](https://huggingface.co/papers/2510.02250))|  | The paper explores scaling computer-use agents (CUAs) to automate digital tasks. It introduces Behavior Best-of-N (bBoN), a method that generates multiple agent rollouts and selects the best using behavior narratives. bBoN achieves a new state-of-the-art of 69.9% on OSWorld, outperforming prior methods and approaching human-level performance (72%).  The method also demonstrates strong generalization to WindowsAgentArena and AndroidWorld. The findings suggest that effective scaling of CUAs requires structured trajectory understanding and selection. |
| Multi-Modal | Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation (Read more on [arXiv](https://arxiv.org/abs/2510.01284) or [HuggingFace](https://huggingface.co/papers/2510.01284))|  | The paper introduces OVI, a unified audio-video generation paradigm using twin DiT modules for synchronized content creation. OVI aims to model audio and video as a single generative process, eliminating separate pipelines and post-hoc alignment. The method employs blockwise cross-modal fusion, initializing an audio tower with a pretrained video model architecture and jointly training them. Subjective human evaluation revealed that OVI achieves a preference rate above 80% for combined audio and video quality, as well as synchronization when compared to the baselines JavisDiT and UniVerse-1. OVI provides AI practitioners with a simplified, scalable approach for generating synchronized, high-quality audio-visual content without heuristic add-ons. |
| Computer Vision | DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag
  Editing (Read more on [arXiv](https://arxiv.org/abs/2510.02253) or [HuggingFace](https://huggingface.co/papers/2510.02253))| Zhuming Lian, Shaocong Zhang, Shuli Leng, Shilin Lu, Zihan Zhou | The paper introduces DragFlow, a novel region-based drag editing framework designed to leverage the strong generative priors of Diffusion Transformers (DiTs). It addresses the limitations of applying point-based drag editing methods to DiTs, which suffer from insufficient feature structure for reliable motion supervision. DragFlow employs affine transformations for richer feature supervision, integrates personalization adapters for subject consistency, and utilizes gradient mask-based hard constraints for background fidelity. Extensive experiments on DragBench-DR and a newly curated Region-based Dragging benchmark (ReD Bench) demonstrate that DragFlow surpasses state-of-the-art baselines. This work provides AI practitioners a method to unlock stronger generative priors for drag editing with significantly improved controllability and faithfulness. |
| Natural Language Processing | Learning to Reason for Hallucination Span Detection (Read more on [arXiv](https://arxiv.org/abs/2510.02173) or [HuggingFace](https://huggingface.co/papers/2510.02173))| Hadi Pouransari, Kundan Krishna, Hema Swetha Koppula, Ting-Yao Hu, Hsuan Su | This paper addresses hallucination span detection in large language models using reinforcement learning. The research questions are whether explicit reasoning can aid in hallucination span detection and how to effectively train reasoning models for this task. The proposed method, RL4HS, utilizes Group Relative Policy Optimization with a span-level reward function and Class-Aware Policy Optimization to improve performance. Experiments on the RAGTruth benchmark showed that RL4HS surpasses pretrained reasoning models and supervised fine-tuning, achieving improved span-F1 scores. This suggests that reinforcement learning with span-level rewards is necessary for detecting hallucination spans, and the method can effectively align reasoning with hallucination detection. |
| Machine Learning | TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP
  Environments (Read more on [arXiv](https://arxiv.org/abs/2510.01179) or [HuggingFace](https://huggingface.co/papers/2510.01179))|  | The paper introduces TOUCAN, a large-scale tool-agentic dataset for training language models to effectively use external tools. It addresses the lack of high-quality, permissively licensed training data for tool-using agents by synthesizing 1.5 million trajectories from nearly 500 real-world Model Context Protocol (MCP) environments. The methodology involves generating diverse tasks, agentic trajectories, and rigorous rule-based/model-based validation. Models fine-tuned on TOUCAN outperform larger closed-source counterparts on the BFCL V3 benchmark, demonstrating superior function calling accuracy. This suggests that TOUCAN can enable smaller, more efficient models to achieve state-of-the-art tool usage, furthering the development of open-source tool-augmented language agents. |
| Natural Language Processing | F2LLM Technical Report: Matching SOTA Embedding Performance with 6
  Million Open-Source Data (Read more on [arXiv](https://arxiv.org/abs/2510.02294) or [HuggingFace](https://huggingface.co/papers/2510.02294))|  | The paper introduces F2LLM, a suite of state-of-the-art embedding models, finetuned from foundation models on 6 million open-source, non-synthetic query-document-negative tuples. The research objective is to achieve competitive embedding performance with reduced training cost and reliance on synthetic data. F2LLM utilizes direct fine-tuning with a curated dataset and margin-based adaptive hard negative mining. F2LLM-4B ranks 2nd among models with approximately 4B parameters and 7th overall on the MTEB English leaderboard. This demonstrates a strong, reproducible, and budget-friendly baseline for text embedding tasks. |
| Multi-Modal | Visual Multi-Agent System: Mitigating Hallucination Snowballing via
  Visual Flow (Read more on [arXiv](https://arxiv.org/abs/2509.21789) or [HuggingFace](https://huggingface.co/papers/2509.21789))| Zhangquan Chen, Yongbo He, Guibin Zhang, Chengming Xu, Xinlei Yu | The paper addresses multi-agent visual hallucination snowballing, a failure mode where hallucinations are amplified across agents in visual language models. The research aims to mitigate this issue by preserving visual information flow. ViF, a plug-and-play method, relays inter-agent messages using selected visual relay tokens and attention reallocation. Experiments demonstrate ViF reduces hallucination snowballing, improving performance across eight benchmarks with a consistent improvement of 2.4-3.8%. The main implication is that visual flow can effectively reduce hallucination propagation in multi-agent VLMs. |
| Computer Vision | VideoNSA: Native Sparse Attention Scales Video Understanding (Read more on [arXiv](https://arxiv.org/abs/2510.02295) or [HuggingFace](https://huggingface.co/papers/2510.02295))| Xiaojun Shan, Ethan Armand, Shusheng Yang, Wenhao Chai, Enxin Song | The paper introduces VideoNSA, a hardware-aware sparse attention mechanism for video understanding. It addresses the challenge of long video context by adapting Native Sparse Attention (NSA) to video-language models, preserving dense attention for text and employing NSA for video. Through end-to-end training on a 216K video instruction dataset, VideoNSA achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks. VideoNSA achieves competitive results, narrowing the gap with state-of-the-art methods, and obtains the highest accuracy on the Tomato benchmark for visual temporal reasoning. The method enables more efficient and scalable video understanding models for AI practitioners. |
| Multi-Modal | Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and
  Reasoning in Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.01304) or [HuggingFace](https://huggingface.co/papers/2510.01304))| Yukun Qi, Xikun Bao, Shiting Huang, Wenxuan Huang, Yu Zeng | This paper introduces AGILE, an agentic jigsaw interaction learning framework for enhancing visual perception and reasoning in VLMs. The research aims to improve VLMs' perceptual and reasoning abilities, which are often limited, even on simple jigsaw tasks. AGILE formulates jigsaw solving as an interactive process where the model generates executable code to perform actions based on the current state and receives fine-grained visual feedback. Experiments show that AGILE significantly boosts performance on jigsaw tasks, increasing accuracy from 9.5% to 82.8% in the 2x2 setting, and demonstrates strong generalization across nine general vision tasks, achieving an average improvement of 3.1%. The framework offers a scalable solution to multimodal reinforcement learning data scarcity, advancing reasoning and generalization in multimodal models. |
| Multi-Modal | Automated Structured Radiology Report Generation with Rich Clinical
  Context (Read more on [arXiv](https://arxiv.org/abs/2510.00428) or [HuggingFace](https://huggingface.co/papers/2510.00428))| Won Hwa Kim, Dongseop Kim, Juho Jung, Dong Bok Lee, Seongjae Kang | This paper presents a contextualized structured radiology report generation (C-SRRG) framework to address the lack of clinical context in existing automated radiology report generation systems. The research aims to improve report generation quality by incorporating multi-view X-ray images, clinical indication, imaging techniques, and prior studies. The proposed methodology curates a C-SRRG dataset and benchmarks state-of-the-art multimodal large language models. Results demonstrate that incorporating clinical context significantly improves report generation quality, achieving F1-SRR-BERT increases of +2.3~4.2/+1.3~7.1 on findings/impression. The study implies that scaling up MLLMs for SRRG requires the inclusion of detailed clinical context for optimal performance. |
| Computer Vision | Go with Your Gut: Scaling Confidence for Autoregressive Image Generation (Read more on [arXiv](https://arxiv.org/abs/2509.26376) or [HuggingFace](https://huggingface.co/papers/2509.26376))| Disen Lan, Rongjin Guo, Wen-Jie Shu, Xianfeng Wu, Harold Haodong Chen | The paper introduces ScalingAR, a novel test-time scaling framework for next-token prediction in autoregressive image generation. It addresses the limitations of existing TTS methods in visual AR by using token entropy as a confidence signal, eliminating the need for early decoding or auxiliary rewards. ScalingAR operates at two levels: Profile Level, fusing intrinsic and conditional signals for confidence estimation, and Policy Level, adaptively terminating low-confidence trajectories and scheduling guidance. Experiments show a 12.5% improvement on GenEval and a 15.2% improvement on TIIF-Bench compared to baseline models. The proposed method provides AI practitioners with a more effective and efficient approach for scaling autoregressive image generation models by leveraging intrinsic token confidence. |
| Reinforcement Learning | Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming
  Attacks (Read more on [arXiv](https://arxiv.org/abs/2510.02286) or [HuggingFace](https://huggingface.co/papers/2510.02286))| Alan Ritter, Miguel Ballesteros, Roshan Sridhar, Afshin Oroojlooy, Ruohao Guo | This paper introduces DIALTREE-RPO, a tree-based reinforcement learning framework for discovering multi-turn adversarial attacks on large language models. The research aims to develop an automated method for red-teaming by treating dialogue as a sequential decision-making problem. The methodology integrates dialogue tree rollout with pruning, a specialized reward function, and adaptive masking to improve exploration and stability. The results show a 25.9% higher Attack Success Rate (ASR) compared to state-of-the-art approaches across ten target models. DIALTREE-RPO enables AI practitioners to uncover diverse, adaptive attack strategies and improve the robustness of safety mechanisms. |
| Natural Language Processing | A Rigorous Benchmark with Multidimensional Evaluation for Deep Research
  Agents: From Answers to Reports (Read more on [arXiv](https://arxiv.org/abs/2510.02190) or [HuggingFace](https://huggingface.co/papers/2510.02190))| Tianle Gu, Yi Lu, Yuxuan Zhang, Yixu Wang, Yang Yao | The paper introduces a new benchmark, Rigorous Bench, and a multidimensional evaluation framework for Deep Research Agents (DRAs) producing report-style outputs. The research aims to address the limitations of existing benchmarks in evaluating DRA capabilities comprehensively. The methodology involves a benchmark with 214 expert-curated queries across 10 domains and an evaluation framework assessing semantic quality, topical focus, and retrieval trustworthiness. Experiments with mainstream DRAs show DRAs outperform tool-augmented models, but further improvement is possible; quantitative results include IntegratedScore, a combined metric for evaluating DRA performance. The implication for AI practitioners is a robust foundation for capability assessment and architectural refinement in DRA systems. |
| Computer Vision | Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject
  Fidelity (Read more on [arXiv](https://arxiv.org/abs/2510.02315) or [HuggingFace](https://huggingface.co/papers/2510.02315))| Thomas Hofmann, Enis Simsar, Eric Tillmann Bill | This paper addresses the challenge of generating high-fidelity multi-subject images using text-to-image (T2I) models. The primary objective is to disentangle multiple subjects described in a prompt, overcoming issues like attribute leakage and subject omission. By formulating flow matching (FM) as a stochastic optimal control (SOC) problem, the authors introduce a test-time controller and an adjoint matching fine-tuning rule. Experiments on Stable Diffusion 3.5 and FLUX show improved multi-subject alignment, with FOCUS achieving state-of-the-art fidelity. This approach offers AI practitioners a principled method to improve the reliability of T2I models in generating complex scenes with multiple subjects. |
| Machine Learning | Transformers Discover Molecular Structure Without Graph Priors (Read more on [arXiv](https://arxiv.org/abs/2510.02259) or [HuggingFace](https://huggingface.co/papers/2510.02259))|  | This paper explores whether Transformers can approximate molecular energies and forces without predefined graphs or physical priors. It investigates this by training a standard Transformer directly on Cartesian coordinates. The key methodology involves training Transformers on the OMol25 dataset and comparing performance against equivariant GNNs under a matched compute budget. Results show that Transformers achieve comparable energy and force mean absolute errors (MAE) to state-of-the-art GNNs, while scaling to 1B parameters. The implication for AI practitioners is a challenge to the necessity of hard-coded graph inductive biases in molecular modeling, pointing towards standardized, scalable architectures. |
| Machine Learning | Rethinking the shape convention of an MLP (Read more on [arXiv](https://arxiv.org/abs/2510.01796) or [HuggingFace](https://huggingface.co/papers/2510.01796))|  | This paper challenges the conventional narrow-wide-narrow shape of MLPs by proposing a wide-narrow-wide (Hourglass) architecture with skip connections at expanded dimensions. The primary objective is to improve incremental refinement by leveraging higher-dimensional spaces while maintaining computational efficiency. The methodology involves a systematic architectural search and performance-parameter Pareto frontier analysis on generative tasks. Results demonstrate that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs, with optimal configurations favoring deeper networks with wider skip connections; for example, in image denoising tasks, Hourglass models achieve PSNR of 22.31dB with 66M parameters compared to 75M parameters required for the conventional model to achieve the same PSNR. This suggests a reconsideration of skip connection placement for improved performance and parameter efficiency in residual networks. |
| Multi-Modal | VLA-R1: Enhancing Reasoning in Vision-Language-Action Models (Read more on [arXiv](https://arxiv.org/abs/2510.01623) or [HuggingFace](https://huggingface.co/papers/2510.01623))| Dapeng Zhang, Xiaofeng Wang, Boyuan Wang, Zeyu Zhang, Angen Ye | The paper introduces VLA-R1, a reasoning-enhanced Vision-Language-Action model. It aims to improve step-by-step reasoning and execution in VLA models through RLVR and GRPO. The method utilizes a post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, along with a new dataset, VLA-CoT-13K. VLA-R1 achieves an IoU of 36.51 on the in-domain affordance benchmark, a 17.78% improvement over the baseline. This suggests a way to enhance generalization and real-world applicability for VLA models in embodied AI systems. |
| Multi-Modal | VIRTUE: Visual-Interactive Text-Image Universal Embedder (Read more on [arXiv](https://arxiv.org/abs/2510.00523) or [HuggingFace](https://huggingface.co/papers/2510.00523))| Yuki Mitsufuji, Shusuke Takahashi, Qiyu Wu, Kazuya Tateishi, Wei-Yao Wang | The paper introduces VIRTUE, a visual-interactive text-image universal embedder that enhances multimodal representation learning. It addresses the lack of visual-interactive capabilities in existing models, enabling users to specify regions of interest through visual prompts. VIRTUE combines a segmentation model with a VLM, allowing the embedder to handle complex scenarios and learn entity-level information. Evaluated on a new Segmentation-and-Scene Caption Retrieval (SCaR) benchmark and MMEB, VIRTUE achieves state-of-the-art performance, improving MMEB by 3.1%-8.5% and SCaR by 15.2%-20.3%. This provides AI practitioners with a model that better understands user intent through both visual and textual modalities, enhancing retrieval precision and alignment. |
| Reinforcement Learning | Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm:
  Demystifying Some Myths About GRPO and Its Friends (Read more on [arXiv](https://arxiv.org/abs/2509.24203) or [HuggingFace](https://huggingface.co/papers/2509.24203))| Wenhao Zhang, Yushuo Chen, Yuchang Sun, Yanxi Chen, Chaorui Yao | This paper presents an off-policy interpretation of group-relative REINFORCE (GRPO), challenging its conventional on-policy classification. It investigates adapting REINFORCE to off-policy settings by focusing on regularizing policy updates and actively shaping data distribution. The methodology involves a first-principles derivation of group-relative REINFORCE without specific data distribution assumptions, revealing its native off-policy nature. Experiments on GSM8k demonstrate that GRPO's effectiveness stems from clipping as regularization. The findings offer new opportunities for principled algorithm design in off-policy RL for LLMs by demystifying common myths and unifying recent algorithms under a REINFORCE loss framework. |
| Natural Language Processing | SKYLENAGE Technical Report: Mathematical Reasoning and
  Contest-Innovation Benchmarks for Multi-Level Math Evaluation (Read more on [arXiv](https://arxiv.org/abs/2510.01241) or [HuggingFace](https://huggingface.co/papers/2510.01241))| Weiqi Zhai, Linlin Miao, Boyu Yang, Ze Xu, Hu Wei | The paper introduces SKYLENAGE, a novel mathematical reasoning benchmark designed to address ceiling effects in existing math suites by evaluating multi-level math evaluation of large language models. It aims to diagnose structural reasoning ability and capture contest-style difficulty spanning multiple academic stages using two complementary benchmarks: SKYLENAGE-ReasoningMATH and SKYLENAGE-MATH. The methodology involves evaluating 15 contemporary LLM variants under a unified protocol with chain-of-thought prompting and small-sample self-consistency. Results show that the strongest model on the contest suite reaches 44% accuracy, while the best model on the reasoning set attains 81% overall. The findings imply that single-score leaderboards are insufficient for characterizing mathematical reasoning ability, necessitating fine-grained analysis for model development and deployment. |
| Natural Language Processing | Parallel Scaling Law: Unveiling Reasoning Generalization through A
  Cross-Linguistic Perspective (Read more on [arXiv](https://arxiv.org/abs/2510.02272) or [HuggingFace](https://huggingface.co/papers/2510.02272))|  | This paper investigates cross-lingual reasoning generalization in large reasoning models (LRMs). It addresses whether reasoning capabilities learned from English reinforcement post-training (RPT) effectively transfer to other languages. The study evaluates English-centric LRMs on multilingual benchmarks, introduces a multilingual transferability index, and conducts parallel training experiments. Results reveal a 'Parallel Scaling Law,' showing cross-lingual reasoning performance scales with the number of training parallel languages and identify a 'Monolingual Generalization Gap'. The study challenges the assumption that LRM reasoning mirrors human cognition and provides insights for developing language-agnostic LRMs. |
| Machine Learning | Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness (Read more on [arXiv](https://arxiv.org/abs/2510.01670) or [HuggingFace](https://huggingface.co/papers/2510.01670))|  | This paper introduces and characterizes Blind Goal-Directedness (BGD) in Computer-Use Agents (CUAs), highlighting their tendency to pursue goals regardless of feasibility, safety, or context. The research aims to systematically evaluate and mitigate BGD in CUAs. The methodology involves creating BLIND-ACT, a benchmark with 90 tasks across three patterns of BGD, and evaluating nine frontier models using LLM-based judges. The primary results show high average BGD rates (80.8%) across models, even with prompting interventions only partially reducing risk. This implies that stronger training- or inference-time interventions are needed to ensure the safe deployment of CUAs, as current models often prioritize execution over safety and reliability. |
| Multi-Modal | ModernVBERT: Towards Smaller Visual Document Retrievers (Read more on [arXiv](https://arxiv.org/abs/2510.01149) or [HuggingFace](https://huggingface.co/papers/2510.01149))|  | This paper introduces ModernVBERT, a compact vision-language encoder for document retrieval, aiming to improve upon the resource intensity of VLM-based approaches. The primary research objective is to determine the optimal design choices for visual document retrievers, particularly attention masking, image resolution, and modality alignment. The methodology involves controlled experiments systematically disentangling these design decisions and their impact on retrieval performance. ModernVBERT, a 250M-parameter model, achieves state-of-the-art results on ViDoRe, outperforming models up to 10 times larger. The implication is that practitioners can achieve efficient document retrieval with smaller, well-tuned models by focusing on late interaction methods and token-level objectives, rather than solely relying on large VLMs. |
| Natural Language Processing | Generalized Parallel Scaling with Interdependent Generations (Read more on [arXiv](https://arxiv.org/abs/2510.01143) or [HuggingFace](https://huggingface.co/papers/2510.01143))| Mrinal Kumar, Yun He, Eryk Helenowski, David Brandfonbrener, Harry Dong | The paper introduces Bridge, a method to improve parallel LLM inference by enabling interdependent generations. It addresses the problem of independent parallel generations by treating batched hidden states as holistic tensors, allowing information sharing between tokens stemming from the same prompt. Bridge improves the relative mean accuracy gains from reinforcement learning with verifiable rewards by up to 50% compared to independent generations while using only 2.8%-5.1% of new parameters. This method unlocks a more general mode of parallel scaling, effectively leveraging information between sequences. The implication for AI practitioners is a more efficient way to generate high-quality and consistent responses without extensive post-training. |
| Reinforcement Learning | RLAD: Training LLMs to Discover Abstractions for Solving Reasoning
  Problems (Read more on [arXiv](https://arxiv.org/abs/2510.02263) or [HuggingFace](https://huggingface.co/papers/2510.02263))| Ruslan Salakhutdinov, Amrith Setlur, Yoonho Lee, Anikait Singh, Yuxiao Qu | This paper introduces RLAD, a two-player reinforcement learning framework for training LLMs to discover and utilize reasoning abstractions for solving complex problems. The research aims to improve reasoning performance by training models to propose concise, natural language abstractions that guide solution generation. The key methodology involves jointly training an abstraction generator and a solution generator, incentivizing the use of provided abstractions during solution generation.  The approach achieves an average 44% improvement over state-of-the-art long chain-of-thought RL approaches on AIME 2025. The main implication is that training models to discover and leverage reasoning abstractions can improve generalization and guide meaningful exploration in solving reasoning problems. |
| Computer Vision | MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment
  Abilities in MLLMs (Read more on [arXiv](https://arxiv.org/abs/2510.01691) or [HuggingFace](https://huggingface.co/papers/2510.01691))| Junzhi Ning, Chenglong Ma, Wanying Qu, Jinjie Wei, Jiyao Liu | The paper introduces MedQ-Bench, a benchmark for evaluating medical image quality assessment abilities in multimodal large language models (MLLMs). It investigates how well MLLMs can perceive and reason about image quality compared to human experts. The methodology involves two tasks: MedQ-Perception for low-level attributes and MedQ-Reasoning for higher-level assessment, using a dataset spanning five modalities and over 40 quality attributes. Results show that state-of-the-art MLLMs achieve only preliminary and unstable perceptual and reasoning skills, with GPT-5 leading at 68.97% accuracy. The main implication is the need for targeted optimization of MLLMs to enhance their performance in medical IQA tasks for reliable clinical use. |
| Machine Learning | TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis (Read more on [arXiv](https://arxiv.org/abs/2510.01538) or [HuggingFace](https://huggingface.co/papers/2510.01538))| Yuting He, Yiwei Xu, Jiaqi Wei, Xiang Zhang, Haokun Zhao | The paper introduces TimeSeriesScientist (TSci), an LLM-driven agentic framework for general time series forecasting. The research aims to minimize human intervention by automating preprocessing, model selection, validation, and ensembling. TSci employs specialized agents for data curation, planning, forecasting, and reporting, augmented by external tools and multimodal diagnostics. Empirical results on eight benchmarks show TSci outperforms both statistical and LLM-based baselines, reducing forecast error by an average of 10.4% and 38.2%, respectively. TSci transforms the forecasting workflow into a more transparent and interpretable system, facilitating wider adoption and practical deployment for AI practitioners. |
| Natural Language Processing | Spectral Scaling Laws in Language Models: How Effectively Do
  Feed-Forward Networks Use Their Latent Space? (Read more on [arXiv](https://arxiv.org/abs/2510.00537) or [HuggingFace](https://huggingface.co/papers/2510.00537))|  | The paper investigates the spectral utilization of Feed-Forward Networks (FFNs) in Large Language Models (LLMs). It explores how effectively FFNs utilize their latent space by analyzing the eigenspectrum of post-activation covariance matrices. The study introduces metrics like Hard Rank and Soft Rank, revealing an asymmetric spectral scaling law where soft rank scales near-linearly with FFN width while hard rank grows sublinearly, indicating a tail-first capacity allocation. The analysis of LLaMA models shows soft rank scaling with R^2 â‰ˆ 0.93 and beta=1.06, but hard rank growing with R^2=0.68 and beta=0.60, suggesting width expansion primarily adds low-energy directions. The findings imply that FFN width selection should balance tail capacity and dominant-mode capacity, offering insights for inference-efficient LLM design and layer-wise resource allocation. |
| Computer Vision | FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame
  Spotlighting (Read more on [arXiv](https://arxiv.org/abs/2509.24304) or [HuggingFace](https://huggingface.co/papers/2509.24304))| Daizong Liu, Siyuan Huang, Yafu Li, Xiaoye Qu, Zefeng He | The paper introduces FrameThinker, a novel framework enabling Large Vision-Language Models (LVLMs) to efficiently reason about long videos through multi-turn frame spotlighting. The research addresses the challenge of inefficient uniform frame sampling in long video reasoning by developing an iterative interrogation approach. FrameThinker employs a two-phase training strategy involving supervised fine-tuning and reinforcement learning to optimize action capabilities. Experiments on benchmarks like LongVideo-Reason demonstrate a 76.1% accuracy using only 20.6 frames, outperforming LongVILA-R1's 72.0% with 512 frames. This approach offers AI practitioners a more efficient and effective method for long video understanding. |
| Machine Learning | AReUReDi: Annealed Rectified Updates for Refining Discrete Flows with
  Multi-Objective Guidance (Read more on [arXiv](https://arxiv.org/abs/2510.00352) or [HuggingFace](https://huggingface.co/papers/2510.00352))| Pranam Chatterjee, Yinuo Zhang, Tong Chen | The paper introduces AReUReDi, a novel algorithm for multi-objective sequence design in therapeutic and biomolecular engineering. It addresses the problem of designing sequences that satisfy multiple conflicting objectives by developing a discrete optimization algorithm with theoretical guarantees of convergence to the Pareto front. AReUReDi combines Tchebycheff scalarization, locally balanced proposals, and annealed Metropolis-Hastings updates within a rectified discrete flow framework. Experiments on peptide and SMILES sequence design demonstrate simultaneous optimization of up to five therapeutic properties, outperforming evolutionary and diffusion-based baselines. The results establish AReUReDi as a powerful sequence-based framework for multi-property biomolecule generation, offering AI practitioners a tool for biomolecular design. |
| Computer Vision | SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking
  for Training-free Zero-Shot Composed Image Retrieval (Read more on [arXiv](https://arxiv.org/abs/2509.26330) or [HuggingFace](https://huggingface.co/papers/2509.26330))| Huei-Fang Yang, Yu-Yen Lin, Ren-Di Wu | This paper introduces SQUARE, a training-free framework for zero-shot composed image retrieval (ZS-CIR). The research aims to improve retrieval accuracy by effectively capturing user intent in multimodal queries. SQUARE uses a Semantic Query-Augmented Fusion (SQAF) stage to enrich queries with MLLM-generated captions and an Efficient Batch Reranking (EBR) stage for joint visual-semantic reasoning across candidate images. Experiments on four standard CIR benchmarks demonstrate strong performance; for instance, SQUARE achieves a Recall@K of 45.04 on CIRR with a ViT-B/32 backbone. The framework's simplicity and effectiveness makes it a practical solution for ZS-CIR, even with lightweight pre-trained models. |
| Natural Language Processing | IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol (Read more on [arXiv](https://arxiv.org/abs/2510.01260) or [HuggingFace](https://huggingface.co/papers/2510.01260))| Yiming Li, Yiyi Lu, Mingchen Ma, Guanliang Lyu, Ningyuan Yang | The paper introduces IoT-MCP, a novel framework bridging Large Language Models (LLMs) and Internet-of-Things (IoT) systems via a Model Context Protocol. It aims to address challenges in hardware heterogeneity and control complexity by providing standardized communication between LLMs and physical devices. The methodology involves edge-deployed servers implementing MCP to bridge LLMs and IoT ecosystems, rigorously evaluated using IoT-MCP Bench, a new benchmark with 114 Basic Tasks and 1,140 Complex Tasks. Results show a 100% task success rate, 205ms average response time, and 74KB peak memory footprint using IoT-MCP. This framework enables standardized evaluation and integration for LLM-IoT systems, facilitating natural language interaction with IoT devices. |
