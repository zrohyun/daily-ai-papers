

## Papers for 2025-10-27

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | DeepAgent: A General Reasoning Agent with Scalable Toolsets (Read more on [arXiv](https://arxiv.org/abs/2510.21618) or [HuggingFace](https://huggingface.co/papers/2510.21618))| Jiajie Jin, Jiarui Jin, Xiaoxi Li, dongguanting, wxjiao | The paper introduces DeepAgent, an end-to-end deep reasoning agent for autonomous task completion through dynamic tool retrieval and execution. It addresses context length limitations in long-horizon interactions using an autonomous memory folding mechanism that compresses past interactions into episodic, working, and tool memories. The agent is trained with ToolPO, a reinforcement learning strategy leveraging LLM-simulated APIs and tool-call advantage attribution. Experimental results on eight benchmarks, including ToolBench and ALFWorld, demonstrate DeepAgent's superior performance compared to baselines in both labeled-tool and open-set tool retrieval scenarios, achieving, for example, 69% success on ToolBench. The work facilitates more general and capable agents for real-world applications by enabling scalable tool use. |
| Computer Vision | Video-As-Prompt: Unified Semantic Control for Video Generation (Read more on [arXiv](https://arxiv.org/abs/2510.20888) or [HuggingFace](https://huggingface.co/papers/2510.20888))|  | The paper introduces Video-As-Prompt (VAP), a unified framework for semantic control in video generation using a reference video as a direct prompt. It aims to overcome limitations of existing methods that introduce artifacts or lack generalizability by using in-context learning. VAP leverages a plug-and-play Mixture-of-Transformers (MOT) expert within a frozen Video Diffusion Transformer (DiT), guided by a temporally biased position embedding for robust context retrieval. The model achieves a 38.7% user preference rate, rivaling leading condition-specific commercial models. VAP's strong zero-shot generalization and support for various downstream applications suggest a significant advancement towards general-purpose, controllable video generation. |
| Multi-Modal | From Denoising to Refining: A Corrective Framework for Vision-Language
  Diffusion Model (Read more on [arXiv](https://arxiv.org/abs/2510.19871) or [HuggingFace](https://huggingface.co/papers/2510.19871))|  | The paper introduces ReDiff, a refining-enhanced diffusion framework to address error cascades in vision-language diffusion models. It aims to mitigate the train-inference discrepancy by reframing generation as active refining rather than passive denoising. The methodology involves a two-stage training process: foundational revision via synthetic errors and online self-correction using expert-revised flawed drafts. Experiments demonstrate that ReDiff improves coherence and factual accuracy, achieving a CLAIR score of 76.74 on CapMAS. ReDiff offers AI practitioners a more stable and efficient parallel generation approach by breaking the error cascade. |
| Computer Vision | Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image
  Generation (Read more on [arXiv](https://arxiv.org/abs/2510.21583) or [HuggingFace](https://huggingface.co/papers/2510.21583))|  | The paper introduces Chunk-GRPO, a novel chunk-level reinforcement learning approach for text-to-image generation that optimizes groups of consecutive timesteps. The research aims to address limitations in GRPO, specifically inaccurate advantage attribution and neglecting temporal dynamics. The method groups timesteps into chunks reflecting temporal dynamics, optimizing them as units with a chunk-level importance ratio, and introduces an optional weighted sampling strategy. Experiments demonstrate Chunk-GRPO achieves superior results in preference alignment, increasing gains up to 23% over baselines, and outperforms on standard T2I benchmarks like WISE. The method highlights the potential of chunk-level optimization for GRPO-based text-to-image generation. |
| Natural Language Processing | Sparser Block-Sparse Attention via Token Permutation (Read more on [arXiv](https://arxiv.org/abs/2510.21270) or [HuggingFace](https://huggingface.co/papers/2510.21270))|  | This paper introduces Permuted Block-Sparse Attention (PBS-Attn) to enhance the computational efficiency of LLM prefilling by leveraging token permutation. The research aims to improve block-level sparsity in self-attention mechanisms for long sequences. PBS-Attn reorganizes query and key sequences to improve block-level sparsity, using a segmented permutation strategy and custom permuted-FlashAttention kernels. Experiments on long-context datasets demonstrate that PBS-Attn outperforms existing block-sparse attention methods and achieves up to 2.75x speedup in prefilling while closely matching full attention accuracy. The method offers AI practitioners a plug-and-play approach to accelerate LLM prefilling with minimal performance degradation. |
| Multi-Modal | UI-Ins: Enhancing GUI Grounding with Multi-Perspective
  Instruction-as-Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.20286) or [HuggingFace](https://huggingface.co/papers/2510.20286))|  | This paper introduces UI-Ins, a novel approach to enhance GUI grounding by treating instructions as dynamic reasoning pathways. The research aims to address the overlooked impact of instruction diversity and quality on grounding performance. The key methodology involves a two-stage training framework: supervised fine-tuning on diverse instructions followed by reinforcement learning to optimize pathway selection. UI-Ins achieves state-of-the-art results, with UI-Ins-32B scoring 87.3% on UI-I2E-Bench, demonstrating improved grounding accuracy. This work implies that leveraging instruction diversity and quality significantly enhances GUI agent capabilities, thus providing a more effective and adaptive approach to GUI grounding for AI practitioners. |
| Other | A Definition of AGI (Read more on [arXiv](https://arxiv.org/abs/2510.18212) or [HuggingFace](https://huggingface.co/papers/2510.18212))| Yarin Gal, Honglak Lee, Christian Szegedy, Dawn Song, Dan Hendrycks | This paper introduces a quantifiable framework for defining and evaluating Artificial General Intelligence (AGI). It seeks to bridge the gap between specialized AI and human-level cognition by defining AGI as matching the cognitive versatility and proficiency of a well-educated adult. The framework adapts established human psychometric batteries grounded in Cattell-Horn-Carroll (CHC) theory to evaluate AI systems across ten core cognitive domains. Applying this framework reveals a "jagged" cognitive profile in current AI, with GPT-4 achieving 27% and GPT-5 57% AGI scores. The results quantify rapid progress but highlight substantial gaps in foundational cognitive machinery, especially long-term memory storage, emphasizing the need for AI practitioners to address these deficits. |
| Machine Learning | Reasoning with Sampling: Your Base Model is Smarter Than You Think (Read more on [arXiv](https://arxiv.org/abs/2510.14901) or [HuggingFace](https://huggingface.co/papers/2510.14901))|  | The paper explores improving reasoning capabilities of large language models (LLMs) without additional training by leveraging base model likelihoods. It introduces an iterative sampling algorithm inspired by MCMC to elicit comparable reasoning abilities to reinforcement learning (RL) methods. The algorithm demonstrates boosts in reasoning on tasks like MATH500, HumanEval, and GPQA, nearly matching or outperforming RL, but without any explicit training. Results show performance gains (e.g., improvements in MATH500). The study suggests that existing base models may be more capable at single-shot reasoning tasks than previously recognized, offering a training-free alternative to RL-posttraining. |
| Natural Language Processing | RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via
  Hierarchical Model Merging (Read more on [arXiv](https://arxiv.org/abs/2510.20479) or [HuggingFace](https://huggingface.co/papers/2510.20479))|  | The paper introduces RECALL, a representation-aware model merging framework for continual learning to alleviate catastrophic forgetting in LLMs. RECALL aims to preserve task knowledge in a data-free, task-agnostic way by analyzing inter-model similarities based on intermediate representations over clustered typical samples and performing adaptive hierarchical parameter fusion. Experiments across five NLP tasks demonstrate that RECALL outperforms baselines in knowledge retention and generalization, achieving up to 45.00 average performance. The main implication is that RECALL offers a scalable and data-free solution for evolving LLMs, enabling seamless multi-domain knowledge fusion and resistance to catastrophic forgetting. The method enhances LLM's abilities in multiple domains without requiring access to historical data. |
| Computer Vision | Visual Diffusion Models are Geometric Solvers (Read more on [arXiv](https://arxiv.org/abs/2510.21697) or [HuggingFace](https://huggingface.co/papers/2510.21697))| Or Patashnik, Andrey Voynov, Omer Dahary, Shai Yehezkel, Nir Goren | This paper introduces a novel application of visual diffusion models as solvers for hard geometric problems. The research aims to demonstrate that visual diffusion models can effectively reason about and discover geometric structures directly in pixel space. The methodology involves training a standard visual diffusion model on images representing valid solutions to geometric problems like the Inscribed Square, Steiner Tree, and Simple Polygon problems. The models achieved strong results, for example, producing inscribed squares with a high squareness metric of 0.891 after snapping, demonstrating accurate approximations of valid inscribed squares. The implication is that standard visual diffusion models can bridge the gap between generative modeling and geometric problem solving by operating directly on visual representations of problems. |
| Computer Vision | WorldGrow: Generating Infinite 3D World (Read more on [arXiv](https://arxiv.org/abs/2510.21682) or [HuggingFace](https://huggingface.co/papers/2510.21682))| Jia Lu, Taoran Yi, Chen Yang, Sikuang Li, JieminFang | The paper introduces WorldGrow, a novel framework for generating infinitely extendable 3D worlds. It addresses the challenge of creating large, continuous 3D environments by leveraging pre-trained 3D models for structured scene block generation. The method employs a data curation pipeline, 3D block inpainting, and a coarse-to-fine generation strategy to ensure coherence and detail.  Evaluated on the large-scale 3D-FRONT dataset, WorldGrow achieves state-of-the-art geometry reconstruction, specifically superior connectivity with SOTA performance across all geometry metrics (Table 1).  WorldGrow enables AI practitioners to construct large-scale virtual environments for embodied AI and simulation. |
| Computer Vision | RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via
  Data Alignment and Test-Time Scaling (Read more on [arXiv](https://arxiv.org/abs/2510.20206) or [HuggingFace](https://huggingface.co/papers/2510.20206))|  | The paper introduces RAPO++, a cross-stage prompt optimization framework for text-to-video (T2V) generation. It aims to improve T2V generation by addressing the challenges of short, unstructured, and misaligned user prompts. RAPO++ unifies training-data-aligned refinement, test-time iterative scaling, and LLM fine-tuning to optimize prompts without altering the underlying generative backbone. Experiments across five T2V models demonstrate significant gains in semantic alignment, compositional reasoning, and temporal stability; RAPO++ achieves a total score of 82.65% on VBench. This framework offers AI practitioners a model-agnostic, cost-efficient, and scalable solution for enhancing T2V generation. |
| Computer Vision | Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs (Read more on [arXiv](https://arxiv.org/abs/2510.13251) or [HuggingFace](https://huggingface.co/papers/2510.13251))| Bohyung Han, taekyung-k, byminji | This paper investigates the internal information flow of Video Large Language Models (VideoLLMs). It aims to understand how VideoLLMs extract and propagate video and textual information for VideoQA tasks using mechanistic interpretability techniques. The study reveals that temporal reasoning begins with cross-frame interactions in early layers, followed by video-language integration in middle layers, enabling answer generation. Attention Knockout analysis shows a retention of VideoQA performance by selecting effective information pathways, suppressing up to 58% of attention edges in LLaVA-NeXT-7B-Video-FT. This provides insights for improving model interpretability and downstream generalization for VideoLLMs. |
| Machine Learning | Model Merging with Functional Dual Anchors (Read more on [arXiv](https://arxiv.org/abs/2510.21223) or [HuggingFace](https://huggingface.co/papers/2510.21223))|  | The paper introduces Functional Dual Anchors (FDAs) for model merging, integrating knowledge from multiple fine-tuned checkpoints. It aims to address parameter inconsistencies in existing parameter-space merging methods by modeling the input-representation space. The key methodology involves generating synthetic inputs whose induced gradients align with task vectors, effectively capturing task-specific functional shifts relative to the pretrained model. Experiments demonstrate that FDAs are complementary to parameter-space model merging, leading to improvements in multi-task performance across various benchmarks (e.g., achieving 87.26 on a vision benchmark). The use of FDAs offers AI practitioners a robust and flexible approach to model merging by focusing on the input space rather than directly manipulating parameters. |
| Natural Language Processing | Document Understanding, Measurement, and Manipulation Using Category
  Theory (Read more on [arXiv](https://arxiv.org/abs/2510.21553) or [HuggingFace](https://huggingface.co/papers/2510.21553))|  | This paper explores document understanding, measurement, and manipulation using category theory. It aims to extract multimodal document structure and develop information-theoretic measures for content summarization and extension. The methodology involves representing a document as a category of question-answer pairs and applying orthogonalization procedures. The authors propose a self-supervised method using RLVR to improve large pretrained models with consistency constraints, though quantitative metrics are not specified. The work implies potential advancements in information retrieval, document understanding, and large language model improvement by leveraging category theory for structured semantic analysis. |
| Computer Vision | PhysWorld: From Real Videos to World Models of Deformable Objects via
  Physics-Aware Demonstration Synthesis (Read more on [arXiv](https://arxiv.org/abs/2510.21447) or [HuggingFace](https://huggingface.co/papers/2510.21447))| Hui Li, Yihan Zeng, Xiang Zhang, Yu Yang, cszhilu1998 | The paper introduces PhysWorld, a framework for building accurate and efficient world models of deformable objects from real-world videos. It addresses the challenge of data scarcity by synthesizing physically plausible demonstrations using a physics-consistent MPM simulator and a global-to-local physical property optimization. The framework then trains a GNN-based world model using these synthetic demonstrations and refines physical properties using real-world videos. PhysWorld achieves competitive performance on future prediction tasks, enabling inference speeds 47 times faster than a state-of-the-art method. This suggests an effective approach to combining simulators and lightweight models for applications in robotics, VR, and AR. |
| Natural Language Processing | Are Large Reasoning Models Good Translation Evaluators? Analysis and
  Performance Boost (Read more on [arXiv](https://arxiv.org/abs/2510.20780) or [HuggingFace](https://huggingface.co/papers/2510.20780))| Min Yang, Lidia S. Chao, Xinyi Yang, Zhihong Huang, rzzhan | This paper investigates the potential of Large Reasoning Models (LRMs) as evaluators for machine translation (MT) quality. It addresses the underexplored application of LRMs in MT evaluation, identifying challenges such as the need for tailored evaluation materials and overestimation issues. The authors propose calibrating LRMs by training them on synthetic, human-like thinking trajectories, leading to a ~35x reduction in thinking budget and up to +8.7 correlation point improvement (R1-Distill-Qwen-7B). Calibrated LRMs offer an efficient approach to advance fine-grained automatic MT evaluation by improved evaluation performance and efficiency. The main implication is that carefully calibrated LRMs can substantially improve automatic MT evaluation while controlling for computational costs. |
| Natural Language Processing | ARC-Encoder: learning compressed text representations for large language
  models (Read more on [arXiv](https://arxiv.org/abs/2510.20535) or [HuggingFace](https://huggingface.co/papers/2510.20535))|  | The paper introduces ARC-Encoder, a method for compressing text representations in LLMs to reduce inference costs without modifying the decoder. It addresses the quadratic complexity of Transformer attention mechanisms by learning compressed, continuous representations that replace token embeddings. The key methodology involves training an encoder with a pooling mechanism and an MLP projector to reduce the number of representations, while preserving few-shot abilities. Results show that ARC-Encoder achieves state-of-the-art performance on benchmarks, improving computational efficiency, with a reported 45.5% average exact match score on various tasks with a ×4 compression factor. This enables more efficient portable encoders that seamlessly work with multiple LLMs. |
| Multi-Modal | Taming Modality Entanglement in Continual Audio-Visual Segmentation (Read more on [arXiv](https://arxiv.org/abs/2510.17234) or [HuggingFace](https://huggingface.co/papers/2510.17234))| Zhaojin Fu, Zili Wang, Tao Zhang, Qi Yang, hongyuyang23casia | The paper addresses continual learning in audio-visual segmentation (CAVS) to mitigate modality entanglement. It aims to overcome multi-modal semantic drift and co-occurrence confusion in fine-grained continual learning. The proposed Collision-based Multi-modal Rehearsal (CMR) framework uses Multi-modal Sample Selection (MSS) and Collision-based Sample Rehearsal (CSR) to address these challenges. Experiments on AVSBench datasets demonstrate improved mIoU compared to single-modal continual learning methods. The work enables more effective continual learning in multi-modal scenarios by addressing modality entanglement issues. |
| Machine Learning | AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research
  Suite (Read more on [arXiv](https://arxiv.org/abs/2510.21652) or [HuggingFace](https://huggingface.co/papers/2510.21652))| Bhavana Dalvi, Dan Bareket, Nishant Balepur, Mike D'Arcy, Jonathan Bragg | The paper introduces AstaBench, a benchmark suite for rigorously evaluating AI agents in scientific research. AstaBench addresses limitations of existing benchmarks by providing a holistic measure of agentic ability, reproducible agent tools, and accounting for confounders. The suite comprises 2400+ problems across multiple scientific domains and includes nine science-optimized agent classes. Evaluation of 57 agents reveals that AI remains far from solving the challenge of science research assistance. AstaBench offers a clear path to enable continuous improvement in AI for scientific research. |
| Multi-Modal | Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video (Read more on [arXiv](https://arxiv.org/abs/2510.21581) or [HuggingFace](https://huggingface.co/papers/2510.21581))|  | Foley Control is a lightweight framework for video-guided Foley sound generation using a frozen text-to-audio (T2A) model. The research aims to achieve accurate temporal and semantic alignment between video and audio by learning a small cross-attention bridge between a V-JEPA2 video encoder and a Stable Audio Open DiT. The methodology involves inserting compact video cross-attention modules after the model's text cross-attention within the transformer blocks. Evaluation on video-audio benchmarks demonstrates competitive alignment with fewer trainable parameters compared to recent multimodal systems. Foley Control offers a practical and modular approach to Foley generation, enabling control and upgradability of components without retraining the entire model. |
| Natural Language Processing | Soft Instruction De-escalation Defense (Read more on [arXiv](https://arxiv.org/abs/2510.21057) or [HuggingFace](https://huggingface.co/papers/2510.21057))|  | The paper introduces Soft Instruction Control (SIC), a prompt sanitization loop to defend against prompt injection attacks in tool-augmented LLM agents. SIC iteratively rewrites potentially malicious instructions within untrusted data, checking for residual instructions after each pass. The methodology involves masking, rephrasing, or removing instructions based on predefined prompts and utilizes multiple passes to increase robustness. Experiments on the AgentDojo benchmark show SIC achieves a 0% attack success rate with minimal utility degradation across various models, outperforming baselines such as RUP and SWD. The method improves the security of LLM agents interacting with external data sources by making prompt injection attacks significantly more difficult. |
| Multi-Modal | PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language
  Models in Physical Environments (Read more on [arXiv](https://arxiv.org/abs/2510.21111) or [HuggingFace](https://huggingface.co/papers/2510.21111))| Chaoyang Zhao, Manli Tao, Yi Peng, Xuantang Xiong, JettZhou | The paper introduces Active Visual Reasoning (AVR), extending visual reasoning to partially observable, interactive environments. It aims to address the limitations of existing multimodal large language models (MLLMs) in real-world environments with incomplete information. The paper proposes CLEVR-AVR, a simulation benchmark, and AVR-152k, a large-scale dataset with Chain-of-Thought annotations, and PhysVLM-AVR, an MLLM achieving state-of-the-art performance on CLEVR-AVR. PhysVLM-AVR achieves 90.6% Information Sufficiency Judgment Accuracy on CLEVR-AVR. The study reveals a gap in active reasoning capabilities, highlighting that existing embodied MLLMs struggle to actively acquire and integrate new information through interaction. |
| Reinforcement Learning | Stabilizing MoE Reinforcement Learning by Aligning Training and
  Inference Routers (Read more on [arXiv](https://arxiv.org/abs/2510.11370) or [HuggingFace](https://huggingface.co/papers/2510.11370))|  | This paper addresses the instability issues in reinforcement learning (RL) with Mixture-of-Experts (MoE) models. The research identifies discrepancies in routing behaviors between training and inference phases, leading to catastrophic RL training collapse. To mitigate this, the authors propose Rollout Routing Replay (R3), a method that replays inference-time routing distributions during training. R3 significantly reduces training-inference policy KL divergence and stabilizes RL training without compromising speed. Extensive experiments demonstrate R3's effectiveness, outperforming existing methods such as GSPO and TIS in various settings. |
