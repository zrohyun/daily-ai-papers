

## Papers for 2025-10-29

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | InteractComp: Evaluating Search Agents With Ambiguous Queries (Read more on [arXiv](https://arxiv.org/abs/2510.24668) or [HuggingFace](https://huggingface.co/papers/2510.24668))| Fashen Ren, Jiayi Zhang, Yani Fan, Lijun Huang, Mingyi Deng | The paper introduces INTERACTCOMP, a benchmark for evaluating search agents' ability to resolve ambiguous queries through interaction. It investigates whether search agents can recognize query ambiguity and actively interact to gather disambiguating information during search. The methodology involves expert-curated questions and simulated user interaction to test agents' performance in ambiguous scenarios. Evaluation of 17 models reveals a striking failure, with the best model achieving only 13.73% accuracy in full interaction despite 71.50% with complete context. This implies that current search agents are overconfident and lack effective interaction mechanisms, highlighting a critical blind spot in agent development. |
| Natural Language Processing | Tongyi DeepResearch Technical Report (Read more on [arXiv](https://arxiv.org/abs/2510.24701) or [HuggingFace](https://huggingface.co/papers/2510.24701))|  | The paper introduces Tongyi DeepResearch, an agentic large language model designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous research, the model is developed through an end-to-end training framework combining agentic mid-training and post-training with a highly scalable automatic data synthesis pipeline. Tongyi DeepResearch, featuring 30.5 billion parameters, achieves state-of-the-art performance across agentic deep research benchmarks, including Humanity's Last Exam (32.9) and BrowseComp (43.4). The authors open-source the model and framework to empower the community, potentially accelerating research in autonomous agentic systems. |
| Natural Language Processing | AgentFold: Long-Horizon Web Agents with Proactive Context Management (Read more on [arXiv](https://arxiv.org/abs/2510.24699) or [HuggingFace](https://huggingface.co/papers/2510.24699))|  | AgentFold is a novel agent paradigm for long-horizon web tasks centered on proactive context management to mitigate context saturation and irreversible information loss. It addresses the trade-off in context management by learning a ‘folding’ operation that manages historical trajectories at multiple scales. The key methodology involves learning granular condensations to preserve fine-grained details or deep consolidations to abstract away multi-step sub-tasks. AgentFold-30B-A3B achieves 36.2% on BrowseComp and 47.3% on BrowseComp-ZH with supervised fine-tuning. This indicates the potential for improved performance on long-horizon tasks via proactive context management without continual pre-training or reinforcement learning. |
| Multi-Modal | RoboOmni: Proactive Robot Manipulation in Omni-modal Context (Read more on [arXiv](https://arxiv.org/abs/2510.23763) or [HuggingFace](https://huggingface.co/papers/2510.23763))|  | The paper introduces RoboOmni, a framework for proactive robot manipulation using omni-modal context, integrating speech, environmental sounds, and vision. The primary objective is to enable robots to proactively infer user intent from multimodal cues rather than relying on explicit instructions. RoboOmni employs an end-to-end omni-modal LLM architecture for intention recognition, interaction confirmation, and action execution, trained on a newly created dataset called OmniAction comprising 140k episodes. In simulations, RoboOmni achieves substantially higher accuracy compared to text- and ASR-based baselines, reaching up to 85.6% overall success rate. This suggests that integrating diverse sensory inputs significantly improves a robot's ability to assist users effectively and proactively. |
| Computer Vision | Uniform Discrete Diffusion with Metric Path for Video Generation (Read more on [arXiv](https://arxiv.org/abs/2510.24717) or [HuggingFace](https://huggingface.co/papers/2510.24717))|  | The paper introduces Uniform discRete diffusion with metric path (URSA), a novel framework for scalable video generation using discrete generative modeling. URSA aims to bridge the performance gap between discrete and continuous video generation approaches by formulating video generation as an iterative global refinement of discrete spatiotemporal tokens. The method integrates a Linearized Metric Path and a Resolution-dependent Timestep Shifting mechanism for efficient high-resolution and long-duration video synthesis. Experiments show URSA achieves a text-to-video score of 82.4 on VBench, surpassing existing discrete methods. URSA provides AI practitioners with a performant discrete video generation approach, comparable to continuous methods while requiring fewer inference steps. |
| Natural Language Processing | Repurposing Synthetic Data for Fine-grained Search Agent Supervision (Read more on [arXiv](https://arxiv.org/abs/2510.24694) or [HuggingFace](https://huggingface.co/papers/2510.24694))|  | This paper introduces Entity-aware Group Relative Policy Optimization (E-GRPO) to improve LLM-based search agent training. It addresses the limitation of standard GRPO by leveraging entity information to formulate a dense reward function that assigns partial rewards proportional to the entity match rate. The key methodology involves incorporating entity matching scores into the reward function to guide policy optimization using synthetic data. Results show that E-GRPO consistently outperforms the GRPO baseline on question-answering and deep research benchmarks (e.g., achieving a higher Pass@1 score), while also inducing more efficient reasoning policies. E-GRPO offers a more effective and sample-efficient approach for aligning search agents by enabling models to learn from informative "near-miss" samples. |
| Computer Vision | OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents (Read more on [arXiv](https://arxiv.org/abs/2510.24563) or [HuggingFace](https://huggingface.co/papers/2510.24563))|  | This paper introduces OSWorld-MCP, a benchmark for evaluating computer-use agents' tool invocation, GUI operation, and decision-making abilities in real-world environments. The research aims to provide a comprehensive and fair evaluation framework that addresses the lack of consistency in tool sets and evaluation metrics of current benchmarks. They design an automated code-generation pipeline to create high-quality MCP tools and evaluate them on state-of-the-art multimodal agents. Results demonstrate that MCP tools generally improve task success rates (e.g., OpenAI 03 increases from 8.3% to 20.4% at 15 steps) while tool invocation rates remain relatively low (36.3% for Claude 4 Sonnet), indicating potential for improvement.  OSWorld-MCP serves as a new standard for evaluating multimodal agents in complex, tool-assisted environments, enabling more nuanced assessment and development. |
| Natural Language Processing | WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling
  Info-Rich Seeking (Read more on [arXiv](https://arxiv.org/abs/2510.24697) or [HuggingFace](https://huggingface.co/papers/2510.24697))|  | The paper introduces WebLeaper, a framework to enhance the efficiency and efficacy of LLM-based web agents. It addresses the problem of low search efficiency by constructing high-coverage information seeking tasks and generating efficient solution trajectories via a tree-structured reasoning approach using curated Wikipedia tables. Experiments on benchmarks like GAIA and BrowserComp demonstrate improvements; for example, the framework attains 73.2 accuracy on GAIA in comprehensive training settings. The implication for AI practitioners is a more efficient and effective approach to web-based information seeking for LLM agents, which benefits from the proposed task construction and training methodology. |
| Computer Vision | Group Relative Attention Guidance for Image Editing (Read more on [arXiv](https://arxiv.org/abs/2510.24657) or [HuggingFace](https://huggingface.co/papers/2510.24657))|  | This paper introduces Group Relative Attention Guidance (GRAG) for improved control in diffusion-based image editing. The research addresses the challenge of balancing instruction adherence and image fidelity by analyzing and modulating the attention mechanism in Diffusion-in-Transformers (DiT) models. GRAG reweights token deviations from a layer-dependent bias vector to control editing intensity, achieving a smoother and more precise effect than Classifier-Free Guidance. Experiments on image editing tasks show enhanced editing quality, with Step1X-Edit exhibiting improved consistency (Cons) rising from 8.4714 to 8.6240 using GRAG. GRAG offers a simple yet effective method for AI practitioners to enhance the controllability and quality of image editing applications based on DiT models. |
| Computer Vision | Routing Matters in MoE: Scaling Diffusion Transformers with Explicit
  Routing Guidance (Read more on [arXiv](https://arxiv.org/abs/2510.24711) or [HuggingFace](https://huggingface.co/papers/2510.24711))|  | This paper introduces ProMoE, an MoE framework for scaling Diffusion Transformers (DiTs) by addressing the limitations of applying MoE to vision tasks. It tackles spatial redundancy and functional heterogeneity in visual tokens via a two-step router with explicit routing guidance. The approach partitions tokens into conditional and unconditional sets before refining assignments via prototypical routing with learnable prototypes and a routing contrastive loss. Experiments on ImageNet demonstrate ProMoE surpasses state-of-the-art methods, achieving FID50K of 2.59 on ProMoE-XL-Flow with CFG=1.5.  ProMoE offers AI practitioners an efficient approach to scaling DiTs while improving image generation quality. |
| Natural Language Processing | ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking (Read more on [arXiv](https://arxiv.org/abs/2510.24698) or [HuggingFace](https://huggingface.co/papers/2510.24698))|  | The paper introduces ParallelMuse, a two-stage paradigm to improve parallel thinking in deep information-seeking agents. It aims to enhance exploration breadth and answer generation quality by addressing inefficiency and context capacity limitations. The methodology involves Functionality-Specified Partial Rollout, partitioning sequences for uncertainty-guided path reuse, and Compressed Reasoning Aggregation, which compresses reasoning to derive coherent answers. Experiments demonstrate up to 62% performance improvement with a 10-30% reduction in token consumption. The main implication is a computationally efficient approach to scaling deep information-seeking agents through parallel thinking. |
| Natural Language Processing | AgentFrontier: Expanding the Capability Frontier of LLM Agents with
  ZPD-Guided Data Synthesis (Read more on [arXiv](https://arxiv.org/abs/2510.24695) or [HuggingFace](https://huggingface.co/papers/2510.24695))|  | The paper introduces AgentFrontier, a data synthesis approach inspired by the Zone of Proximal Development (ZPD) to train large language model agents. It addresses the challenge of expanding LLM capabilities by creating high-quality, multidisciplinary data within the model's ZPD. AgentFrontier Engine, an automated pipeline, synthesizes data for both continued pre-training and targeted post-training. Results show that AgentFrontier-30B-A3B achieves state-of-the-art performance on benchmarks like Humanity's Last Exam, surpassing some proprietary agents and achieving 28.6% on HLE. The work demonstrates ZPD-guided data synthesis as an effective method for building more capable LLM agents. |
| Multi-Modal | Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal
  Reasoning in MLLMs (Read more on [arXiv](https://arxiv.org/abs/2510.24514) or [HuggingFace](https://huggingface.co/papers/2510.24514))|  | The paper introduces Latent Sketchpad, a framework that equips MLLMs with an internal visual sketchpad to enhance reasoning. It addresses the research question of how to leverage pretrained visual features in MLLMs for generative visual thought. The methodology involves integrating visual generation directly into the MLLM's autoregressive reasoning process via a Context-Aware Vision Head and a Sketch Decoder. Experiments on MAZEPLANNING show that Latent Sketchpad delivers comparable or superior reasoning performance with improvements up to +3.80% in success rate over baselines. This extends model's textual reasoning to visual thinking, opening new opportunities for richer human-computer interaction. |
| Machine Learning | VisCoder2: Building Multi-Language Visualization Coding Agents (Read more on [arXiv](https://arxiv.org/abs/2510.23642) or [HuggingFace](https://huggingface.co/papers/2510.23642))|  | The paper introduces VisCoder2, a family of multi-language visualization coding agents designed to generate, execute, and self-debug visualization code across 12 programming languages. VisCoder2 aims to improve upon existing models that suffer from limited language coverage and unreliable execution. The methodology involves a large-scale supervised dataset (VisCode-Multi-679K) and a benchmark (VisPlotBench) for systematic evaluation. Experiments show that VisCoder2 significantly outperforms open-source baselines, achieving an 82.4% execution pass rate at the 32B scale through iterative self-debugging. This suggests that multi-language coverage and iterative refinement are critical for building robust visualization coding agents, making them more practical for AI practitioners. |
| Multi-Modal | From Spatial to Actions: Grounding Vision-Language-Action Model in
  Spatial Foundation Priors (Read more on [arXiv](https://arxiv.org/abs/2510.17439) or [HuggingFace](https://huggingface.co/papers/2510.17439))|  | This paper introduces FALCON, a novel vision-language-action (VLA) model designed for improved spatial reasoning in robotic tasks. The research aims to bridge the spatial reasoning gap in existing VLAs by incorporating 3D spatial priors without compromising modality transferability or vision-language alignment. FALCON leverages spatial foundation models to inject rich 3D spatial tokens into the action head and includes an Embodied Spatial Model for optional multi-modal input fusion. Evaluations across simulation benchmarks and real-world tasks show FALCON achieves state-of-the-art performance, consistently surpassing competitive baselines and exhibiting robustness to variations in object scale and height; for example, achieving a 70% success rate in real-world experiments. FALCON's design offers AI practitioners a robust framework for developing generalist robotic policies capable of complex spatial reasoning and manipulation. |
| Natural Language Processing | FunReason-MT Technical Report: Overcoming the Complexity Barrier in
  Multi-Turn Function Calling (Read more on [arXiv](https://arxiv.org/abs/2510.24645) or [HuggingFace](https://huggingface.co/papers/2510.24645))|  | This paper presents FunReason-MT, a novel data synthesis framework designed to improve multi-turn function calling capabilities in large language models. The research aims to overcome limitations in existing data generation methods by creating high-quality, logically consistent training data for complex tool use scenarios. The framework incorporates Environment-API Graph Interactions, Advanced Tool-Query Synthesis, and a Guided Iterative Chain to generate targeted training data. Evaluation on the BFCLv3 benchmark demonstrates that a 4B model trained on FunReason-MT data achieves state-of-the-art performance among comparable-sized models, improving multi-turn accuracy to 56.5%. FunReason-MT offers AI practitioners a reliable source for agentic learning, particularly in environments requiring complex reasoning and tool use. |
| Natural Language Processing | ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers? (Read more on [arXiv](https://arxiv.org/abs/2510.24591) or [HuggingFace](https://huggingface.co/papers/2510.24591))| Ian L. V. Roque, Steven Dillmann, Suchetha Cooray, Sihan Yuan, Christine Ye | ReplicationBench is introduced as an evaluation framework to test the ability of AI agents to replicate astrophysics research papers, assessing their faithfulness and correctness. The main research question is whether AI agents can replicate entire research papers from the astrophysics literature, requiring replication of experimental setup, derivations, data analysis, and codebase. The methodology involves splitting papers into tasks co-developed with original paper authors, targeting key scientific results for objective evaluation. Frontier language models achieved scores under 20% on ReplicationBench, revealing diverse failure modes in scientific research. ReplicationBench provides a benchmark for paper-scale astrophysics research tasks and insights into agent performance applicable to data-driven science. |
| Computer Vision | Rethinking Visual Intelligence: Insights from Video Pretraining (Read more on [arXiv](https://arxiv.org/abs/2510.24448) or [HuggingFace](https://huggingface.co/papers/2510.24448))| Ahmad Rahimi, Sebastian Stapf, Mariam Hassan, Aram Davtyan, Pablo Acuaviva | The paper explores video pretraining using Video Diffusion Models (VDMs) as a foundation for visual intelligence. It aims to determine if VDMs pre-trained on spatiotemporal data exhibit strong inductive biases that support broad task adaptability. The methodology involves pretraining a VDM and an LLM and evaluating them on tasks including ARC-AGI, ConceptARC, and visual games using lightweight adapters. Results demonstrate that VDMs exhibit higher data efficiency compared to LLMs across these benchmarks. This suggests that video pretraining offers inductive biases that facilitate progress toward visual foundation models with both generative and problem-solving strengths. |
| Natural Language Processing | Generalization or Memorization: Dynamic Decoding for Mode Steering (Read more on [arXiv](https://arxiv.org/abs/2510.22099) or [HuggingFace](https://huggingface.co/papers/2510.22099))|  | The paper introduces a framework for understanding and controlling generalization versus memorization in Large Language Models (LLMs). It aims to distinguish and manage these reasoning modes to improve LLM reliability. The methodology involves a lightweight probe to identify memorization reliance and a dynamic activation steering mechanism to nudge computation towards generalization circuits, framing it as adaptive self-contrastive decoding. Experiments show that Dynamic Mode Steering (DMS) improves logical consistency and factual accuracy, achieving up to a 6.2% increase in Pass@1 accuracy on GSM8K for Llama-3 8B. The study provides a principled approach to enhance LLM reliability by dynamically managing reasoning modes at inference time. |
| Multi-Modal | VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations (Read more on [arXiv](https://arxiv.org/abs/2510.22373) or [HuggingFace](https://huggingface.co/papers/2510.22373))| Jiayi Zhang, Sirong Lu, Yifan Wu, Zhiyang Zhang, Yupeng Xie | The paper introduces VISJUDGE-BENCH, the first comprehensive benchmark for evaluating multimodal large language models (MLLMs) in assessing visualization aesthetics and quality. It addresses the challenge that MLLMs, while promising in natural image aesthetics, lack systematic evaluation in visualization, which requires simultaneous judgment across data encoding, expressiveness, and visual aesthetics. The benchmark contains 3,090 expert-annotated samples from real-world scenarios and 32 chart types, revealing significant gaps in advanced MLLMs (e.g., GPT-5) compared to human experts, with an MAE of 0.551 and a correlation of 0.429. The paper proposes VISJUDGE, a specialized model that narrows this gap, reducing MAE to 0.442 (19.8% reduction) and increasing correlation to 0.681 (58.7% improvement), demonstrating the necessity for domain-specific fine-tuning of MLLMs for visualization assessment. |
| Multi-Modal | VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a
  Unified Concept Set (Read more on [arXiv](https://arxiv.org/abs/2510.21323) or [HuggingFace](https://huggingface.co/papers/2510.21323))|  | The paper introduces VL-SAE, a sparse autoencoder to improve the interpretability and alignment of vision-language models (VLMs) by mapping multi-modal representations to a unified concept set. It addresses the challenge of interpreting VL alignment by encoding vision and language representations into hidden activations correlated with semantically similar image and text concepts. The methodology involves a distance-based encoder and modality-specific decoders to ensure activation consistency, explicitly aligning representations using cosine similarity. Experiments on models like CLIP and LLaVA demonstrate improved interpretability and enhancement in tasks like zero-shot image classification (e.g., improved zero-shot classification by 0.3% on ViT-H/14). VL-SAE provides a tool for understanding and strengthening vision-language alignment at a conceptual level, enabling better performance in downstream applications. |
| Computer Vision | PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D
  Part Understanding (Read more on [arXiv](https://arxiv.org/abs/2510.20155) or [HuggingFace](https://huggingface.co/papers/2510.20155))| Lan Xu, Yukai Zhou, Xin Lv, Yiyang He, Penghao Wang | PartNeXt introduces a new dataset for fine-grained and hierarchical 3D part understanding to address limitations in existing datasets like PartNet. The paper aims to improve scalability and usability by providing textured 3D models and a web-based annotation interface. The dataset, containing 23,519 high-quality, textured 3D models across 50 object categories, is annotated with fine-grained, hierarchical part labels. Benchmarking on class-agnostic part segmentation reveals that state-of-the-art methods perform noticeably worse on PartNeXt, especially for leaf-level parts; training Point-SAM on PartNeXt yields substantial gains over PartNet. PartNeXt enables more comprehensive part-level 3D understanding and reasoning. |
| Natural Language Processing | PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text
  Embedding (Read more on [arXiv](https://arxiv.org/abs/2510.22264) or [HuggingFace](https://huggingface.co/papers/2510.22264))| Denis Cavallucci, Iliass Ayaou | The paper introduces PatenTEB, a comprehensive benchmark and model family for patent text embedding. It addresses the research question of creating a comprehensive evaluation framework for patent text understanding and developing models optimized for both benchmark performance and real-world generalization. The methodology involves constructing a 15-task benchmark and developing the patembed model family, employing multi-task training and distillation. PatenTEB achieves state-of-the-art performance on external benchmarks, including 0.494 V-measure on MTEB BigPatentClustering.v2. The patembed model family provides AI practitioners with a suite of patent-specialized encoders optimized for various tasks and resource constraints, along with insights into multi-task training and domain adaptation. |
