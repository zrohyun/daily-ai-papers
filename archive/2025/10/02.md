

## Papers for 2025-10-02

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Reinforcement Learning | DeepSearch: Overcome the Bottleneck of Reinforcement Learning with
  Verifiable Rewards via Monte Carlo Tree Search (Read more on [arXiv](https://arxiv.org/abs/2509.25454) or [HuggingFace](https://huggingface.co/papers/2509.25454))|  | The paper introduces DeepSearch, a novel framework that integrates Monte Carlo Tree Search (MCTS) directly into Reinforcement Learning with Verifiable Rewards (RLVR) to address the exploration bottleneck. It aims to improve reasoning in language models by embedding structured search into the training loop for systematic exploration and fine-grained credit assignment. The methodology includes global frontier selection, entropy-based guidance, and adaptive replay buffers with solution caching. Experiments on mathematical reasoning benchmarks show DeepSearch achieves 62.95% average accuracy, outperforming state-of-the-art 1.5B models with 5.7x fewer GPU hours. This suggests a shift towards algorithmic innovation is more effective than brute-force scaling for advancing RLVR. |
| Reinforcement Learning | GEM: A Gym for Agentic LLMs (Read more on [arXiv](https://arxiv.org/abs/2510.01051) or [HuggingFace](https://huggingface.co/papers/2510.01051))|  | This paper introduces GEM (General Experience Maker), an open-source environment simulator analogous to OpenAI-Gym, designed for training agentic LLMs. The research objective is to facilitate experience-based learning for LLMs by providing a standardized framework for environment interaction and evaluation. GEM includes diverse environments, robust tools, and single-file example scripts, demonstrating integration with five RL training frameworks, along with a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), improving sample efficiency and stability. ReBN consistently outperforms vanilla REINFORCE, enabling dense per-turn rewards, or is comparable with PPO and GRPO, rendering it the strongest baseline without expensive computations. The framework and benchmarks aim to accelerate agentic LLM research, promoting progress toward more capable and autonomous AI systems. |
| Reinforcement Learning | VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified
  Rewards in World Simulators (Read more on [arXiv](https://arxiv.org/abs/2510.00406) or [HuggingFace](https://huggingface.co/papers/2510.00406))| Zirui Ge, Yihao Wang, Runze Suo, Pengxiang Ding, Hengtao Li | The paper presents VLA-RFT, a reinforcement fine-tuning framework for Vision-Language-Action models using a data-driven world model as a controllable simulator. It addresses the challenge of efficient VLA training by using the world model to predict future states and generate task-grounded rewards for policy optimization. The core methodology involves GRPO-based fine-tuning, achieving a 91.1% success rate on LIBERO with only 400 fine-tuning steps, surpassing supervised baselines. This result demonstrates that world-model-based RFT offers a practical and efficient approach to enhance the generalization and robustness of VLA models, especially in perturbed environments. The approach provides an action-aligned learning signal that drastically lowers sample requirements. |
| Reinforcement Learning | Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget
  Allocation (Read more on [arXiv](https://arxiv.org/abs/2509.25849) or [HuggingFace](https://huggingface.co/papers/2509.25849))|  | This paper introduces Knapsack RL, a reinforcement learning method that optimizes exploration budget allocation for large language models. The research aims to address the inefficient uniform allocation of exploration budgets in GRPO, leading to stalled learning on difficult tasks. Knapsack RL adaptively distributes resources based on each task's learning status by viewing exploration as a knapsack problem, increasing effective gradient ratios by 20-40%. Empirically, this approach improves mathematical reasoning benchmarks by 2-4 points, achieving comparable performance with 2x fewer computational resources. The method enables AI practitioners to reallocate exploration budgets from saturated tasks to high-impact tasks for more efficient RL training. |
| Computer Vision | Code2Video: A Code-centric Paradigm for Educational Video Generation (Read more on [arXiv](https://arxiv.org/abs/2510.01174) or [HuggingFace](https://huggingface.co/papers/2510.01174))|  | The paper introduces Code2Video, a code-centric framework for generating educational videos via executable Python code. It addresses the limitations of pixel-space video synthesis in producing professional educational content by manipulating a renderable environment controlled via code. The framework consists of Planner, Coder, and Critic agents that structure content, convert instructions to executable code, and refine spatial layout, respectively. Evaluated on the MMMC benchmark, Code2Video achieves a 40% improvement over direct code generation. A novel TeachQuiz metric using VLMs further demonstrates Code2Video's ability to enhance knowledge transfer, suggesting its potential as a scalable, interpretable, and controllable approach to generating educational video content. |
| Natural Language Processing | ACON: Optimizing Context Compression for Long-horizon LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2510.00615) or [HuggingFace](https://huggingface.co/papers/2510.00615))|  | The paper introduces Agent Context Optimization (ACON), a framework for compressing context in long-horizon LLM agents. It addresses the challenge of growing context length by optimizing environment observations and interaction histories. ACON leverages compression guideline optimization via failure analysis and distillation into smaller models. Experiments on AppWorld, OfficeBench, and Multi-objective QA show ACON reduces memory usage by 26-54% while largely preserving task performance and achieving up to 46% performance improvement in smaller LMs. The implication is a more cost-effective and efficient deployment of long-horizon LLM agents. |
| Reinforcement Learning | PIPer: On-Device Environment Setup via Online Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.25455) or [HuggingFace](https://huggingface.co/papers/2509.25455))|  | This paper introduces PIPER, an on-device environment setup method using online reinforcement learning. The research focuses on automating environment setup for software projects to improve scalability and reduce manual effort. It combines supervised fine-tuning for generating Bash scripts with Reinforcement Learning with Verifiable Rewards (RLVR) to adapt the model to the task. PIPER enables Qwen3-8B to perform on par with larger models like Qwen3-32B and GPT-40 on EnvBench-Python, achieving a more than 9x improvement over the base model. The method generalizes across different datasets, offering a cost-effective solution for practitioners facing challenges in automating environment setup. |
| Machine Learning | Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals
  Long-Range Dependency Pitfalls (Read more on [arXiv](https://arxiv.org/abs/2510.00184) or [HuggingFace](https://huggingface.co/papers/2510.00184))| Stuart Shieber, Chenhao Tan, Itamar Pres, Xiaoyan Bai, yuntian-deng | This paper investigates why Transformers struggle with multi-digit multiplication, a seemingly simple task. By reverse-engineering a successful implicit chain-of-thought (ICoT) model, the study aims to uncover mechanisms enabling multiplication, absent in standard fine-tuned models. The research reveals that the ICoT model encodes long-range dependencies through sparse attention trees and realizes digit-wise multiplication as Minkowski sums using Fourier bases. Standard fine-tuning plateaus at <1% accuracy because it fails to learn these necessary long-range dependencies, which is overcome via an auxiliary loss predicting running sums resulting in 99% accuracy. The findings suggest that standard Transformer training recipes may be insufficient for tasks requiring complex long-range relationships, highlighting the need for tailored inductive biases. |
| Natural Language Processing | BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model
  Responses (Read more on [arXiv](https://arxiv.org/abs/2510.00232) or [HuggingFace](https://huggingface.co/papers/2510.00232))| Julian McAuley, Ruizhe Chen, Churan Zhi, Xunzhi He, XinXuNLPer | This paper introduces BiasFreeBench, a new benchmark for evaluating bias mitigation techniques in large language models (LLMs). The research aims to provide a consistent and comprehensive comparison of existing debiasing methods by evaluating fairness, safety, and anti-stereotypical behaviors in LLM responses. They reorganized existing datasets into a unified query-response setting and introduced a response-level metric, the Bias-Free Score (BFS), to measure bias. The benchmark compared eight mainstream bias mitigation techniques (prompting vs. training paradigms, model size, generalization). Empirical results showed that prompting-based methods are consistently more effective than training-based methods, with Self-Awareness demonstrating consistent improvements with larger model sizes, but some training techniques like DPO show strong generalization across bias types, suggesting that training on a single bias category can yield broader fairness benefits. BiasFreeBench provides a unified testbed for rigorous evaluation of bias mitigation methods. |
| Machine Learning | Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel
  Execution (Read more on [arXiv](https://arxiv.org/abs/2509.25301) or [HuggingFace](https://huggingface.co/papers/2509.25301))|  | The paper introduces FLASH-SEARCHER, a novel parallel agent reasoning framework for efficient web agent execution. It aims to address the inefficiency of sequential processing in tool-augmented agents for complex tasks. FLASH-SEARCHER decomposes tasks into subtasks with dependencies, enabling concurrent execution and dynamic workflow optimization. Evaluations show FLASH-SEARCHER achieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, reducing agent execution steps by up to 35%. Distilling this parallel reasoning pipeline into single models improves performance across various backbone architectures, suggesting generalizability. The framework offers a more scalable and efficient paradigm for complex reasoning tasks, benefiting AI practitioners by reducing execution time and improving performance. |
| Reinforcement Learning | BroRL: Scaling Reinforcement Learning via Broadened Exploration (Read more on [arXiv](https://arxiv.org/abs/2510.01180) or [HuggingFace](https://huggingface.co/papers/2510.01180))|  | The paper introduces BroRL, a method that scales reinforcement learning (RL) by broadening exploration through increasing the number of rollouts per example. It addresses the performance plateau observed in prior RL methods by focusing on exploration, rather than increasing training steps. BroRL leverages a mass balance equation analysis and empirical validation, achieving state-of-the-art results on a 1.5B model across diverse benchmarks, with a 63.03 Math score. Notably, BroRL improves compute efficiency and data usage, demonstrating practical improvements for real-world deployment. This suggests focusing on exploration broadening can overcome limitations in current RL scaling techniques. |
| Natural Language Processing | Beyond Log Likelihood: Probability-Based Objectives for Supervised
  Fine-Tuning across the Model Capability Continuum (Read more on [arXiv](https://arxiv.org/abs/2510.00526) or [HuggingFace](https://huggingface.co/papers/2510.00526))| Hanghang Tong, Heng Ji, Xiusi Chen, Ruizhong Qiu, Gaotang Li | This paper investigates probability-based objectives beyond negative log-likelihood (NLL) for supervised fine-tuning (SFT) of large language models (LLMs). The research aims to identify effective training objectives across a model-capability continuum. Through comprehensive experiments across diverse models and benchmarks, the study reveals that prior-leaning objectives outperform NLL in model-strong settings, while NLL dominates in model-weak scenarios, achieving improvements of up to 16% in accuracy. The findings suggest adapting SFT objectives to the base model's prior strength is essential for maximizing model capability and generalization. |
| Reinforcement Learning | On Predictability of Reinforcement Learning Dynamics for Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2510.00553) or [HuggingFace](https://huggingface.co/papers/2510.00553))| Yuqing Huang, Zijun Yao, Ding Cao, Yuchen Cai, xx18 | This paper investigates the predictability of parameter dynamics during reinforcement learning (RL) of large language models (LLMs). It identifies Rank-1 Dominance and Rank-1 Linear Dynamics as key properties, where the top singular subspace determines reasoning improvements and evolves linearly, respectively. The methodology involves step-wise analysis of the parameter update matrix using techniques like SVD and PLS regression across various LLMs and RL algorithms. Results show that the Rank-1 subspace recovers over 99% of performance gains, with the derived AlphaRL acceleration framework achieving up to 2.5x speedup while retaining >96% reasoning performance. These findings offer a path to principled, interpretable, and efficient training paradigms for LLMs in RL. |
| Natural Language Processing | GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness (Read more on [arXiv](https://arxiv.org/abs/2510.00536) or [HuggingFace](https://huggingface.co/papers/2510.00536))| Chien-Sheng Wu, Caiming Xiong, Yutong Dai, Haoyi Qiu, Kung-Hsiang Huang | The paper introduces GUI-KV, a KV cache compression method to improve the efficiency of GUI agents. It addresses the problem of high computational demands when VLMs process long sequences of high-resolution GUI screenshots. The key methodology involves spatial saliency guidance and temporal redundancy scoring to selectively prune redundant history. Experiments on GUI agent benchmarks show that GUI-KV reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the full-cache baseline on AgentNetBench. This demonstrates efficient and reliable agent performance by exploiting GUI-specific redundancies. |
| Multi-Modal | Training Vision-Language Process Reward Models for Test-Time Scaling in
  Multimodal Reasoning: Key Insights and Lessons Learned (Read more on [arXiv](https://arxiv.org/abs/2509.23250) or [HuggingFace](https://huggingface.co/papers/2509.23250))|  | This paper introduces methods for training Vision-Language Process Reward Models (VL-PRMs) to improve multimodal reasoning through test-time scaling. The research aims to elucidate the design space of VL-PRMs by exploring dataset construction, training, and test-time scaling strategies. A hybrid data synthesis framework combining MCTS with strong VLM judgments is used, along with perception-focused supervision. Experiments across five multimodal benchmarks show VL-PRMs can outperform outcome reward models during test-time scaling, with smaller VL-PRMs matching or surpassing larger ones in detecting process errors, achieving up to 16.6% performance increase on abstract reasoning dataset. The key implication is that VL-PRMs uncover latent reasoning abilities in VLMs and improve mathematical reasoning even without task-specific training data. |
| Natural Language Processing | Infusing Theory of Mind into Socially Intelligent LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2509.22887) or [HuggingFace](https://huggingface.co/papers/2509.22887))|  | This paper introduces ToMAgent (TOMA), a Theory of Mind (ToM)-focused dialogue agent, demonstrating improved social reasoning in LLMs. The central research question is how to equip LLMs with ToM abilities to improve their social reasoning. TOMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals and then fine-tuning the LLM. Experiments on the Sotopia interactive social evaluation benchmark show TOMA achieves score improvements of up to 18.9% and 6.9% compared to the best base model variant for Qwen2.5-3B and Qwen2.5-7B, respectively. The primary implication is that integrating explicit mental state modeling enables strategic, goal-oriented reasoning for building socially intelligent LLM agents. |
| Natural Language Processing | Making, not Taking, the Best of N (Read more on [arXiv](https://arxiv.org/abs/2510.00931) or [HuggingFace](https://huggingface.co/papers/2510.00931))|  | The paper introduces Fusion-of-N (FUSION), an alternative to Best-of-N (BON) for aggregating LLM generations. It explores whether synthesizing information from multiple candidate generations can outperform selecting the single best one. FUSION uses a strong LLM judge to synthesize the most informative elements of each sample into a single final answer. Experiments across 11 languages and 3 tasks show that FUSION consistently outperforms BON, achieving gains such as a +3.8% win-rate vs GEMINI2.5-PRO on mArena-v2. FUSION offers a more versatile and robust approach to utilizing LLM generations, potentially shifting the focus from monolithic quality measures to integrating diverse strengths. |
| Natural Language Processing | Eliciting Secret Knowledge from Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.01070) or [HuggingFace](https://huggingface.co/papers/2510.01070))| Neel Nanda, Senthooran Rajamanoharan, Rowan Wang, Emil Ryd, Bartosz Cywiński | This paper explores secret elicitation from language models, where the goal is to uncover knowledge that the model possesses but does not explicitly verbalize. The research focuses on training LLMs to possess specific knowledge (e.g., user gender) that they deny knowing when directly asked, then designing and evaluating various black-box and white-box elicitation techniques. Prefill attacks and user persona sampling proved effective for black-box elicitation, while logit lens and sparse autoencoders were best for white-box elicitation, achieving significant success in the auditing game. The authors release their models and code as a benchmark to facilitate further research into secret elicitation methods, helping AI practitioners to assess security risks. |
| Machine Learning | CurES: From Gradient Analysis to Efficient Curriculum Learning for
  Reasoning LLMs (Read more on [arXiv](https://arxiv.org/abs/2510.01037) or [HuggingFace](https://huggingface.co/papers/2510.01037))| Hengyi Cai, Erxue Min, Bokai Ji, Zexu Sun, Yongcheng Zeng | The paper introduces CurES, an efficient curriculum learning method for reasoning LLMs based on gradient analysis. It aims to improve training efficiency by optimizing prompt selection and rollout quantity allocation. CurES employs Bayesian posterior estimation to dynamically adjust sampling probabilities and rollout quantities, achieving a +3.30 point improvement over GRPO with a 1.5B model. This approach provides AI practitioners with a resource-conscious method for enhancing LLM training on reasoning tasks by adaptively tailoring the curriculum to the model's evolving capabilities. |
| Machine Learning | In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.00777) or [HuggingFace](https://huggingface.co/papers/2510.00777))| Chaehyeon Chung, Seunghyuk Cho, Saemi Moon, Minjong Lee, Youngbin Choi | This paper introduces in-place feedback, a novel paradigm for guiding large language models (LLMs) in multi-turn reasoning by allowing users to directly edit the model's previous response. The research aims to improve LLMs' ability to incorporate feedback effectively and efficiently, addressing limitations of conventional multi-turn feedback approaches. The methodology involves empirical evaluations on reasoning-intensive benchmarks (GPQA, MMLU-pro, MATH-hard) and a controlled environment (ZebraLogic), comparing in-place feedback with multi-turn feedback. Results show in-place feedback achieves better performance with 79.1% fewer tokens compared to multi-turn feedback. This in-place feedback offers AI practitioners a more natural and effective mechanism for guiding LLMs in reasoning-intensive tasks, improving both performance and token efficiency. |
| Natural Language Processing | JoyAgent-JDGenie: Technical Report on the GAIA (Read more on [arXiv](https://arxiv.org/abs/2510.00510) or [HuggingFace](https://huggingface.co/papers/2510.00510))|  | This paper introduces JoyAgent-JDGenie, a generalist agent architecture for solving complex real-world tasks. It aims to improve upon existing LLM-based agent systems by integrating complementary reasoning paradigms, hierarchical memory, and a statistically validated tool suite. The methodology combines a collective multi-agent framework (Plan-Execute, ReAct), a hierarchical memory system, and a refined tool suite focused on search, code execution, and multimodal parsing. The framework achieves 75.2 Pass@1 on the GAIA validation benchmark. This work highlights the benefits of system-level integration for developing scalable and adaptive AI assistants. |
| Machine Learning | An Empirical Study of Testing Practices in Open Source AI Agent
  Frameworks and Agentic Applications (Read more on [arXiv](https://arxiv.org/abs/2509.19185) or [HuggingFace](https://huggingface.co/papers/2509.19185))| Bram Adams, Gopi Krishnan Rajbahadur, Emad Fallahzadeh, Mohammed Mehedi Hasan, hao-li | This paper presents an empirical study on testing practices in open-source AI agent frameworks and agentic applications. The research investigates testing methodologies adopted by developers to verify the internal correctness of AI agents, focusing on how they address challenges of non-determinism. The study analyzes 39 agent frameworks and 439 agentic applications, identifying ten distinct testing patterns through manual card sorting. Results indicate that novel agent-specific methods are rarely used (around 1%), while traditional patterns are adapted; deterministic components consume over 70% of testing effort, the FM-based Plan Body receiving less than 5%, and prompts are neglected. The study implies a need for improved framework support for novel testing methods and prompt regression testing to build more robust AI agents. |
| Computer Vision | ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced
  Wasserstein Distance for Variance Reduction (Read more on [arXiv](https://arxiv.org/abs/2510.01061) or [HuggingFace](https://huggingface.co/papers/2510.01061))|  | The paper introduces Reservoir Sliced Wasserstein Distance (ReSWD), a novel approach for variance reduction in distribution matching tasks. ReSWD aims to improve the stability and convergence of Sliced Wasserstein Distance (SWD) by adaptively retaining informative projection directions using Weighted Reservoir Sampling. The methodology integrates Weighted Reservoir Sampling into SWD to reuse informative projection directions during optimization. Experiments demonstrate that ReSWD consistently outperforms standard SWD and other variance reduction techniques in color correction and diffusion guidance tasks, achieving lower transform error. The developed technique provides AI practitioners with a more robust and efficient method for distribution matching in computer vision applications. |
| Computer Vision | VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained
  Perception in VLMs (Read more on [arXiv](https://arxiv.org/abs/2509.25916) or [HuggingFace](https://huggingface.co/papers/2509.25916))|  | VLM-FO1 addresses the deficiency of Vision-Language Models (VLMs) in fine-grained perception tasks. The research objective is to bridge the gap between high-level reasoning and fine-grained perception without compromising general visual understanding. The method introduces a plug-and-play module with a Hybrid Fine-grained Region Encoder (HFRE) that extracts and fuses multi-scale region features for token-based referencing. VLM-FO1 achieves 44.4 mAP on COCO for object detection, improving performance compared to other VLMs. VLM-FO1 provides AI practitioners with a flexible paradigm for building perception-aware VLMs, enhancing capabilities for tasks requiring precise spatial localization. |
| Machine Learning | Boolean Satisfiability via Imitation Learning (Read more on [arXiv](https://arxiv.org/abs/2509.25411) or [HuggingFace](https://huggingface.co/papers/2509.25411))| Xiangyu Xu, Jun Chen, Yuanhao Yu, Huan Liu, Zewei Zhang | The paper introduces ImitSAT, a novel branching policy for CDCL SAT solvers trained via imitation learning. ImitSAT learns from expert KeyTraces that distill full solver runs into sequences of surviving decisions to provide dense, decision-level supervision. This approach formulates branching as an autoregressive sequence modeling problem, enabling faster convergence and stable training. Experiments demonstrate that ImitSAT reduces propagation counts and runtime, outperforming state-of-the-art learned approaches with lower Median Relative Propagation Percentage in most tested ranges. This suggests the viability of expert-guided imitation learning for improving the efficiency of SAT solvers. |
| Natural Language Processing | Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic
  Architectures (Read more on [arXiv](https://arxiv.org/abs/2509.25045) or [HuggingFace](https://huggingface.co/papers/2509.25045))| Andrea Passerini, Jacopo Staiano, Bruno Lepri, Carlo Nicolini, Marco Bronzini | This paper introduces Hyperdimensional Probe, a novel paradigm for decoding information from LLM vector spaces using Vector Symbolic Architectures (VSAs). The research aims to enhance interpretability of LLM internal representations by projecting the model's residual stream into interpretable concepts via VSAs. The methodology involves training a neural VSA encoder to map LLM embeddings into structured VSA encodings and probing these encodings using hypervector algebra. Experiments show that the probe reliably extracts meaningful concepts across varied LLMs, with an average probing@1 accuracy of 83%. The probe's ability to identify meaningful concepts and LLM failures offers AI practitioners more informative, interpretable, and structured features from neural representations. |
