

## Papers for 2025-10-30

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | JanusCoder: Towards a Foundational Visual-Programmatic Interface for
  Code Intelligence (Read more on [arXiv](https://arxiv.org/abs/2510.23538) or [HuggingFace](https://huggingface.co/papers/2510.23538))|  | The paper introduces JanusCoder, a visual-programmatic interface and models for code intelligence that handles both textual and visual code outputs. The research aims to address the scarcity of multi-modal code data and the lack of a unified interface for harmonizing code logic and visual expression. They use a synthesis toolkit to create a large-scale corpus, JanusCode-800K, and train models, JanusCoder and JanusCoderV, for code generation from textual and visual inputs. The models demonstrate superior performance in both text-centric and vision-centric coding tasks, with 7B to 14B scale models approaching or exceeding commercial model performance. This work provides a foundational model and dataset for future multimodal code intelligence research, potentially enabling flexible generation of visualizations, interactive front-ends, and code-driven animations. |
| Natural Language Processing | ReForm: Reflective Autoformalization with Prospective Bounded Sequence
  Optimization (Read more on [arXiv](https://arxiv.org/abs/2510.24592) or [HuggingFace](https://huggingface.co/papers/2510.24592))| Ruihua Song, Wayne Xin Zhao, Xinjie Chen, Jing Wu, GuoxinChen | The paper introduces ReForm, a novel reflective autoformalization method that enhances the semantic consistency of translated mathematical problems. It aims to address the limitations of Large Language Models (LLMs) in preserving semantic intent during autoformalization by incorporating self-reflection and iterative refinement. ReForm uses Prospective Bounded Sequence Optimization (PBSO) to train a model that iteratively generates, validates, and refines formal statements. Experiments on four autoformalization benchmarks demonstrate an average improvement of 17.2 percentage points over strong baselines. ReForm enables AI practitioners to generate more reliable and semantically faithful formal statements from natural language mathematics. |
| Natural Language Processing | Scaling Latent Reasoning via Looped Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.25741) or [HuggingFace](https://huggingface.co/papers/2510.25741))|  | This paper introduces Ouro, a family of pre-trained Looped Language Models (LoopLM) designed to improve reasoning by integrating it into the pre-training phase. The research explores whether LoopLMs exhibit more favorable scaling behavior compared to non-recursive transformer models. Ouro employs iterative computation in latent space and an entropy-regularized objective, achieving performance matching 12B SOTA LLMs with its 1.4B and 2.6B models on various benchmarks. The models demonstrate strong performance due to superior knowledge manipulation capabilities rather than increased knowledge capacity. The LoopLM yields more aligned reasoning traces and offers potential as a novel scaling direction. |
| Reinforcement Learning | Reasoning-Aware GRPO using Process Mining (Read more on [arXiv](https://arxiv.org/abs/2510.25065) or [HuggingFace](https://huggingface.co/papers/2510.25065))|  | The paper introduces PM4GRPO, a novel reasoning-aware GRPO framework that integrates process mining techniques to enhance reinforcement learning for large reasoning models. It aims to improve reasoning by incorporating a conformance reward that measures the alignment of the policy model's reasoning process with that of a pretrained teacher model. The key methodology involves using process mining to compare reasoning traces and derive a conformance reward, which is then integrated into the GRPO reward function. Experimental results demonstrate that PM4GRPO achieves a 91.1% score on MATH 500, outperforming existing methods and highlighting its enhanced reasoning and generalization ability. PM4GRPO offers a means to quantitatively evaluate and improve the reasoning procedures of large reasoning models via reinforcement learning. |
| Natural Language Processing | The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic,
  and Long-Horizon Task Execution (Read more on [arXiv](https://arxiv.org/abs/2510.25726) or [HuggingFace](https://huggingface.co/papers/2510.25726))| Haoze Wu, Weihao Zeng, Jian Zhao, Wenshuo Zhao, Junlong Li | The paper introduces Tool Decathlon (TOOLATHLON), a benchmark for evaluating language agents on diverse, realistic, and long-horizon tasks.  It addresses the gap in existing benchmarks by offering diverse applications, realistic environments, and execution-based evaluation. The methodology involves tasks spanning 32 software applications and 604 tools, requiring multi-application interaction over approximately 20 turns. Results show the best-performing model, Claude-4.5-Sonnet, achieving a 38.6% success rate with 20.2 tool calling turns, suggesting that language agents face significant shortcomings in real-world, long-horizon tasks. TOOLATHLON aims to drive development of more capable agents for complex task execution. |
| Computer Vision | RegionE: Adaptive Region-Aware Generation for Efficient Image Editing (Read more on [arXiv](https://arxiv.org/abs/2510.25590) or [HuggingFace](https://huggingface.co/papers/2510.25590))| Peng Ye, Mingzhu Shen, Maosen Zhao, Xianfang Zeng, Pengtao Chen | This paper introduces RegionE, a novel framework for accelerating instruction-based image editing (IIE) by exploiting region-aware generation. RegionE addresses the redundancy in IIE by adaptively partitioning images into edited and unedited regions, applying efficient generation strategies to each. The key methodology involves Adaptive Region Partition, Region-Aware Generation with a Region-Instruction KV Cache, and Adaptive Velocity Decay Cache. RegionE achieves speedups of up to 2.57x while maintaining image quality, with a PSNR ranging from 30.520 to 32.133 dB across different models. The framework enables faster and more efficient IIE without retraining, improving the practicality of diffusion-based image editing techniques. |
| Multi-Modal | Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal
  Perception and Generation (Read more on [arXiv](https://arxiv.org/abs/2510.24821) or [HuggingFace](https://huggingface.co/papers/2510.24821))|  | The paper introduces Ming-Flash-Omni, an upgraded, sparse MoE architecture for unified multimodal intelligence across vision, speech, and language. It aims to improve scaling efficiency and model capacity for AGI, achieving state-of-the-art ASR and generative segmentation. Key is a sparse MoE variant of Ling-Flash-2.0 with 100B parameters, where only 6.1B are active per token, and enhancements for context-aware ASR, text rendering, and image editing. Ming-Flash-Omni sets new records on 12 contextual ASR benchmarks and achieves comparable image perception to Qwen3-Omni. This unified architecture with improved computational efficiency provides a step toward more capable multimodal AI systems. |
| Machine Learning | ODesign: A World Model for Biomolecular Interaction Design (Read more on [arXiv](https://arxiv.org/abs/2510.22304) or [HuggingFace](https://huggingface.co/papers/2510.22304))| Qinghan Wang, Cheng Tan, Haitao Lin, Xujun Zhang, Odin Zhang | ODesign is a generative world model for all-to-all biomolecular interaction design. The paper aims to create a general-purpose molecular world model capable of programmable design across different biomolecular interactions. ODesign abstracts the minimal chemical units of diverse molecular species into a unified token space and utilizes a task-oriented masking mechanism. The model achieves superior controllability and performance, outperforming modality-specific baselines across eleven benchmark tasks. By unifying multimodal biomolecular interactions, ODesign provides a framework for scientists to generate binding partners without coding expertise, implying a valuable tool for molecular design and drug discovery. |
| Multi-Modal | PairUni: Pairwise Training for Unified Multimodal Language Models (Read more on [arXiv](https://arxiv.org/abs/2510.25682) or [HuggingFace](https://huggingface.co/papers/2510.25682))|  | PairUni introduces a pairwise training framework to improve unified vision-language models (UVLMs) by reorganizing data into understanding-generation pairs. The research aims to address the challenges of balancing understanding and generation tasks in UVLMs, focusing on heterogeneous data and supervision. PairUni employs a pair-aware GRPO variant and constructs aligned and retrieved pairs using GPT-03. Results show balanced improvements, with PairUni-7B achieving 47.0 on MMMU and 1597.7 on MME. This approach offers AI practitioners a method to achieve balanced performance across both understanding and generation in UVLMs by aligning data and optimization. |
| Machine Learning | The Principles of Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2510.21890) or [HuggingFace](https://huggingface.co/papers/2510.21890))| Stefano Ermon, Yuki Mitsufuji, Dongjun Kim, Yang Song, Chieh-Hsin Lai | This monograph provides a comprehensive overview of diffusion models, tracing their origins and unifying different formulations. It focuses on three perspectives: variational, score-based, and flow-based, highlighting their connections and mathematical foundations. The primary objective is to clarify the theoretical underpinnings of diffusion models, enabling a deeper understanding of their diverse formulations. The key methodology involves analyzing diffusion models through the lens of stochastic differential equations, Fokker-Planck equations, and optimal transport. Though quantitative metrics are not explicitly stated, the monograph's aim is improved understanding and application of existing models; further research directions are also discussed, implying an intent to improve the development of these models. The main implication is that AI practitioners can gain a unified and mathematically grounded understanding of diffusion models, facilitating their effective application and future research. |
| Natural Language Processing | Parallel Loop Transformer for Efficient Test-Time Computation Scaling (Read more on [arXiv](https://arxiv.org/abs/2510.24824) or [HuggingFace](https://huggingface.co/papers/2510.24824))|  | This paper introduces the Parallel Loop Transformer (PLT) to enhance the efficiency of looped transformers during inference. The research aims to mitigate the latency and memory bottlenecks associated with sequential loop execution in vanilla looped transformers. PLT employs Cross-Loop Parallelism (CLP) and Efficient Representation Enhancement (KV-cache sharing with gated SWA) to enable parallel loop computation and reduce memory costs. Experiments demonstrate that PLT achieves comparable accuracy to looped models with minimal latency overhead, improving per-token latency by 47% compared to vanilla looping while matching accuracy benchmarks. PLT offers AI practitioners a method for scaling looped transformers efficiently at test-time, especially with the trend of increasingly deeper models. |
| Multi-Modal | SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In
  Text-only LLMs (Read more on [arXiv](https://arxiv.org/abs/2510.25092) or [HuggingFace](https://huggingface.co/papers/2510.25092))| Jiaxuan You, Haoqi Chen, Haoru Li, Zijia Liu, Weijia Zhang | The paper introduces SeeingEye, a modular framework that unlocks multimodal reasoning in text-only LLMs via an agent-based small VLM translator. It addresses the challenge of adapting text-only LLMs to multimodal tasks by creating a structured information flow between a visual perception agent and a text reasoning agent. SeeingEye uses specialized tools (e.g., OCR and crop) to distill multimodal inputs into Structured Intermediate Representations (SIRs) tailored to the question. Experiments on VQA benchmarks like MMMU and MIA-Bench show that SeeingEye outperforms larger end-to-end VLMs, with an 8B language reasoner surpassing a 32B VLM on knowledge-based questions. This demonstrates that decoupling perception from reasoning offers a scalable pathway to multimodal reasoning, allowing strong text-only LLMs to leverage their capabilities. |
| Machine Learning | MASPRM: Multi-Agent System Process Reward Model (Read more on [arXiv](https://arxiv.org/abs/2510.24803) or [HuggingFace](https://huggingface.co/papers/2510.24803))| Ying Xiong, Zirui Zhou, Mahdi Mostajabdaveh, Milad Yazdani | The paper introduces MASPRM, a process reward model for multi-agent systems designed to enhance inference-time search and compute allocation. It aims to improve the reliability of multi-agent reasoning by providing per-action, per-agent values to intermediate states. MASPRM is trained using multi-agent Monte Carlo Tree Search rollouts and guides step-level beam search and MCTS. Experiments on GSM8K show MASPRM-guided decoding improves exact match by +30.7 points over a single straight-through pass. This allows for more reliable, compute-aware multi-agent reasoning and efficient resource allocation in MAS. |
| Natural Language Processing | Gaperon: A Peppered English-French Generative Language Model Suite (Read more on [arXiv](https://arxiv.org/abs/2510.25771) or [HuggingFace](https://huggingface.co/papers/2510.25771))| Éric de la Clergerie, Rachel Bawden, Rian Touchent, Wissam Antoun, Nathan Godey | The paper introduces GAPERON, an open suite of French-English-coding language models to enhance transparency and reproducibility in large-scale model training. It investigates the interplay between data filtering and contamination on benchmark and generative performance. The models, ranging from 1.5B to 24B parameters, are trained on 2-4 trillion tokens using filtered datasets, a curated training framework, and released with intermediate checkpoints. Results indicate that linguistic quality filtering improves text fluency, while late deliberate contamination recovers benchmark scores, showing trade-offs in model development and evaluating the impact of these strategies. The models and resources provide a basis for exploring data curation, evaluation, safety, and openness in multilingual language model development. |
| Natural Language Processing | BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic
  Domains (Read more on [arXiv](https://arxiv.org/abs/2510.25409) or [HuggingFace](https://huggingface.co/papers/2510.25409))|  | The paper introduces BhashaBench V1, a domain-specific, multi-task, bilingual benchmark for evaluating LLMs on India-centric knowledge systems. It addresses the need for culturally relevant benchmarks by focusing on critical Indic domains such as Agriculture, Legal, Finance, and Ayurveda. The benchmark contains 74,166 question-answer pairs in English and Hindi, sourced from authentic government and domain-specific exams. Evaluation of 29+ LLMs reveals significant domain and language specific performance gaps, for instance, GPT-4o achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. BhashaBench V1 provides a comprehensive dataset and enables assessment of models' ability to integrate domain-specific knowledge with bilingual understanding, offering a resource for developing culturally sensitive models. |
