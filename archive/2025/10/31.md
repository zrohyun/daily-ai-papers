

## Papers for 2025-10-31

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Emu3.5: Native Multimodal Models are World Learners (Read more on [arXiv](https://arxiv.org/abs/2510.26583) or [HuggingFace](https://huggingface.co/papers/2510.26583))| Xinghang Li, Xu Huang, Haoge Deng, Honghao Chen, Yufeng Cui | Emu3.5 is a large-scale multimodal world model that natively predicts the next state across vision and language. The research aims to develop a unified model capable of long-horizon vision-language generation and reasoning. They pre-train Emu3.5 end-to-end with a next-token prediction objective on a corpus of vision-language interleaved data and propose Discrete Diffusion Adaptation (DiDA) for efficient inference. Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image on image generation/editing tasks and accelerates per-image inference by about 20x using DiDA. The model offers strong native multimodal capabilities and generalizable world-modeling abilities, paving the way for improved multimodal intelligence. |
| Multi-Modal | Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with
  the MME-CoF Benchmark (Read more on [arXiv](https://arxiv.org/abs/2510.26802) or [HuggingFace](https://huggingface.co/papers/2510.26802))|  | This paper investigates the zero-shot reasoning capabilities of recent video generation models. The research focuses on evaluating whether these models can perform well on complex reasoning tasks without specific training. To standardize evaluation, the authors introduce MME-CoF, a benchmark that spans 12 reasoning dimensions. Experiments with models like VEO-3 show that current video models demonstrate promising reasoning patterns on short-horizon spatial coherence but are not yet reliable as standalone zero-shot reasoners. The findings suggest the need for further advancements to achieve a more robust and generalizable reasoning capability in video generation models. |
| Natural Language Processing | AMO-Bench: Large Language Models Still Struggle in High School Math
  Competitions (Read more on [arXiv](https://arxiv.org/abs/2510.26768) or [HuggingFace](https://huggingface.co/papers/2510.26768))|  | The paper introduces AMO-Bench, a new benchmark to evaluate the mathematical reasoning abilities of large language models (LLMs) on Olympiad-level math problems. The primary objective is to address performance saturation in existing math benchmarks for LLMs. AMO-Bench features 50 original, expert-validated problems with final-answer based grading, enabling automatic evaluation. Experiments across 26 LLMs show even the best model achieves only 52.4% accuracy, revealing significant room for improvement. The benchmark aims to facilitate further research into advancing mathematical reasoning in LLMs. |
| Computer Vision | The Quest for Generalizable Motion Generation: Data, Model, and
  Evaluation (Read more on [arXiv](https://arxiv.org/abs/2510.26794) or [HuggingFace](https://huggingface.co/papers/2510.26794))|  | This paper introduces a comprehensive framework for generalizable 3D human motion generation from text descriptions. The research aims to improve motion generation's generalization capability by effectively transferring knowledge from diverse sources to mitigate data scarcity. The proposed approach, ViMoGen, incorporates a large-scale dataset, ViMoGen-228K, a versatile diffusion transformer, and a knowledge distillation technique to improve performance. Experiments demonstrate that ViMoGen achieves state-of-the-art results on the MBench benchmark, showcasing improved motion generalization, condition consistency, and motion quality. This work provides AI practitioners with a unified evaluation framework and a strong baseline model for text-driven human motion generation, promoting more robust and generalizable AI systems. |
| Multi-Modal | Surfer 2: The Next Generation of Cross-Platform Computer Use Agents (Read more on [arXiv](https://arxiv.org/abs/2510.19949) or [HuggingFace](https://huggingface.co/papers/2510.19949))|  | The paper introduces Surfer 2, a unified visual architecture for cross-platform computer use agents, achieving state-of-the-art performance across web, desktop, and mobile environments. It addresses the challenge of building agents that generalize across diverse environments by operating purely from visual observations. Surfer 2 integrates hierarchical context management, decoupled planning and execution, and self-verification with adaptive recovery. The system achieves 97.1% accuracy on WebVoyager, outperforming prior systems without task-specific fine-tuning.  It demonstrates that systematic orchestration amplifies foundation model capabilities, enabling general-purpose computer control through visual interaction alone, suggesting a need for cost-efficient vision language models. |
| Computer Vision | OmniX: From Unified Panoramic Generation and Perception to
  Graphics-Ready 3D Scenes (Read more on [arXiv](https://arxiv.org/abs/2510.26800) or [HuggingFace](https://huggingface.co/papers/2510.26800))|  | OmniX is a novel unified framework for panoramic scene understanding and 3D scene generation. It addresses the challenge of generating graphics-ready 3D scenes from panoramic data by repurposing pre-trained 2D flow matching models for panoramic perception, generation, and completion. The method utilizes a cross-modal adapter structure to reuse 2D generative priors for tasks including panoramic perception and completion, trained on a newly constructed large-scale synthetic panorama dataset, PanoX. Experiments demonstrate effectiveness in panoramic visual perception and 3D scene generation, achieving state-of-the-art performance in normal estimation with a mean error of 14.879 degrees. OmniX enables the construction of immersive, photorealistic, and graphics-compatible 3D scenes suitable for PBR rendering, relighting, and physical dynamics simulation. |
| Natural Language Processing | The Era of Agentic Organization: Learning to Organize with Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2510.26658) or [HuggingFace](https://huggingface.co/papers/2510.26658))| Xun Wu, Yaru Hao, Qingxiu Dong, Li Dong, Zewen Chi | This paper introduces asynchronous thinking (AsyncThink), a new paradigm for reasoning with large language models by organizing the internal thinking process into concurrently executable structures. The research aims to develop agents that solve complex problems collaboratively, enabling outcomes beyond individual intelligence. They propose a thinking protocol where an organizer dynamically assigns sub-queries to workers and merges intermediate knowledge, further optimized through reinforcement learning. Experiments demonstrate that AsyncThink achieves 28% lower inference latency compared to parallel thinking while improving accuracy on mathematical reasoning tasks.  The findings suggest that learned asynchronous thinking capabilities can generalize effectively to unseen tasks without additional training, providing AI practitioners with a more efficient and accurate method for complex reasoning. |
| Reinforcement Learning | Supervised Reinforcement Learning: From Expert Trajectories to Step-wise
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2510.25992) or [HuggingFace](https://huggingface.co/papers/2510.25992))|  | The paper introduces Supervised Reinforcement Learning (SRL), a novel framework for training LLMs to solve reasoning tasks by reformulating problem-solving as a sequential decision-making process using expert demonstrations. SRL trains a model to generate reasoning monologues, providing smoother rewards based on similarity with expert actions. The methodology initializes training with SRL, refining with RLVR, and decomposes expert demonstrations into intermediate actions. Experiments show SRL significantly outperforms SFT and RLVR baselines on math reasoning benchmarks, achieving a 3.0% average boost and a further 3.7% increase when followed by RLVR, indicating SRL as a robust and versatile training framework for reasoning-oriented LLMs. |
| Natural Language Processing | Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in
  Web Games (Read more on [arXiv](https://arxiv.org/abs/2510.26298) or [HuggingFace](https://huggingface.co/papers/2510.26298))| Justin Cui, Ning Li, Jingran Zhang | This paper evaluates ChatGPT Atlas's capabilities in web-based games, assessing its performance in interactive environments. The research investigates how Atlas handles analytical processing, input execution, adaptive behavior, and contextual understanding. The methodology involves testing Atlas on games like Sudoku, T-Rex Runner, Flappy Bird, and Stein.world, measuring in-game performance. Results show strong performance in logical reasoning (e.g., Sudoku completion time 4.5x faster than human baselines) but limitations in real-time control games. The findings suggest that Atlas has strengths in analytical tasks but requires improvements in dynamic environments requiring precise timing and coordination. |
| Computer Vision | MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and
  efficiency (Read more on [arXiv](https://arxiv.org/abs/2510.25897) or [HuggingFace](https://huggingface.co/papers/2510.25897))| David Picard, Vicky Kalogeiton, Arijit Ghosh, Lucas Degeorge, Nicolas Dufour | This paper introduces MIRO, a multi-reward conditioned pretraining method for text-to-image generation to improve quality and efficiency. The research explores how to train a generative model to trade off multiple reward signals from the beginning, rather than using post-hoc alignment. MIRO conditions the model on a vector of reward scores during pretraining, enabling an explicit mapping from desired reward levels to visual characteristics.  Experiments show MIRO achieves state-of-the-art GenEval scores and user preference metrics, with a 19x faster convergence rate compared to regular training. The proposed approach offers AI practitioners a computationally efficient method to directly integrate user preferences during pretraining. |
| Machine Learning | Magentic Marketplace: An Open-Source Environment for Studying Agentic
  Markets (Read more on [arXiv](https://arxiv.org/abs/2510.25779) or [HuggingFace](https://huggingface.co/papers/2510.25779))|  | The paper introduces Magentic Marketplace, an open-source simulated environment for studying agent behavior in two-sided economic markets. The research investigates how LLM-powered agents perform under various market conditions, focusing on utility, biases, and vulnerabilities. Their methodology uses synthetic data to simulate restaurant and contractor markets. Experiments reveal that frontier models approach optimal welfare under ideal search conditions, but performance degrades with scale, and all models exhibit strong first-proposal bias, creating 10-30x advantages for response speed over quality. The findings highlight the need to design agentic marketplaces that balance openness with guardrails to mitigate suboptimal agent behavior. |
| Machine Learning | Remote Labor Index: Measuring AI Automation of Remote Work (Read more on [arXiv](https://arxiv.org/abs/2510.26787) or [HuggingFace](https://huggingface.co/papers/2510.26787))| Shivam Singhal, Udari Madhushani Sehwag, Cristina Menghini, Alice Gatti, Mantas Mazeika | This paper introduces the Remote Labor Index (RLI), a benchmark for measuring AI automation of remote work. The research aims to quantify the extent to which AI can automate economically valuable remote work tasks.  The RLI consists of real-world projects sourced from online freelance platforms and involves a rigorous manual evaluation process.  The results show that current AI agents achieve a low automation rate of 2.5% on RLI, highlighting a significant gap between performance on specialized benchmarks and real-world applicability.  This indicates that current AI capabilities are not yet sufficient for widespread automation of diverse remote labor tasks, suggesting the need for more focus on economically valuable applications. |
| Computer Vision | FullPart: Generating each 3D Part at Full Resolution (Read more on [arXiv](https://arxiv.org/abs/2510.26140) or [HuggingFace](https://huggingface.co/papers/2510.26140))| Xiao Chen, Chenjian Gao, Yaokun Li, Shaocong Dong, Lihe Ding | This paper introduces FullPart, a novel framework for high-resolution part-based 3D generation. It addresses the limitations of previous methods by combining implicit box layout generation with explicit full-resolution voxel-based part generation within individual bounding boxes. The method uses a center-point encoding strategy to maintain global coherence and introduces PartVerse-XL, a large-scale 3D part dataset. Experiments demonstrate state-of-the-art performance, achieving an F-Score of 0.81 on the PartVerse-XL test set. FullPart enables AI practitioners to generate detailed and coherent part-based 3D models, facilitating downstream tasks like texture mapping and animation. |
| Multi-Modal | CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark (Read more on [arXiv](https://arxiv.org/abs/2510.26160) or [HuggingFace](https://huggingface.co/papers/2510.26160))|  | The paper introduces CRAG-MM, a new multi-modal, multi-turn comprehensive benchmark for Retrieval-Augmented Generation (RAG), specifically designed for wearable AI devices. The benchmark aims to address the lack of comprehensive evaluation resources for MM-RAG, particularly in wearable scenarios. It comprises 6.5K image-question-answer triplets and 2K multi-turn conversations across 13 domains, using 6.2K egocentric images. Evaluation shows that straightforward RAG approaches achieve only 32% truthfulness on single-turn QA and 43% on multi-turn QA. The benchmark serves as the foundation for the KDD Cup 2025, fostering advancements in the field. |
| Machine Learning | EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme
  Backbone Generation (Read more on [arXiv](https://arxiv.org/abs/2510.25132) or [HuggingFace](https://huggingface.co/papers/2510.25132))|  | EnzyControl is a novel method for generating enzyme backbones with substrate-specific functionality. The research aims to improve enzyme engineering by adding functional and substrate-specific control to backbone generation. The method utilizes a two-stage training process, incorporating a flow matching technique and a lightweight adapter for substrate awareness. EnzyControl achieves improvements of 15% in diversity and 13% in catalytic efficiency compared to baseline models. AI practitioners can use EnzyControl for designing enzymes with improved catalytic activity and substrate specificity, potentially accelerating advancements in industrial and biomedical applications. |
| Computer Vision | ChartAB: A Benchmark for Chart Grounding & Dense Alignment (Read more on [arXiv](https://arxiv.org/abs/2510.26781) or [HuggingFace](https://huggingface.co/papers/2510.26781))|  | The paper introduces ChartAB, a benchmark for evaluating Vision-Language Models (VLMs) in chart grounding and dense alignment tasks. It aims to address the lack of fine-grained evaluation for VLMs' ability to extract tabular data, localize visualization elements, and recognize attributes from charts. The methodology involves creating a dataset of chart pairs with controlled perturbations in data and attributes and developing a two-stage inference workflow for evaluating alignment capabilities. Experiments reveal weaknesses in VLMs' perception biases and hallucinations, achieving only ~60% accuracy on text weight grounding. The benchmark provides AI practitioners with a tool for identifying and improving specific skills in VLMs for chart understanding, highlighting the need for strengthening fine-grained perception and reasoning. |
| Natural Language Processing | CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction
  Tuning for BabyLMs (Read more on [arXiv](https://arxiv.org/abs/2510.25364) or [HuggingFace](https://huggingface.co/papers/2510.25364))|  | This paper investigates instruction tuning for small-scale language models (LMs). The research explores whether BabyLMs can benefit from conversational and question-answering instruction tuning datasets. The methodology involves training decoder-only models with 100M and 140M parameters, using merged or sequential curricula. Results show instruction tuning yields consistent gains in fine-tuning scenarios, with sequential curricula outperforming merged data, but improvements do not consistently transfer to zero-shot tasks. The findings suggest that instruction tuning can enhance fine-tuning performance in BabyLMs, but with potential trade-offs for zero-shot generalization capabilities. |
