

## Papers for 2025-06-17

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning
  Attention (Read more on [arXiv](https://arxiv.org/abs/2506.13585) or [HuggingFace](https://huggingface.co/papers/2506.13585))| ManTle, windlx, LINMUJIE-judy, enochzhang, sheep33333 | MiniMax-M1 is a large-scale reasoning model leveraging a hybrid Mixture-of-Experts architecture and lightning attention for improved test-time compute efficiency. The paper focuses on scaling test-time compute for large language models by developing a novel architecture and reinforcement learning techniques. It employs a hybrid attention mechanism with both softmax and lightning attention, combined with the CISPO RL algorithm for training. The model achieves comparable or superior performance to other open-weight models like DeepSeek-R1 on benchmarks such as software engineering and long-context tasks, with M1 consuming 25% of the FLOPs compared to DeepSeek R1 at a generation length of 100K tokens. This advancement offers AI practitioners a more efficient foundation for building next-generation language model agents capable of complex reasoning tasks. |
| Multi-Modal | Scientists' First Exam: Probing Cognitive Abilities of MLLM via
  Perception, Understanding, and Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.10521) or [HuggingFace](https://huggingface.co/papers/2506.10521))| Ruoyao Xiao, Xuming He, Yiheng Wang, Yuhao Zhou, WilsonHwang | This paper introduces the Scientists' First Exam (SFE), a benchmark to assess the scientific cognitive abilities of Multimodal Large Language Models (MLLMs). The research aims to evaluate MLLMs' capabilities in scientific signal perception, attribute understanding, and comparative reasoning. The SFE benchmark comprises 830 expert-verified VQA pairs across 66 multimodal tasks in five disciplines. Experiments show that state-of-the-art models like GPT-03 and InternVL-3 achieve only 34.08% and 26.52% accuracy respectively, indicating significant room for improvement. The SFE is expected to facilitate advancements in AI-enhanced scientific discovery by providing a granular tool for evaluating MLLM performance in scientific domains. |
| Natural Language Processing | DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents (Read more on [arXiv](https://arxiv.org/abs/2506.11763) or [HuggingFace](https://huggingface.co/papers/2506.11763))| Zhendong Mao, Xiaorui Wang, Benfeng Xu, IgnoraZ, Ayanami0730 | DeepResearch Bench is introduced as a comprehensive benchmark for evaluating deep research agents (DRAs). The study addresses the lack of systematic evaluation for LLM-based agents by providing 100 PhD-level research tasks across 22 domains.  Evaluation involves two novel methodologies: RACE (Reference-based Adaptive Criteria-driven Evaluation) for report quality, and FACT (Factual Abundance and Citation Trustworthiness) for information retrieval capabilities. Experimental results show Gemini-2.5-Pro Deep Research achieves an average of 111.21 effective citations. The benchmark facilitates the development of more powerful and human-centric AI agent systems. |
| Natural Language Processing | DoTA-RAG: Dynamic of Thought Aggregation RAG (Read more on [arXiv](https://arxiv.org/abs/2506.12571) or [HuggingFace](https://huggingface.co/papers/2506.12571))| Peerawat Rojratchadakorn, Natthapath Rungseesiripak, natnitaract, montholscbx, saksornr | The paper introduces DoTA-RAG, a Retrieval-Augmented Generation system designed for high throughput and large-scale web knowledge indexes. It aims to address the limitations of traditional RAG pipelines, such as high latency and limited accuracy on massive datasets, through a three-stage pipeline involving query rewriting, dynamic routing to specialized sub-indexes, and multi-stage retrieval and ranking. The system achieves an answer correctness score of 1.478, compared to a baseline of 0.752, while maintaining low latency. DoTA-RAG offers a practical solution for deploying RAG in domains requiring fast and reliable access to extensive and evolving knowledge resources, but the exact parameters around data preparation for the baseline is unclear. |
| Computer Vision | Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.13654) or [HuggingFace](https://huggingface.co/papers/2506.13654))| Yuhao Dong, Penghao Wu, Hongming Guo, ruiqiw, shulin16 | The paper introduces Ego-R1, a framework for reasoning over ultra-long egocentric videos. It addresses the challenge of understanding videos spanning days or weeks by leveraging a structured Chain-of-Tool-Thought (CoTT) process orchestrated by a reinforcement learning-trained agent. The key methodology involves dynamically calling specialized perception tools like Hierarchical_RAG, Video LLM, and VLM for temporal retrieval and multi-modal understanding. Evaluations on the Ego-R1 Bench demonstrate that Ego-R1 achieves 46.0% accuracy, outperforming existing approaches. The work's main implication is that dynamic, tool-augmented CoTT reasoning can effectively tackle the unique challenges of understanding ultra-long egocentric videos, extending time coverage to a week. |
| Natural Language Processing | Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves
  Reasoning Efficiency (Read more on [arXiv](https://arxiv.org/abs/2506.08343) or [HuggingFace](https://huggingface.co/papers/2506.08343))| Ranjay Krishna, Zhaoyang Chu, Dongping Chen, Yuanning Feng, Chenlong Wang | The paper introduces NOWAIT, a training-free approach to improve the reasoning efficiency of large language models by removing explicit self-reflection tokens. It examines whether tokens like 'Wait' and 'Hmm' are necessary for advanced reasoning. NOWAIT suppresses the generation of these tokens during inference, leading to shorter chain-of-thought trajectories. Experiments on ten benchmarks demonstrate up to a 27%-51% reduction in CoT trajectory length without significantly compromising model utility. The approach offers a plug-and-play solution for efficient and utility-preserving multimodal reasoning. |
| Multi-Modal | Discrete Diffusion in Large Language and Multimodal Models: A Survey (Read more on [arXiv](https://arxiv.org/abs/2506.13759) or [HuggingFace](https://huggingface.co/papers/2506.13759))| Xinchao Wang, Qi Li, Runpeng Yu | This survey provides a systematic overview of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). The paper aims to categorize representative models and summarize applications across language, vision-language, and biological domains. dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm and achieve up to 10x acceleration in inference speed compared to autoregressive models. The models demonstrate performance comparable to autoregressive counterparts on code and mathematics benchmarks. This work serves as a foundation for future research and development in this fast-evolving area. |
| Natural Language Processing | TaskCraft: Automated Generation of Agentic Tasks (Read more on [arXiv](https://arxiv.org/abs/2506.10055) or [HuggingFace](https://huggingface.co/papers/2506.10055))| Weizhen Li, Weichen Sun, Qianben Chen, Jingyi Cao, Dingfeng Shi | The paper introduces TaskCraft, an automated workflow for generating verifiable agentic tasks that require multi-step problem solving, tool use, and adaptive reasoning. It addresses the scalability limitations of existing instruction data and agentic benchmarks through automated generation. TaskCraft uses depth-based and width-based extensions to create structurally and hierarchically complex challenges, achieving improved prompt optimization and enhanced supervised fine-tuning of agentic foundation models. Empirical results demonstrate performance gains on HotpotQA, Musique, and Bamboogle after fine-tuning with TaskCraft-generated data. The authors release a synthetic dataset of approximately 36,000 tasks to support future research on agent tuning and evaluation. |
| Multi-Modal | VGR: Visual Grounded Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.11991) or [HuggingFace](https://huggingface.co/papers/2506.11991))| Haiyong Jiang, Haochen Wang, Zijiang Kang, bongbohong, stormthunder | The paper introduces VGR, a novel reasoning multimodal large language model enhancing fine-grained visual perception. It addresses the limitation of existing methods that rely heavily on language bias by incorporating visual grounding into the reasoning process. VGR first detects relevant regions in the image and then uses these regions to provide precise answers. Experiments on the LLaVA-NeXT-7B baseline show VGR achieves superior performance, delivering a +4.1 score on MMStar while using only 30% of the image token count. This implies a more efficient and accurate reasoning process through selective visual attention for complex visual reasoning tasks. |
| Natural Language Processing | PersonaFeedback: A Large-scale Human-annotated Benchmark For
  Personalization (Read more on [arXiv](https://arxiv.org/abs/2506.12915) or [HuggingFace](https://huggingface.co/papers/2506.12915))| Yuchen Eleanor Jiang, Tiannan Wang, Dongyi Ding, Chenghao Zhu, Meiling Tao | The paper introduces PersonaFeedback, a large-scale benchmark for evaluating LLMs' personalization capabilities. The primary objective is to directly assess LLMs' ability to generate tailored responses given predefined user personas and queries, decoupling persona inference from personalization assessment. The benchmark comprises 8298 human-annotated test cases across varying difficulty levels, evaluating models using a binary-choice task. Empirical results show that even state-of-the-art LLMs struggle on the hard tier, achieving significantly lower accuracy than on complex reasoning tasks, and RAG systems underperform baseline models. The benchmark, annotation protocols, and evaluation pipeline are made publicly available to facilitate future research on LLM personalization. |
| Natural Language Processing | From Real to Synthetic: Synthesizing Millions of Diversified and
  Complicated User Instructions with Attributed Grounding (Read more on [arXiv](https://arxiv.org/abs/2506.03968) or [HuggingFace](https://huggingface.co/papers/2506.03968))| Zhendong Mao, Xiaorui Wang, Benfeng Xu, IgnoraZ | This paper presents a method for synthesizing large-scale, diverse, and complex instruction data for aligning large language models (LLMs). The research aims to address the limitations of existing synthetic instruction generation methods by incorporating attributed grounding. The approach involves a top-down attribution process grounding real instructions to situated users and a bottom-up synthesis process leveraging web documents to generate situations and instructions. The method constructs a dataset of 1 million instructions, SYNTHQUESTIONS, and shows leading performance on benchmarks, with model improvements scaling with web corpora. Fine-tuning on SYNTHQUESTIONS achieves comparable results to models trained with 10x more data and preference training, implying the effectiveness of attributed grounding for improving instruction following capabilities. |
| Computer Vision | Test3R: Learning to Reconstruct 3D at Test Time (Read more on [arXiv](https://arxiv.org/abs/2506.13750) or [HuggingFace](https://huggingface.co/papers/2506.13750))| Xinchao Wang, Xingyi Yang, Shizun Wang, Yuheng Yuan, florinshum | The paper introduces Test3R, a test-time learning technique to improve 3D reconstruction by maximizing cross-pair consistency. The research aims to address limitations in pairwise pointmap prediction, which restricts global geometric consistency in dense matching methods like DUSt3R. Test3R optimizes a network at test time using a self-supervised objective that aligns reconstructions from image triplets relative to a common image. Experiments demonstrate Test3R significantly outperforms state-of-the-art methods, achieving improved accuracy and completion rates compared to vanilla DUSt3R (e.g., in the 7Scenes dataset, accuracy improved from 0.146 to 0.105). This near cost-free method can be easily applied to existing models and implemented with minimal overhead, improving their 3D reconstruction quality. |
| Multi-Modal | BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning
  with Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.07961) or [HuggingFace](https://huggingface.co/papers/2506.07961))| Xiangnan Wu, Xiao Ma, Hongtao Wu, Yixiang Chen, LPY | The paper introduces BridgeVLA, a novel 3D vision-language-action model for efficient 3D manipulation learning. It aims to improve sample efficiency by leveraging 3D data and pre-trained vision-language models. The key methodology involves projecting 3D inputs to multiple 2D images and utilizing 2D heatmaps for action prediction, aligning input and output spaces. BridgeVLA achieves an average success rate of 88.2% on RLBench, outperforming state-of-the-art methods. This input-output alignment strategy provides AI practitioners a pathway to develop more data-efficient 3D manipulation models by exploiting spatial information. |
| Natural Language Processing | Language Surgery in Multilingual Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.12450) or [HuggingFace](https://huggingface.co/papers/2506.12450))| Muhammad Ilham Ghozali, samuel-cahyawijaya, tackhwa, muhammadravi251001, joanitolopo | This paper explores representation alignment in multilingual LLMs and its implications for language control. The research aims to disentangle language-specific and language-agnostic information to improve cross-lingual performance. They empirically validate this alignment, propose Inference-Time Language Control (ITLC) leveraging latent injection, and demonstrate its effectiveness in cross-lingual language control and mitigating language confusion. Experiments show ITLC achieves strong cross-lingual control while preserving semantic integrity, and alleviates language confusion in current LLMs. The work provides a practical solution for enhancing LLM cross-lingual capabilities without significant semantic degradation. |
| Other | AI Agent Behavioral Science (Read more on [arXiv](https://arxiv.org/abs/2506.06366) or [HuggingFace](https://huggingface.co/papers/2506.06366))| Honglin Zhang, Haoye Chai, Yunke Zhang, Lin Chen, JJ-TMT | This paper introduces AI Agent Behavioral Science, a paradigm emphasizing the systematic observation and theory-guided interpretation of AI agent behavior in context. It examines how AI agents act, adapt, and interact in individual, multi-agent, and human-agent interaction scenarios. The methodology involves synthesizing research to observe, measure, theorize, and adapt behaviors across various settings, re-framing fairness, safety, and other principles as behavioral properties. The paper synthesizes a growing body of work to highlight behavioral patterns that can be observed, measured, theorized, and adapted, which indicates new possibilities to achieve responsible AI. The study suggests that AI practitioners should adopt a behavioral lens to understand, evaluate, and govern real-world AI system behavior. |
| Machine Learning | ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm
  Engineering (Read more on [arXiv](https://arxiv.org/abs/2506.09050) or [HuggingFace](https://huggingface.co/papers/2506.09050))| Kensho Aoki, Yoichi Iwata, Kohki Horie, Yuki Imajuku, iwiwi | ALE-Bench is introduced as a benchmark for evaluating AI systems in algorithm engineering for computationally hard optimization problems. The research aims to assess AI's capabilities in iterative solution refinement over long time horizons, drawing on tasks from AtCoder Heuristic Contests. The methodology involves an interactive agent framework supporting test-run feedback and visualizations, evaluated using frontier LLMs and a custom ALE-Agent. Results show that while LLMs perform well on specific problems, a gap remains compared to humans in consistency and long-horizon problem-solving; for example, o4-mini-high achieved a performance score of 1520 in an iterative refinement setting. ALE-Bench serves as a valuable tool for fostering future AI advancements in complex optimization challenges. |
| Machine Learning | LETS Forecast: Learning Embedology for Time Series Forecasting (Read more on [arXiv](https://arxiv.org/abs/2506.06454) or [HuggingFace](https://huggingface.co/papers/2506.06454))| Yin Li, Nada Magdi Elkordi, Satya Sai Srinath Namburi GNVV, viswa-98, alphaomeaga | The paper introduces DeepEDM, a novel framework integrating dynamical systems modeling with deep learning for time series forecasting. DeepEDM addresses the limitation of existing deep learning approaches by explicitly modeling the underlying system dynamics using time-delayed embeddings and kernel regression. The model leverages softmax attention for efficient computation and allows for accurate multi-step prediction. Experiments on synthetic and real-world datasets demonstrate that DeepEDM outperforms state-of-the-art methods in forecasting accuracy, with results showing a significantly lower Mean Squared Error (MSE) in noisy environments. DeepEDM provides a robust and accurate forecasting tool applicable to practitioners working with complex, nonlinear time series data. |
| Natural Language Processing | Supernova Event Dataset: Interpreting Large Language Model's Personality
  through Critical Event Analysis (Read more on [arXiv](https://arxiv.org/abs/2506.12189) or [HuggingFace](https://huggingface.co/papers/2506.12189))| Ioana Ciucă, pranavAL2109 | This paper introduces the Supernova Event Dataset to interpret LLM personality through critical event analysis. The research objective is to benchmark LLMs on the subjective task of extracting and ranking key events from diverse text sources. The methodology involves evaluating LLMs (Phi-4, Orca 2, Qwen 2.5, Claude 3.7, Gemini 2.5, OpenAI o3) and using another LLM as a judge to infer personality traits. The analysis reveals distinct personality traits; for instance, Orca 2 demonstrates emotional reasoning, while Qwen 2.5 displays analytical style. The main implication is enhanced model interpretability and user-friendliness for diverse applications through understanding LLM decision-making. |
| Machine Learning | Forecasting Time Series with LLMs via Patch-Based Prompting and
  Decomposition (Read more on [arXiv](https://arxiv.org/abs/2506.12953) or [HuggingFace](https://huggingface.co/papers/2506.12953))| Anish Gupta, Sri Harsha Vardhan Prasad Jella, Anshul Vemulapalli, Mayank Bumb, Franck-Dernoncourt | The paper introduces PatchInstruct, a prompt-based framework for time series forecasting using LLMs without fine-tuning or complex architectures. It addresses the challenge of efficient and accurate time series prediction by employing patch-based tokenization and time series decomposition. The method involves prompting LLMs with time series data decomposed into patches and structured natural language instructions. PatchInstruct demonstrates superior forecasting accuracy, achieving almost always top forecasting accuracy on small values of horizons (at most 12), in regards to MSE/MAE, while reducing inference overhead by 10x-100x compared to baselines in datasets like Weather and Traffic. This lightweight approach enables scalable and domain-adaptable time series forecasting. |
| Multi-Modal | MS4UI: A Dataset for Multi-modal Summarization of User Interface
  Instructional Videos (Read more on [arXiv](https://arxiv.org/abs/2506.12623) or [HuggingFace](https://huggingface.co/papers/2506.12623))| Jiuxiang Gu, Seunghyun Yoon, Hao Tan, Yuan Zang, Franck-Dernoncourt | The paper introduces MS4UI, a new dataset for multi-modal summarization of User Interface (UI) instructional videos. It addresses the need for step-by-step executable instructions, crucial for UI learning, which are lacking in existing video summarization benchmarks. The methodology involves collecting and annotating 2,413 UI instructional videos for video segmentation, text summarization, and video summarization. Experimental results show that current state-of-the-art multi-modal summarization methods struggle on the MS4UI dataset. This dataset enables the AI community to address the need for more specialized methods suitable for UI instructional videos. |
| Natural Language Processing | Profiling News Media for Factuality and Bias Using LLMs and the
  Fact-Checking Methodology of Human Experts (Read more on [arXiv](https://arxiv.org/abs/2506.12552) or [HuggingFace](https://huggingface.co/papers/2506.12552))| Preslav Nakov, Maha Tufail Agro, Dilshod Azizov, Zain Muhammad Mujahid | This paper introduces a novel methodology for profiling news media outlets' factuality and political bias using Large Language Models (LLMs). It addresses the challenge of automatically assessing news source reliability by emulating expert fact-checkers' criteria via custom prompts to elicit LLMs' responses, which are then aggregated for prediction. Experiments using BERT-based models fine-tuned on LLM-generated data achieved 93.5% accuracy in political bias prediction, outperforming baselines. The approach provides a scalable method for characterizing news outlets, aiding AI practitioners in developing tools for mitigating mis- and disinformation. |
| Machine Learning | SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification
  and LLM Assistance (Read more on [arXiv](https://arxiv.org/abs/2506.09968) or [HuggingFace](https://huggingface.co/papers/2506.09968))| Weiyang He, Haoyue Zheng, Ziyan Wang, Yuqing Sun, Owenngt | The paper presents SRLAgent, a Minecraft-based system enhanced by gamification and LLM assistance, to improve college students' self-regulated learning (SRL) skills. It investigates how embedding SRL scaffolding and real-time AI support in a gamified environment fosters goal setting, strategy execution, and self-reflection. A between-subjects study with 59 college students showed significant improvements in SRL skills within the SRLAgent group (p < .001, Cohen's d = 0.234) compared to baseline systems. The results suggest that embedding SRL scaffolding and AI support within gamified environments can enhance deeper learning and metacognitive skill development. The main implication is a design methodology for interactive AI educational technologies to promote student learning and metacognitive skill development. |
| Natural Language Processing | Incorporating Domain Knowledge into Materials Tokenization (Read more on [arXiv](https://arxiv.org/abs/2506.11115) or [HuggingFace](https://huggingface.co/papers/2506.11115))| SangKeun Lee, SungHo Kim, Junho Kim, Jun-Hyung Park, yerim0210 | The paper introduces MATTER, a novel tokenization framework for materials science that integrates domain knowledge. It addresses the issue of excessive fragmentation and semantic loss caused by typical frequency-centric tokenization methods. MATTER uses a material concept identifier (MatDetector) and a re-ranking method to prioritize material-related subwords during token merging. Experiments show that MATTER outperforms existing tokenization methods, achieving an average performance gain of 4% on generation tasks and 2% on classification tasks. This framework offers AI practitioners a domain-specific tokenization strategy that preserves the structural and semantic integrity of material concepts, leading to improved performance in downstream materials science tasks. |
| Natural Language Processing | Steering LLM Thinking with Budget Guidance (Read more on [arXiv](https://arxiv.org/abs/2506.13752) or [HuggingFace](https://huggingface.co/papers/2506.13752))| Chuang Gan, Yang Zhang, Wenshuo Zhao, Junyan Li | This paper introduces budget guidance, a method for controlling the reasoning length of large language models (LLMs) without fine-tuning. It aims to balance performance and inference costs by steering LLMs toward a target thinking budget. The approach uses a lightweight predictor that models a Gamma distribution over remaining thinking length to guide generation. Experiments on MATH-500 show a 26% accuracy gain under tight budgets compared to baseline methods, while using only 63% of the full-thinking model's tokens. Budget guidance enables natural control of thinking length and significant token efficiency improvements. |
| Computer Vision | Uncertainty-Aware Remaining Lifespan Prediction from Images (Read more on [arXiv](https://arxiv.org/abs/2506.13430) or [HuggingFace](https://huggingface.co/papers/2506.13430))| Barbara Hammer, Philip Kenneweg, TristanKe | The paper presents a method for predicting remaining lifespan from images using pretrained vision transformers with uncertainty quantification. It investigates how accurately lifespan can be estimated from facial and whole-body images and models predictive uncertainty as a Gaussian distribution. The method achieves state-of-the-art results, with a mean absolute error (MAE) of 7.48 years on a legacy dataset and improved MAEs of 4.79 and 5.07 years on newly curated datasets, with an average calibration error of 0.62 years. The findings suggest the potential for extracting medically relevant information from images, but the authors acknowledge the limitations of the dataset and emphasize that it's a proof of concept, not a clinically relevant tool. |
| Natural Language Processing | Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging
  Unsubstantiated Claims and Ambiguous Pronouns (Read more on [arXiv](https://arxiv.org/abs/2506.13172) or [HuggingFace](https://huggingface.co/papers/2506.13172))| PChemGuy | This paper explores using Large Language Models (LLMs) for high-level semantic and linguistic analysis of scholarly manuscripts, focusing on unsubstantiated claims and ambiguous pronouns. The main objective is to assess the LLM's ability to mimic human hierarchical reasoning in these tasks. A structured workflow with prompts guides LLMs like Gemini Pro 2.5 Pro and ChatGPT Plus 03. Results indicate significant performance divergence; for example, ChatGPT failed to identify unsubstantiated adjectival modifiers (0% success) that Gemini correctly flagged (95% success). The study implies that structured prompting is viable for complex textual analysis but requires model-specific testing due to task and context dependencies. |
| Multi-Modal | QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety (Read more on [arXiv](https://arxiv.org/abs/2506.12299) or [HuggingFace](https://huggingface.co/papers/2506.12299))| Yunho Maeng, Soo Yong Kim, Hyoungseo Cho, Jeonghwa Yoo, Taegyeong Lee | The paper introduces QGuard, a zero-shot safety guard method for multi-modal LLMs to block harmful prompts. It addresses the challenge of protecting LLMs from malicious attacks without fine-tuning by utilizing question prompting. The methodology involves categorizing harmful prompts, generating guard questions, extracting logits, and applying a PageRank algorithm for filtering. The model achieves competitive performance on text-only and multi-modal harmful datasets, demonstrating its effectiveness in identifying malicious content. The approach enables white-box analysis and provides a practical solution for mitigating security risks in real-world LLM services. |
| Computer Vision | EgoPrivacy: What Your First-Person Camera Says About You? (Read more on [arXiv](https://arxiv.org/abs/2506.12258) or [HuggingFace](https://huggingface.co/papers/2506.12258))| Xiaojun Shan, Yi Li, Jiacheng Cheng, Genpei Zhang, Yijiang Li | This paper introduces EgoPrivacy, a benchmark for evaluating privacy risks in egocentric videos. The study aims to quantify how much private information about a camera wearer can be inferred from their first-person view videos. A novel retrieval-augmented attack (RAA) is proposed that uses ego-to-exo retrieval to enhance demographic privacy attacks. Experiments show foundation models can compromise wearer privacy even in zero-shot settings, achieving 70-80% accuracy in recovering attributes like identity, scene, gender, and race. The benchmark and attack strategies can aid AI practitioners in developing privacy-preserving techniques for wearable cameras. |
| Natural Language Processing | Hatevolution: What Static Benchmarks Don't Tell Us (Read more on [arXiv](https://arxiv.org/abs/2506.12148) or [HuggingFace](https://huggingface.co/papers/2506.12148))| Albert Meroño-Peñuela, Yulan He, Barbara McGillivray, Chiara Di Bonaventura | This paper investigates the reliability of static hate speech benchmarks in the context of evolving language. The central question explores how static benchmarks correlate with evolving language patterns in hate speech detection. The study evaluates 20 language models across two experiments: time-sensitive shifts and vocabulary expansion, using metrics like time-sensitive macro F1 and counterfactual invariance. Results show language models exhibit volatility over time, with label flip rates exceeding 10% in some cases, indicating a misalignment between static benchmarks and evolving hate speech. The findings suggest a need for time-sensitive benchmarks to accurately evaluate language model safety in the hate speech domain. |
