

## Papers for 2025-06-04

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning
  Capabilities of VLMs (Read more on [arXiv](https://arxiv.org/abs/2505.24120) or [HuggingFace](https://huggingface.co/papers/2505.24120))| xuchensong, rockman24, jiangbopei, shawn0wang, qiuwj | The paper introduces CSVQA, a novel Chinese multimodal benchmark for evaluating STEM reasoning in VLMs. It aims to address the lack of scientific reasoning evaluation in existing multimodal benchmarks by creating a domain-specific VQA dataset. The methodology involves curating 1,378 question-answer pairs across STEM disciplines, each requiring domain knowledge and visual evidence integration. Evaluation of 15 VLMs reveals that even the top proprietary model achieves only 49.6% accuracy, highlighting significant room for improvement. The CSVQA benchmark is released to promote further research into scientific reasoning capabilities of VLMs. |
| Computer Vision | UniWorld: High-Resolution Semantic Encoders for Unified Visual
  Understanding and Generation (Read more on [arXiv](https://arxiv.org/abs/2506.03147) or [HuggingFace](https://huggingface.co/papers/2506.03147))| Yuwei Niu, Xinhua Cheng, Zongjian Li, BestWishYsh, LanguageBind | The paper introduces UniWorld, a unified generative framework for image perception and manipulation tasks. The main objective is to explore visual feature integration in unified generative models, hypothesizing that semantic encoders, rather than VAEs, are more suitable. UniWorld leverages pre-trained visual-language models and contrastive semantic encoders, achieving competitive performance on image editing benchmarks like ImgEdit-Bench, outperforming BAGEL with only 2.7M training samples compared to BAGEL's 2665M samples. UniWorld achieves a score of 3.37 on the ImgEdit-Bench overall, closest to GPT-4o among open-source alternatives. The framework offers a more efficient and effective approach to building unified models for visual understanding and generation, benefiting AI practitioners by providing a strong, open-source baseline. |
| Multi-Modal | VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in
  Multi-Agent Environments (Read more on [arXiv](https://arxiv.org/abs/2506.02387) or [HuggingFace](https://huggingface.co/papers/2506.02387))| Xinlei Chen, Xiangmin Yi, Zhexuan Xu, HuiningYuan, zelaix | The paper introduces VS-BENCH, a multimodal benchmark for evaluating Vision Language Models (VLMs) in strategic reasoning and decision-making within multi-agent environments. It aims to address the limitations of existing benchmarks that focus on single-agent or text-only settings by providing vision-grounded environments. The methodology involves evaluating VLMs both offline using next-action prediction accuracy and online using normalized episode return across eight diverse environments. Results show a significant gap between current VLMs and optimal performance, with the best models achieving 47.8% prediction accuracy and 24.3% normalized return. VS-BENCH serves as a platform to evaluate VLMs for strategy and decision-making in complex multi-agent environments. |
| Multi-Modal | OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for
  Vision Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.03135) or [HuggingFace](https://huggingface.co/papers/2506.03135))| Xinqiang Yu, Wenyao Zhang, Shaochen Zhang, Mengdi Jia, qizekun | The paper introduces OmniSpatial, a new comprehensive benchmark for evaluating spatial reasoning capabilities of vision-language models (VLMs). It aims to address the limitations of existing benchmarks that focus on basic spatial understanding by providing a more challenging and diverse set of spatial reasoning tasks grounded in cognitive psychology. OmniSpatial comprises over 1.5K question-answer pairs across four major categories: dynamic reasoning, complex spatial logic, spatial interaction, and perspective-taking. Experiments reveal that current VLMs, including both open- and closed-source models, exhibit significant limitations in comprehensive spatial understanding, achieving a peak accuracy of only 57%. The benchmark enables practitioners to identify and address the spatial reasoning deficiencies in VLMs, potentially leading to improved performance in embodied AI and robotics. |
| Multi-Modal | Visual Embodied Brain: Let Multimodal Large Language Models See, Think,
  and Control in Spaces (Read more on [arXiv](https://arxiv.org/abs/2506.00123) or [HuggingFace](https://huggingface.co/papers/2506.00123))| Guanzhou Chen, Gen Luo, robot-haonan, Cusyoung, ganlinyang | The paper introduces Visual Embodied Brain (VeBrain), a unified framework enabling multimodal large language models (MLLMs) to perceive, reason, and control real-world robotic tasks. It addresses the challenge of integrating multimodal understanding, visual-spatial reasoning, and physical interaction capabilities in MLLMs for embodied agents. VeBrain reformulates robotic control into text-based MLLM tasks, using keypoint detection and embodied skill recognition in 2D visual space, and introduces a novel robotic adapter to convert textual control signals into motion policies. The framework is trained on VeBrain-600k, a high-quality instruction dataset, and achieves a +5.6% gain on MMVet compared to Qwen2.5-VL. This approach allows MLLMs to be effectively deployed in robotic applications, enhancing their adaptability and compositional abilities. |
| Multi-Modal | SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis (Read more on [arXiv](https://arxiv.org/abs/2506.02096) or [HuggingFace](https://huggingface.co/papers/2506.02096))| Hang Yan, Zichen Liu, Xiangyan Liu, Jinjie Ni, Jakumetsu | The paper introduces SynthRL, a pipeline for scaling visual reasoning by synthesizing verifiable data for reinforcement learning. It investigates improving vision-language model (VLM) performance via synthesized RL data with correctness and distribution guarantees. SynthRL selects seed questions, augments them into challenging variants, and verifies correctness and difficulty. Experiments on the MMK12 dataset, SynthRL synthesizes 3.3K verifiable questions, leading to consistent gains on out-of-domain benchmarks, achieving 58.0% average accuracy compared to 57.0% baseline on the 8K data scale. SynthRL enhances VLM reasoning by providing a scalable, data-centric approach with automated data synthesis. |
| Multi-Modal | MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal
  LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.01674) or [HuggingFace](https://huggingface.co/papers/2506.01674))| Rui Xie, Kepan Nan, Tiehan Fan, Yipeng Du, yingtai | The paper introduces MotionSight, a zero-shot method to improve fine-grained video motion understanding in multimodal LLMs by decoupling object and camera motion. The research addresses the limitation of existing MLLMs in capturing subtle inter-frame dynamics by using object-centric visual spotlight and motion blur as visual prompts. They curated MotionVid-QA, a large-scale dataset containing 40K video clips and 87K QAs for fine-grained video motion understanding with SFT and preference data. Experiments show MotionSight achieves state-of-the-art open-source performance, improving category average by 3.4% on MotionBench when using Qwen2.5VL as the backbone. This approach enables more effective modeling of nuanced inter-frame dynamics in video, potentially improving performance in tasks requiring detailed motion analysis. |
| Computer Vision | Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate
  Video Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2506.03065) or [HuggingFace](https://huggingface.co/papers/2506.03065))| Maosen Zhao, Xianfang Zeng, skicy, wchengad, PengtaoChen | The paper introduces Sparse-vDiT, a method for accelerating Video Diffusion Transformers (vDiT) by exploiting sparsity in attention mechanisms. It aims to reduce the computational cost of 3D full-attention vDiT models while maintaining generation quality by identifying and leveraging recurring sparsity patterns in attention maps. The proposed method includes pattern-optimized sparse kernels and an offline sparse diffusion search algorithm to select the optimal sparse computation strategy, achieving up to 1.85x inference speedup on HunyuanVideo. Sparse-vDiT effectively balances computational efficiency and generation quality, leading to improved video synthesis. This approach allows for more efficient deployment of vDiT models in long video synthesis tasks by systemically using structural sparsity. |
| Multi-Modal | GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents (Read more on [arXiv](https://arxiv.org/abs/2506.03143) or [HuggingFace](https://huggingface.co/papers/2506.03143))| Jianwei Yang, vyokky, Ray2333, cckevinn, qianhuiwu | The paper introduces GUI-Actor, a VLM-based method for coordinate-free visual grounding in GUI agents. It aims to address limitations of coordinate generation approaches by aligning a dedicated <ACTOR> token with relevant visual patches using an attention mechanism. GUI-Actor proposes multiple action regions and selects the most plausible using a grounding verifier. Experiments show GUI-Actor-7B achieves 40.7 on ScreenSpot-Pro with Qwen2-VL, outperforming UI-TARS-72B (38.1) with fewer parameters. This implies that GUI-Actor effectively endows VLMs with GUI grounding capabilities without compromising general strengths, offering a more robust and generalizable approach. |
| Computer Vision | Native-Resolution Image Synthesis (Read more on [arXiv](https://arxiv.org/abs/2506.03131) or [HuggingFace](https://huggingface.co/papers/2506.03131))| Yiyuan Zhang, Wanli Ouyang, Xiangyu Yue, Lei Bai, GoodEnough | The paper introduces Native-resolution Image Synthesis, a novel generative modeling paradigm enabling image synthesis at arbitrary resolutions and aspect ratios. It addresses the limitations of fixed-resolution image generation by natively handling variable-length visual tokens. The methodology involves a Native-resolution diffusion Transformer (NiT) that explicitly models varying resolutions and aspect ratios, achieving state-of-the-art performance on both ImageNet-256 × 256 and 512 × 512 benchmarks with a single model achieving a FID of 1.45 on the 512x512 benchmark. The results indicate the potential for native-resolution modeling to bridge visual generative modeling and advanced LLM methodologies. This capability extends far beyond conventional fixed-resolution, square-image generation, demonstrating strong generalization. |
| Computer Vision | AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2506.03126) or [HuggingFace](https://huggingface.co/papers/2506.03126))| Ying Shan, Yixiao Ge, Yuying Ge, liyz, qiulu66 | The paper introduces AnimeShooter, a new reference-guided multi-shot animation dataset for video generation. It addresses the lack of reference images and hierarchical annotations in existing datasets for training coherent animation video generation models. The dataset features story-level and shot-level annotations, and a subset includes synchronized audio tracks with descriptions.  Experiments using AnimeShooterGen, a proposed model leveraging MLLMs and diffusion models, demonstrate superior cross-shot visual consistency, achieving higher CLIP scores compared to baselines. The dataset enables further research into coherent animated video generation from references. |
| Reinforcement Learning | Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in
  Robotics (Read more on [arXiv](https://arxiv.org/abs/2506.00070) or [HuggingFace](https://huggingface.co/papers/2506.00070))| Jaehyung Kim, Jinwoo Shin, Huiwon Jang, Sumin Park, Dongyoung Kim | This paper introduces ROBOT-R1, a reinforcement learning framework to enhance embodied reasoning in robotics. The research addresses limitations of supervised fine-tuning (SFT) in robotic control, such as heuristic data construction and catastrophic forgetting. ROBOT-R1 trains LVLMs to predict the next keypoint state for task completion and optimizes reasoning via reinforcement learning, using a multiple-choice question-answering formulation. Experiments show ROBOT-R1 achieves over a 28% improvement in embodied reasoning, outperforming SFT methods and even surpassing GPT-40 on specific low-level reasoning tasks. The method improves generalization, enabling effective transfer to other tasks like manipulation and spatial reasoning. |
| Reinforcement Learning | Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2506.03136) or [HuggingFace](https://huggingface.co/papers/2506.03136))| Mengdi Wang, Ke Shen, Ye Tian, Ling Yang, Yinjie Wang | The paper introduces CURE, a reinforcement learning framework for co-evolving LLM coders and unit testers without ground-truth code supervision. CURE aims to improve LLM coding abilities by enabling the coder and tester to learn from their interaction outcomes. The methodology involves a dedicated reward design to facilitate mutual supervision and continuous improvement. ReasonFlux-Coder models (7B and 14B) trained with CURE improve code generation accuracy by 5.3% and Best of N accuracy by 9.0% on Qwen2.5-Instruct models. CURE can also serve as an effective reward model for RL fine-tuning, enhancing coding performance without labeled unit tests. |
| Natural Language Processing | DINGO: Constrained Inference for Diffusion LLMs (Read more on [arXiv](https://arxiv.org/abs/2505.23061) or [HuggingFace](https://huggingface.co/papers/2505.23061))| Gagandeep Singh, Sasa Misailovic, Shubham Ugare, Debangshu Banerjee, Tarun Suresh | The paper introduces DINGO, a constrained decoding algorithm for diffusion LLMs to enforce user-specified regular expressions on generated text. It addresses the challenge of parallel token prediction in diffusion models, which makes traditional constrained decoding methods ineffective. DINGO uses dynamic programming to ensure output strings maximize probability under the model's distribution while adhering to constraints. Experiments on symbolic math and JSON generation show DINGO achieves up to 68% improvement over unconstrained inference. DINGO offers a way to reliably generate structured outputs from diffusion LLMs, useful for downstream tasks. |
| Computer Vision | RelationAdapter: Learning and Transferring Visual Relation with
  Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2506.02528) or [HuggingFace](https://huggingface.co/papers/2506.02528))| Yin Zhang, Chenglin Li, Yicheng Li, Yiren Song, Yan Gong | RelationAdapter introduces a novel approach to visual prompt-based image editing using Diffusion Transformers, aiming to effectively capture and transfer visual relations from exemplar image pairs to novel query images. The method decouples editing intent extraction with a dual-branch adapter module and integrates an in-context editor using positional encoding cloning for precise alignment.  The study demonstrates significant improvements in understanding and transferring editing intent, leading to enhancements in generation quality and editing performance. Evaluation on a newly introduced Relation252K dataset showed a reduced Mean Squared Error to 0.020 compared to Edit Transfer's 0.043, alongside CLIP-I score increase to 0.905 from 0.827, indicating better pixel-level accuracy and semantic consistency. RelationAdapter offers AI practitioners a data-efficient and effective way to achieve fine-grained image editing with limited training samples. |
| Multi-Modal | FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation (Read more on [arXiv](https://arxiv.org/abs/2505.24714) or [HuggingFace](https://huggingface.co/papers/2505.24714))| Jinsheng Huang, Xiao Luo, chunfenri, alan1027, luojunyu | The paper introduces FINMME, a novel benchmark dataset for evaluating multimodal large language models (MLLMs) in the financial domain. It aims to address the lack of specialized evaluation datasets for financial MLLMs, focusing on high data quality and comprehensive financial knowledge coverage. The authors created a dataset of over 11,000 financial samples across 18 domains and 6 asset classes, featuring chart images, captions, and QA annotations, further incorporating FinScore to penalize hallucination and provide unbiased results. Experimental results show that even state-of-the-art models like GPT-4o achieve unsatisfactory performance (around 50%), and exhibits high robustness with prediction variations below 1%, highlighting the challenging nature of the benchmark. The FINMME benchmark offers AI practitioners a robust and reliable tool for developing and evaluating MLLMs for complex financial applications. |
| Computer Vision | PCoreSet: Effective Active Learning through Knowledge Distillation from
  Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.00910) or [HuggingFace](https://huggingface.co/papers/2506.00910))| Sung Ju Hwang, Dongseop Kim, Hyungjoon Jang, Dong Bok Lee, Seongjae Kang | This paper introduces ActiveKD, a framework to improve active learning by integrating knowledge distillation (KD) with vision-language models (VLMs). The research aims to address the scarcity of task-specific labeled data in active learning scenarios by leveraging VLMs' zero- and few-shot capabilities. The core methodology involves a novel selection strategy called Probabilistic CoreSet (PCoreSet), which maximizes coverage in the probability space derived from VLM predictions. Evaluation across 11 datasets demonstrates that PCoreSet consistently outperforms existing selection methods, with ActiveKD improving accuracy by 27.33% on ImageNet with random selection. ActiveKD provides an effective strategy to harness the knowledge embedded within VLMs for more data-efficient training of compact, task-specific models. |
| Computer Vision | LumosFlow: Motion-Guided Long Video Generation (Read more on [arXiv](https://arxiv.org/abs/2506.02497) or [HuggingFace](https://huggingface.co/papers/2506.02497))| Jiazheng Xing, Jingyun Liang, Yichen Qian, Hangjie Yuan, Jiahao Chen | The paper introduces LumosFlow, a motion-guided framework for generating temporally coherent and visually compelling long videos. The research addresses the challenge of synthesizing long video sequences by employing a hierarchical approach with explicit motion guidance. LumosFlow utilizes a Large Motion Text-to-Video Diffusion Model (LMTV-DM) for key frame generation, a Latent Optical Flow Diffusion Model (LOF-DM) for motion synthesis, and Motion ControlNet for refinement, achieving 15x interpolation. Experiments demonstrate improved motion smoothness (0.9898) and dynamic degree (0.5700) compared to existing methods. LumosFlow offers AI practitioners a pipeline for controllable long video synthesis with enhanced motion and appearance consistency. |
| Computer Vision | FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2506.01144) or [HuggingFace](https://huggingface.co/papers/2506.01144))| Lior Wolf, Ariel Shaulov, Hila, itayhzn | The paper introduces FlowMo, a training-free inference-time guidance method to enhance temporal coherence in text-to-video diffusion models. FlowMo addresses the research question of improving motion coherence without additional training or explicit conditioning signals. It leverages latent space dynamics by computing and minimizing patch-wise variance of appearance-debiased representations to guide denoising steps. Experiments on Wan2.1 and CogVideoX-5B demonstrate a 6.2% and 5.26% improvement in overall video quality (Final Score), respectively, according to VBench. FlowMo offers AI practitioners an effective plug-and-play solution for enhancing the temporal fidelity of pre-trained video diffusion models. |
| Natural Language Processing | OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for
  Over-Reasoning Mitigation (Read more on [arXiv](https://arxiv.org/abs/2506.02397) or [HuggingFace](https://huggingface.co/papers/2506.02397))| Changwang Zhang, Jiawei Chen, Junjie Wu, jwanglux, Cynthia-1628 | The paper introduces OThink-R1, a novel large reasoning model (LRM) framework designed to mitigate over-reasoning by dynamically switching between fast and slow thinking modes. The primary objective is to enhance the computational efficiency of LRMs without sacrificing accuracy on complex tasks. OThink-R1 utilizes a LLM-Judge to classify reasoning trajectories as either redundant or essential, pruning unnecessary steps and guiding supervised fine-tuning with a dual-reference KL-divergence loss. Experiments on mathematical and question-answering tasks demonstrate an average reduction of 23.4% in generated tokens while maintaining or improving accuracy. This implies that OThink-R1 offers practical guidelines for developing more efficient and adaptable reasoning models in AI applications. |
| Machine Learning | Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and
  Accountability (Read more on [arXiv](https://arxiv.org/abs/2506.01789) or [HuggingFace](https://huggingface.co/papers/2506.01789))| David Anugraha, Genta Indra Winata, cryptexcode, seungone, patrickamadeus | This paper introduces DataRubrics, a framework for assessing the quality of both human- and model-generated datasets. It addresses the lack of standardized, quantifiable methods for evaluating data quality, often overlooked in dataset paper submissions. DataRubrics leverages LLM-based evaluation to provide reproducible and scalable dataset assessment across ten critical dimensions. The paper demonstrates an upward trend in papers including guidelines and quality assurance practices, although there are still gaps for improvement. DataRubrics offers actionable insights for both authors and reviewers to uphold higher standards in data-centric AI research. |
| Multi-Modal | ReFoCUS: Reinforcement-guided Frame Optimization for Contextual
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2506.01274) or [HuggingFace](https://huggingface.co/papers/2506.01274))| Yong Man Ro, Hyunjun Kim, arkimjh, lakelee | The paper introduces ReFoCUS, a reinforcement-guided frame optimization framework for improving contextual understanding in video-LLMs. It addresses the issue of suboptimal frame selection by learning a frame-level policy via reinforcement learning, using rewards derived from a reference LMM. ReFoCUS employs an autoregressive, conditional selection architecture to efficiently explore the frame space and ensure temporal coherence. The approach improves reasoning performance across video QA benchmarks, demonstrating the benefits of aligning frame selection with model-internal utility. For example, the approach increased performance by ~2% on Video-MME. |
| Reinforcement Learning | Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.24726) or [HuggingFace](https://huggingface.co/papers/2505.24726))| Kiran Kamble, Christopher Bryant, Umar Jamil, Shelly Bensal, melisa | The paper explores a method for improving large language models through self-reflection and reinforcement learning. It investigates incentivizing the model to generate better self-reflections when answering incorrectly, even when synthetic data generation is infeasible. The key methodology involves a two-stage framework where the model first generates a self-reflective commentary upon failing a task, then retries the task with the self-reflection in context, rewarding tokens from successful self-reflections using GRPO. Results show substantial performance gains across various model architectures, including a 34.7% improvement in math equation writing and an 18.1% improvement in function calling. This approach provides a pathway for more useful and reliable language models that can self-improve on challenging tasks with limited external feedback. |
| Computer Vision | ORV: 4D Occupancy-centric Robot Video Generation (Read more on [arXiv](https://arxiv.org/abs/2506.03079) or [HuggingFace](https://huggingface.co/papers/2506.03079))| Chongjie Ye, Nan Wang, Shaocong Xu, Bohan Li, gzzyyxy | The paper introduces ORV, a novel framework for generating action-conditioned robot manipulation videos using 4D occupancy as a fine-grained intermediate representation for precise control and strong generalizability. ORV leverages 4D semantic occupancy sequences to provide accurate semantic and geometric guidance for video generation, enabling seamless simulation-to-real transfer and multi-view video synthesis. The core methodology involves an occupancy-centric pipeline integrating physical simulators and generative models with occupancy-derived visual signals for localized supervision. Experimental results demonstrate that ORV outperforms existing baselines in generation quality, achieving a PSNR of 28.258 on the BridgeV2 dataset. The method offers AI practitioners a scalable and fine-grained system for advancing robot video generation and data synthesis, potentially enhancing robot learning and simulation. |
| Computer Vision | FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens (Read more on [arXiv](https://arxiv.org/abs/2506.03096) or [HuggingFace](https://huggingface.co/papers/2506.03096))| Matthias Hein, Nicolas Flammarion, Francesco Croce, chs20 | The paper introduces FuseLIP, a multimodal embedding model that aligns text and images via early fusion of discrete tokens. It aims to improve multimodal encoding by using a single transformer operating on an extended vocabulary of text and image tokens, enabling richer interactions between modalities. FuseLIP outperforms late fusion approaches on tasks like VQA and text-guided image transformation retrieval. Specifically, FuseLIP achieves up to 94.3% accuracy on the CC3M-TGIT dataset, while being comparable on unimodal tasks. The architecture benefits multimodal tasks where visual information is key, and demonstrates that early fusion enhances representation learning. |
| Multi-Modal | Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports
  From Scratch with Agentic Framework (Read more on [arXiv](https://arxiv.org/abs/2506.02454) or [HuggingFace](https://huggingface.co/papers/2506.02454))| Xingyu Liu, Yiyao Wang, Han Wang, Bo Pan, Zhaorui Yang | The paper introduces Multimodal DeepResearcher, an agentic framework for generating text-chart interleaved reports from scratch, addressing the underexplored task of automated visualization within deep research. The primary objective is to design informative visualizations and effectively integrate them into text reports using a Formal Description of Visualization (FDV) to enable LLMs to learn and generate high-quality charts. The framework decomposes the task into researching, exemplar report textualization, planning, and multimodal report generation. Evaluated on the MultimodalReportBench, Multimodal DeepResearcher achieves an 82% overall win rate over a baseline method (DataNarrative) using the same Claude 3.7 Sonnet model. This work offers a novel approach for AI practitioners to generate comprehensive research reports with integrated visualizations, improving readability and practical utility. |
| Natural Language Processing | One Missing Piece for Open-Source Reasoning Models: A Dataset to
  Mitigate Cold-Starting Short CoT LLMs in RL (Read more on [arXiv](https://arxiv.org/abs/2506.02338) or [HuggingFace](https://huggingface.co/papers/2506.02338))| Sunghyun Park, Beong-woo Kwak, Jihyuk Kim, Dongjin Kang, hyungjoochae | This paper introduces the Long CoT Collection, a dataset designed to mitigate the cold-start problem for short Chain-of-Thought (CoT) Large Language Models (LLMs) in Reinforcement Learning (RL). The study aims to construct a long CoT dataset from short CoT LLMs by inducing novel reasoning strategies and controlling the thought budget. The methodology involves creating a pipeline that enables short CoT LLMs to generate longer and more controllable rationales through a step-by-step process, guided by existing LRMs. Experiments demonstrate that models initialized on the Long CoT Collection achieve 2-3x larger gains with RLVR, suggesting an improved foundation for RL. The main implication is that the Long CoT Collection provides a reliable starting point for RL by strengthening general reasoning skills and alleviating the cold-start issue for short CoT LLMs. |
| Natural Language Processing | Accelerating Diffusion LLMs via Adaptive Parallel Decoding (Read more on [arXiv](https://arxiv.org/abs/2506.00413) or [HuggingFace](https://huggingface.co/papers/2506.00413))| Aditya Grover, Guy Van den Broeck, danielmisrael | The paper introduces Adaptive Parallel Decoding (APD) to accelerate diffusion Large Language Models (dLLMs) inference. The research aims to improve the generation speed of dLLMs without sacrificing quality by dynamically adjusting the number of tokens sampled in parallel. APD uses a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. Experiments show APD provides markedly higher throughput (e.g., over 100 tokens per second is possible on GSM8K) with minimal quality degradations on downstream benchmarks. The method offers AI practitioners tunable parameters for flexibly trading off throughput and quality in dLLM inference. |
| Machine Learning | R^2ec: Towards Large Recommender Models with Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.16994) or [HuggingFace](https://huggingface.co/papers/2505.16994))| Wenjie Wang, Xinyu Lin, izhx, tensorslow, dd101bb | The paper introduces R²ec, a unified large recommender model with intrinsic reasoning capabilities. It addresses the limitations of decoupled reasoning modules by interleaving reasoning and recommendation in an autoregressive process. The key methodology involves a reinforcement learning framework called RecPO, which simultaneously optimizes reasoning and recommendation using a fused reward scheme. Experiments on three datasets show R²ec outperforms baselines, achieving a 68.67% relative improvement in Hit@5 and 45.21% in NDCG@20. This work provides a pathway for enhancing recommender systems by tightly integrating reasoning without specialized reasoning annotations. |
| Multi-Modal | MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition
  Query (Read more on [arXiv](https://arxiv.org/abs/2506.03144) or [HuggingFace](https://huggingface.co/papers/2506.03144))| Qi Xu, Xian Wang, Linfeng Li, Yuan Gao, WeiChow | The paper introduces MERIT, a multilingual dataset for interleaved multi-condition semantic retrieval across 5 languages and 7 product categories. It addresses the research question of how to comprehensively measure and improve existing models for such complex queries. The authors propose CORAL, a fine-tuning framework integrating embedding reconstruction and contrastive learning to adapt pre-trained MLLMs.  Experiments demonstrate CORAL achieves a 45.9% performance improvement over conventional approaches on MERIT. The study provides AI practitioners with a novel benchmark and a fine-tuning technique for handling complex, real-world multimodal retrieval scenarios. |
| Natural Language Processing | M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial
  Meeting Understanding Evaluation Dataset (Read more on [arXiv](https://arxiv.org/abs/2506.02510) or [HuggingFace](https://huggingface.co/papers/2506.02510))| Lifan Guo, Xiandong Li, Yalong Wen, Junhui Li, amazingj | The paper introduces M³FinMeeting, a novel multilingual, multi-sector, and multi-task dataset for evaluating financial meeting understanding in LLMs. The research aims to address the limitations of current financial benchmarks by capturing real-world dynamics of financial meetings across English, Chinese, and Japanese. The dataset includes three NLP tasks: summarization, question-answer pair extraction, and question answering, utilizing transcripts from meetings across all 11 GICS sectors. Experimental results using seven LLMs demonstrate significant room for improvement, with Qwen2.5-72B-Instruct achieving overall GPT-4-Judge scores near 70.0, indicating that M³FinMeeting can effectively benchmark comprehension skills for LLMs in financial settings. |
| Computer Vision | QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large
  Language Model Adaptation (Read more on [arXiv](https://arxiv.org/abs/2506.02295) or [HuggingFace](https://huggingface.co/papers/2506.02295))| Omar Elshehy, Mahmoud Reda, Abdelakreem Elkhateb, Omer Nacar, oddadmix | The paper presents Qari-OCR, a fine-tuned vision-language model for high-fidelity Arabic text recognition. The research aims to improve Arabic OCR accuracy, particularly in handling diacritics and diverse typography, using a multimodal large language model. The methodology involves iterative fine-tuning of a Qwen2-VL-2B-Instruct model on specialized synthetic Arabic datasets. Qari v0.2 achieves state-of-the-art results with a Character Error Rate (CER) of 0.061 on diacritically-rich texts. This work provides AI practitioners with a significantly improved open-source solution for Arabic OCR, enhancing accessibility and research capabilities. |
| Natural Language Processing | Knowing Before Saying: LLM Representations Encode Information About
  Chain-of-Thought Success Before Completion (Read more on [arXiv](https://arxiv.org/abs/2505.24362) or [HuggingFace](https://huggingface.co/papers/2505.24362))| Florian Matthes, yziser, galchechik, anumafzal94 | This paper investigates the predictability of Chain-of-Thought (CoT) success before completion by examining LLM representations. The central question is whether LLMs implicitly encode information about the success of CoT reasoning early in the process. The authors train a probing classifier on top of LLM internal representations at various CoT steps to predict answer correctness.  Results show that the classifier achieves up to 76.4% accuracy even before token generation, outperforming a BERT baseline. The findings suggest opportunities for early stopping or adaptive CoT strategies to optimize computational efficiency. |
| Natural Language Processing | How Much Backtracking is Enough? Exploring the Interplay of SFT and RL
  in Enhancing LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.24273) or [HuggingFace](https://huggingface.co/papers/2505.24273))| Bhuwan Dhingra, Junlin Wang, chenyn66, jamescai20 | This paper investigates the interplay between Supervised Finetuning (SFT) and Reinforcement Learning (RL) in enhancing Large Language Model (LLM) reasoning. The study aims to determine how much backtracking during SFT is optimal for subsequent RL training across eight reasoning tasks. The methodology involves controlled experiments using synthetic datasets with systematically varied backtracking steps to isolate the influence of correctness versus structure. Results indicate that longer CoTs with backtracks generally induce better RL training, and more challenging problems require higher numbers of backtracks in the SFT stage, achieving up to 69.7% accuracy on Countdown. The research suggests that RL prioritizes structural patterns over content correctness, providing insights for designing optimal training strategies for scaling reasoning in LLMs. |
| Computer Vision | Deep Video Discovery: Agentic Search with Tool Use for Long-form Video
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2505.18079) or [HuggingFace](https://huggingface.co/papers/2505.18079))| Bin Li, Jiahao Li, Zongyu Guo, Zhaoyang Jia, Xiaoyi Zhang | The paper introduces Deep Video Discovery (DVD), an agentic search framework for long-form video understanding. It addresses the challenges of extended temporal-spatial complexity in long videos. The DVD agent leverages LLMs and a search-centric toolkit with multi-granular video database to autonomously plan and iteratively extract relevant information. The DVD agent achieves state-of-the-art performance on LVBench, reaching 74.2% accuracy, significantly surpassing prior works. This approach offers a scalable and flexible solution for analyzing and understanding long videos by combining LLM reasoning with targeted tool use. |
| Machine Learning | Revisiting LRP: Positional Attribution as the Missing Ingredient for
  Transformer Explainability (Read more on [arXiv](https://arxiv.org/abs/2506.02138) or [HuggingFace](https://huggingface.co/papers/2506.02138))| Lior Wolf, Hila Chefer, Itamar Zimerman, Yarden Bakish | This paper addresses a gap in transformer explainability by incorporating positional encoding (PE) attribution in Layer-wise Relevance Propagation (LRP). The research question is how to develop theoretically-sound LRP rules that propagate attributions across various positional encoding methods to better explain transformer models. The proposed Positional-Aware LRP (PA-LRP) method reformulates the input space as position-token pairs and introduces specialized LRP rules for different PE techniques. Experiments on fine-tuned classifiers and zero-shot foundation models demonstrate significant performance improvements, achieving up to a 14.5% improvement in AU-MSE scores for generation tasks with LLaMa-2 7B. The PA-LRP method offers a more faithful and comprehensive approach to transformer explainability, enabling practitioners to gain better insights into model behavior. |
| Multi-Modal | Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural
  Understanding and Transcreation (Read more on [arXiv](https://arxiv.org/abs/2506.01565) or [HuggingFace](https://huggingface.co/papers/2506.01565))| Wenyan Li, Shaohuan Cheng, Dongchu Xie, Lutong Yu, Li Zhou | The paper introduces Hanfu-Bench, a multimodal benchmark dataset for cross-temporal cultural understanding and transcreation, focusing on traditional Chinese Hanfu. It investigates the temporal dimension of cultural understanding often overlooked by existing VLMs. The benchmark comprises two tasks: cultural visual understanding via VQA and cultural image transcreation, transforming traditional attire into modern designs. Evaluation of VLMs reveals limitations in capturing cultural nuances; closed VLMs achieve comparable performance to non-experts on VQA, while the best transcreation model achieves only a 42% success rate. The benchmark highlights the need for improved VLMs in temporal cultural understanding and creative adaptation for tasks like cultural heritage preservation and innovative creative applications. |
