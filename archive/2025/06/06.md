

## Papers for 2025-06-06

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Robotics | RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language
  Models for Robotics (Read more on [arXiv](https://arxiv.org/abs/2506.04308) or [HuggingFace](https://huggingface.co/papers/2506.04308))| Shanyu Rong, Yi Han, Cheng Chi, Jingkun An, Zhoues | The paper introduces RoboRefer, a 3D-aware VLM for spatial referring with reasoning in robotics, addressing the limitations of current VLMs in complex 3D scenes. The research focuses on enabling robots to accurately understand and reason about spatially constrained instructions. The methodology involves supervised fine-tuning (SFT) with a dedicated depth encoder and reinforcement fine-tuning (RFT) using metric-sensitive process reward functions. Experiments demonstrate that SFT-trained RoboRefer achieves state-of-the-art spatial understanding, reaching an average success rate of 89.6% on single-step spatial reasoning tasks, and RFT-trained RoboRefer surpasses Gemini-2.5-Pro by 17.4% on a new benchmark. The implication is that RoboRefer enhances the ability of robots to interact with and manipulate objects in complex, real-world environments through explicit reasoning. |
| Computer Vision | SeedVR2: One-Step Video Restoration via Diffusion Adversarial
  Post-Training (Read more on [arXiv](https://arxiv.org/abs/2506.05301) or [HuggingFace](https://huggingface.co/papers/2506.05301))| Meng Wei, Yuxi Ren, Zhijie Lin, Shanchuan Lin, Jianyi Wang | The paper introduces SeedVR2, a one-step diffusion-based video restoration model leveraging adversarial post-training to achieve high-quality results. The primary research objective is to enable efficient high-resolution video restoration using a single forward pass, overcoming the computational cost of multi-step methods. SeedVR2 incorporates an adaptive window attention mechanism and feature matching loss to stabilize training and improve detail recovery. Experiments demonstrate comparable or better performance compared to existing VR approaches with a reported 4x speed improvement over previous diffusion-based models. This provides AI practitioners with a computationally efficient approach to high-resolution video restoration without sacrificing visual quality. |
| Computer Vision | Video World Models with Long-term Spatial Memory (Read more on [arXiv](https://arxiv.org/abs/2506.05284) or [HuggingFace](https://huggingface.co/papers/2506.05284))| Ziwei Liu, Yinghao Xu, Ryan Po, Shuai Yang, Tong Wu | The paper introduces a novel framework to improve the long-term consistency of video world models by incorporating a geometry-grounded long-term spatial memory. It aims to address the issue of scene inconsistency and forgetting in autoregressive video generation models due to limited temporal context windows. The approach integrates short-term working memory with long-term spatial memory represented by a point cloud and episodic memory using historical reference frames, updated through TSDF fusion. Evaluations show improved quality and consistency, achieving a PSNR of 19.10 compared to baseline methods. The research enables AI practitioners to develop video world models with improved long-term memory capabilities, paving the way for more consistent world generation. |
| Natural Language Processing | ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow
  Development (Read more on [arXiv](https://arxiv.org/abs/2506.05010) or [HuggingFace](https://huggingface.co/papers/2506.05010))| Zijiao Wu, Qingli Hu, Yiyu Wang, Xue Yang, imryanxu | ComfyUI-Copilot is introduced as an LLM-powered plugin to enhance the usability and efficiency of ComfyUI for AI-driven art creation. The paper aims to address challenges in ComfyUI such as limited documentation and complex workflow design. It employs a hierarchical multi-agent framework with a central assistant agent and specialized worker agents supported by curated knowledge bases. Experimental results demonstrate accurate node recommendations and accelerated workflow development, with recall rates for workflows and nodes exceeding 88.5%. This system lowers the entry barrier for beginners and enhances workflow efficiency for experienced users in AI art creation. |
| Multi-Modal | Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights (Read more on [arXiv](https://arxiv.org/abs/2506.02865) or [HuggingFace](https://huggingface.co/papers/2506.02865))| Emilien Bir√©, Breno Baldas Skuk, Mathieu Andreux, tonywu71, hamza-hcompany | This paper introduces Surfer-H, a cost-efficient web agent, and Holo1, a collection of open-weight VLMs specialized for web navigation and information extraction. The research focuses on integrating VLMs to perform user-defined web tasks efficiently. Surfer-H uses a policy, localizer, and validator, powered by Holo1, trained on curated web content and agentic data. Surfer-H achieves 92.2% state-of-the-art performance on WebVoyager while maintaining cost-efficiency. The open-sourcing of WebClick and Holo1 model weights offers AI practitioners a new resource for agentic systems development, balancing accuracy and cost. |
| Natural Language Processing | Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers
  for Long Contexts (Read more on [arXiv](https://arxiv.org/abs/2506.05229) or [HuggingFace](https://huggingface.co/papers/2506.05229))| Ivan Oseledets, Yuri Kuratov, Gleb Kuzmin, Ivan Rodkin, Danil Sivtsov | This paper introduces Diagonal Batching to enhance parallelism in Recurrent Memory Transformers (RMTs) for long contexts. The research addresses the sequential execution bottleneck in RMTs by enabling parallel segment processing while preserving recurrence. Diagonal Batching reorganizes layer-segment computations into independent diagonals for concurrent execution. Applied to a LLaMA-1B ARMT model, it achieves a 3.3x speedup over standard LLaMA-1B and a 1.8x speedup over sequential RMT on 131,072-token sequences. The main implication is a more practical and efficient solution for long-context applications using RMTs by reducing inference cost and latency. |
| Computer Vision | VideoREPA: Learning Physics for Video Generation through Relational
  Alignment with Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2505.23656) or [HuggingFace](https://huggingface.co/papers/2505.23656))| Xiangpeng Wan, Fanqing Meng, Shaofeng Zhang, Jiaqi Liao, aHapBean | The paper introduces VideoREPA, a novel framework for enhancing the physical plausibility of text-to-video generation by distilling physics understanding from video foundation models (VFMs). It addresses the limitations of current T2V models in generating physically realistic content by aligning token-level relations between VFMs and VDMs. The key methodology involves a Token Relation Distillation (TRD) loss that leverages spatio-temporal alignment for fine-tuning pre-trained T2V models. Empirical evaluations show that VideoREPA significantly improves the physical commonsense of the baseline method, CogVideoX, achieving a state-of-the-art Physical Commonsense score of 40.1 on VideoPhy (a 24.1% improvement). This framework offers AI practitioners a method to improve the physics understanding of T2V models without relying on explicit physics datasets. |
| Natural Language Processing | Qwen3 Embedding: Advancing Text Embedding and Reranking Through
  Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2506.05176) or [HuggingFace](https://huggingface.co/papers/2506.05176))| Huan Lin, Mingxin Li, Yanzhao Zhang, izhx, thenlper | The paper introduces Qwen3 Embedding, an advancement over GTE-Qwen in text embedding and reranking, based on the Qwen3 foundation models. The research aims to improve text embedding and reranking capabilities, particularly for multilingual understanding and generation. It employs a multi-stage training pipeline, combining large-scale unsupervised pre-training with supervised fine-tuning and model merging. Empirical evaluations show Qwen3 Embedding achieves state-of-the-art results, with Qwen3-8B-Embedding reaching 70.58 on the MTEB Multilingual benchmark. The series, open-sourced under Apache 2.0, provides practitioners with robust models and customizable options for text embedding and reranking. |
| Machine Learning | Aligning Latent Spaces with Flow Priors (Read more on [arXiv](https://arxiv.org/abs/2506.05240) or [HuggingFace](https://huggingface.co/papers/2506.05240))| Ping Luo, Ying Shan, Yixiao Ge, Yuying Ge, liyz | The paper introduces a novel framework for aligning latent spaces with arbitrary target distributions using flow-based generative models as priors. It aims to efficiently align a learnable latent space to a target distribution using a pre-trained flow model. The method uses a fixed flow model to regularize the latent space via an alignment loss derived from the flow matching objective, avoiding expensive likelihood evaluations and ODE solving during optimization. Empirically, the alignment loss landscape closely approximates the negative log-likelihood of the target distribution, validated through large-scale ImageNet generation experiments. This approach provides a computationally tractable method for incorporating rich distributional priors into latent models, paving the way for structured representation learning. |
| Multi-Modal | Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual
  Simulations (Read more on [arXiv](https://arxiv.org/abs/2506.04633) or [HuggingFace](https://huggingface.co/papers/2506.04633))| Yinuo Yang, Zixian Ma, Mahtab Bigverdi, Linjie Li, kuvvi | The paper introduces STARE, a new benchmark for evaluating multimodal models on spatial cognition tasks requiring visual simulation. STARE aims to rigorously assess how well models perform geometric transformations, integrated spatial reasoning, and real-world spatial reasoning tasks. The evaluation reveals that models excel at simpler 2D transformations but struggle with more complex tasks like 3D cube net folding and tangram puzzles, achieving near-random accuracy. Humans achieve near-perfect accuracy but are significantly sped up with intermediate visual simulations, unlike models that may improve or decline inconsistently (ex: tangram puzzles decline using intermediate views for certain models). STARE serves as a critical tool to advance human-level spatial reasoning capabilities in AI. |
| Multi-Modal | SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs (Read more on [arXiv](https://arxiv.org/abs/2506.05344) or [HuggingFace](https://huggingface.co/papers/2506.05344))| Jiwen Lu, Yongming Rao, Jiahui Wang, Zuyan | The paper introduces SparseMM, an approach to accelerate multimodal large language models (MLLMs) by leveraging head sparsity. It investigates attention mechanisms in MLLMs and finds that only a small subset of attention heads actively contribute to visual understanding. SparseMM prioritizes KV-Cache retention for these visually relevant heads using precomputed visual scores, while aggressively compressing other heads. Experiments show SparseMM achieves 1.38x real-time acceleration and 52% memory reduction while maintaining performance parity on efficiency tests. This facilitates more efficient deployment of MLLMs in resource-constrained environments. |
| Multi-Modal | AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual
  Counting for MLLMs (Read more on [arXiv](https://arxiv.org/abs/2506.05328) or [HuggingFace](https://huggingface.co/papers/2506.05328))| Tong Lu, Yicheng Liu, Zhiqi Li, cg1177, lulidong | The paper introduces CG-AV-Counting, a clue-grounded audio-visual counting benchmark for evaluating MLLMs in long videos. The research aims to improve MLLMs' counting capabilities by addressing limitations in existing benchmarks, such as short video duration and lack of clue annotations. The paper proposes AV-Reasoner, a model trained with GRPO and curriculum learning, to generalize counting ability from related tasks. AV-Reasoner achieves state-of-the-art results across multiple benchmarks, demonstrating the effectiveness of reinforcement learning, and reaches a WCS score of 6.35 on the CG-AV Counting dataset. This work provides a comprehensive testbed and methodology for improving and evaluating MLLMs in complex audio-visual counting scenarios. |
| Natural Language Processing | StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence
  Training of LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.03077) or [HuggingFace](https://huggingface.co/papers/2506.03077))| Xiao Li, Lei Zhao, Qijun Luo, Kullpar | StreamBP is a memory-efficient backpropagation method for training language models on long sequences. The research aims to reduce memory costs associated with storing activation values during backpropagation for long sequence training. StreamBP achieves this through a linear decomposition of the chain rule along the sequence dimension, and leverages the causal structure of language models. Empirical results demonstrate that StreamBP scales the maximum sequence length by 2.8 ‚Äì 5.5√ó larger than gradient checkpointing with comparable or less BP time. This enables AI practitioners to train LLMs with longer sequences and potentially larger batch sizes with less memory overhead. |
| Multi-Modal | MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical
  Chain-of-Thought Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.05331) or [HuggingFace](https://huggingface.co/papers/2506.05331))| Shilin Yan, Aojun Zhou, Renrui Zhang, CaraJ, xy06 | This paper introduces MINT-CoT, a method for enabling interleaved visual tokens in mathematical chain-of-thought reasoning. The research aims to enhance mathematical reasoning in multimodal LLMs by incorporating visual information within the reasoning steps. The key methodology involves an Interleave Token, which adaptively selects visual regions of any shape within math figures and a progressive three-stage training strategy. The MINT-CoT-7B model outperforms the baseline by +34.08% on MathVista, demonstrating effective visual interleaved reasoning. MINT-CoT facilitates more effective grounding of visual information in mathematical reasoning tasks, potentially leading to improved performance in multimodal mathematical problem solving. |
| Multi-Modal | VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal
  Understanding in Videos (Read more on [arXiv](https://arxiv.org/abs/2506.05349) or [HuggingFace](https://huggingface.co/papers/2506.05349))| Ming-Hsuan Yang, Muhammad Maaz, Anqi Tang, Abdelrahman Shaker, Hanoona Rasheed | The paper introduces VideoMathQA, a new benchmark for evaluating mathematical reasoning in videos, requiring models to interpret multimodal information including visual content, instructional narratives, and textual data. The main objective is to assess temporal cross-modal reasoning capabilities on videos. The key methodology involves a dataset of 420 annotated real-world video-question pairs spanning ten diverse mathematical domains, featuring detailed reasoning steps. Results show that state-of-the-art multimodal models achieve a 44.8% accuracy (CoT MBin +Sub) suggesting current approaches still struggle. VideoMathQA serves as a valuable tool for advancing the development of AI systems that can effectively reason across modalities in complex, temporally extended environments. |
| Computer Vision | Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2506.05327) or [HuggingFace](https://huggingface.co/papers/2506.05327))| Jia-Wang Bian, Zeyu Zhang, Donny Y. Chen, lhmd, dc-walker | The paper introduces PM-Loss, a novel regularization technique to enhance the geometric quality of feed-forward 3D Gaussian Splatting (3DGS). It addresses the issue of depth discontinuities at object boundaries, which degrade rendering quality in depth map-based 3DGS pipelines. PM-Loss leverages a pre-trained transformer to predict a pointmap, enforcing geometric smoothness, particularly around object boundaries, and uses Chamfer loss for regularization in 3D space. Experiments on RealEstate10K demonstrate that incorporating PM-Loss improves PSNR by over 2 dB. The method's plug-and-play nature and significant rendering improvements make it a valuable tool for training more robust feed-forward 3DGS models. |
| Natural Language Processing | Inference-Time Hyper-Scaling with KV Cache Compression (Read more on [arXiv](https://arxiv.org/abs/2506.05345) or [HuggingFace](https://huggingface.co/papers/2506.05345))| Edoardo M. Ponti, Piotr Nawrot, Konrad Staniszewski, Adrian ≈Åa≈Ñcucki | This paper introduces inference-time hyper-scaling by compressing the key-value (KV) cache in Transformer LLMs to improve reasoning accuracy. The research investigates whether KV cache compression can enhance inference-time scaling by generating longer or more parallel sequences within the same compute budget. The authors propose Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches with minimal training, delaying token eviction to implicitly merge representations and preserve information. Experiments show that DMS enhances Qwen-R1 32B's accuracy by 9.1 points on AIME 24 and improves scores on GPQA and LiveCodeBench. The main implication is that KV cache compression can effectively improve LLM reasoning capabilities under comparable runtime and memory load constraints. |
| Computer Vision | EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an
  Egocentric World? (Read more on [arXiv](https://arxiv.org/abs/2506.05287) or [HuggingFace](https://huggingface.co/papers/2506.05287))| Dian Jiao, Wentong Li, Long Li, Ronghao Dang, CircleRadon | The paper introduces EOC-Bench, a new benchmark for evaluating object-centric embodied cognition in multimodal large language models (MLLMs) within dynamic egocentric scenarios. It aims to address the lack of evaluation regarding dynamic object interactions in existing benchmarks. The methodology involves a meticulously annotated dataset of 3,277 question-answer pairs across past, present, and future temporal dimensions, along with a novel multi-scale temporal accuracy metric. Results show that GPT-4o achieves 61.83% accuracy, but MLLMs exhibit deficiencies in temporal perception. EOC-Bench provides AI practitioners with a tool to advance embodied object cognitive capabilities in MLLMs for more reliable core models in embodied systems. |
| Computer Vision | Language-Image Alignment with Fixed Text Encoders (Read more on [arXiv](https://arxiv.org/abs/2506.04209) or [HuggingFace](https://huggingface.co/papers/2506.04209))| Yi Ma, Yue Zhao, robinwuzy, JingfengY | This paper proposes Language-Image alignment with a Fixed Text encoder (LIFT) for language-image alignment. It investigates whether a pre-trained fixed large language model (LLM) offers a sufficient text encoder for guiding visual representation learning. LIFT trains only the image encoder to align visual representations with text embeddings from a frozen LLM. The results show that LIFT outperforms CLIP in scenarios involving compositional understanding, achieving an average accuracy gain of 7.4% across seven relevant tasks. This suggests that LLM-based text embeddings can effectively guide visual learning, providing an alternative approach to joint training. |
| Computer Vision | FlexPainter: Flexible and Multi-View Consistent Texture Generation (Read more on [arXiv](https://arxiv.org/abs/2506.02620) or [HuggingFace](https://huggingface.co/papers/2506.02620))| Luozhou Wang, Jiantao Lin, Leyi Wu, yingcongchen, StarYDY | FlexPainter is a novel texture generation pipeline enabling flexible multi-modal conditional guidance and consistent multi-view texture generation. The research aims to improve control and consistency in texture generation by constructing a shared conditional embedding space and employing an image-based classifier-free guidance (CFG) method. The methodology incorporates a multi-view image grid representation, view synchronization, and adaptive weighting during diffusion sampling, followed by 3D-aware texture completion and enhancement. Experiments demonstrate a significant outperformance over state-of-the-art methods, achieving better FID scores and user preference compared to other methods. This framework provides AI practitioners with a more flexible and higher-quality solution for generating 3D textures, reducing the need for manual adjustments. |
| Natural Language Processing | The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly
  Licensed Text (Read more on [arXiv](https://arxiv.org/abs/2506.05209) or [HuggingFace](https://huggingface.co/papers/2506.05209))| Stella Biderman, Colin Raffel, Brian Lester, Nikhil Kandpal, storytracer | This paper introduces the Common Pile v0.1, an 8TB dataset comprising public domain and openly licensed text intended for LLM pretraining. It addresses the scarcity of high-quality, permissively licensed data for training large language models (LLMs) in an ethical manner. The authors collected and curated text from 30 diverse sources and validated the data by training two 7B parameter LLMs, Comma v0.1-1T and Comma v0.1-2T, achieving competitive performance with models like Llama 7B. The resulting models demonstrated strong performance, with Comma v0.1-1T outperforming other compute-matched models on standard benchmarks, particularly those focused on knowledge and coding, indicating the potential for training performant LLMs on openly licensed data. |
| Computer Vision | Autoregressive Images Watermarking through Lexical Biasing: An Approach
  Resistant to Regeneration Attack (Read more on [arXiv](https://arxiv.org/abs/2506.01011) or [HuggingFace](https://huggingface.co/papers/2506.01011))| Wenli Huang, Ye Deng, Sanping Zhou, Yiren Song, Siqi Hui | The paper presents Lexical Bias Watermarking (LBW), a novel in-generation watermarking framework for autoregressive image generation models to resist regeneration attacks. LBW embeds watermarks by biasing token selection during image generation toward a predefined green list, ensuring seamless integration with AR models and extending to post-hoc watermarking. To increase security, it samples a green list per image from a pool. Experiments demonstrate that LBW achieves superior robustness, particularly against regeneration attacks, evidenced by an AUC of 0.995 and TPR@1FPR of 0.937 when LBW-Post is applied to RAR. LBW provides a resilient watermarking approach for AR image generation, enhancing traceability and preventing misuse. |
| Natural Language Processing | MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at
  Scale (Read more on [arXiv](https://arxiv.org/abs/2506.04405) or [HuggingFace](https://huggingface.co/papers/2506.04405))| Yue Yu, Yishan Zhong, Yuchen Zhuang, Ran Xu, wshi83 | MedAgentGym is introduced as a publicly available training environment for enhancing coding-based medical reasoning in LLM agents. The research focuses on optimizing LLM agents' coding capabilities for medical reasoning using real-world biomedical scenarios. MedAgentGym comprises 72,413 task instances across 129 categories from 12 authentic real-world biomedical scenarios encapsulated within executable coding environments. Benchmarking over 25 LLMs, Med-Copilot-7B achieves gains through fine-tuning (+36.44%) and reinforcement learning (+42.47%). The environment provides both a benchmark and expandable training resources for developing LLM-based coding assistants. |
| Computer Vision | Geometry-Editable and Appearance-Preserving Object Compositon (Read more on [arXiv](https://arxiv.org/abs/2505.20914) or [HuggingFace](https://huggingface.co/papers/2505.20914))| Liang Lin, Zhijing Yang, Chunmei Qing, Haojie Li, Jianman Lin | The paper introduces a Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) model for general object composition. It addresses the challenge of simultaneously enabling geometric editing and preserving fine-grained appearance details when integrating objects into scenes. The DGAD model leverages semantic embeddings for geometric manipulation and a cross-attention retrieval mechanism to align appearance features. Experiments demonstrate that DGAD achieves superior performance, with a CLIP score of 89.38, in preserving appearance while allowing geometry edits. The model provides AI practitioners with a method to manipulate and composite objects while maintaining visual fidelity. |
| Computer Vision | FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene
  Reconstruction (Read more on [arXiv](https://arxiv.org/abs/2506.05348) or [HuggingFace](https://huggingface.co/papers/2506.05348))| Zhanhua Zhang, Jiaming Sun, Zhen Xu, Peishan Yang, Yifan Wang | The paper introduces FreeTimeGS, a novel 4D Gaussian representation for dynamic scene reconstruction that enables real-time rendering with complex motions. It aims to improve dynamic scene modeling by allowing Gaussian primitives to appear at arbitrary times and locations, assigning each a motion function. The method optimizes Gaussian parameters with rendering and regularization losses and introduces a periodic relocation strategy to mitigate local minima. Experiments on the SelfCap dataset show a PSNR improvement of 2.4dB compared to 4DGS and real-time rendering at 450 FPS with a single RTX 4090 GPU. FreeTimeGS provides AI practitioners with a more flexible and efficient representation for dynamic scene reconstruction, enhancing rendering quality and speed. |
| Computer Vision | Rectified Point Flow: Generic Point Cloud Pose Estimation (Read more on [arXiv](https://arxiv.org/abs/2506.05282) or [HuggingFace](https://huggingface.co/papers/2506.05282))| Iro Armeni, Shuran Song, Shengyu Huang, Liyuan Zhu, Tao Sun | The paper introduces Rectified Point Flow, a unified conditional generative model for pairwise point cloud registration and multi-part shape assembly. It addresses the problem of estimating rigid part poses from unordered 3D point clouds by learning a continuous point-wise velocity field. The method uses a self-supervised encoder pre-trained on overlapping points and achieves state-of-the-art performance on six benchmarks, demonstrating improved accuracy (e.g., achieving Rotation Error of 7.4 degrees on BreakingBad-Everyday). This approach enables effective joint training on diverse datasets and facilitates learning shared geometric priors, offering a more generalizable solution for pose estimation and shape assembly tasks in robotics and computer vision. |
| Natural Language Processing | Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning
  Capabilities Through Evaluation Design (Read more on [arXiv](https://arxiv.org/abs/2506.04734) or [HuggingFace](https://huggingface.co/papers/2506.04734))| Xiaoqi Jian, Yongfu Zhu, Jinzhu Wu, Weihong Lin, lincharliesun | The paper examines the reliability of benchmark evaluations for Large Language Models (LLMs), particularly the Deepseek-R1-Distill series, and reveals significant performance fluctuations due to subtle variations in evaluation conditions. The research investigates how factors like seed initialization, dataset version, instruction position, option bias, and Tensor Parallelism affect evaluation outcomes. Experimental results demonstrate fluctuations exceeding 1 percentage point due to variations in the value of N and highlight performance differences across various models using different evaluation datasets. The study advocates for a more rigorous and transparent evaluation paradigm to avoid misleading claims and improve the reliability of LLM assessments. |
| Multi-Modal | SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video
  Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2506.00830) or [HuggingFace](https://huggingface.co/papers/2506.00830))| Youqiang Zhang, Baoxuan Gu, Hao Jiang, Zhengcong Fei, diqiu7 | SkyReels-Audio is a novel framework for generating high-fidelity talking portrait videos conditioned on multimodal inputs. The paper addresses the challenge of audio-driven avatar behavior modulation and aims to achieve robust synchronization across modalities using a pretrained video diffusion transformer. The methodology involves a hybrid curriculum learning strategy for aligning audio with facial motion, a facial mask loss, and a sliding-window denoising approach. Quantitative evaluations demonstrate superior performance in lip-sync accuracy, identity consistency, and realistic facial dynamics compared to existing methods on the HDTF dataset. The framework enables AI practitioners to generate controllable and temporally coherent talking portraits from diverse inputs, with potential applications in digital storytelling and virtual communication. |
| Natural Language Processing | Contextual Integrity in LLMs via Reasoning and Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2506.04245) or [HuggingFace](https://huggingface.co/papers/2506.04245))| Janardhan Kulkarni, Huseyin A. Inan, wulu, sahar-abdelnabi, Eric-Lan | This paper introduces a novel approach to improve contextual integrity (CI) in large language models (LLMs) by combining reasoning and reinforcement learning. The primary research question is how to enhance LLMs' ability to discern appropriate information disclosure based on context. The methodology involves prompting LLMs to reason explicitly about CI and developing a reinforcement learning framework to further instill reasoning skills. Using a synthetic dataset, the method reduces inappropriate information disclosure while maintaining task performance; improvements on the PrivacyLens benchmark show a reduction in privacy leakage rate by up to 40%. This work implies that combining structured reasoning and reinforcement learning can significantly improve the safety and contextual awareness of LLM-based agents. |
| Natural Language Processing | Micro-Act: Mitigate Knowledge Conflict in Question Answering via
  Actionable Self-Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.05278) or [HuggingFace](https://huggingface.co/papers/2506.05278))| Xiaolong Li, Ge Qu, Bowen Qin, Jinyang Li, NanHUO | The paper introduces MICRO-ACT, a framework to mitigate knowledge conflicts in Retrieval-Augmented Generation (RAG) systems for question answering. It addresses the issue where retrieved external knowledge contradicts a language model's inherent knowledge, negatively impacting QA accuracy. MICRO-ACT employs a hierarchical action space to automatically perceive context complexity and decompose knowledge sources into fine-grained, actionable comparisons. Experiments on five benchmark datasets demonstrated significant accuracy increases over state-of-the-art baselines, particularly in temporal and semantic conflict types, achieving improvements up to 9.4% on ConflictBank and 6.65% on KRE for GPT-40-mini. The framework's ability to resolve knowledge conflicts contributes to more reliable and robust RAG systems for real-world applications. |
| Computer Vision | RobustSplat: Decoupling Densification and Dynamics for Transient-Free
  3DGS (Read more on [arXiv](https://arxiv.org/abs/2506.02751) or [HuggingFace](https://huggingface.co/papers/2506.02751))| Yuan Xiong, Guanying Chen, Kunbin Yao, Yuqi Zhang, fcy99 | The paper introduces RobustSplat, a novel approach for transient-free 3D Gaussian Splatting (3DGS) in dynamic scenes. It addresses the challenge of artifacts caused by transient objects by decoupling densification and dynamics. The key methodology involves a delayed Gaussian growth strategy and a scale-cascaded mask bootstrapping approach. Extensive experiments demonstrate RobustSplat's superior performance, achieving improved PSNR, SSIM and LPIPS metrics over existing methods on challenging datasets. RobustSplat offers AI practitioners a more robust and reliable technique for 3DGS optimization, reducing artifacts and improving rendering quality in dynamic environments. |
| Computer Vision | Diffusion-Based Generative Models for 3D Occupancy Prediction in
  Autonomous Driving (Read more on [arXiv](https://arxiv.org/abs/2505.23115) or [HuggingFace](https://huggingface.co/papers/2505.23115))| Yingshi Liang, Yucheng Mao, Tianyuan Yuan, Yicheng Liu, Yunshen Wang | The paper introduces a diffusion-based generative model for 3D occupancy prediction in autonomous driving to address limitations of discriminative methods. It aims to improve prediction consistency and handle complex 3D structures by learning the underlying data distribution and incorporating 3D scene priors through conditional sampling. The methodology involves framing occupancy prediction as generative modeling using diffusion models. Experimental results show that the proposed method outperforms state-of-the-art discriminative approaches, with a reported mIoU of 43.08 using PanoOcc as a visual encoder, improving over the PanoOcc baseline. This suggests that generative models can enhance the realism and accuracy of occupancy predictions, benefiting downstream planning tasks for autonomous vehicles. |
| Computer Vision | Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric
  Approach (Read more on [arXiv](https://arxiv.org/abs/2506.03238) or [HuggingFace](https://huggingface.co/papers/2506.03238))| Weidi Xie, Yanfeng Wang, Ya Zhang, Lisong Dai, zzh99 | This paper presents a new approach to whole-body CT image interpretation by focusing on abnormality detection and description. The work introduces a comprehensive hierarchical classification system with 404 representative abnormal findings and a new dataset, OminiAbnorm-CT, containing over 14.5K CT images with abnormality grounding annotations. The proposed model, OminiAbnorm-CT, automatically grounds and describes abnormal findings based on text queries, demonstrating significant outperformance over existing methods across various tasks and metrics.  Specifically, it improves the RaTEScore by an average of 2.7 points compared to the best baseline on visual prompted report generation. The research facilitates improved abnormality-centric AI systems by providing a new labeled dataset and a novel model that can robustly interpret text and visual prompts. |
| Computer Vision | BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View
  Representations (Read more on [arXiv](https://arxiv.org/abs/2506.02587) or [HuggingFace](https://huggingface.co/papers/2506.02587))| Konstantinos Karydis, Divyank Shah, Justin Yue, Jerry Li, Yewandou | The paper introduces BEVCALIB, a novel target-less LiDAR-camera calibration method using bird's-eye view (BEV) representations. It aims to improve calibration accuracy by explicitly enforcing geometric constraints during the calibration process. The methodology involves extracting and fusing camera and LiDAR BEV features, followed by a geometry-guided feature selection and refinement process. Evaluations on KITTI and NuScenes datasets show that BEVCALIB outperforms the best baseline by an average of (47.08%, 82.32%) on KITTI dataset in terms of (translation, rotation), respectively. The approach provides AI practitioners with a new state-of-the-art method that enhances multi-modal sensor fusion in autonomous systems. |
| Computer Vision | PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill
  Assessment (Read more on [arXiv](https://arxiv.org/abs/2506.04996) or [HuggingFace](https://huggingface.co/papers/2506.04996))| Antonio Liotta, EdBianchi | The paper introduces Proficiency-Aware Temporal Sampling (PATS) for improved multi-view sports skill assessment. It addresses the disruption of temporal continuity in existing video sampling methods by preserving complete fundamental movements within continuous temporal segments. PATS adaptively segments videos to maximize information coverage while maintaining temporal coherence, enhancing multi-view fusion. Evaluated on the EgoExo4D benchmark, PATS improves state-of-the-art accuracy by +0.65% to +3.05% across viewing configurations. This adaptive temporal sampling approach advances automated skill assessment in real-world applications by capturing more coherent and meaningful temporal patterns. |
| Natural Language Processing | What do self-supervised speech models know about Dutch? Analyzing
  advantages of language-specific pre-training (Read more on [arXiv](https://arxiv.org/abs/2506.00981) or [HuggingFace](https://huggingface.co/papers/2506.00981))| Willem Zuidema, Gaofei Shen, Charlotte Pouw, Hosein Mohebbi, Marianne de Heer Kloots | This paper investigates language-specific speech representations in self-supervised models by analyzing Dutch phonetic and lexical encoding. The study examines the impact of pre-training Wav2Vec2 models exclusively on Dutch compared to English and multilingual data. The methodology involves training and evaluating phone and word identity probes, ABX tasks, clustering, and representational similarity analysis on the SSL-NL dataset. Results show that Dutch-specific pre-training improves Dutch linguistic feature encoding, reflected by enhanced probe accuracy and lower word error rates on ASR tasks. The findings suggest that language-specific pre-training enhances linguistic representation in speech models, which benefits downstream ASR performance, and highlights the importance of data selection and analysis methods. |
