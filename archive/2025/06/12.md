

## Papers for 2025-06-12

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.06395) or [HuggingFace](https://huggingface.co/papers/2506.06395))| Ivan Oseledets, Andrey Kuznetsov, Alexander Zubrey, Matvey Skripkin, LiPengyi29 | This paper introduces Reinforcement Learning via Self-Confidence (RLSC), a novel method for fine-tuning language models. The research aims to improve LLM task alignment using the model's own confidence as a reward signal, eliminating the need for external labels or reward models. RLSC fine-tunes Qwen2.5-Math-7B using only 16 samples per question and 10-20 training steps. The approach achieves significant accuracy improvements, including +13.4% on AIME2024 and +21.2% on MATH500, demonstrating its effectiveness in enhancing reasoning capabilities. RLSC offers a scalable and efficient post-training method for inference models by leveraging self-confidence analysis, potentially streamlining LLM optimization workflows. |
| Computer Vision | Seedance 1.0: Exploring the Boundaries of Video Generation Models (Read more on [arXiv](https://arxiv.org/abs/2506.09113) or [HuggingFace](https://huggingface.co/papers/2506.09113))| Lu Jiang, Weilin Huang, Tuyen Hoang, Haoyuan Guo, Yu Gao | Seedance 1.0 is presented as a high-performance video generation model addressing challenges in prompt following, motion plausibility, and visual quality. The paper aims to improve video generation by integrating multi-source data curation, an efficient architecture, and post-training optimization. The methodology involves a decoupled spatial and temporal architecture with interleaved multimodal positional encoding and a video-tailored RLHF algorithm. Seedance 1.0 achieves 10x inference speedup through multi-stage distillation, generating 5-second 1080p videos in 41.4 seconds on an NVIDIA-L20. The model enables practitioners to generate high-quality videos with improved spatiotemporal fluidity and instruction adherence, offering native support for multi-shot narrative coherence. |
| Computer Vision | PlayerOne: Egocentric World Simulator (Read more on [arXiv](https://arxiv.org/abs/2506.09995) or [HuggingFace](https://huggingface.co/papers/2506.09995))| Fan Wang, Xiang Bai, Xi Chen, Hao Luo, Yuanpeng Tu | The paper introduces PlayerOne, an egocentric realistic world simulator for immersive and unrestricted exploration within dynamic environments. It aims to generate egocentric videos aligned with real-scene human motion captured by an exocentric camera and ensure world consistency. The methodology involves a coarse-to-fine training pipeline, a part-disentangled motion injection scheme, and a joint reconstruction framework for modeling 4D scenes and video frames. Experimental results demonstrate the model's ability to achieve precise control over human movements and world-consistent modeling, achieving superior performance across metrics like DINO-Score (67.8) and CLIP-Score (88.2). This marks a step towards egocentric real-world simulation, enabling fresh applications in world modeling. |
| Computer Vision | Autoregressive Adversarial Post-Training for Real-Time Interactive Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2506.09350) or [HuggingFace](https://huggingface.co/papers/2506.09350))| Yuxi Ren, Jianwen Jiang, Hao He, Ceyuan Yang, Shanchuan Lin | The paper introduces an autoregressive adversarial post-training (AAPT) method for real-time interactive video generation. It aims to transform pre-trained latent video diffusion models into efficient, interactive generators. The method employs adversarial training for autoregressive generation, leveraging KV caching and student-forcing to reduce error accumulation. Experiments show the 8B parameter model achieves 24fps streaming video generation at 736x416 resolution on a single H100 GPU, or 1280x720 on 8xH100 GPUs. The AAPT framework enables practitioners to convert existing video diffusion models into real-time interactive systems. |
| Multi-Modal | ComfyUI-R1: Exploring Reasoning Models for Workflow Generation (Read more on [arXiv](https://arxiv.org/abs/2506.09790) or [HuggingFace](https://huggingface.co/papers/2506.09790))| Weihua Luo, Longyue Wang, Xue Yang, Yiyu Wang, Zhenran Xu | The paper introduces ComfyUI-R1, a large reasoning model for automated workflow generation in ComfyUI. It addresses the challenge of crafting effective workflows in ComfyUI by automating the orchestration of specialized components. The model is trained using a two-stage framework including CoT fine-tuning and reinforcement learning with a rule-metric hybrid reward. Experiments show the model achieves a 97% format validity rate with high pass rate, node-level and graph-level F1 scores. This work enables AI practitioners to more easily generate complex workflows for generative tasks within ComfyUI. |
| Machine Learning | SeerAttention-R: Sparse Attention Adaptation for Long Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.08889) or [HuggingFace](https://huggingface.co/papers/2506.08889))| Yu Cheng, Yuqing Xia, Shijie Cao, Shuming Guo, Yizhao Gao | The paper introduces SeerAttention-R, a sparse attention framework for improving long decoding efficiency in reasoning models. It addresses the challenge of increasing compute and memory demands during autoregressive decoding. The method utilizes a self-distilled gating mechanism to learn attention sparsity without modifying the original model parameters. Evaluated on models like Qwen3 and DeepSeek-R1, SeerAttention-R maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmarks, achieving near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. This enables AI practitioners to deploy more efficient and scalable long-context reasoning models. |
| Machine Learning | SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner (Read more on [arXiv](https://arxiv.org/abs/2506.09003) or [HuggingFace](https://huggingface.co/papers/2506.09003))| Mouxiang Chen, Jian Yang, Min Yang, Jiaxi Yang, Lei Zhang | The paper introduces SWE-Flow, a data synthesis framework for generating software engineering data in a test-driven manner. It aims to address the limitations of existing software engineering datasets by automatically inferring incremental development steps from unit tests. The framework constructs a Runtime Dependency Graph (RDG) to capture function interactions, enabling the generation of structured development schedules and verifiable TDD tasks. Experiments show that fine-tuning open models on data generated by SWE-Flow significantly improves performance in TDD-based coding. Specifically, fine-tuning Qwen2.5-Coder-32B-Instruct on SWE-Flow data led to performance improvements. |
| Computer Vision | InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio
  Conditions (Read more on [arXiv](https://arxiv.org/abs/2506.09984) or [HuggingFace](https://huggingface.co/papers/2506.09984))| Gaojie Lin, Chao Liang, Jianwen Jiang, Jiaqi Yang, Zhenzhi Wang | The paper introduces InterActHuman, a novel framework for multi-concept human animation with layout-aligned audio conditions. It addresses the limitation of existing methods in handling multiple interacting concepts (humans/objects) by enforcing region-specific binding of conditions to each entity's spatiotemporal footprint. The method leverages a mask predictor to infer layout information and injects local audio conditions into corresponding regions for accurate modality matching. Experimental results show that the proposed design achieves improved high-quality controllable multi-concept human-centric videos compared to implicit approaches. Specifically, it achieves a FVD score of 22.881 on a multi-person test set demonstrating effective video synthesis and improved lip-synchronization compared to current state-of-the-art methods. |
| Machine Learning | SAFE: Multitask Failure Detection for Vision-Language-Action Models (Read more on [arXiv](https://arxiv.org/abs/2506.09937) or [HuggingFace](https://huggingface.co/papers/2506.09937))| Haruki Nishimura, Igor Gilitschenski, Shengxiang Sun, Yuanliang Ju, Qiao Gu | The paper introduces SAFE, a multitask failure detection method for vision-language-action models (VLAs). It addresses the challenge of detecting failures in generalist robot policies across diverse and unseen tasks. SAFE leverages VLA internal features to predict the likelihood of task failure, trained on both successful and failed rollouts across multiple tasks. Evaluated on OpenVLA, πο, and πο-FAST, SAFE achieves state-of-the-art failure detection performance with an improved trade-off between accuracy and detection time, with ROC-AUC improved up to 15% compared to baseline methods. This enables safer and more reliable deployment of VLA policies by facilitating early intervention in potential failure scenarios. |
| Machine Learning | Reparameterized LLM Training via Orthogonal Equivalence Transformation (Read more on [arXiv](https://arxiv.org/abs/2506.08001) or [HuggingFace](https://huggingface.co/papers/2506.08001))| Bernhard Schölkopf, Maximilian Dax, Tim Z. Xiao, Simon Buchholz, Zeju Qiu | This paper introduces POET, a reparameterized training algorithm for large language models using Orthogonal Equivalence Transformation to optimize neurons. The research aims to enhance training stability and generalization by preserving spectral properties of weight matrices. POET reparameterizes neurons with learnable orthogonal matrices and a fixed random weight matrix and develops efficient approximations, including stochastic primitive optimization and Cayley-Neumann parameterization, for scalability. Experiments validate that POET achieves improved performance with fewer parameters, demonstrating, for example, a validation perplexity of 13.70 on a 1.3B LLaMA model, better than AdamW's 14.73. The method offers AI practitioners a more parameter-efficient and stable approach to training large language models. |
| Computer Vision | MIRAGE: Multimodal foundation model and benchmark for comprehensive
  retinal OCT image analysis (Read more on [arXiv](https://arxiv.org/abs/2506.08900) or [HuggingFace](https://huggingface.co/papers/2506.08900))| Taha Emre, Ronald Fecso, Emese Sükei, Botond Fazekas, José Morano | The paper introduces MIRAGE, a multimodal foundation model for retinal OCT and SLO image analysis. It aims to develop a robust AI system for comprehensive retinal image understanding, overcoming limitations of existing single-modality models and annotation requirements. MIRAGE utilizes a paired multimodal masked autoencoding approach on a large dataset of OCT/SLO images, significantly outperforming state-of-the-art models in both classification (84.01% balanced accuracy) and segmentation tasks. The model's superior performance highlights its suitability as a foundation for developing robust AI systems in retinal OCT image analysis, offering improved generalizability and reduced reliance on labeled data for AI practitioners. |
| Machine Learning | Branched Schrödinger Bridge Matching (Read more on [arXiv](https://arxiv.org/abs/2506.09007) or [HuggingFace](https://huggingface.co/papers/2506.09007))| Pranam Chatterjee, Alexander Tong, Yinuo Zhang, Sophia Tang | The paper introduces BranchSBM, a novel framework for learning branched stochastic transport maps from an initial distribution to multiple target distributions. It addresses the limitation of existing methods that cannot capture branched or divergent evolution. BranchSBM parameterizes diverging velocity fields and branch-specific growth rates, enabling the representation of population-level divergence. The framework is evaluated on tasks such as 3D navigation, cell differentiation, and perturbation response, achieving improved reconstruction accuracy (e.g., lower Wasserstein distances compared to single-branch SBM). This offers AI practitioners a more expressive tool for modeling dynamic systems with branching behavior. |
