

## Papers for 2025-06-02

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time (Read more on [arXiv](https://arxiv.org/abs/2505.24863) or [HuggingFace](https://huggingface.co/papers/2505.24863))| Haoran Geng, Xuying Ning, Han Wang, RunpeiDong, jyzhang1208 | This paper introduces ALPHAONE (a1), a framework for modulating reasoning progress in large reasoning models (LRMs) at test time. The research aims to develop a universally applicable method for controlling reasoning speed and efficiency in LRMs. a1 dynamically schedules slow thinking transitions before a scaled 'a moment' and deterministically terminates slow thinking after it, fostering fast reasoning. Empirical results on mathematical, coding, and scientific benchmarks show a1 achieves superior reasoning with, for example, a margin of +6.15% improvement for a 1.5B LRM. The framework offers AI practitioners a more efficient and controllable approach to leverage slow and fast thinking in LRMs. |
| Computer Vision | Time Blindness: Why Video-Language Models Can't See What Humans Can? (Read more on [arXiv](https://arxiv.org/abs/2505.24867) or [HuggingFace](https://huggingface.co/papers/2505.24867))| Mohamed Elhoseiny, Zhiqiang Shen, mukul54, ujjwal9 | This paper investigates the limitations of video-language models (VLMs) in purely temporal understanding. It introduces SpookyBench, a benchmark with information encoded solely in temporal sequences of noise-like frames. Results show state-of-the-art VLMs achieve 0% accuracy, while humans achieve over 98% accuracy on SpookyBench tasks. This performance gap reveals that VLMs over-rely on frame-level spatial features and lack mechanisms for extracting meaning from purely temporal cues. The implication is that novel architectures or training paradigms are needed to decouple spatial dependencies from temporal processing in VLMs. |
| Multi-Modal | Don't Look Only Once: Towards Multimodal Interactive Reasoning with
  Selective Visual Revisitation (Read more on [arXiv](https://arxiv.org/abs/2505.18842) or [HuggingFace](https://huggingface.co/papers/2505.18842))| Min Soo Kim, Jaeyoung Lee, Jiwan Chung, siyeolkim, kjunh | The paper introduces v1, a lightweight extension for Multimodal Large Language Models enabling selective visual revisitation during inference. It addresses the limitation of current MLLMs that consume visual input only once by introducing a point-and-copy mechanism for dynamic image region retrieval. The methodology involves constructing v1g, a dataset of 300K multimodal reasoning traces with visual grounding annotations, to train the model. Experiments on MathVista, MathVision, and MathVerse demonstrate that v1 improves performance, particularly in tasks requiring fine-grained visual reference, outperforming comparable baselines and improving average mini performance to 50.6%. Dynamic visual access during reasoning is a promising direction for enhancing grounded multimodal reasoning capabilities. |
| Machine Learning | Large Language Models for Data Synthesis (Read more on [arXiv](https://arxiv.org/abs/2505.14752) or [HuggingFace](https://huggingface.co/papers/2505.14752))| Lijun Sun, Menglin Kong, HYTYH | This paper introduces LLMSYNTHOR, a framework for data synthesis using Large Language Models (LLMs) that aligns real and synthetic data distributions. It aims to address the inefficiencies and limitations of standard LLM-based sampling for data synthesis. The method leverages LLMs as nonparametric copula simulators guided by distributional feedback. In experiments on heterogeneous datasets, LLMSYNTHOR demonstrates high statistical fidelity, achieving consistently lower divergence scores compared to baselines. This approach offers a scalable and adaptable tool for generating high-quality synthetic data for use in economics, social science, urban studies, and beyond. |
| Machine Learning | HardTests: Synthesizing High-Quality Test Cases for LLM Coding (Read more on [arXiv](https://arxiv.org/abs/2505.24098) or [HuggingFace](https://huggingface.co/papers/2505.24098))| Jiabao Ji, Kexun Zhang, Yee Man Choi, Zhongmou He, JuntingZhou | This paper introduces HARDTESTGEN, a pipeline for synthesizing high-quality test cases for large language model (LLM) coding. The research aims to improve the reliability of verifiers used in post-training techniques such as reinforcement learning for coding tasks. HARDTESTGEN utilizes LLMs to generate test generator programs and filters them using human-written oracles, creating the HARDTESTS dataset. The resulting HARDTESTS dataset demonstrates 11.3 percentage points higher precision and 17.5 percentage points higher recall when evaluating LLM-generated code, improving downstream code generation performance. The implication is that higher-quality test cases significantly enhance the effectiveness of LLM post-training techniques like reinforcement learning and self-distillation for coding tasks. |
| Computer Vision | ViStoryBench: Comprehensive Benchmark Suite for Story Visualization (Read more on [arXiv](https://arxiv.org/abs/2505.24862) or [HuggingFace](https://huggingface.co/papers/2505.24862))| Yaoqi Hu, Jingwei Wu, Ailin Huang, Cailin Zhuang, wchengad | The paper introduces ViStoryBench, a comprehensive benchmark for evaluating story visualization frameworks. It addresses the need for a diverse dataset and evaluation metrics to assess story visualization models in real-world scenarios. ViStoryBench includes 80 diverse story segments and 344 roles, evaluated using a range of metrics, including Character Identification Similarity (CIDS). Evaluation results demonstrate that different models exhibit varying strengths and weaknesses, highlighting areas for targeted improvements. The benchmark aids researchers in identifying model capabilities and deficiencies, fostering advancements in story visualization. |
| Multi-Modal | Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and
  Benchmarking Multimodal LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2505.24878) or [HuggingFace](https://huggingface.co/papers/2505.24878))| Xiaohan Zhao, Jiacheng Liu, Zhaoyi Li, Yaxin Luo, jiachengcui888 | The paper introduces Open CaptchaWorld, a web-based benchmark for evaluating multimodal LLM agents' ability to solve diverse CAPTCHA puzzles. The primary research objective is to assess the visual reasoning and interactive capabilities of these agents in complex, dynamic environments. The methodology involves developing a benchmark with 20 CAPTCHA types and introducing a new metric, CAPTCHA Reasoning Depth, to quantify task complexity. Results indicate that even state-of-the-art MLLM agents, like Browser-Use Openai-03, achieve a maximum success rate of 40.0%, significantly lower than human performance (93.3%). This highlights the need for more robust multimodal reasoning systems capable of handling interactive, multi-step challenges. |
| Natural Language Processing | CLaSp: In-Context Layer Skip for Self-Speculative Decoding (Read more on [arXiv](https://arxiv.org/abs/2505.24196) or [HuggingFace](https://huggingface.co/papers/2505.24196))| Ziqiang Liu, Lu Wang, Huiming Wang, Renke Shan, Longze Chen | The paper introduces CLaSp, a novel self-speculative decoding framework for accelerating Large Language Model (LLM) inference. It addresses the challenge of consistency between draft and verify models by dynamically adjusting layer-skipping strategies based on the context. CLaSp leverages a dynamic programming algorithm to optimize layer skipping with minimal additional latency. Experiments on the LLaMA3 series models demonstrate a 1.3x to 1.7x speedup compared to conventional autoregressive decoding. CLaSp offers AI practitioners a lossless acceleration technique that doesn't require additional drafting modules or training, potentially simplifying deployment across various LLMs. |
| Multi-Modal | Vision Language Models are Biased (Read more on [arXiv](https://arxiv.org/abs/2505.23941) or [HuggingFace](https://huggingface.co/papers/2505.23941))| Vy Tuong Dang, Khai-Nguyen Nguyen, An Vo, knguyennguyen, taesiri | The paper investigates biases in Vision Language Models (VLMs) on objective visual tasks. It aims to quantify how pre-existing knowledge sways VLMs, impairing accuracy in counting and identification. The methodology involves creating counterfactual images with subtle alterations to common objects and assessing VLM performance. Results indicate VLMs exhibit strong biases, achieving only 17.05% accuracy in counting tasks across diverse domains; inserting subject names further decreases accuracy. The study highlights the need for bias mitigation strategies in VLMs to enhance robustness and reliability in downstream applications. |
| Computer Vision | CoDA: Coordinated Diffusion Noise Optimization for Whole-Body
  Manipulation of Articulated Objects (Read more on [arXiv](https://arxiv.org/abs/2505.21437) or [HuggingFace](https://huggingface.co/papers/2505.21437))| Taku Komura, Zhiyang Dou, Zhi Cen, Huaijin Pi | The paper introduces CoDA, a framework for generating coordinated whole-body manipulation of articulated objects. It addresses the challenge of achieving realistic and coordinated body and hand motion for manipulating articulated objects. The method optimizes latent noise inputs of decoupled diffusion models for the body, left hand, and right hand, guided by end-effector tracking, penetration, and regularization losses. Experiments on ARCTIC and GRAB datasets demonstrate state-of-the-art performance, achieving superior motion quality and physical plausibility, with a Frechet Inception Distance (FID) of 2.283 on ARCTIC. The approach enables various capabilities such as object pose control and simultaneous manipulation and locomotion, offering AI practitioners a new method to generate human-object interaction. |
| Computer Vision | UniGeo: Taming Video Diffusion for Unified Consistent Geometry
  Estimation (Read more on [arXiv](https://arxiv.org/abs/2505.24521) or [HuggingFace](https://huggingface.co/papers/2505.24521))| Yuan-Chen Guo, Yi-Hua Huang, Zehuan Huang, Xin Yu, Yang-Tian Sun | The paper introduces UniGeo, a framework for consistent video geometry estimation using video diffusion models. It aims to improve geometric estimation consistency across video frames by reformulating the task as a video generation problem in a global coordinate system. UniGeo employs a shared positional encoding strategy and joint training of multiple geometric attributes. Experiments on the ScanNet++ dataset demonstrate state-of-the-art results for normal and radius estimation. The framework enables direct application to reconstruction tasks with improved inter-frame consistency, offering a unified approach to various geometry-related tasks. |
| Computer Vision | EasyText: Controllable Diffusion Transformer for Multilingual Text
  Rendering (Read more on [arXiv](https://arxiv.org/abs/2505.24417) or [HuggingFace](https://huggingface.co/papers/2505.24417))| Yiren Song, Haifa Wang, Jailing Liu, Yuxuan Zhang, Runnan Lu | EasyText is a novel framework for controllable multilingual text rendering using a diffusion transformer. The paper addresses the challenge of generating accurate text in multiple languages and complex layouts. It employs a character-aware encoding and positional control mechanisms (Implicit Character Position Alignment) alongside a two-stage training strategy with a large synthetic dataset and a fine-tuning stage. Experiments show superior performance in multilingual text rendering achieving OCR accuracy of 88.72% surpassing existing methods. This work enables AI practitioners to generate realistic images with accurate multilingual text and controllable layouts. |
| Machine Learning | Harnessing Negative Signals: Reinforcement Distillation from Teacher
  Data for LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.24850) or [HuggingFace](https://huggingface.co/papers/2505.24850))| Wei Chu, Weidi Xu, Jiangxuan Long, Cheng Peng, Tim-Xu | This paper introduces Reinforcement Distillation (REDI), a two-stage framework for distilling reasoning abilities from teacher models to smaller LLMs by leveraging both positive and negative reasoning traces. The research aims to maximize LLM reasoning performance in an offline setting by utilizing previously discarded incorrect reasoning examples. REDI employs supervised fine-tuning on positive traces followed by a reference-free objective to refine the model with both positive and negative traces, using an asymmetric weighting strategy for stability. The Qwen-REDI-1.5B model achieves 83.1% on MATH-500 (pass@1) using 131k open data, surpassing a model trained on 800k proprietary data. This suggests a more data-efficient approach to distillation and provides a pathway to improving reasoning skills for efficient student models. |
| Natural Language Processing | Large Language Models are Locally Linear Mappings (Read more on [arXiv](https://arxiv.org/abs/2505.24293) or [HuggingFace](https://huggingface.co/papers/2505.24293))| jamesgolden1 | The paper demonstrates that inference operations of open-weight large language models (LLMs) can be mapped to equivalent linear systems for a given input sequence. The objective is to show that LLMs, despite their global nonlinearity, exhibit local linearity during next-token prediction. The methodology involves strategically altering the gradient computation to produce a Jacobian that accurately reproduces the forward prediction with a linear system, then analyzing the detached Jacobian. The paper demonstrates that this linear approximation holds up to Llama 3.3 70B Q4 and finds a relative reconstruction error on the order of 10-6 for float32 precision. This work implies that LLMs can be interpreted through locally linear decompositions, offering insights into internal representations and semantic structures. |
| Natural Language Processing | Harnessing Large Language Models for Scientific Novelty Detection (Read more on [arXiv](https://arxiv.org/abs/2505.24615) or [HuggingFace](https://huggingface.co/papers/2505.24615))| Erik Cambria, Thanh-Son Nguyen, Soujanya Poria, Yan Liu, ZonglinY | The paper introduces a novel approach to scientific novelty detection (ND) by harnessing large language models (LLMs). It addresses the challenge of accurately identifying novel research ideas by constructing ND-tailored benchmark datasets and developing an LLM-based knowledge distillation (KD) framework. The methodology involves extracting closure sets of papers, generating structured summaries with LLMs, and training a lightweight retriever using synthesized idea pairs to bridge textual similarity and idea conception. Experiments on marketing and NLP datasets demonstrate that the proposed method consistently outperforms others on idea retrieval and ND tasks, achieving an average improvement of 5.40% and 15.19% compared to the top-performing baseline in marketing and NLP, respectively. The LLM-based KD retriever can significantly improve the process of scientific innovation by more accurately identifying novel research ideas. |
| Computer Vision | un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via
  Inverting unCLIP (Read more on [arXiv](https://arxiv.org/abs/2505.24517) or [HuggingFace](https://huggingface.co/papers/2505.24517))| Shiguang Shan, Ruibing Hou, Hong Chang, Jiahe Zhao, yinqi | This paper introduces un²CLIP, a method to improve CLIP's visual detail capturing ability by inverting unCLIP. The research aims to enhance CLIP's image encoder to capture more visual details while maintaining language-alignment. The methodology involves finetuning the CLIP image encoder within the unCLIP framework to transfer the generator's visual knowledge. Experiments show un²CLIP significantly improves CLIP, achieving a 3.5 mIoU improvement on the dense vision task of open-vocabulary segmentation with ClearCLIP. This method offers AI practitioners a way to enhance CLIP models for finer-grained visual understanding tasks. |
| Natural Language Processing | EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,
  Expressiveness, and Linguistic Challenges Using Model-as-a-Judge (Read more on [arXiv](https://arxiv.org/abs/2505.23009) or [HuggingFace](https://huggingface.co/papers/2505.23009))| Alex Smola, Mu Li, Xingjian Shi, Yuzhi Tang, ruskinmanku | The paper introduces EmergentTTS-Eval, a new benchmark for evaluating Text-to-Speech (TTS) systems across six challenging categories including questions, foreign words, and paralinguistics. The research aims to provide a comprehensive evaluation scheme that assesses not only naturalness but also expressiveness, prosody, and linguistic complexities in TTS. The methodology involves creating a diverse dataset of 1,645 test cases and employing a Model-as-a-Judge (MAJ) approach using a large language model (Gemini 2.5 Pro) to automatically evaluate TTS outputs. Experiments reveal that model performance varies significantly across different categories, with GPT-to-audio generally achieving higher win rates against other models, suggesting a potential gap in handling more complex prosodic and linguistic structures. The benchmark offers AI practitioners a robust and reproducible methodology for TTS evaluation, facilitating targeted improvements in model capabilities beyond basic intelligibility. |
| Reinforcement Learning | DexUMI: Using Human Hand as the Universal Manipulation Interface for
  Dexterous Manipulation (Read more on [arXiv](https://arxiv.org/abs/2505.21864) or [HuggingFace](https://huggingface.co/papers/2505.21864))| Linxi Fan, Zhenjia Xu, Yifan Hou, Han Zhang, mengdaxu | DexUMI is a data collection and policy learning framework for transferring dexterous manipulation skills from human hands to robot hands. The paper aims to minimize the embodiment gap between human and robot hands through hardware and software adaptations. The hardware adaptation uses a wearable hand exoskeleton, while the software adaptation uses robot hand inpainting in video data. Real-world experiments on two robot hand platforms demonstrate an average task success rate of 86%. The system offers AI practitioners a scalable and efficient approach to collecting real-world dexterous hand data and learning manipulation policies. |
