

## Papers for 2025-06-25

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion
  Models (Read more on [arXiv](https://arxiv.org/abs/2506.19851) or [HuggingFace](https://huggingface.co/papers/2506.19851))| lsheng2024, pookiefoof, Yang-Tian, fenghora, huanngzh | AnimaX is a feed-forward 3D animation framework for animating diverse articulated meshes. The research aims to transfer motion priors from video diffusion models to skeleton-based animation. It represents 3D motion as multi-view, multi-frame 2D pose maps and uses a joint video-pose diffusion model conditioned on template renderings and textual prompts. AnimaX achieves state-of-the-art results on VBench in generalization, motion fidelity, and efficiency. This approach offers a scalable solution for category-agnostic 3D animation generation. |
| Computer Vision | Matrix-Game: Interactive World Foundation Model (Read more on [arXiv](https://arxiv.org/abs/2506.18701) or [HuggingFace](https://huggingface.co/papers/2506.18701))| Qingcheng Zhu, Puyi Wang, Boyang Wang, Chunli Peng, Vanint | Matrix-Game introduces an interactive world foundation model for controllable game world generation. The research aims to enable precise control over character actions in generated videos while maintaining visual quality and temporal coherence. It employs a two-stage pipeline involving unlabeled pretraining and action-labeled training, utilizing a custom Minecraft dataset. Experiments demonstrate that Matrix-Game outperforms existing Minecraft world models across various metrics, showing gains in controllability and physical consistency; GameWorld Score demonstrates superior performance with over 88% accuracy on all actions. This work facilitates the development of more interactive and controllable virtual environments for AI agents. |
| Multi-Modal | GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.16141) or [HuggingFace](https://huggingface.co/papers/2506.16141))| Junhao Cheng, Yixiao Ge, Rui Wang, Yuying Ge, Yi Chen | The paper introduces GRPO-CARE, a consistency-aware reinforcement learning framework for multimodal reasoning in video understanding. It addresses the limitation of standard outcome-supervised GRPO, which often compromises logical consistency between reasoning and answers, hindering interpretability. GRPO-CARE introduces a two-tiered reward system with a base reward for answer correctness and an adaptive consistency bonus derived from a slowly-updated reference model. Experiments on SEED-Bench-R1 demonstrate a 6.7% performance gain on the most challenging evaluation level and a 24.5% improvement in consistency rate. This implies that jointly optimizing for answer correctness and reasoning coherence, without explicit process supervision, can improve the performance and interpretability of MLLMs. |
| Machine Learning | Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in
  LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.19290) or [HuggingFace](https://huggingface.co/papers/2506.19290))| Changshi Li, Yuzhen Xiao, chrisliu298, lycfight, zengliangcs | This paper presents Skywork-SWE, a new dataset and model for software engineering tasks in LLMs, exploring data scaling laws. The research aims to address limitations in existing SWE datasets, focusing on sustained iterative problem-solving and long-context dependency resolution. The authors curated a dataset of 10,169 real-world Python tasks with automated unit-test validation and fine-tuned the Skywork-SWE model, achieving 38.0% pass@1 accuracy on the SWE-bench Verified benchmark. The study reveals a data scaling phenomenon in LLMs for software engineering and provides guidelines to advance LLM-driven software engineering, suggesting task-specific high-quality training data is key. |
| Computer Vision | ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality
  Debiasing (Read more on [arXiv](https://arxiv.org/abs/2506.19848) or [HuggingFace](https://huggingface.co/papers/2506.19848))| Pan Zhang, Xiaoyi Dong, Long Xing, yuhangzang, shikiw | ScaleCap is an inference-time scalable image captioning strategy leveraging dual-modality debiasing to generate detailed captions. The paper addresses challenges in high-quality image captioning related to multimodal and linguistic biases in LVLMs. It proposes a scalable debiased captioning strategy using heuristic question answering and contrastive sentence rating to enrich and calibrate captions. Experiments demonstrate its effectiveness, achieving best performance on 11 benchmarks when pretraining LVLMs with ScaleCap-annotated data. ScaleCap improves the quality and richness of generated captions, offering a cost-effective approach to generating high-quality multimodal data for AI practitioners. |
| Machine Learning | SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in
  Real-World Applications (Read more on [arXiv](https://arxiv.org/abs/2506.18951) or [HuggingFace](https://huggingface.co/papers/2506.18951))| Per Jacobsson, Ge Qu, Jinyang Li, Tebmer, xia01ongLi | The paper introduces SWE-SQL, a benchmark for evaluating LLMs on SQL issue debugging. It addresses the gap in rigorously evaluating LLMs on the challenging task of SQL debugging, focusing on authentic user issues. The study employs BIRD-CRITIC, a dataset comprising PostgreSQL and multi-dialect tasks, along with SIX-GYM, a training environment leveraging SQL-Rewind and f-Plan Boosting. The BIRD-FIXER agent, fine-tuned on Qwen-2.5-Coder-14B, achieves a success rate of 38.11% on BIRD-CRITIC-PG, surpassing several proprietary models. This signifies a move toward democratizing SQL debugging capabilities for research and industry. |
| Natural Language Processing | SRFT: A Single-Stage Method with Supervised and Reinforcement
  Fine-Tuning for Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.19767) or [HuggingFace](https://huggingface.co/papers/2506.19767))| Xihuai Wang, Jiajun Chai, Tinghong Chen, SONGJUNTU, Yuqian-Fu | The paper introduces SRFT, a single-stage fine-tuning method unifying Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for enhanced reasoning in large language models (LLMs). It addresses the challenge of optimally integrating SFT and RL by proposing an entropy-aware weighting mechanism to simultaneously leverage demonstrations and self-exploration. SRFT achieves 59.1% average accuracy on five mathematical reasoning benchmarks, outperforming zero-RL methods by 9.0%. The key methodology involves an entropy-aware mechanism to balance the strengths of SFT's knowledge distillation with RL's policy optimization. SRFT provides AI practitioners with an efficient single-stage approach for improving LLM reasoning capabilities by integrating demonstrations and self-exploration in a balanced manner. |
| Computer Vision | JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo
  Retouching Agent (Read more on [arXiv](https://arxiv.org/abs/2506.17612) or [HuggingFace](https://huggingface.co/papers/2506.17612))| Panwang Pan, Jinbin Bai, Kunjie Lin, Zixu Lin, LYL1015 | JarvisArt is introduced as an MLLM-driven agent designed for intelligent photo retouching. The paper aims to bridge the gap between professional tools and automated solutions by enabling user intent understanding and fine-grained control over retouching. It utilizes a two-stage training process: Chain-of-Thought supervised fine-tuning followed by Group Relative Policy Optimization for Retouching (GRPO-R). Results on MMArt-Bench show a 60% improvement in average pixel-level metrics compared to GPT-40 while maintaining comparable instruction-following. The main implication is a new direction for intelligent photo retouching with user-friendly interaction and superior generalization, though details on specific architectural choices may be missing. |
| Computer Vision | SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution (Read more on [arXiv](https://arxiv.org/abs/2506.19838) or [HuggingFace](https://huggingface.co/papers/2506.19838))| Xintao Wang, Menghan Xia, Shian Du, Yu Li, Liangbin Xie | SimpleGVR is presented as a simple yet effective baseline for cascaded video super-resolution on latent representations from large text-to-video models. The paper focuses on key design principles for cascaded video super-resolution (VSR) models, specifically aiming to enhance the low-resolution latent outputs of a base text-to-video model. The methodology involves two degradation strategies (flow-based and model-guided) to generate training pairs, detail-aware timestep sampling, and a sparse local attention mechanism for efficient training and inference. Experiments demonstrate SimpleGVR achieves superior performance over existing methods, achieving a MUSIQ score of 62.35, indicating improved perceptual quality. The framework provides practical insights for developing efficient cascaded synthesis systems for high-resolution video generation, potentially reducing computational overhead. |
| Computer Vision | Guidance in the Frequency Domain Enables High-Fidelity Sampling at Low
  CFG Scales (Read more on [arXiv](https://arxiv.org/abs/2506.19713) or [HuggingFace](https://huggingface.co/papers/2506.19713))| Farnood Salehi, Tobias Vontobel, RMW, msadat97 | This paper introduces frequency-decoupled guidance (FDG) to enhance conditional diffusion models. It addresses the trade-off between image quality and diversity in classifier-free guidance (CFG) by analyzing CFG's impact in the frequency domain. FDG decomposes CFG into low- and high-frequency components, applying distinct guidance strengths. Experiments show FDG improves FID scores while preserving diversity, for example, reducing the FID score from 9.77 to 5.44 on EDM2-S. The implication is improved image generation quality at lower guidance scales, offering a plug-and-play alternative to standard CFG. |
| Multi-Modal | Unified Vision-Language-Action Model (Read more on [arXiv](https://arxiv.org/abs/2506.19850) or [HuggingFace](https://huggingface.co/papers/2506.19850))| Yingyan Li, Junbo Zhang, Wenxuan Wang, Xinghang Li, Yuqi Wang | The paper presents UniVLA, a unified vision-language-action model that autoregressively models vision, language, and action as discrete tokens. It addresses the challenge of jointly modeling heterogeneous modalities by using a shared vocabulary and autoregressive sequence learning. UniVLA incorporates world modeling during post-training to capture causal dynamics from videos, facilitating effective transfer to downstream policy learning. The approach achieves state-of-the-art results on CALVIN, LIBERO, and SimplerEnv-Bridge, significantly surpassing existing methods, for example, achieving 95.5% average success rate on the LIBERO benchmark. UniVLA provides AI practitioners with a new architecture that better integrates cross-modal information and enables scalable, video-based training. |
| Natural Language Processing | Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic
  Empirical Study (Read more on [arXiv](https://arxiv.org/abs/2506.19794) or [HuggingFace](https://huggingface.co/papers/2506.19794))| Ziheng Zhang, Jintian Zhang, Yi Zhong, Yuqi Zhu, Ningyu | This paper investigates the limitations of open-source Large Language Models (LLMs) in data analysis tasks. It evaluates models across data understanding, code generation, and strategic planning dimensions using a curated dataset. The study finds that strategic planning quality is the primary determinant of model performance. Through a data synthesis methodology based on these findings, the analytical reasoning capabilities of open-source LLMs were significantly improved, achieving comparable performance to GPT-40 on some tasks after fine-tuning. This highlights the importance of focusing on strategic planning and data quality over sheer diversity when fine-tuning LLMs for analytical tasks. |
| Natural Language Processing | Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text (Read more on [arXiv](https://arxiv.org/abs/2506.14012) or [HuggingFace](https://huggingface.co/papers/2506.14012))| Michalis Vazirgiannis, Yang Zhang, guokan-shang, amr-mohamed | This paper evaluates the understanding of Large Language Models (LLMs) on code-switched text by generating code-switched variants of established reasoning benchmarks. The study aims to assess how LLMs process and reason about mixed-language inputs, examining performance degradation when foreign tokens disrupt English text and improvements when embedding English into other languages. Using a constrained, multi-step LLM pipeline, results show consistent drops in LLM performance with non-English tokens in English matrix sentences; however, embedding English into other languages often improves comprehension. While prompting yields mixed results, fine-tuning offers a more stable path to degradation mitigation, indicating that code-switching introduces comprehension challenges distinct from monolingual processing. |
| Natural Language Processing | Can Large Language Models Capture Human Annotator Disagreements? (Read more on [arXiv](https://arxiv.org/abs/2506.19467) or [HuggingFace](https://huggingface.co/papers/2506.19467))| Alexander Hoyle, Donya Rooein, Vilém Zouhar, Yu Fan, JingweiNi | This paper investigates whether Large Language Models (LLMs) can capture human annotator disagreements without access to repeated human labels.  The study evaluates LLMs' ability to predict annotation disagreements across various NLP tasks using variance correlation and distributional alignment metrics. The primary finding reveals that Reinforcement Learning from Human Feedback (RLVR) reasoning degrades LLM performance in disagreement prediction.  The results suggest that practitioners should exercise caution when using LLM annotators, especially with RLVR LLMs, and for subjective tasks due to the potential oversight of critical human disagreements. |
| Machine Learning | USAD: Universal Speech and Audio Representation via Distillation (Read more on [arXiv](https://arxiv.org/abs/2506.18843) or [HuggingFace](https://huggingface.co/papers/2506.18843))| Alexander H. Liu, James Glass, saurabhati, vectominist | The paper introduces Universal Speech and Audio Distillation (USAD), a unified model for learning audio representations across speech, sound, and music domains. The research aims to create a single, general-purpose audio encoder by distilling knowledge from domain-specific self-supervised learning (SSL) models. USAD employs efficient layer-to-layer distillation from pre-trained speech and audio SSL models using a mixture of multi-domain audio data. Results show that USAD achieves competitive performance on SUPERB and HEAR benchmarks, often approaching state-of-the-art, including a SUPERB score of 851.7 for the large model. USAD provides a unified audio representation enabling AI practitioners to use a single encoder for diverse audio tasks, simplifying deployment and potentially improving performance across domains. |
| Reinforcement Learning | KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality (Read more on [arXiv](https://arxiv.org/abs/2506.19807) or [HuggingFace](https://huggingface.co/papers/2506.19807))| Huajun Chen, Wenhao Yu, Shuofei Qiao, Baochang Ren, Ningyu | The paper introduces KnowRL, a knowledge-enhanced reinforcement learning framework to mitigate hallucinations in slow-thinking large language models. It addresses the lack of factual supervision in RL by integrating a factuality reward based on knowledge verification during training. The method guides models to perform fact-based reasoning and recognize knowledge boundaries. Experiments on hallucination and reasoning datasets demonstrate that KnowRL effectively reduces hallucinations while maintaining or improving reasoning capabilities; for instance, accuracy reaches 16.23% on ChineseSimpleQA. KnowRL offers a technique to improve the reliability of LLMs by incorporating factual rewards into the reinforcement learning process. |
| Machine Learning | Intelligent Operation and Maintenance and Prediction Model Optimization
  for Improving Wind Power Generation Efficiency (Read more on [arXiv](https://arxiv.org/abs/2506.16095) or [HuggingFace](https://huggingface.co/papers/2506.16095))| Jiaqi He, Xiaobin Wu, Xun Liu, rajandasgupta | This paper investigates optimizing wind turbine operation and maintenance using predictive modeling. The research aims to improve wind power generation efficiency by refining predictive maintenance models and intelligent O&M systems. The study employs a qualitative approach, conducting structured interviews with wind farm engineers and maintenance managers, using thematic analysis to derive insights. The results indicate that predictive maintenance models can reduce downtime by up to 20% but struggle with minor fault detection, highlighting the need for AI refinement and real-time data integration. These improvements in model accuracy and data handling can lead to enhanced wind energy production and more efficient resource allocation. |
| Computer Vision | Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments
  with a Hierarchical Spatial-Cognition Long-Short Memory System (Read more on [arXiv](https://arxiv.org/abs/2506.19433) or [HuggingFace](https://huggingface.co/papers/2506.19433))| Jie Feng, Yangcheng Yu, Zhenxing Chen, Haoyu Dong, Lixuan He | Mem4Nav introduces a hierarchical spatial-cognition long-short memory system to enhance vision-and-language navigation (VLN) in urban environments. The paper addresses the challenge of long-term memory and spatial reasoning in VLN agents. The proposed system combines a sparse octree and a semantic topology graph, integrated with long-term and short-term memory modules utilizing reversible Transformers. Evaluated on Touchdown and Map2Seq, Mem4Nav achieves a 7-13 pp gain in Task Completion. AI practitioners can leverage the hierarchical memory system to improve VLN agents' performance in complex, large-scale environments. |
