

## Papers for 2025-06-13

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.09513) or [HuggingFace](https://huggingface.co/papers/2506.09513))| Weiwen Xu, Xingyu Qian, Swrooy, 26hzhang, YuSun-AI | The paper introduces ReasonMed, a large-scale medical reasoning dataset for enhancing knowledge-intensive question answering. It addresses the underexplored capabilities of LLMs in medical QA by creating a 370k example dataset distilled from 1.7 million LLM-generated reasoning paths through a multi-agent verification and refinement process. The methodology utilizes an Error Refiner to improve reasoning paths by identifying and correcting error-prone steps. ReasonMed-7B achieves state-of-the-art performance among sub-10B models, outperforming previous benchmarks by 4.17% and exceeding LLaMA3.1-70B on PubMedQA by 4.60%. The results suggest that combining detailed Chain-of-Thought reasoning with concise answer summaries yields the most effective fine-tuning strategy for medical reasoning models. |
| Machine Learning | SWE-Factory: Your Automated Factory for Issue Resolution Training Data
  and Evaluation Benchmarks (Read more on [arXiv](https://arxiv.org/abs/2506.10954) or [HuggingFace](https://huggingface.co/papers/2506.10954))| Pengyu Yang, Caihua Li, Yanlin Wang, Lianghong Guo, itaowe | The paper introduces SWE-Factory, an automated pipeline for constructing GitHub issue resolution datasets. It addresses the challenges of evaluation environment setup, grading, and validation by integrating a multi-agent system (SWE-Builder), exit-code-based grading, and automated fail2pass validation. Experiments on 671 issues show SWE-Factory with GPT-4.1-mini constructs 269 valid instances at $0.045 per instance, and exit-code grading achieves 100% accuracy. The tool automates benchmark creation, reducing manual labor.  Filtering of error2pass cases is crucial to avoid underestimation of model capabilities. |
| Computer Vision | Text-Aware Image Restoration with Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2506.09993) or [HuggingFace](https://huggingface.co/papers/2506.09993))| Jihye Park, Jaeeun Lee, paulcho98, jinlovespho, Min-Jaewon | The paper introduces Text-Aware Image Restoration (TAIR), a novel computer vision task for simultaneously recovering visual content and textual fidelity in degraded images. It addresses the challenge of text-image hallucination in existing diffusion-based restoration methods that fail to reconstruct textual regions accurately. The authors propose a multi-task diffusion framework called TeReDiff, which integrates a text-spotting module and diffusion features to extract rich text representations as prompts for denoising. They also introduce SA-Text, a large-scale benchmark of 100K scene images densely annotated with text instances. Experiments demonstrate that TeReDiff outperforms state-of-the-art restoration methods and achieves a higher text recognition accuracy. |
| Multi-Modal | AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven
  Clip Generation (Read more on [arXiv](https://arxiv.org/abs/2506.10540) or [HuggingFace](https://huggingface.co/papers/2506.10540))| Baotian Hu, Longyue Wang, Xinyu Chen, YunxinLi, MrSunshy | The paper introduces AniMaker, a multi-agent framework for automated storytelling animation from text. It aims to generate coherent, multi-character, multi-scene animation by integrating specialized agents for storyboard creation, video clip generation, evaluation, and post-production. The key methodology involves an MCTS-inspired strategy (MCTS-Gen) for efficient clip candidate generation and a novel evaluation framework (AniEval) for multi-shot animation. Experiments demonstrate AniMaker's superior performance, achieving a 14.6% higher score in AniEval compared to baseline methods. AniMaker improves the efficiency of multi-candidate generation, bringing AI-generated storytelling animation closer to production standards. |
| Natural Language Processing | Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture
  without Training (Read more on [arXiv](https://arxiv.org/abs/2506.10952) or [HuggingFace](https://huggingface.co/papers/2506.10952))| Xipeng Qiu, Lu Wang, Howe77, mzzhang | The paper introduces Domain2Vec, a novel approach to identify the optimal data mixture for language model pretraining without training. It aims to decompose datasets into a linear combination of meta-domains, represented by domain vectors generated by a meta-domain classifier. The core methodology involves Distribution Alignment Assumption (DA2), suggesting better training/validation alignment yields lower validation loss. Experiments show Domain2Vec achieves the same validation loss on Pile-CC using 51.5% of the original computation and improves downstream performance by 2.83% under equivalent compute budget. Domain2Vec enables more efficient and scalable language model pretraining by avoiding computationally intensive proxy model training. |
| Multi-Modal | Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable
  Task Experts (Read more on [arXiv](https://arxiv.org/abs/2506.10357) or [HuggingFace](https://huggingface.co/papers/2506.10357))| Weili Guan, Gongwei Chen, Rui Shao, Yuquan Xie, Zaijing Li | This paper introduces Optimus-3, a generalist multimodal agent for Minecraft capable of perception, planning, action, grounding, and reflection. The research aims to develop an agent addressing challenges like insufficient domain-specific data, heterogeneous task interference, and visual diversity. Optimus-3 utilizes a knowledge-enhanced data generation pipeline, a task-level routing Mixture-of-Experts architecture, and Multimodal Reasoning-Augmented Reinforcement Learning. Experimental results show Optimus-3 outperforms existing agents, achieving a 20% improvement in planning tasks. The architecture facilitates creating more versatile agents applicable to other complex open-world environments. |
| Natural Language Processing | AutoMind: Adaptive Knowledgeable Agent for Automated Data Science (Read more on [arXiv](https://arxiv.org/abs/2506.10974) or [HuggingFace](https://huggingface.co/papers/2506.10974))| Lanning Wei, Jingsheng Zheng, Yujie Luo, Ningyu, OE-Heart | The paper introduces AUTOMIND, an adaptive, knowledgeable LLM agent framework for automated data science. It aims to enhance LLM agents in data science by incorporating rich empirical expertise and flexible coding strategies. AUTOMIND utilizes an expert knowledge base, an agentic knowledgeable tree search, and a self-adaptive coding strategy to tailor code generation to task complexity. Evaluated on two automated data science benchmarks, AUTOMIND demonstrates superior performance, outperforming 56.8% of human participants on the MLE-Bench leaderboard. It also improves time efficiency by 300% and reduces token costs by 63%, offering a robust step toward fully automated data science for AI practitioners. |
| Computer Vision | VideoDeepResearch: Long Video Understanding With Agentic Tool Using (Read more on [arXiv](https://arxiv.org/abs/2506.10821) or [HuggingFace](https://huggingface.co/papers/2506.10821))| Zhicheng Dou, Ji-Rong Wen, Junjie Zhou, Zheng Liu, Huaying Yuan | The paper introduces VideoDeepResearch, an agentic framework for long video understanding (LVU) that uses a text-only large reasoning model (LRM) with a multi-modal toolkit. It aims to overcome the limitations of current multi-modal large language models (MLLMs) in handling complex LVU tasks. The methodology involves reasoning to formulate a problem-solving strategy and selectively accessing video content via tools like multimodal retrievers and visual perceivers. Experiments on MLVU, LVBench, and LongVideoBench show VideoDeepResearch improves performance over MLLM baselines, achieving gains of 9.6%, 6.6%, and 3.9%, respectively. The framework offers a practical and efficient approach to LVU, suggesting agentic systems can effectively address key challenges without relying on large, monolithic models. |
| Computer Vision | PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a
  Unified Framework (Read more on [arXiv](https://arxiv.org/abs/2506.10741) or [HuggingFace](https://huggingface.co/papers/2506.10741))| Haoyu Chen, Tian Ye, Jialin Gao, Jianyu Lai, Ephemeral182 | The paper introduces PosterCraft, a unified framework for high-quality aesthetic poster generation. It addresses the challenge of creating visually coherent posters by rethinking the traditional modular design paradigm. PosterCraft utilizes a cascaded workflow including text rendering optimization, supervised fine-tuning, and reinforcement learning, culminating in vision-language feedback refinement. Experiments demonstrate PosterCraft significantly outperforms open-source baselines, achieving competitive performance with SOTA commercial systems. The framework's ability to generate visually compelling posters directly from textual input offers AI practitioners a robust method for automated design. |
| Natural Language Processing | ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark (Read more on [arXiv](https://arxiv.org/abs/2506.10960) or [HuggingFace](https://huggingface.co/papers/2506.10960))| Bozhong Tian, Siyuan Cheng, Kangwei Liu, Jasonchen123, Ningyu | This paper introduces ChineseHarm-Bench, a comprehensive benchmark for detecting harmful content in Chinese. The research aims to address the lack of Chinese-specific resources for harmful content detection, which are often limited in scope. They constructed the benchmark from real-world data, covering six categories, and manually curated a knowledge rule base to assist LLMs. Results show that their knowledge-augmented baseline achieved a macro-F1 score of 0.77 using smaller models, comparable to state-of-the-art LLMs. The benchmark and methodology can help AI practitioners build more effective and efficient Chinese harmful content detection systems. |
| Reinforcement Learning | Magistral (Read more on [arXiv](https://arxiv.org/abs/2506.10910) or [HuggingFace](https://huggingface.co/papers/2506.10910))| Gabrielle Berrada, Andy Lo, Albert Q. Jiang, Abhinav Rastogi, Mistral-AI | The paper introduces Magistral, a reasoning model trained using reinforcement learning (RL) from scratch without distillation from existing reasoning models. It explores the limits of pure RL training for LLMs, focusing on forcing reasoning language and maintaining initial checkpoint capabilities. Magistral Medium, trained with RL, achieves a 50% boost in AIME-24 (pass@1) compared to the Mistral Medium 3 checkpoint. The work demonstrates that RL maintains or improves multimodal understanding, instruction following, and function calling, providing insights for training reasoning models at scale. |
| Computer Vision | CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic
  Design Generation (Read more on [arXiv](https://arxiv.org/abs/2506.10890) or [HuggingFace](https://huggingface.co/papers/2506.10890))| Yutao Cheng, ShiLayne, YangMaoke, hxxxl, zbrl | The paper introduces CreatiPoster, a framework for generating editable, multi-layer graphic compositions from optional natural-language instructions or assets. It aims to improve on existing AI tools by accurately incorporating user-supplied assets, maintaining editability, and achieving professional visual appeal. The framework uses a protocol model and conditional background model to produce a JSON specification detailing every layer, along with a concise background prompt, for coherent graphic design. Evaluated on a new benchmark with automated metrics, CreatiPoster outperforms existing systems; quantitative results are not specified in this document. This allows AI practitioners to democratize AI-assisted graphic design, supporting diverse applications such as canvas editing, text overlay, responsive resizing, multilingual adaptation, and animated posters. |
| Natural Language Processing | Resa: Transparent Reasoning Models via SAEs (Read more on [arXiv](https://arxiv.org/abs/2506.09967) or [HuggingFace](https://huggingface.co/papers/2506.09967))| Ömer Faruk Akgül, Julian Asilis, willieneis, deqing, upup-ashton-wang | The paper introduces Resa, a family of 1.5B parameter reasoning models trained using a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. The research aims to elicit strong reasoning abilities in language models cost-effectively by leveraging underlying representations. SAE-Tuning involves training an SAE to capture reasoning abilities from a source model and then using it to guide supervised fine-tuning of a target model using verified question-answer data without reasoning traces. SAE-Tuning retains >97% of RL-trained counterpart's reasoning performance while reducing training costs by >2000x. This work enables AI practitioners to elicit strong reasoning in smaller models at significantly reduced computational cost and provides insights into model transparency. |
| Multi-Modal | Ming-Omni: A Unified Multimodal Model for Perception and Generation (Read more on [arXiv](https://arxiv.org/abs/2506.09344) or [HuggingFace](https://huggingface.co/papers/2506.09344))| Chunluan Zhou, Chuanyang Zheng, Cheng Zou, Biao Gong, Inclusion AI | Ming-Omni is presented as a unified multimodal model for perception and generation across images, text, audio, and video. The research aims to develop a single model efficiently processing and fusing multimodal inputs. Ming-Omni employs dedicated modality encoders processed by a Mixture-of-Experts architecture, Ling, with modality-specific routers. It integrates an audio decoder and Ming-Lite-Uni for speech and image generation, reaching a GenEval score of 0.64 and an FID score of 4.85 in image generation. This architecture offers AI practitioners a single model for versatile cross-modal tasks without task-specific fine-tuning. |
| Natural Language Processing | Eliciting Fine-Tuned Transformer Capabilities via Inference-Time
  Techniques (Read more on [arXiv](https://arxiv.org/abs/2506.08060) or [HuggingFace](https://huggingface.co/papers/2506.08060))| codelion | This paper investigates approximating supervised fine-tuning (SFT) capabilities of large language models using inference-time techniques. The research question is whether a base transformer can elicit SFT capabilities via in-context learning (ICL) without parameter updates. The authors theoretically prove that, under idealized assumptions, a base transformer can approximate SFT capabilities via ICL within a quantifiable error margin, and they extend this to practical scenarios with finite context lengths. Specifically, for text generation, datasets of size O(m(V/ε)^2 log(V/δ)) or, with fixed context, O((V/ε)^2 log(m/δ)) are sufficient to approximate fine-tuned distributions across m contexts; for linear classification, datasets of size O(d/ε^2) or, with fixed context, O(d log(1/δ)/ε^2) suffice, where d is the input dimension. The results provide a theoretical foundation for resource-efficient deployment of large language models by leveraging ICL. |
| Computer Vision | Attention, Please! Revisiting Attentive Probing for Masked Image
  Modeling (Read more on [arXiv](https://arxiv.org/abs/2506.10178) or [HuggingFace](https://huggingface.co/papers/2506.10178))| Tilemachos Aravanis, Ioannis Kakogeorgiou, Eirini Baltzi, Dionysis Christopoulos, Bill Psomas | This paper revisits attentive probing for evaluating Masked Image Modeling (MIM) models by addressing limitations of standard linear probing. The research aims to improve the accuracy-efficiency trade-off in attentive probing. They introduce efficient probing (EP), a multi-query cross-attention mechanism that reduces parameterization and computational cost. EP achieves up to a 10x speed-up over conventional multi-head attention and outperforms linear probing and prior attentive probing approaches across seven benchmarks; for instance, EP64 achieves 75.6% top-1 accuracy on ImageNet-1k with MAE ViT-B. The proposed method offers AI practitioners a lightweight yet expressive alternative for probing frozen models, enabling better assessment of self-supervised learning representations. |
| Computer Vision | UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal
  Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2506.09952) or [HuggingFace](https://huggingface.co/papers/2506.09952))| Jiwen Lu, Jie Zhou, Yanran21, LavenderLA | The paper introduces UniPre3D, a unified pre-training method for 3D point cloud models applicable across varying scales and architectures. It addresses the challenge of scale diversity in 3D data by predicting Gaussian primitives and rendering images via differentiable Gaussian splatting for pixel-level supervision. The method integrates pre-trained 2D image features to enhance geometric structure learning, using scale-adaptive fusion techniques. Experiments on ScanObjectNN demonstrate UniPre3D achieves 87.93% accuracy on the PB_T50_RS benchmark, outperforming previous MAE-based methods. UniPre3D enables AI practitioners to use a single pre-training framework for both object- and scene-level 3D perception tasks, improving performance and efficiency. |
| Natural Language Processing | VerIF: Verification Engineering for Reinforcement Learning in
  Instruction Following (Read more on [arXiv](https://arxiv.org/abs/2506.09942) or [HuggingFace](https://huggingface.co/papers/2506.09942))| Lei Hou, Bin Xu, Xiaozhi Wang, Yunjia Qi, Wesleythu | The paper introduces VERIF, a novel verification method for reinforcement learning in instruction following, combining rule-based code verification with LLM-based verification. It addresses the underexplored challenge of verification in RL for instruction following. The key methodology involves constructing a high-quality instruction-following dataset (VERINSTRUCT) with verification signals and applying RL training. Experiments demonstrate significant improvements across various benchmarks, with models achieving state-of-the-art performance. The main implication is that VERIF provides a valuable approach to enhance LLMs in instruction following tasks by ensuring reliable reward signals, improving overall model performance. |
| Machine Learning | Build the web for agents, not agents for the web (Read more on [arXiv](https://arxiv.org/abs/2506.10953) or [HuggingFace](https://huggingface.co/papers/2506.10953))| Siva Reddy, Marius Mosbach, Gaurav Kamath, xhluca | This paper proposes a shift from adapting AI agents to existing web interfaces to designing new "Agentic Web Interfaces" (AWIs) optimized for agentic capabilities. The research aims to address the fundamental mismatch between human-designed interfaces and LLM capabilities for web navigation. The paper outlines six guiding principles for AWI design, emphasizing safety, efficiency, and standardization. The authors advocate for collaborative effort, involving the broader ML community, to overcome limitations of existing interfaces. While no quantitative metrics are provided, the main implication is a call to action for the ML community to develop web interfaces designed specifically for AI agents, potentially leading to more efficient and reliable web agent design. |
| Natural Language Processing | Compound AI Systems Optimization: A Survey of Methods, Challenges, and
  Future Directions (Read more on [arXiv](https://arxiv.org/abs/2506.08234) or [HuggingFace](https://huggingface.co/papers/2506.08234))| Guan-Bo Yang, Jui-Chao Lu, Mei-Yi Liu, Guan-Ting Yi, Yu-Ang Lee | This survey reviews recent progress in optimizing compound AI systems, which integrate multiple components like LLMs, code interpreters, and RAG modules. The paper formalizes the notion of compound AI system optimization and classifies existing methods based on structural flexibility and learning signals. It examines 26 representative works, highlighting open research challenges and future directions in automating the design and optimization of these systems. While the paper does not contain specific quantitative metrics, it identifies manual hyperparameter configuration, excessive computation burden, and limited experimental scope as key challenges. The main implication is a need for more standardized terminology and frameworks to guide the development and deployment of complex AI workflows. |
| Natural Language Processing | LLM Unlearning Should Be Form-Independent (Read more on [arXiv](https://arxiv.org/abs/2506.07795) or [HuggingFace](https://huggingface.co/papers/2506.07795))| Shu Wu, Mengqi Zhang, Acruxos | This paper addresses the form-dependent bias in LLM unlearning, where unlearning effectiveness varies with the expression of knowledge. The research investigates the failure of existing unlearning methods to generalize across different knowledge expressions. It introduces a benchmark, ORT, to quantify this bias and proposes Rank-One Concept Redirection (ROCR), a training-free method that redirects dangerous concepts to harmless ones. Experiments demonstrate that ROCR significantly improves unlearning effectiveness, outperforming traditional methods while maintaining natural outputs, achieving up to a 71.47% probability reduction on knowledge recall. The findings imply that LLM unlearning should focus on form-independent concept manipulation for robust knowledge suppression. |
| Natural Language Processing | What Makes a Good Natural Language Prompt? (Read more on [arXiv](https://arxiv.org/abs/2506.06950) or [HuggingFace](https://huggingface.co/papers/2506.06950))| Nancy F. Chen, Kenji Kawaguchi, Ngoc-Hai Nguyen, Duy Dinh, Do Xuan Long | This paper investigates the factors that define a good natural language prompt for large language models (LLMs). It meta-analyzes over 150 prompting-related papers to propose a property- and human-centric framework with 21 properties across six dimensions for evaluating prompt quality. The study identifies imbalances in prior research, analyzes property correlations, and explores multi-property prompt enhancements in reasoning tasks. Empirically, single-property enhancements often have the greatest impact, and instruction-tuning on property-enhanced prompts yields better reasoning models. The findings provide a foundation for property-centric prompt evaluation and optimization, with the Cohen's Kappa reaching 0.94 under specific conditions. |
| Machine Learning | Breaking Data Silos: Towards Open and Scalable Mobility Foundation
  Models via Generative Continual Learning (Read more on [arXiv](https://arxiv.org/abs/2506.06694) or [HuggingFace](https://huggingface.co/papers/2506.06694))| Yong Li, Chonghua Han, Yukun Liu, Yuan Yuan, JJ-TMT | The paper introduces MoveGCL, a novel framework for training mobility foundation models via generative continual learning in a privacy-preserving manner. It aims to address the challenges of data silos and privacy concerns in human mobility modeling by enabling decentralized model evolution without sharing raw data. The key methodology involves replaying synthetic trajectories generated from a frozen teacher model and using a tailored distillation strategy to mitigate catastrophic forgetting, along with a Mixture-of-Experts Transformer to handle data heterogeneity. Experiments on six real-world urban datasets demonstrate that MoveGCL achieves performance comparable to joint training and significantly outperforms federated learning baselines. This framework offers a practical blueprint for open, scalable, and privacy-preserving model development in the mobility domain, suggesting a pathway to unlock foundation models for mobility data. |
| Computer Vision | Token Perturbation Guidance for Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2506.10036) or [HuggingFace](https://huggingface.co/papers/2506.10036))| Babak Taati, Soroush Mehraban, Javad Rajabi, msadat97 | This paper introduces Token Perturbation Guidance (TPG), a novel training-free technique to improve diffusion model generation quality. TPG directly manipulates intermediate token representations via a norm-preserving shuffling operation to provide stable guidance signals without architectural changes, addressing the limitations of CFG. Experiments on SDXL show TPG achieves nearly a 2x improvement in FID for unconditional generation compared to the baseline, while closely matching CFG in prompt alignment. TPG offers a condition-agnostic guidance method bringing CFG-like benefits to a broader class of diffusion models. The technique can enhance both conditional and unconditional generation capabilities. |
| Natural Language Processing | Draft-based Approximate Inference for LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.08373) or [HuggingFace](https://huggingface.co/papers/2506.08373))| Hyung Il Koo, Minjae Lee, Wonjun Kang, Ethan Ewer, Kevin Galim | The paper introduces a novel draft-based approximate inference framework for long-context LLMs to improve efficiency. It aims to accurately predict token and KV pair importance using small draft models. The methodology includes two instantiations: SpecKV for KV cache dropping and SpecPC for prompt compression, leveraging the draft model's attention activations. Experiments on long-context benchmarks show that the methods consistently achieve higher accuracy than baselines, with up to 25 points improvement on RULER, while preserving memory and latency improvements. This approach offers practitioners a means to accelerate LLM inference without significant loss of accuracy by exploiting draft model predictions. |
| Multi-Modal | LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure
  Profiles (Read more on [arXiv](https://arxiv.org/abs/2506.06561) or [HuggingFace](https://huggingface.co/papers/2506.06561))| Branislav Kveton, Aashish Anantha Ramakrishnan, Ting-Yao Hsu, Ho Yin 'Sam' Ng, Franck-Dernoncourt | This paper introduces LAMP-CAP, a dataset for personalized figure caption generation utilizing multimodal figure profiles. The research aims to improve AI-generated figure captions by personalizing them to match an author's style and domain. LAMP-CAP provides multimodal context including figure images, captions, and related paragraphs from the same document to characterize the context for personalization. Experiments with four LLMs demonstrate that profile information consistently enhances caption generation quality, with ablation studies showing that profile images are more helpful than figure-mentioning paragraphs; personalization enhances the similarity to ground-truth captions. The work offers a new benchmark for multimodal personalization in text generation. |
| Computer Vision | MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness
  Against VLM-based Attacks (Read more on [arXiv](https://arxiv.org/abs/2506.05982) or [HuggingFace](https://huggingface.co/papers/2506.05982))| Yiren Song, Xin Wei, Yule Xue, Zonglin Wu | The paper introduces MCA-Bench, a multimodal benchmark for evaluating CAPTCHA robustness against VLM-based attacks. It addresses the need for a unified framework to assess CAPTCHA security across various modalities. The methodology involves fine-tuning specialized cracking agents for each CAPTCHA category using a shared vision-language model backbone. Experiments reveal MCA-Bench effectively maps the vulnerability spectrum of modern CAPTCHA designs, with VLMs achieving over 96% accuracy on simple tasks but as low as 2.5% on complex ones. The benchmark provides actionable design principles for CAPTCHA hardening and facilitates broader community collaboration. |
| Computer Vision | Fine-Grained Perturbation Guidance via Attention Head Selection (Read more on [arXiv](https://arxiv.org/abs/2506.10978) or [HuggingFace](https://huggingface.co/papers/2506.10978))| Jaewon Min, Minjae Kim, Sanghyun Lee, Jiwon Kang, Donghoon Ahn | The paper introduces a novel approach to diffusion model guidance by selectively perturbing attention heads. It aims to improve generation quality and visual attribute control in diffusion models by investigating the granularity of attention perturbations. The proposed "HeadHunter" framework iteratively selects attention heads based on user-centric objectives and introduces SoftPAG, which linearly interpolates attention maps to tune perturbation strength; experiments on DiT models like Stable Diffusion 3 show superior performance in general quality enhancement and style-specific guidance. The method enables targeted manipulation of visual styles through compositional head selection, achieving, for instance, improved FID scores compared to layer-level guidance. This work provides interpretable specialization within attention layers, enabling practical design of effective perturbation strategies for AI practitioners. |
| Machine Learning | Discovering Hierarchical Latent Capabilities of Language Models via
  Causal Representation Learning (Read more on [arXiv](https://arxiv.org/abs/2506.10378) or [HuggingFace](https://huggingface.co/papers/2506.10378))| Hanlin Zhang, Sham Kakade, Vasilis Syrgkanis, Jikai Jin | The paper proposes a causal representation learning framework to discover hierarchical latent capabilities in language models. It aims to address challenges in evaluating LM capabilities due to complex confounding effects and computational costs. The methodology involves modeling benchmark performance as a linear transformation of latent capability factors, identified as causally interrelated after controlling for the base model. Applying this to 1500 models on the Open LLM Leaderboard reveals a three-node linear causal structure explaining performance variations; the general problem-solving capabilities causally affect reasoning ability.  This reveals the role of controlling for base model variations for evaluation and provides practitioners with insights into capability target for post-training. |
| Computer Vision | StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated
  Video Streams (Read more on [arXiv](https://arxiv.org/abs/2506.08862) or [HuggingFace](https://huggingface.co/papers/2506.08862))| Renjie Liao, Lele Wang, Xuanyu Yi, Qi Yan, Zike Wu | The paper introduces StreamSplat, a feed-forward framework for online dynamic 3D reconstruction from uncalibrated video streams. It aims to achieve real-time, accurate, and stable dynamic scene reconstruction without calibrated inputs. The approach employs probabilistic sampling in the static encoder for 3D Gaussian Splatting (3DGS) position prediction and a bidirectional deformation field in the dynamic decoder. Experiments on static and dynamic benchmarks demonstrate that StreamSplat outperforms prior works in reconstruction quality and dynamic scene modeling, achieving a PSNR of 41.60 on RE10K with given views. StreamSplat enables online reconstruction of arbitrarily long video streams, offering a practical solution for real-time 3D scene understanding. |
