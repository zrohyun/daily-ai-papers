

## Papers for 2025-06-30

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing (Read more on [arXiv](https://arxiv.org/abs/2506.17450) or [HuggingFace](https://huggingface.co/papers/2506.17450))| Sanghyun Woo, Saining Xie, Xuhui Jia, Ramin Mehran, cccjc | BlenderFusion is a generative visual compositing framework that synthesizes new scenes by recomposing objects, camera, and background. It addresses the challenge of precise 3D-aware visual editing by integrating generative models with a 3D graphics engine. The method employs a layering-editing-compositing pipeline, fine-tuning a diffusion model with source masking and simulated object jittering. BlenderFusion demonstrates improved object-level and image-level metrics across multiple datasets (e.g., 21.32 PSNR on MOVi-E), outperforming prior methods. The implication is a more flexible and controllable approach to complex visual content creation tasks. |
| Computer Vision | LLaVA-Scissor: Token Compression with Semantic Connected Components for
  Video LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.21862) or [HuggingFace](https://huggingface.co/papers/2506.21862))| Qibin Hou, Xihan Wei, Jiaxing Zhao, Boyuan Sun | LLaVA-Scissor introduces a training-free token compression strategy for video multimodal large language models. The research aims to compress tokens in videos while maintaining comprehensive semantic coverage to address token redundancy issues.  A two-step spatio-temporal compression strategy utilizing Semantic Connected Components (SCC) is employed in both spatial and temporal domains. Experimental results show that LLaVA-Scissor outperforms existing methods across various video understanding benchmarks, with superior performance at low token retention ratios. The method offers a practical approach for AI practitioners to improve the efficiency and scalability of video processing in LLMs by reducing computational costs without significant performance degradation. |
| Computer Vision | XVerse: Consistent Multi-Subject Control of Identity and Semantic
  Attributes via DiT Modulation (Read more on [arXiv](https://arxiv.org/abs/2506.21416) or [HuggingFace](https://huggingface.co/papers/2506.21416))| Xu Wang, Li Chen, Haomiao Sun, Mengyi Zhao, chenbowen | The paper introduces XVerse, a novel method for consistent multi-subject control of identity and semantic attributes in text-to-image generation using Diffusion Transformers (DiTs). It aims to address the challenges of editability and attribute entanglement in multi-subject image synthesis by transforming reference images into offsets for token-specific text-stream modulation. XVerse learns offsets within DiT blocks and incorporates VAE-encoded image features to enhance details, thus preserving image quality and editability. Quantitative evaluations on XVerseBench demonstrate a superior overall score of 73.40 compared to other methods. This advancement provides AI practitioners with improved personalized and complex scene generation capabilities while maintaining subject identity and semantic control. |
| Computer Vision | ShotBench: Expert-Level Cinematic Understanding in Vision-Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2506.21356) or [HuggingFace](https://huggingface.co/papers/2506.21356))| Yuhao Dong, Dian Zheng, Yi Jin, Jingwen He, Hongbo Liu | The paper introduces ShotBench, a novel benchmark for evaluating cinematic understanding in vision-language models (VLMs). It aims to address the lack of robust evaluation for fine-grained cinematic grammar comprehension in current VLMs. The study curates a dataset of over 3.5k expert-annotated QA pairs across eight cinematography dimensions and evaluates 24 leading VLMs, and the results showed that even the top-performing models achieve less than 60% average accuracy, highlighting limitations in nuanced cinematic understanding. The authors then developed ShotQA dataset and ShotVL model which achieves substantial improvements on ShotBench. This work provides AI practitioners with a specialized benchmark and methodology to enhance cinematic understanding in VLMs, potentially improving AI-driven video generation and analysis. |
| Computer Vision | From Ideal to Real: Unified and Data-Efficient Dense Prediction for
  Real-World Scenarios (Read more on [arXiv](https://arxiv.org/abs/2506.20279) or [HuggingFace](https://huggingface.co/papers/2506.20279))| Minnan Luo, Zhuohang Dang, Chengyou Jia, Changliang Xia | The paper introduces a unified framework for data-efficient dense prediction across real-world scenarios. It addresses the challenge of limited generalization of existing methods from idealized conditions to complex real-world data. The authors propose DenseDiT, a generative model-based approach that leverages visual priors and incorporates a parameter-reuse mechanism with lightweight branches for multi-scale context integration. Evaluations on a newly introduced DenseWorld benchmark, comprising 25 diverse tasks, show that DenseDiT achieves superior performance, reaching an average D-Score of 0.944. This demonstrates the framework's potential for practical and scalable dense prediction in real-world applications with limited supervision. |
| Computer Vision | MiCo: Multi-image Contrast for Reinforcement Visual Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.22434) or [HuggingFace](https://huggingface.co/papers/2506.22434))| Xiaogang Xu, Xiaoyang Wu, Shaoteng Liu, Mingkang Zhu, Xi Chen | The paper introduces MiCo, a novel approach for multi-image visual reasoning using contrastive learning and reinforcement learning. It aims to enhance Vision-Language Models (VLMs) by enabling them to link visual cues across multiple images without relying on human-annotated question-answer pairs. The key methodology involves constructing image triplets comprising augmented views and optimizing the model using rule-based reinforcement learning with Augmented GRPO. Experiments on the VLM2-Bench dataset show significant improvements, achieving a new state-of-the-art performance (exact quantitative metric unspecified). MiCo offers a data-efficient method to train VLMs to perform fine-grained visual reasoning across multiple images, which is beneficial for AI applications where labelled data is scarce. |
| Multi-Modal | Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs (Read more on [arXiv](https://arxiv.org/abs/2506.21656) or [HuggingFace](https://huggingface.co/papers/2506.21656))| Xiaofeng Zhang, Xu Cao, Jingyuan Zhu, Yuanzhe Liu, Yifan Shen | The paper introduces SpatialReasoner-R1, a VLM designed for fine-grained spatial reasoning by utilizing multi-step logic. It addresses the limitation of VLMs in scenarios involving complex object arrangements by employing interpretable LongCoT reasoning. The methodology uses a Multi-Model Monte Carlo Tree Search (M3CTS) to generate LongCoT reasoning trajectories and a fine-grained Direct Preference Optimization (fDPO) approach. Experimental results show that fDPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1 sets a new SoTA on SPATIALRGPT-BENCH, indicating enhanced spatial understanding capabilities, with minimal impact on general vision-language performance. |
| Natural Language Processing | Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity (Read more on [arXiv](https://arxiv.org/abs/2505.21411) or [HuggingFace](https://huggingface.co/papers/2505.21411))| Wei Guo, Xiaosong Li, MightyCrane, Fangcheng2, tangyehui | This paper introduces Mixture of Grouped Experts (MoGE), an architecture designed to balance expert workload in large language models (LLMs). MoGE groups experts during selection, constraining tokens to activate an equal number of experts within each group to ensure computational load balance across devices. Experiments on the Pangu Pro MoE model, a 72 billion parameter model based on MoGE, achieved 1148 tokens/s per card during inference on Ascend NPUs. The architecture and associated optimizations lead to improved expert load balancing and more efficient execution, providing an alternative to traditional MoE. |
| Machine Learning | Ark: An Open-source Python-based Framework for Robot Learning (Read more on [arXiv](https://arxiv.org/abs/2506.21628) or [HuggingFace](https://huggingface.co/papers/2506.21628))| Jiacheng Qiu, Huang Helong, Sarthak Das, Christopher E. Mower, Magnus Dierking | The paper introduces Ark, an open-source Python-based framework designed to streamline robot learning by bridging the gap between AI and robotics. The primary objective is to provide a Gym-style environment interface with seamless toggling between simulation and physical robots. Ark employs a lightweight client-server architecture with optional C/C++ bindings, facilitating imitation learning and ROS interoperability. The framework aims to lower entry barriers for autonomous robot development and research. Though no explicit performance metrics are available, the framework demonstrated rapid prototyping and hardware swapping. |
| Computer Vision | Noise Consistency Training: A Native Approach for One-Step Generator in
  Learning Additional Controls (Read more on [arXiv](https://arxiv.org/abs/2506.19741) or [HuggingFace](https://huggingface.co/papers/2506.19741))| Jing Tang, Tianyang Hu, Shuchen Xue, Yihong Luo | The paper introduces Noise Consistency Training (NCT), a novel approach for integrating new control signals into pre-trained one-step generators. It addresses the challenge of adapting efficient one-step generators to new control conditions without extensive retraining. NCT employs an adapter module and a noise consistency loss in the generator's noise space to align generation behavior across conditionally dependent noises. Experiments demonstrate that NCT achieves state-of-the-art controllable generation in a single forward pass, outperforming existing methods (e.g., achieving FID scores of 13.67 for Canny edge control). NCT offers AI practitioners a modular, data-efficient, and easily deployable solution for controllable generation. |
| Machine Learning | The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT
  Improvements (Read more on [arXiv](https://arxiv.org/abs/2506.22419) or [HuggingFace](https://huggingface.co/papers/2506.22419))| Roberta Raileanu, Xian Li, Minqi Jiang, Despoina Magka, Bingchen Zhao | The paper introduces the Automated LLM Speedrunning Benchmark to assess LLMs' ability to reproduce research findings in LLM training. The research question explores the extent to which LLMs can automate scientific reproduction. The methodology involves challenging LLMs with NanoGPT speedrun tasks, providing previous records and optional hints. Results show recent LLMs, even with detailed hints, struggle to match speedup improvements, recovering less than 20% of the human achieved speedup without hints. The benchmark offers a non-saturated measure of LLMs' capacity for automating scientific reproduction, highlighting current limitations. |
| Natural Language Processing | Gazal-R1: Achieving State-of-the-Art Medical Reasoning with
  Parameter-Efficient Two-Stage Training (Read more on [arXiv](https://arxiv.org/abs/2506.21594) or [HuggingFace](https://huggingface.co/papers/2506.21594))| Amr Fawzy, Mostafa Samy, Ahmed M. Adly | The paper introduces Gazal-R1, a 32B-parameter language model for state-of-the-art medical reasoning with transparent explanations. It aims to achieve high medical reasoning performance using parameter-efficient training methods. Gazal-R1 utilizes a two-stage training pipeline: supervised fine-tuning on synthetic medical examples and reinforcement learning via Group Relative Policy Optimization (GRPO). The model achieves 87.1% on MedQA, outperforming larger models. The study offers a reproducible framework for developing high-capability, domain-specific language models that balance performance, efficiency, and explainability. |
| Natural Language Processing | Confucius3-Math: A Lightweight High-Performance Reasoning LLM for
  Chinese K-12 Mathematics Learning (Read more on [arXiv](https://arxiv.org/abs/2506.18330) or [HuggingFace](https://huggingface.co/papers/2506.18330))| Yitao Duan, Jiachen Wang, Qiao Cheng, Na Cai, nomadlx | The paper introduces Confucius3-Math, a 14B parameter large language model optimized for Chinese K-12 mathematics learning. It addresses the challenge of creating a high-performance, low-cost reasoning LLM for education. The methodology involves post-training with reinforcement learning and novel techniques like Targeted Entropy Regularization, Recent Sample Recovery, and Policy-Specific Hardness Weighting. Confucius3-Math achieves SOTA performance on K-12 math benchmarks, outperforming larger models and reaching 96.24% accuracy on the CK12-MATH benchmark, while costing only $26K to train. This demonstrates the feasibility of building strong, domain-specific reasoning models at significantly reduced costs, making advanced AI education tools more accessible. |
| Computer Vision | RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation
  Models (Read more on [arXiv](https://arxiv.org/abs/2506.22149) or [HuggingFace](https://huggingface.co/papers/2506.22149))| Hrvoje Bogunović, Ursula Schmidt-Erfurth, José Morano, Ronald Fecso | The paper introduces RetFiner, a vision-language refinement scheme for improving retinal foundation models using self-supervised learning. It aims to enhance the semantic understanding of existing FMs and facilitate adaptation to specific populations. The method refines the model by training a vision-language model with diverse objectives using EHRs as supervisory signals. Results show significant improvements in linear probing performance, with an average increase of 5.8% in balanced accuracy across seven OCT classification tasks. RetFiner provides a way to adapt models to specific data distributions without requiring manual annotation, enhancing semantic understanding. |
