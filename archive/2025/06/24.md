

## Papers for 2025-06-24

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | Light of Normals: Unified Feature Representation for Universal
  Photometric Stereo (Read more on [arXiv](https://arxiv.org/abs/2506.18882) or [HuggingFace](https://huggingface.co/papers/2506.18882))| Bohan Li, Zhaoxi Chen, Chongjie Ye, Houyuan Chen, Hong Li | The paper introduces LINO-UniPS, a novel approach to universal photometric stereo for recovering high-quality surface normals under varying lighting conditions. The research addresses the challenge of decoupling illumination and normal features while preserving high-frequency geometric details. LINO-UniPS employs learnable light register tokens, wavelet transform-based sampling, and a normal-gradient confidence loss. Experiments on synthetic and real datasets demonstrate state-of-the-art performance, achieving higher feature consistency (SSIM and CSIM) and improving accuracy up to 3.89° mean angular error. The work offers AI practitioners an improved method and synthetic dataset (PS-Verse) for training robust photometric stereo models, enhancing 3D reconstruction in unconstrained lighting environments. |
| Multi-Modal | OmniGen2: Exploration to Advanced Multimodal Generation (Read more on [arXiv](https://arxiv.org/abs/2506.18871) or [HuggingFace](https://huggingface.co/papers/2506.18871))| yzwang, sienna223, Shitao, Ruiran, wcyno23 | The paper introduces OmniGen2, a versatile and open-source generative model for diverse generation tasks including text-to-image, image editing, and in-context generation. The research aims to provide a unified solution for multimodal tasks by employing distinct decoding pathways for text and image modalities and a decoupled image tokenizer. The methodology includes developing comprehensive data construction pipelines and introducing a reflection mechanism for image generation. OmniGen2 achieves competitive results on multiple benchmarks including achieving state-of-the-art performance among open-source models on the OmniContext benchmark in terms of consistency. OmniGen2 offers AI practitioners a unified framework for multimodal generation with competitive performance and strong text generation capabilities. |
| Natural Language Processing | LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2506.18841) or [HuggingFace](https://huggingface.co/papers/2506.18841))| Juanzi Li, Roy Ka-Wei Lee, Yushi Bai, Yuhao Wu, Zhiqiang007 | This paper introduces LongWriter-Zero, a reinforcement learning (RL) approach to ultra-long text generation without relying on synthetic or annotated data. The research explores how RL can foster long-form generation capabilities in LLMs by optimizing length control, writing quality, and structural formatting through specialized reward models. LongWriter-Zero, trained on Qwen2.5-32B, outperforms supervised fine-tuning methods, achieving state-of-the-art results on WritingBench and Arena-Write, including an Elo rating of 1447 on Arena-Write. The main implication is that RL can effectively scale LLMs for coherent ultra-long text production, offering an alternative to synthetic data-dependent approaches. |
| Computer Vision | Phantom-Data : Towards a General Subject-Consistent Video Generation
  Dataset (Read more on [arXiv](https://arxiv.org/abs/2506.18851) or [HuggingFace](https://huggingface.co/papers/2506.18851))| Crayon-Shinchan, onion-liu, TianxiangMa, lbc402, ZhuoweiChen | The paper introduces Phantom-Data, a large-scale cross-pair subject-to-video consistency dataset. It addresses the copy-paste problem in subject-to-video generation by disentangling subject identity from background and contextual attributes. The dataset is constructed using a three-stage pipeline involving subject detection, cross-context retrieval, and identity verification. Experiments demonstrate improved prompt alignment and visual quality while maintaining identity consistency, achieving a CLIP score of 0.416 compared to 0.354 of face cross-pair. This dataset aims to provide AI practitioners with resources to create more controllable and realistic AI-generated video content. |
| Natural Language Processing | ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought
  Reasoning in LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.18896) or [HuggingFace](https://huggingface.co/papers/2506.18896))| Ke Shen, Jiahao Qiu, Jingwen Gu, Ling Yang, Jiaru Zou | The paper introduces ReasonFlux-PRM, a trajectory-aware process reward model (PRM) for evaluating chain-of-thought reasoning in LLMs. It addresses the limitations of existing PRMs in supervising intermediate reasoning steps by incorporating both step-level and trajectory-level supervision. ReasonFlux-PRM is adapted for offline data selection and online policy optimization, demonstrating a 12.1% average gain in supervised fine-tuning and 4.5% in reinforcement learning. The model enhances data selection quality and improves reasoning performance by identifying high-quality reasoning traces. ReasonFlux-PRM offers improved supervision for long chain-of-thought reasoning in LLMs. |
| Reinforcement Learning | RLPR: Extrapolating RLVR to General Domains without Verifiers (Read more on [arXiv](https://arxiv.org/abs/2506.18254) or [HuggingFace](https://huggingface.co/papers/2506.18254))| Zefan Wang, Shu Yao, Shouli Wang, Bo Ji, Tianyu Yu | The paper introduces RLPR, a verifier-free framework for extending Reinforcement Learning with Verifiable Rewards (RLVR) to general domains. It addresses the limitation of existing RLVR methods that heavily rely on domain-specific verifiers by using the LLM's intrinsic probability of generating a correct answer as the reward signal. The key methodology involves using a probability-based reward calculated from the average decoding probabilities of reference answer tokens, along with a debiasing method and adaptive curriculum learning. Experiments on seven benchmarks show that RLPR improves reasoning capabilities, achieving, for example, a 7.6 point increase on TheoremQA compared to VeriFree. RLPR offers a more scalable and generalizable approach to RLVR by eliminating the need for complex, domain-specific verifiers. |
| Multi-Modal | Vision as a Dialect: Unifying Visual Understanding and Generation via
  Text-Aligned Representations (Read more on [arXiv](https://arxiv.org/abs/2506.18898) or [HuggingFace](https://huggingface.co/papers/2506.18898))| Qi Zhao, Yang Zhao, Hao Chen, hywang66, csuhan | This paper introduces Tar, a multimodal framework that unifies visual understanding and generation through a shared, discrete, text-aligned representation. The research aims to bridge the gap between visual understanding and generation within a single model. Tar utilizes a Text-Aligned Tokenizer (TA-Tok) to convert images into discrete tokens based on a text-aligned codebook and employs scale-adaptive encoding/decoding with generative de-tokenizers. Experiments show Tar matches or surpasses existing multimodal LLM methods and achieves an 82.96 score on DPG Bench using Tar-1.5B, demonstrating its effectiveness in unifying these modalities. The framework's shared representation and efficient training could streamline the development of multimodal AI systems. |
| Natural Language Processing | OAgents: An Empirical Study of Building Effective Agents (Read more on [arXiv](https://arxiv.org/abs/2506.15741) or [HuggingFace](https://huggingface.co/papers/2506.15741))| Yeyi Guan, Heyuan Huang, He Zhu, kangz, tianyue818 | The paper presents an empirical study on building effective agents, focusing on addressing the lack of standardization in agent research. It investigates the impact of various design choices in agent frameworks, particularly on the GAIA benchmark, to enhance scientific rigor and facilitate fair comparisons. Through systematic experimentation and evaluation, the study identifies crucial components for effective agents, such as planning, memory, and test-time scaling strategies. The paper introduces OAGENTS, a new foundation agent framework, achieving state-of-the-art performance with an overall average score of 73.93% on the GAIA benchmark, outperforming existing open-source projects. The main implication is to offer a modular design and standardized evaluation protocol for future research in Agentic AI. |
| Computer Vision | VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed
  View Memory (Read more on [arXiv](https://arxiv.org/abs/2506.18903) or [HuggingFace](https://huggingface.co/papers/2506.18903))| Tomas Jakab, Andrea Vedaldi, Philip Torr, Runjia Li | The paper introduces VMem, a novel memory mechanism for consistent interactive video scene generation. It addresses the challenge of long-term coherence in autoregressive video generation by geometrically indexing past views using 3D surfels. VMem efficiently retrieves relevant past views to condition new view generation, reducing computational cost and improving consistency. Experiments on long-term scene synthesis benchmarks demonstrate superior performance compared to existing methods, with improvements observed in Fréchet Image Distance (FID). This provides AI practitioners with a scalable approach for creating realistic and consistent interactive environments. |
| Machine Learning | LettinGo: Explore User Profile Generation for Recommendation System (Read more on [arXiv](https://arxiv.org/abs/2506.18309) or [HuggingFace](https://huggingface.co/papers/2506.18309))| Jianfeng Liu, Pu Zhao, Fangkai Yang, Di Zhang, Lu Wang | This paper introduces LETTINGO, a novel framework for generating diverse user profiles to improve recommendation systems. The research aims to enhance user profile generation by exploring different formats and incorporating task-specific feedback. LETTINGO uses multiple LLMs for profile exploration, evaluates profile quality based on recommendation performance, and aligns profile generation through pairwise preference data using Direct Preference Optimization (DPO). Experimental results demonstrate that LETTINGO significantly enhances recommendation accuracy, achieving an average accuracy improvement of 20 percentage points compared to baseline methods on the LLaMA3 8B Instruct model. The approach provides AI practitioners with a flexible framework for creating adaptive, high-quality profiles, improving recommendation performance and generalizability. |
| Machine Learning | ReDit: Reward Dithering for Improved LLM Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2506.18631) or [HuggingFace](https://huggingface.co/papers/2506.18631))| Yao Shu, Hande Dong, Ying Tiffany He, Jiarui Yu, Chenxing Wei | This paper introduces ReDit, a reward dithering method to improve LLM policy optimization within reinforcement learning by addressing unstable gradients and slow convergence. The main objective is to mitigate gradient anomalies associated with discrete rule-based reward systems by introducing random noise. ReDit perturbs the reward signal with zero-mean random noise, providing exploratory gradients for smoother updates and accelerated convergence. Experiments across diverse tasks and LLMs show that ReDit achieves comparable performance to vanilla GRPO with only approximately 10% of the training steps, exhibiting a 4% improvement when trained for a similar duration. This suggests that AI practitioners can leverage ReDit to enhance the stability and efficiency of LLM policy optimization, particularly when working with discrete reward functions. |
| Natural Language Processing | FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.16123) or [HuggingFace](https://huggingface.co/papers/2506.16123))| Potsawee Manakul, Panop Pitchayarthorn, Warit Sirichotedumrong, pittawat, natnitaract | The paper introduces FinCoT, a structured chain-of-thought prompting framework grounded in expert financial reasoning. It investigates how domain-specific expert insights can guide reasoning traces of large language models in financial tasks. FinCoT uses Mermaid blueprints to inject expert financial workflows into structured CoT templates, yielding auditable reasoning. Experiments on 1,032 CFA-style questions show FinCoT improves performance from 63.2% to 80.5% and reduces token generation eight-fold compared to structured CoT prompting. This approach enables more interpretable and domain-aligned reasoning traces for AI applications in finance. |
| Computer Vision | ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs (Read more on [arXiv](https://arxiv.org/abs/2506.18792) or [HuggingFace](https://huggingface.co/papers/2506.18792))| Gregory Slabaugh, Zhensong Zhang, Thomas Tanay, Sibi Catley-Chandar, Michal Nazarczuk | The paper introduces ViDAR, a video diffusion-aware 4D reconstruction framework from monocular inputs for dynamic novel view synthesis. It aims to improve reconstruction quality from monocular videos by leveraging personalized diffusion models to generate pseudo multi-view supervision signals. ViDAR uses scene-specific features and a diffusion-aware loss to recover fine-grained details while ensuring spatio-temporal consistency, along with camera pose optimization. Experiments on DyCheck show that ViDAR outperforms state-of-the-art baselines in visual quality and geometric consistency, achieving improved PSNR. The implication is a more effective approach to reconstructing dynamic scenes from limited monocular video data. |
| Computer Vision | Auto-Regressively Generating Multi-View Consistent Images (Read more on [arXiv](https://arxiv.org/abs/2506.18527) or [HuggingFace](https://huggingface.co/papers/2506.18527))| Chen Zhao, Jinbo Wu, Jialun Liu, Yuxiao Yang, JiaKui Hu | The paper introduces Multi-View Auto-Regressive (MV-AR), a novel method for generating consistent multi-view images from various prompts. It addresses the challenge of maintaining consistency across views while synthesizing shapes and textures under diverse conditions by leveraging an auto-regressive model. The key methodology involves a unified model that accommodates multiple conditions via architecture design and progressive training. Experiments demonstrate comparable multi-view image quality and enhanced image-text consistency, achieving a CLIP-Score of 29.49 on GSO. This work provides a new baseline for AR-based multi-view generation and facilitates the development of unified multi-view generation models capable of handling diverse conditional inputs. |
| Machine Learning | SlimMoE: Structured Compression of Large MoE Models via Expert Slimming
  and Distillation (Read more on [arXiv](https://arxiv.org/abs/2506.18349) or [HuggingFace](https://huggingface.co/papers/2506.18349))| Young Jin Kim, Ilgee Hong, Zixuan Zhang, Chen Liang, Pearush | SlimMoE introduces a multi-stage compression framework for Mixture of Experts (MoE) models via expert slimming and knowledge distillation. The research aims to reduce the memory footprint of large MoE models while maintaining inference efficiency, addressing the challenge of deploying them in resource-constrained environments. The SlimMoE method systematically prunes experts and transfers knowledge through intermediate stages, mitigating performance degradation typical of one-shot pruning. Using this approach, Phi-3.5-MoE (41.9B parameters) is compressed to Phi-mini-MoE (7.6B) and Phi-tiny-MoE (3.8B), with Phi-mini-MoE matching or exceeding Phi-3-mini's performance using fewer activated parameters. The framework enables broader adoption of MoE architectures by reducing computational resource requirements for fine-tuning and deployment. |
| Multi-Modal | Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs (Read more on [arXiv](https://arxiv.org/abs/2506.16962) or [HuggingFace](https://huggingface.co/papers/2506.16962))| Wenjie Li, Yujie Zhang, Wenjie Lou, Yankai Jiang, manglu3935 | This paper introduces Mentor-Intern Collaborative Search (MICS) to enhance step-by-step medical reasoning in MLLMs. The research aims to generate rigorous and effective medical chain-of-thought (CoT) data by collaboratively searching for reasoning paths and evaluating them with an MICS-Score. MICS leverages multiple mentor models to initialize reasoning paths, which are then completed by intern models, and the optimal path is selected based on the interns' performance.  Experiments show that Chiron-01, trained on MICS-generated data, achieves state-of-the-art performance across medical VQA benchmarks, for example, significantly outperforming baselines on VQA-RAD. This approach provides a new framework for creating verifiable medical reasoning datasets and models. |
| Machine Learning | ConsumerBench: Benchmarking Generative AI Applications on End-User
  Devices (Read more on [arXiv](https://arxiv.org/abs/2506.17538) or [HuggingFace](https://huggingface.co/papers/2506.17538))| Yiyu Liu, Hoang Nguyen, Rohan Kadekodi, Yile Gu, kamahori | CONSUMERBENCH is introduced as a benchmarking framework for evaluating GenAI applications on end-user devices, addressing the shift from cloud to local environments. The research objective is to evaluate system efficiency and response time of GenAI models under realistic, concurrent application scenarios. CONSUMERBENCH captures application-level and system-level metrics, revealing resource sharing inefficiencies and SLO attainment issues. Experiments demonstrate greedy allocation leads to starvation, with static partitioning underutilizing resources; tailored kernels and SLO-aware strategies are beneficial and can improve performance. The framework provides insights for practitioners on designing efficient GenAI systems that meet SLOs on constrained hardware, improving user experience by optimizing resource management and scheduling. |
| Natural Language Processing | CommVQ: Commutative Vector Quantization for KV Cache Compression (Read more on [arXiv](https://arxiv.org/abs/2506.18879) or [HuggingFace](https://huggingface.co/papers/2506.18879))| Tianle Cai, Talha Chafekar, Muhammad Yusuf Hassan, Yang Zhang, Junyan Li | The paper introduces Commutative Vector Quantization (CommVQ) for compressing the KV cache in long-context Large Language Models (LLMs). It aims to reduce the GPU memory bottleneck associated with growing context lengths in LLMs. CommVQ employs additive quantization with a RoPE-commutative codebook learned via Expectation-Maximization, enabling efficient integration with self-attention. Experiments show CommVQ achieves up to 87.5% KV cache size reduction with 2-bit quantization, outperforming existing methods on benchmarks like LongBench and GSM8K; 1-bit quantization enabled a LLaMA-3.1 8B model to run with 128K context on a single RTX 4090 GPU. This allows practitioners to deploy long-context LLMs in resource-constrained environments. |
| Computer Vision | From Virtual Games to Real-World Play (Read more on [arXiv](https://arxiv.org/abs/2506.18901) or [HuggingFace](https://huggingface.co/papers/2506.18901))| Zilong Chen, Xi Chen, Jinjing Zhao, Fangyun Wei, Wenqiang Sun | The paper introduces RealPlay, a neural network-based real-world game engine enabling interactive video generation from user control signals. It addresses the research question of transferring control from virtual environments to real-world scenarios without relying on labeled real-world data. The methodology involves training a diffusion-based video generation model on a combination of labeled game data and unlabeled real-world videos, incorporating an adaptive LayerNorm mechanism for action control. The results demonstrate a control success rate of 90% when using adaptive LayerNorm for action fusion. RealPlay enables control of diverse real-world entities like bicycles and pedestrians, demonstrating the potential for bridging the gap between simulation and real-world applications in AI. |
| Natural Language Processing | FaithfulSAE: Towards Capturing Faithful Features with Sparse
  Autoencoders without External Dataset Dependencies (Read more on [arXiv](https://arxiv.org/abs/2506.17673) or [HuggingFace](https://huggingface.co/papers/2506.17673))| Andrew Bermingham, Luis Eduardo Rodrigues Vieira, Donghyun Lee, Harryn Oh, seonglae | The paper introduces FaithfulSAE, a method for training sparse autoencoders (SAEs) on language models using self-generated synthetic datasets to improve the interpretability of learned features. It addresses the issue of "Fake Features" arising from training on out-of-distribution external datasets. FaithfulSAE trains SAEs on the model's own synthetic data, resulting in more stable feature sets across different seeds. Experiments show FaithfulSAE outperforms SAEs trained on web-based datasets in the SAE probing task and reduces the Fake Feature Ratio in most models. This approach advances interpretability by better capturing model-internal features, encapsulating the training data within the model’s own synthetic capabilities and potentially yielding more robust feature representations. |
| Machine Learning | A deep learning and machine learning approach to predict neonatal death
  in the context of São Paulo (Read more on [arXiv](https://arxiv.org/abs/2506.16929) or [HuggingFace](https://huggingface.co/papers/2506.16929))| Afia Anjum Tamanna, A Z M Tahmidul Kabir, Plabon Kumar Saha, Mohon Raihan, rajandasgupta | This paper focuses on predicting neonatal death using machine learning and deep learning techniques in São Paulo. The main objective is to develop a robust model for early prediction of endangered newborns using historical data. The study employed machine learning algorithms such as Logistic Regression, KNN, Random Forest, XGBoost, and deep learning models like CNN and LSTM, training the models on a dataset of 1.4 million newborn child records. LSTM achieved the best outcome with 99% accuracy. The research implies that LSTM can be used to predict whether precaution for a child is necessary. |
| Machine Learning | Robust Reward Modeling via Causal Rubrics (Read more on [arXiv](https://arxiv.org/abs/2506.16507) or [HuggingFace](https://huggingface.co/papers/2506.16507))| Sravanti Addepalli, Gandharv Patil, Rahul Madhavan, Harman Singh, Pragya Srivastava | This paper introduces CROME, a framework for training robust reward models (RMs) to mitigate reward hacking in large language model alignment. The research focuses on disentangling causal quality drivers from spurious attributes by employing synthetic targeted augmentations during training. CROME utilizes causal and neutral augmentations generated via an oracle LLM, improving average accuracy on RewardBench by up to 5.4%. The consistent gains in Best-of-N inference suggest improved robustness for AI practitioners by reducing RM reliance on spurious correlations, enhancing the alignment process. |
| Machine Learning | I Know Which LLM Wrote Your Code Last Summer: LLM generated Code
  Stylometry for Authorship Attribution (Read more on [arXiv](https://arxiv.org/abs/2506.17323) or [HuggingFace](https://huggingface.co/papers/2506.17323))| Bertalan Borsos, Nils Gruschka, Richard A. Dubniczky, Tamas Bisztray, Neo111x | This paper presents LLM-generated code stylometry for authorship attribution. It investigates the feasibility of identifying the LLM source of C programs using supervised learning. The study introduces CodeT5-Authorship, a novel encoder-based model, and LLM-AuthorBench, a benchmark dataset of 32,000 C programs generated by eight LLMs. CodeT5-Authorship achieves 97.56% accuracy in binary classification and 95.40% accuracy in multi-class attribution. The findings suggest that LLMs exhibit distinguishable coding styles, enabling authorship attribution for enhanced accountability and security. |
| Natural Language Processing | SoK: Evaluating Jailbreak Guardrails for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.10597) or [HuggingFace](https://huggingface.co/papers/2506.10597))| Daoyuan Wu, Zongjie Li, Wenxuan Wang, Zhenlan Ji, Xunguang Wang | This paper presents a systematization of knowledge (SoK) evaluating jailbreak guardrails for large language models (LLMs). It aims to provide a holistic analysis of existing guardrails by proposing a novel, multi-dimensional taxonomy and a Security-Efficiency-Utility (SEU) evaluation framework. The paper conducts extensive analysis and experiments to identify the strengths and limitations of existing approaches, exploring their universality across attack types. The results show that GuardReasoner (Pre) demonstrates the most robust defense, achieving the lowest attack success rate (ASR) of 0.135 on average, but there exist trade-offs with efficiency and utility. The work offers a structured foundation for future research, guiding the advancement and deployment of robust LLM guardrails for AI practitioners. |
