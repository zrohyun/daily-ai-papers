

## Papers for 2025-06-18

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Scaling Test-time Compute for LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2506.12928) or [HuggingFace](https://huggingface.co/papers/2506.12928))| Dehua Ma, Tianshun Xing, Siwei Wu, Hanhao Li, King Zhu | This paper investigates the impact of scaling test-time compute (TTS) for language agent performance. It systematically explores parallel sampling, sequential revision, and verify-and-merge strategies to enhance language agent effectiveness. The study finds that applying parallel sampling algorithms improves agent performance and that knowing when to revise is important. List-wise methods outperform others in verification and result merging, with increasing diversified rollouts showing a positive impact on agent task performance, achieving improvements of up to 8 points over the baseline. The work implies that TTS methods can be effectively adapted to language agents, aligning with observed test-time scaling phenomena. |
| Natural Language Processing | LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.14429) or [HuggingFace](https://huggingface.co/papers/2506.14429))| Ziwei He, Qipeng Guo, Zengfeng Huang, Zhigeng Liu, Xiaoran Liu | The paper introduces LongLLaDA, a method to enhance long-context capabilities in diffusion LLMs. It investigates the long-context performance of diffusion LLMs compared to autoregressive models, identifying stable perplexity and localized perception during extrapolation, explained by RoPE scaling. LongLLaDA integrates LLaDA with NTK-based RoPE extrapolation without training, validating extrapolation scaling laws. Experiments show diffusion LLMs exhibit superior question answering performance, while matching autoregressive retrieval scores and lagging in aggregation tasks, providing insights into diffusion LLM context handling. |
| Reinforcement Learning | Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes
  Correct Reasoning in Base LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.14245) or [HuggingFace](https://huggingface.co/papers/2506.14245))| Shengyu Ye, Zhijian Xu, Zihan Liu, Xumeng Wen, shun-zheng | The paper addresses the paradox in Reinforcement Learning with Verifiable Rewards (RLVR) where improved Pass@1 does not translate to higher Pass@K scores, suggesting a flaw in the evaluation metric itself. It introduces CoT-Pass@K, which requires both a correct answer and a correct reasoning chain, and theoretically formalizes RLVR's incentivization of logical integrity. Empirically, RLVR shows improved CoT-Pass@K across all tested values of K on math benchmarks, using an LLM-as-a-CoT-Judge paradigm for CoT verification. Analyzing training dynamics reveals that enhanced reasoning emerges early and generalizes, implying that RLVR can advance machine reasoning by promoting correct reasoning paths. |
| Multi-Modal | Stream-Omni: Simultaneous Multimodal Interactions with Large
  Language-Vision-Speech Model (Read more on [arXiv](https://arxiv.org/abs/2506.13642) or [HuggingFace](https://huggingface.co/papers/2506.13642))| Yang Feng, Yan Zhou, Qingkai Fang, Shoutao Guo, Shaolei Zhang | The paper introduces Stream-Omni, a large language-vision-speech model designed for simultaneous interactions across multiple modalities. It addresses the challenge of efficiently aligning text, vision, and speech by using sequence-dimension concatenation for vision-text alignment and CTC-based layer-dimension mapping for speech-text alignment. Stream-Omni achieves strong performance on tasks like visual understanding, speech interaction, and vision-grounded speech interaction using only 23,000 hours of speech data.  By leveraging layer-dimensional mapping, the model provides intermediate text outputs during speech interaction. This approach allows for more flexible modality alignments and enhanced multimodal experiences for users. |
| Reinforcement Learning | Efficient Medical VIE via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2506.13363) or [HuggingFace](https://huggingface.co/papers/2506.13363))| Chong Li, Chenglin Zhu, Lijun Liu, zhaocheng, lryyyy | The paper introduces a reinforcement learning approach for efficient medical visual information extraction (VIE) using a limited dataset of 100 annotated samples. It addresses the challenges of domain-specific schemas and high annotation costs by employing Reinforcement Learning with Verifiable Rewards (RLVR). The objective is to enhance medical VIE performance by ensuring dataset diversity, reducing hallucinations, and improving field coverage through a balanced precision-recall reward mechanism and strategic sampling. Fine-tuning Qwen2.5-VL-7B with RLVR achieves state-of-the-art performance, significantly improving F1 scores, precision, and recall on medical VIE tasks. The approach provides an efficient and robust solution for medical VIE tasks, especially in scenarios with limited annotated data, by leveraging reasoning during training and inference. |
| Reinforcement Learning | Reasoning with Exploration: An Entropy Perspective (Read more on [arXiv](https://arxiv.org/abs/2506.14758) or [HuggingFace](https://huggingface.co/papers/2506.14758))| Wayne Xin Zhao, Bo Dai, Xuekai Zhu, Shaohan Huang, daixuancheng | This paper explores the role of entropy in guiding exploratory reasoning within language models for reinforcement learning. It addresses the performance plateaus encountered by exploitation-focused methods by augmenting the RL advantage function with an entropy-based term to promote longer, deeper reasoning chains. The method achieves significant gains on the Pass@K metric for language model reasoning, even with extremely large K values, indicating improved reasoning capability. Specifically, the study reveals positive correlations between high-entropy regions and exploratory actions like pivotal tokens and self-verification. These findings imply that integrating entropy-based exploration can enhance LM reasoning in RL by fostering more diverse and extended reasoning processes. |
| Natural Language Processing | Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just
  Like an Olympiad Team (Read more on [arXiv](https://arxiv.org/abs/2506.14234) or [HuggingFace](https://huggingface.co/papers/2506.14234))| Md Rizwan Parvez, Md Kishor Morol, Salman Rahman, Md Tanzib Hosain | The paper introduces Xolver, a training-free, multi-agent reasoning framework designed to equip black-box LLMs with a persistent memory of holistic experience. Xolver aims to overcome the limitations of current LLMs that operate in isolation by integrating diverse experience modalities like external retrieval, tool use, and collaborative agent interactions. The methodology involves a multi-agent system with an evolving shared memory that guides reasoning, tool invocation, and iterative refinement. Xolver achieves state-of-the-art results, including 98.1% on GSM8K using a o3-mini-high backbone. This work implies that holistic experience learning is crucial for developing dynamic, generalist agents capable of expert-level reasoning. |
| Natural Language Processing | QFFT, Question-Free Fine-Tuning for Adaptive Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.12860) or [HuggingFace](https://huggingface.co/papers/2506.12860))| Ke Ji, Yukang Lin, Fei Yu, Junxiao Xu, lwl-uestc | The paper introduces Question-Free Fine-Tuning (QFFT) to enable adaptive reasoning in large language models by selectively employing Short and Long Chain-of-Thought (CoT) patterns. It aims to address the issue of overthinking in existing Long CoT models by training models exclusively on Long CoT responses without input questions. The key methodology involves fine-tuning models on Long CoT outputs, allowing them to prioritize Short CoT patterns by default and activate Long CoT reasoning when necessary. Experiments show that QFFT reduces average response length by over 50% while maintaining performance comparable to Supervised Fine Tuning (SFT) on mathematical datasets. The main implication is a more efficient and adaptive reasoning strategy for language models that mitigates overthinking and enhances generalization. |
| Machine Learning | Can LLMs Generate High-Quality Test Cases for Algorithm Problems?
  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure (Read more on [arXiv](https://arxiv.org/abs/2506.12278) or [HuggingFace](https://huggingface.co/papers/2506.12278))| Xue Xia, Zexi Kuang, Zheyuan Yang, yilunzhao | The paper introduces TestCase-Eval, a benchmark for evaluating LLMs in generating test cases for algorithm problems. It investigates whether LLMs can generate test cases that match or surpass human-designed ones in terms of fault coverage and exposure. The methodology involves assessing LLM-generated test cases on 500 algorithm problems from Codeforces, focusing on fault coverage and fault exposure. Results show even top models like Qwen3-32B achieve only 43.8% on the Fault Exposure task, highlighting a significant gap compared to human performance (93.3%). The implication is that current LLMs struggle with generating high-quality test cases for complex algorithmic problems, indicating a need for further research in improving their analytical capabilities for software testing. |
| Natural Language Processing | Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC
  Transpilation with Testing Guarantees (Read more on [arXiv](https://arxiv.org/abs/2506.14606) or [HuggingFace](https://huggingface.co/papers/2506.14606))| Abdulrahman Mahmoud, Celine Lee, Chaimaa Abi, Ahmed Heakl, Sarim-Hash | The paper introduces Guaranteed Guess (GG), a language modeling approach for assembly-to-assembly transpilation, particularly CISC-to-RISC. It aims to accurately translate x86 assembly code into ARM or RISC-V while providing testing guarantees. GG employs a custom-trained LLM with hardware-aware design and integrates it with a software testing framework for validation. Evaluation on HumanEval shows GG achieves 99% functional correctness with high code coverage. The method provides a scalable, test-verified solution for efficient cross-ISA binary translation, outperforming Rosetta 2 in runtime and energy efficiency. |
| Computer Vision | Align Your Flow: Scaling Continuous-Time Flow Map Distillation (Read more on [arXiv](https://arxiv.org/abs/2506.14603) or [HuggingFace](https://huggingface.co/papers/2506.14603))| Karsten Kreis, Sanja Fidler, Amirmojtaba Sabour | This paper introduces Align Your Flow (AYF), a novel continuous-time distillation method for training flow maps. The primary objective is to improve few-step generative modeling performance, addressing limitations of consistency models in multi-step sampling. AYF employs new continuous-time objectives and autoguidance during distillation, achieving state-of-the-art few-step generation performance on ImageNet, reaching a FID of 1.10 on ImageNet 64x64. AYF offers AI practitioners a fast and efficient method for generating high-quality images with reduced computational overhead, outperforming existing non-adversarial few-step samplers in text-conditioned synthesis. |
| Natural Language Processing | CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language
  Models in Tool-Calling Error Scenarios (Read more on [arXiv](https://arxiv.org/abs/2506.13977) or [HuggingFace](https://huggingface.co/papers/2506.13977))| Junjie Ye, Siyu Yuan, Zehui Chen, Shiting Huang, CostaliyA | This paper introduces CRITICTOOL, a novel benchmark for evaluating the self-critique capabilities of Large Language Models (LLMs) in tool-calling scenarios. It focuses on categorizing and addressing errors stemming from both internal model limitations and external environmental factors. The methodology involves an evolutionary strategy for dataset construction, resulting in a diverse set of tool-use errors with varying complexities. Experiments on CRITICTOOL demonstrate that different LLMs exhibit varying self-critique behaviors across different error sources, with GPT-4 achieving the highest overall score of 69.01. The benchmark provides a new perspective for advancing tool learning in LLMs by enabling a more granular evaluation of error handling and recovery. |
| Machine Learning | xbench: Tracking Agents Productivity Scaling with Profession-Aligned
  Real-World Evaluations (Read more on [arXiv](https://arxiv.org/abs/2506.13651) or [HuggingFace](https://huggingface.co/papers/2506.13651))| Haotong Tian, Xiaobo Hu, Yang Liu, Yixin Ren, Kaiyuan Chen | The paper introduces xbench, a dynamic, profession-aligned evaluation suite designed to assess AI agent productivity in real-world scenarios. It aims to bridge the gap between AI capabilities and economic value by targeting commercially significant domains like recruitment and marketing. The methodology involves defining evaluation tasks with industry professionals and creating metrics that correlate with productivity. Initial results evaluate leading contemporary agents, establishing a baseline for these professional domains, with 03 showing leading performance. The framework enables tracking product capabilities over time and predicting Technology-Market Fit (TMF) aligning AI agent development with practical business needs. |
| Machine Learning | Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse
  Autoencoders (Read more on [arXiv](https://arxiv.org/abs/2506.14002) or [HuggingFace](https://huggingface.co/papers/2506.14002))| Zhuoran Yang, Tianhao Wang, Xuyuan Xiong, Heejune Sheen, Siyu Chen | This paper introduces a novel approach to feature recovery in Large Language Models (LLMs) using Sparse Autoencoders (SAEs). The research aims to overcome limitations of existing SAE training algorithms by developing a statistically grounded framework for feature identifiability. To achieve this, they propose Group Bias Adaptation (GBA), which adaptively controls neuron sparsity and provably recovers monosemantic features under a defined statistical model. Empirically, GBA demonstrates superior performance on LLMs up to 1.5B parameters compared to benchmarks, achieving a better sparsity-loss frontier. The algorithm represents a step in enhancing the mechanistic interpretability of LLMs. |
| Multi-Modal | EfficientVLA: Training-Free Acceleration and Compression for
  Vision-Language-Action Models (Read more on [arXiv](https://arxiv.org/abs/2506.10100) or [HuggingFace](https://huggingface.co/papers/2506.10100))| Chang Zou, Luo Zhongwei, Zichen Wen, Yuhao Wang, Yantai Yang | The paper introduces EfficientVLA, a training-free framework for accelerating and compressing Vision-Language-Action models. It addresses the computational and memory bottlenecks inherent in VLA architectures by exploiting redundancies across the entire pipeline. The key methodology involves pruning language module layers, selecting task-relevant visual tokens, and caching intermediate features in the diffusion-based action head. Experiments on the CogACT model in the SIMPLER environment demonstrate a 1.93x inference speedup and a reduction of FLOPs to 28.9% with a minimal 0.6% success rate drop. The main implication is a path for deploying large-scale VLAs on resource-constrained robotics platforms. |
| Computer Vision | VideoMolmo: Spatio-Temporal Grounding Meets Pointing (Read more on [arXiv](https://arxiv.org/abs/2506.05336) or [HuggingFace](https://huggingface.co/papers/2506.05336))| Zhiqiang Shen, Abdelrahman Shaker, Hanan Gani, Ghazi Shazan Ahmad, ahmedheakl | The paper introduces VideoMolmo, a large multimodal model for spatio-temporal pointing conditioned on textual descriptions in videos. It aims to enhance spatio-temporal reasoning in visual grounding by decomposing the task into pointing and mask generation. The model uses a temporal module with attention and a novel mask fusion pipeline with SAM2 to propagate points into coherent masks. Evaluated on the newly introduced VPoS-Bench, VideoMolmo achieves a 5.4 pp improvement over the strongest baseline. This implies improved accuracy and coherence in spatio-temporal localization for AI practitioners. |
| Computer Vision | Ambient Diffusion Omni: Training Good Models with Bad Data (Read more on [arXiv](https://arxiv.org/abs/2506.10038) or [HuggingFace](https://huggingface.co/papers/2506.10038))| Constantinos Daskalakis, Antonio Torralba, Adam Klivans, Giannis Daras, adrianrm | The paper introduces Ambient Diffusion Omni, a framework for training diffusion models using low-quality, synthetic, and out-of-distribution images. The research explores extracting signal from all available images during training by exploiting spectral power law decay and locality properties of natural images. The method trains diffusion models with synthetically corrupted images and achieves state-of-the-art ImageNet FID. A key insight is that noise dampens the skew between high-quality and mixed-quality data distributions, offering a bias-variance trade-off. This allows AI practitioners to leverage previously discarded low-quality data to improve image generation models without compromising diversity. |
| Natural Language Processing | Optimizing Length Compression in Large Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2506.14755) or [HuggingFace](https://huggingface.co/papers/2506.14755))| Mingyang Fu, Dongping Chen, Zhengxiang Cheng, zhoutianyi | The paper presents a method to reduce unnecessary verbosity in Large Reasoning Models (LRMs). The research addresses the problem of "invalid thinking," where models double-check after deriving the correct answer, leading to inefficiencies.  The study introduces LC-R1, a Group Relative Policy Optimization (GRPO) post-training method that incorporates a Length Reward for overall conciseness and a Compress Reward to eliminate invalid reasoning steps.  Experiments on reasoning benchmarks showed a sequence length reduction of ~50% with only a marginal (~2%) drop in accuracy. LC-R1 offers a favorable trade-off, enabling more computationally efficient LRMs without significant performance compromise. |
| Natural Language Processing | Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning
  for LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.14731) or [HuggingFace](https://huggingface.co/papers/2506.14731))| Ding Liu, Deng Zhao, Cai Chen, Bin Hu, Ring Team | The paper introduces Ring-lite, a Mixture-of-Experts language model optimized via reinforcement learning for efficient reasoning. The primary objective is to achieve state-of-the-art reasoning with a fraction of the parameters typically required, addressing challenges in MoE RL training. A joint training pipeline integrating distillation with RL and a novel constrained optimization method, C3PO, are employed to enhance training stability and efficiency. Ring-lite achieves impressive scores of 76.61% and 69.11% on AIME2024 and AIME2025 respectively, matching or surpassing dense models with under 10B parameters. This provides AI practitioners with a more parameter-efficient architecture for complex reasoning tasks, coupled with a stable training methodology. |
| Natural Language Processing | Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time
  Markers (Read more on [arXiv](https://arxiv.org/abs/2506.14702) or [HuggingFace](https://huggingface.co/papers/2506.14702))| Sara Hooker, Ahmet Üstün, Adrien Morisot, Julia Kreutzer, Daniel D'souza | This paper targets improving performance on the long-tail of rare and underrepresented features in large language models. It explores whether training protocols can be optimized to improve controllability and performance on underrepresented use cases at inference time. The methodology involves creating a taxonomy of data characteristics and task provenance to control generation attributes, fine-tuning a base model to infer these markers automatically. Results show a 5.7% win rate increase in open-ended generation quality and up to 14.1% relative lifts on underrepresented tasks like CodeRepair. The approach provides a principled way to improve long-tail performance and offer users control levers without explicit intervention during inference. |
| Natural Language Processing | Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic
  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise
  Pooled Representations (Read more on [arXiv](https://arxiv.org/abs/2506.13901) or [HuggingFace](https://huggingface.co/papers/2506.13901))| Utkarsh Bhatt, Danush Khanna, Chhavi Sharma, Abhilekh Borah, amanchadha | This paper introduces the Alignment Quality Index (AQI) as an intrinsic diagnostic metric for evaluating LLM alignment. It aims to assess LLM alignment beyond behavioral proxies by analyzing the separation of safe and unsafe activations in latent space. The AQI combines measures like the Davies-Bouldin score, Dunn index, Xie-Beni index, and Calinski-Harabasz index across layerwise pooled representations. Empirical tests on the LITMUS dataset show AQI correlates with external judges and reveals vulnerabilities missed by refusal metrics, suggesting it is a robust tool for behavior-agnostic safety auditing. The proposed LITMUS dataset and AQI implementation are made publicly available to foster future research. |
| Natural Language Processing | CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility
  Simulation (Read more on [arXiv](https://arxiv.org/abs/2506.13599) or [HuggingFace](https://huggingface.co/papers/2506.13599))| Yong Li, Jian Yuan, Yuwei Du, JJ-TMT | The paper introduces CAMS, an agentic framework using a CityGPT-powered approach for simulating urban human mobility. It addresses the limitations of existing methods by leveraging language-based urban foundation models to better model urban spaces and integrate individual/collective mobility patterns. CAMS employs modules for pattern extraction, geospatial knowledge generation, and trajectory enhancement using DPO. Experiments on real-world datasets demonstrate CAMS achieves superior performance compared to existing methods, as reflected in an improved CMRR score. This framework offers AI practitioners a new paradigm that fuses agentic frameworks with urban-knowledgeable LLMs for enhanced human mobility simulation. |
| Reinforcement Learning | Mixture-of-Experts Meets In-Context Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2506.05426) or [HuggingFace](https://huggingface.co/papers/2506.05426))| Daoyi Dong, Zican Hu, Haoru Li, Fuhong Liu, Wenhao0 | The paper introduces T2MIR, a mixture-of-experts architecture for in-context reinforcement learning (ICRL), to address the challenges of multi-modality and task diversity. The research objective is to enhance ICRL's adaptability by incorporating token-wise and task-wise MoEs within a transformer-based decision model. T2MIR uses a token-wise MoE to capture distinct input token semantics and a task-wise MoE, guided by contrastive learning, to route diverse tasks to specialized experts. Experiments demonstrate that T2MIR significantly facilitates in-context learning, outperforming baselines; for example, T2MIR achieves superior performance on Cheetah-Vel compared to existing ICRL methods. The findings suggest that MoE architectures provide a scalable enhancement to ICRL, improving generalization capabilities across a range of tasks. |
| Computer Vision | TR2M: Transferring Monocular Relative Depth to Metric Depth with
  Language Descriptions and Scale-Oriented Contrast (Read more on [arXiv](https://arxiv.org/abs/2506.13387) or [HuggingFace](https://huggingface.co/papers/2506.13387))| Hongliang Ren, Long Bai, Yiming Huang, Beilei Cui | The paper introduces TR2M, a framework for transferring monocular relative depth to metric depth. It addresses the scale uncertainty in relative depth estimation by using language descriptions and scale-oriented contrastive learning. The method estimates pixel-wise rescale maps by fusing image and text features and filters confident pseudo metric depths for supervision. TR2M achieves superior zero-shot performance on five unseen datasets, surpassing some SOTA metric depth estimation methods with fewer trainable parameters. This work implies potential improvements in generalizable depth estimation using language assistance for AI practitioners. |
| Natural Language Processing | Universal Jailbreak Suffixes Are Strong Attention Hijackers (Read more on [arXiv](https://arxiv.org/abs/2506.12880) or [HuggingFace](https://huggingface.co/papers/2506.12880))| Mahmood Sharif, Mor Geva, MatanBT | The paper investigates suffix-based jailbreak attacks on large language models (LLMs) by analyzing the widely used GCG attack. It aims to understand the mechanisms driving suffix effectiveness and universality, particularly focusing on attention hijacking. The authors employ attention knockout and dominance metrics to localize the jailbreak mechanism and quantify the contribution of adversarial suffixes. They find that more universal suffixes exhibit stronger hijacking effects, enhancing universality by up to ×5 in some cases while surgical mitigation can halve attack success. The study's findings enable enhanced jailbreak creation and mitigation strategies for AI practitioners, reducing attack success with minimal utility loss. |
| Machine Learning | EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction (Read more on [arXiv](https://arxiv.org/abs/2506.12015) or [HuggingFace](https://huggingface.co/papers/2506.12015))| Yu-Chiang Frank Wang, Kai-Po Chang, Yu-Chu Yu, Hsi-Che Lin | The paper introduces EMLoC, a memory-efficient fine-tuning framework leveraging an emulator and LoRA correction. It addresses the challenge of fine-tuning large models under limited memory budgets by constructing a lightweight, task-specific emulator. EMLoC employs activation-aware SVD for emulator creation and a novel LoRA correction algorithm to mitigate misalignment between the emulator and the original model. Experiments demonstrate that EMLoC outperforms baselines on VQA tasks, achieving 85.2% accuracy on WebSRC, and enables fine-tuning a 38B model on a single 24GB GPU. EMLoC provides AI practitioners with a practical solution for adapting large models to specific tasks within resource constraints. |
