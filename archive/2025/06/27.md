

## Papers for 2025-06-27

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | MMSearch-R1: Incentivizing LMMs to Search (Read more on [arXiv](https://arxiv.org/abs/2506.20670) or [HuggingFace](https://huggingface.co/papers/2506.20670))| Bo You, Yiding Liu, Wei Li, Zihao Deng, kimingng | The paper introduces MMSearch-R1, a reinforcement learning framework for incentivizing large multimodal models (LMMs) to perform on-demand search in real-world internet environments. The research aims to improve LMMs' ability to determine when and how to utilize external search tools for knowledge-intensive VQA tasks. MMSearch-R1 employs a GRPO algorithm with an outcome-based reward and search penalty, trained on a collected multimodal VQA dataset with both search-required and search-free samples. Results show that MMSearch-R1-7B outperforms RAG-based baselines of the same size and matches the performance of a larger RAG-based model while reducing search calls by over 30%. This suggests a more efficient and targeted approach to knowledge acquisition for LMMs, benefiting AI practitioners dealing with dynamic and complex information needs. |
| Computer Vision | MADrive: Memory-Augmented Driving Scene Modeling (Read more on [arXiv](https://arxiv.org/abs/2506.21520) or [HuggingFace](https://huggingface.co/papers/2506.21520))| Maria Golitsyna, Ruslan Musaev, Kirill Struminsky, Polina Karpikova, apryc1 | The paper introduces MADRIVE, a memory-augmented driving scene modeling framework for generating realistic novel views. It addresses the challenge of synthesizing altered driving scenarios by replacing observed vehicles with realistically reconstructed counterparts retrieved from a large-scale car video dataset, MAD-CARS. MADRIVE integrates retrieved 3D car models into scenes through orientation alignment and relighting techniques using Gaussian splatting for representing both static and dynamic components. Experiments show MADRIVE improves tracking accuracy with a MOTA of 0.810 compared to other baselines. This enables more realistic simulations and supports analysis of autonomous driving system behavior in safety-critical scenarios. |
| Multi-Modal | WorldVLA: Towards Autoregressive Action World Model (Read more on [arXiv](https://arxiv.org/abs/2506.21539) or [HuggingFace](https://huggingface.co/papers/2506.21539))| Siteng Huang, Yuming Jiang, Chaohui Yu, Jun Cen, JacobYuan | The paper introduces WorldVLA, an autoregressive action world model unifying action and image understanding with generation. It aims to improve action generation by learning environmental physics through both VLA modeling and world modeling. WorldVLA employs three tokenizers for images, text, and actions sharing a common vocabulary within a single LLM architecture, enhanced with an action attention masking strategy. Experiments on LIBERO show a 4% improvement in grasping success rate compared to an action model with the same backbone. This framework offers a compact and efficient approach to decision-making and environment modeling by unifying perception and action generation. |
| Machine Learning | Where to find Grokking in LLM Pretraining? Monitor
  Memorization-to-Generalization without Test (Read more on [arXiv](https://arxiv.org/abs/2506.21551) or [HuggingFace](https://huggingface.co/papers/2506.21551))| Ziyue Li, zhoutianyi, Fcr09 | This paper investigates grokking during LLM pretraining to understand the memorization-to-generalization transition. It analyzes routing pathways in a 7B-parameter MoE LLM (OLMoE) using two novel metrics: pairwise pathway similarity and single-sample pathway consistency. The study demonstrates that these metrics exhibit strong correlations with downstream test performance (e.g., consistently above 0.97 for pathway consistency in some tasks), indicating a smarter memorization using more shared knowledge. The findings provide a finetuning-free tool to monitor generalization during pretraining by analyzing training data only, enhancing transparency and efficiency for LLM development. |
| Natural Language Processing | Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge (Read more on [arXiv](https://arxiv.org/abs/2506.21506) or [HuggingFace](https://huggingface.co/papers/2506.21506))| Yu Gu, Zanming Huang, yhshu, nnnyt, BoyuNLP | The paper introduces Mind2Web 2, a benchmark designed for evaluating agentic search systems on realistic and complex information-gathering tasks involving real-time web search and browsing. It addresses the challenge of evaluating long-horizon, time-varying answers by proposing a novel Agent-as-a-Judge framework. The methodology involves creating task-specific judge agents based on a tree-structured rubric to automatically assess answer correctness and source attribution. Evaluation of nine frontier agentic search systems revealed that the best-performing system, OpenAI Deep Research, achieves 50-70% of human performance, spending half the time. Mind2Web 2 provides a rigorous foundation for developing and benchmarking next-generation agentic search systems. |
| Computer Vision | SAM4D: Segment Anything in Camera and LiDAR Streams (Read more on [arXiv](https://arxiv.org/abs/2506.21547) or [HuggingFace](https://huggingface.co/papers/2506.21547))| Sheng Yang, Chunyong Hu, Ziqian Ni, Jianyun Xu, songw-zju | The paper introduces SAM4D, a multi-modal and temporal foundation model for promptable segmentation across camera and LiDAR streams. It aims to unify multi-modal and temporal segmentation tasks by fusing camera and LiDAR data using a Unified Multi-modal Positional Encoding (UMPE) and a Motion-aware Cross-modal Memory Attention (MCMA) mechanism. The methodology includes a multi-modal transformer architecture trained on the newly constructed Waymo-4DSeg dataset. Experiments on Waymo-4DSeg show that SAM4D achieves strong cross-modal segmentation ability with an average cross-modal IoU of 0.56 on constructed masklets. SAM4D facilitates more efficient and accurate joint 2D-3D labeling, potentially reducing annotation costs and improving perception in autonomous driving systems. |
| Computer Vision | Whole-Body Conditioned Egocentric Video Prediction (Read more on [arXiv](https://arxiv.org/abs/2506.21552) or [HuggingFace](https://huggingface.co/papers/2506.21552))| Trevor Darrell, Yann LeCun, Amir Bar, dans123, Emma02 | This paper introduces PEVA, a model for predicting ego-centric video frames conditioned on past video and relative 3D body pose as actions. The research aims to simulate how human actions shape the environment from a first-person view by conditioning on kinematic pose trajectories. PEVA uses an auto-regressive conditional diffusion transformer trained on a large-scale dataset of real-world egocentric video and body pose capture, employing a hierarchical encoding to capture the kinematic tree structure of human motion. Experiments demonstrate improved video quality, with a lower FID score compared to baselines, indicating better visual quality and temporal consistency over longer rollouts. PEVA offers AI practitioners a method for modeling complex real-world environments and embodied agent behaviors using video prediction from a human perspective. |
| Computer Vision | FaSTA^*: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient
  Multi-turn Image Editing (Read more on [arXiv](https://arxiv.org/abs/2506.20911) or [HuggingFace](https://huggingface.co/papers/2506.20911))| Dang Nguyen, Rishie Raj, Advait Gupta, zhoutianyi | The paper introduces FaSTA*, a cost-efficient neurosymbolic agent for multi-turn image editing. It addresses the challenge of computationally expensive toolpath searches by mining and reusing symbolic subroutines learned from past experiences. FaSTA* employs a fast-slow planning strategy, leveraging LLMs for high-level subtask planning and A* search for accurate toolpath selection, with subroutines enabling 'fast planning' and A* serving as fallback for 'slow planning'. Compared to CoSTA*, FaSTA* achieves a 49.3% cost reduction with only a 3.2% quality degradation. This approach offers AI practitioners a more scalable and adaptive solution for complex image editing tasks. |
| Natural Language Processing | Arch-Router: Aligning LLM Routing with Human Preferences (Read more on [arXiv](https://arxiv.org/abs/2506.16655) or [HuggingFace](https://huggingface.co/papers/2506.16655))| Adil Hafeez, Co Tran, nehcgs, parachas | This paper introduces Arch-Router, a preference-aligned LLM routing framework that uses human preferences to guide model selection. The research aims to address the limitations of existing LLM routing approaches that rely on benchmarks and limited model pools. Arch-Router uses a compact 1.5B model that learns to map queries to user-defined domain-action preferences for model routing. Experiments demonstrate SOTA results in matching queries with human preferences, outperforming top proprietary models by 7.71%. The framework offers more transparency and flexibility in routing decisions, enabling practical integration with evolving models and use cases. |
| Computer Vision | FairyGen: Storied Cartoon Video from a Single Child-Drawn Character (Read more on [arXiv](https://arxiv.org/abs/2506.21272) or [HuggingFace](https://huggingface.co/papers/2506.21272))| Xiaodong Cun, Jiayi Zheng | The paper introduces FairyGen, a framework for generating multi-shot cartoon videos from a single child-drawn character while preserving its unique style. It addresses the challenge of consistent style and motion in stylized cartoon video generation by disentangling character modeling from background generation and incorporating cinematic shot design. The methodology involves a multimodal large language model for storyboard generation, style propagation adapter for visual consistency, 3D character proxy for motion, and a two-stage motion customization adapter. Experiments demonstrate stylistically faithful animations with quantitative results showing improved style alignment score of 0.6580 compared to baselines, indicating potential for personalized story animation. The work offers AI practitioners a method for creating coherent and engaging cartoon content from simple user-generated drawings. |
| Machine Learning | HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial
  Optimization Challenges (Read more on [arXiv](https://arxiv.org/abs/2506.15196) or [HuggingFace](https://huggingface.co/papers/2506.15196))| Jiang Bian, Lei Song, Haolong Qian, Ling Zhang, VictorYXL | The paper introduces HeurAgenix, a two-stage hyper-heuristic framework using LLMs to solve combinatorial optimization (CO) problems. It addresses the challenge of generalizing heuristic designs across diverse CO instances. HeurAgenix leverages an LLM to evolve heuristics by extracting reusable evolution strategies from contrastive solution tuples and dynamically selects heuristics guided by the LLM's perception ability, fine-tuning a lightweight selector with a dual-reward mechanism for robust selection. Experimental results show that HeurAgenix matches or exceeds specialized solvers on canonical benchmarks. The framework offers a scalable and generalizable solution for complex CO problems, outperforming existing LLM-based hyper-heuristic approaches. |
| Natural Language Processing | Learning to Skip the Middle Layers of Transformers (Read more on [arXiv](https://arxiv.org/abs/2506.21103) or [HuggingFace](https://huggingface.co/papers/2506.21103))| Laurence Aitchison, tim-lawson | This paper explores conditionally skipping middle layers in Transformers to improve efficiency. The research aims to dynamically skip redundant layers based on input token complexity to reduce computational cost. The methodology involves a learned gating mechanism that skips symmetric spans of central blocks and a gated attention mechanism to prevent attending to skipped tokens. Results show that this architecture does not improve the trade-off between validation cross-entropy and estimated FLOPs compared to dense baselines with fewer layers, at the scales investigated. The primary implication is that, with the current architecture, insights regarding redundancy in middle layers do not translate into efficiency gains for language modeling at the investigated scales. |
| Machine Learning | MuseControlLite: Multifunctional Music Generation with Lightweight
  Conditioners (Read more on [arXiv](https://arxiv.org/abs/2506.18729) or [HuggingFace](https://huggingface.co/papers/2506.18729))| Bo-Rui Chen, Sheng-Ping Yang, Weijaw Lee, Shih-Lun Wu, fundwotsai2001 | MuseControlLite is a lightweight mechanism for fine-tuning text-to-music generation models, enabling precise control using time-varying musical attributes and reference audio signals. The research explores using positional embeddings in decoupled cross-attention layers to improve control accuracy. Experiments demonstrated that adding rotary positional embeddings increases control accuracy in melody control from 56.6% to 61.1%, while requiring 6.75 times fewer trainable parameters than state-of-the-art methods. This approach offers AI practitioners a more parameter-efficient method for controllable music generation without significantly increasing training costs. |
