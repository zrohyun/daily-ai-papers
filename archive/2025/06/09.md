

## Papers for 2025-06-09

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Will It Still Be True Tomorrow? Multilingual Evergreen Question
  Classification to Improve Trustworthy QA (Read more on [arXiv](https://arxiv.org/abs/2505.21115) or [HuggingFace](https://huggingface.co/papers/2505.21115))| VityaVitalich, nakrayko, VirVen, zlatamaria, memyprokotow | This paper investigates the impact of question temporality (evergreen vs. mutable) on trustworthy question answering (QA) by large language models (LLMs). The authors introduce EverGreenQA, a multilingual QA dataset with evergreen labels, and benchmark 12 LLMs to assess their encoding of question temporality. They train EG-E5, a multilingual classifier, achieving state-of-the-art performance in evergreen question classification, with a weighted F1-score of 0.906. They demonstrate the utility of EG-E5 in improving self-knowledge estimation, dataset curation, and explaining GPT-40's retrieval behavior, highlighting its role in enhancing the reliability and interpretability of LLMs in QA tasks. The findings suggest that question evergreen-ness is a crucial factor for improving LLM trustworthiness in QA applications. |
| Multi-Modal | FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal
  Contextual Fusion (Read more on [arXiv](https://arxiv.org/abs/2506.01111) or [HuggingFace](https://huggingface.co/papers/2506.01111))| Owen Lee, Liyan Zhao, Zheshu Chen, Shunian Chen, SatsukiVie | The paper introduces FusionAudio-1.2M, a large-scale dataset for fine-grained audio captioning using multimodal contextual fusion. It addresses the lack of detailed and contextually accurate automated audio captions by drawing inspiration from human auditory perception. The methodology involves a two-stage pipeline: multimodal contextual cue extraction using specialized models and LLM-driven contextual synthesis. The dataset comprises 1.2 million detailed captions and 6 million QA pairs, enabling enhanced audio models. Experiment results show the models achieve a 75.5% average recall in audio-text retrieval, suggesting the data set can improve audio understanding. |
| Multi-Modal | Is Extending Modality The Right Path Towards Omni-Modality? (Read more on [arXiv](https://arxiv.org/abs/2506.01872) or [HuggingFace](https://huggingface.co/papers/2506.01872))| Yu Su, Muhao Chen, Kai Zhang, DarthZhu | This paper examines the impact of modality extension on Large Language Models (LLMs) in achieving omni-modality. It investigates whether extending modalities compromises core language abilities, evaluates model merging techniques, and compares omni-modality fine-tuning with modality-specific approaches. The study reveals a trade-off between extending modalities and preserving core language capabilities, with weighted model merging showing promise in maintaining multimodal capabilities while retaining original LLM attributes achieving an accuracy of 68.6% on textual abilities. Furthermore, omni-modality fine-tuning proves less effective than modality-specific training, suggesting the need for refined designs in omni-modal models. The findings advise AI practitioners to carefully consider these trade-offs in developing robust omni-modal models. |
| Multi-Modal | Audio-Aware Large Language Models as Judges for Speaking Styles (Read more on [arXiv](https://arxiv.org/abs/2506.05984) or [HuggingFace](https://huggingface.co/papers/2506.05984))| Linjie Li, Kevin Lin, Chung-Ching Lin, xiaofei-wang, dcml0714 | This paper explores the use of Audio-Aware Large Language Models (ALLMs) as automatic judges for evaluating the speaking styles generated by spoken language models (SLMs). The research investigates whether ALLMs can assess SLMs on voice style instruction following and role-playing tasks. The methodology involves comparing two ALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results. The results show that the agreement between Gemini and human judges is comparable to the agreement between human evaluators, suggesting ALLMs can effectively evaluate SLMs. The primary implication for AI practitioners is that ALLMs offer a potential avenue for automating the evaluation of speaking style quality in SLMs. |
| Natural Language Processing | Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.05629) or [HuggingFace](https://huggingface.co/papers/2506.05629))| sambaran, abhi1nandy2, ananthmuppidi | The paper introduces an input-dependent soft prompting method for parameter-efficient fine-tuning of large language models. The research aims to enhance adaptation to downstream tasks by generating soft prompts based on input tokens and a self-attention mechanism. The proposed ID-SPAM method was evaluated on the GLUE benchmark, achieving improved performance compared to state-of-the-art techniques, with ID-SPAM outperforming other methods on 4 out of 6 GLUE tasks using RoBERTa-BASE. It also shows improved zero-shot domain transfer capability. This approach offers a computationally efficient way for AI practitioners to fine-tune LLMs for specific tasks without modifying the original model parameters. |
| Computer Vision | STARFlow: Scaling Latent Normalizing Flows for High-resolution Image
  Synthesis (Read more on [arXiv](https://arxiv.org/abs/2506.06276) or [HuggingFace](https://huggingface.co/papers/2506.06276))| Yuyang Wang, Huangjie Zheng, David Berthelot, Tianrong Chen, Jiatao Gu | STARFlow is a scalable generative model based on normalizing flows for high-resolution image synthesis. The paper aims to improve the scalability of normalizing flows for high-resolution image generation. It introduces a deep-shallow Transformer Autoregressive Flow (TARFlow) architecture, latent space learning with pre-trained autoencoders, and a novel guidance algorithm. STARFlow achieves competitive results in class- and text-conditional image generation, with a sample quality approaching state-of-the-art diffusion models, achieving an FID score of 2.40 on ImageNet-256. This approach offers AI practitioners a scalable and efficient alternative to diffusion-based and autoregressive models for high-resolution image synthesis. |
| Computer Vision | PartCrafter: Structured 3D Mesh Generation via Compositional Latent
  Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2506.05573) or [HuggingFace](https://huggingface.co/papers/2506.05573))| Yiqiang Feng, Honglei Yan, Panwang Pan, Yuchen Lin, chenguolin | The paper introduces PARTCRAFTER, a structured 3D generative model for jointly synthesizing multiple semantically meaningful 3D meshes from a single RGB image. It aims to achieve part-aware 3D mesh generation without relying on segmented image inputs. The model employs a compositional latent space and a hierarchical attention mechanism within a diffusion transformer, trained using a curated dataset with part-level annotations. Experiments demonstrate PARTCRAFTER outperforms existing methods in generating decomposable 3D meshes, achieving higher generation quality and efficiency. This structured approach to 3D generation provides AI practitioners with a novel method to create and manipulate complex 3D scenes and objects with part-level control. |
| Multi-Modal | MORSE-500: A Programmatically Controllable Video Benchmark to
  Stress-Test Multimodal Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.05523) or [HuggingFace](https://huggingface.co/papers/2506.05523))| Hyunwoo Jae, Ankit Nakhawa, Anirudh Satheesh, Andrew Wang, Zikui | The paper introduces MORSE-500, a new video benchmark to stress-test multimodal reasoning in AI systems. It aims to address limitations in existing benchmarks by focusing on temporal complexity, diverse reasoning skills, and scalability. The methodology involves programmatically generating 500 fully scripted video clips with questions spanning six reasoning categories. Experiments with state-of-the-art systems revealed substantial performance gaps, particularly in abstract and planning tasks, with overall accuracy below 25%. The benchmark enables scalable difficulty and provides a foundation for transparent, reproducible, and forward-looking multimodal reasoning research. |
| Computer Vision | Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence
  with Egocentric-Exocentric Vision (Read more on [arXiv](https://arxiv.org/abs/2506.06253) or [HuggingFace](https://huggingface.co/papers/2506.06253))| Baoqi Pei, Lidong Lu, Yifei Huang, Yuping He, cg1177 | This survey explores cross-view collaborative intelligence by integrating egocentric and exocentric vision for enhanced video understanding. The research focuses on systematically reviewing and organizing existing research into three primary directions: leveraging egocentric data to enhance exocentric understanding, utilizing exocentric data to improve egocentric analysis, and developing joint learning frameworks for both perspectives. The survey analyzes a diverse set of tasks, relevant works, and benchmark datasets, evaluating their scope, diversity, and applicability, but does not contain concrete results. It aims to inspire advancements in video understanding and AI by synthesizing insights from both perspectives, moving machines closer to human-like perception. A GitHub repository is provided for related works. |
| Computer Vision | 3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World
  Model (Read more on [arXiv](https://arxiv.org/abs/2506.06199) or [HuggingFace](https://huggingface.co/papers/2506.06199))| Quanxi Wu, Yubo Dong, Siyuan Zhou, Peihao Chen, Hoyard | This paper introduces 3DFlowAction, a novel approach for robot manipulation that leverages 3D optical flow as a robust and unified action representation. The research aims to enable cross-embodiment manipulation by learning a 3D flow world model from both human and robot video data to predict object movement. The methodology involves synthesizing a large-scale 3D optical flow dataset (ManiFlow-110k) and training a video diffusion-based world model to generate 3D optical flow trajectories conditioned on language instructions. Experiments demonstrate a 70% success rate across diverse manipulation tasks, showcasing strong generalization and cross-embodiment adaptation without hardware-specific training. The primary implication is a reliable method for robots to understand and execute manipulation tasks across different embodiments by understanding the 3D motions. |
| Reinforcement Learning | Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward (Read more on [arXiv](https://arxiv.org/abs/2506.05433) or [HuggingFace](https://huggingface.co/papers/2506.05433))| Junxian Cai, Longteng Guo, Yepeng Tang, Tongtian Yue, Zikang Liu | The paper introduces Prefix Grouper, an efficient algorithm for Group Relative Policy Optimization (GRPO) to reduce redundant prefix computation. It addresses the computational overhead in GRPO training with long shared prefixes by introducing a Shared-Prefix Forward strategy. Prefix Grouper restructures self-attention to encode the shared prefix only once, maintaining full differentiability. Experiments demonstrate that Prefix Grouper achieves comparable performance to standard GRPO while significantly reducing computational cost, especially in long-prefix scenarios. This improvement enables using larger group sizes under similar computational constraints, enhancing GRPO scalability. |
| Computer Vision | Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot
  Data (Read more on [arXiv](https://arxiv.org/abs/2506.04120) or [HuggingFace](https://huggingface.co/papers/2506.04120))| Zhibin Li, Tom Erez, Steven Bohez, Mauro Comi, Ben Moran | This paper introduces an end-to-end real-to-sim framework for creating accurate physical simulations from imperfect robot data. The research aims to jointly optimize scene appearance, object geometry, robot poses, and camera parameters. The key methodology involves a hybrid scene representation called SplatMesh, combining 3D Gaussian Splatting for photorealistic rendering with explicit object meshes suitable for physics simulation. Experiments demonstrate improved object reconstruction with a Chamfer Distance of 0.073 mm² on a synthetic YCB dataset and novel view synthesis with a PSNR of 30.91 on the same dataset. The framework allows AI practitioners to create high-fidelity digital twins and perform annotation-free robot pose calibration from noisy robot trajectories. |
| Machine Learning | HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource
  Utilization (Read more on [arXiv](https://arxiv.org/abs/2506.04255) or [HuggingFace](https://huggingface.co/papers/2506.04255))| Harshil Patel, helloparthshah, guineapig | The paper introduces HASHIRU, a hierarchical multi-agent system designed for efficient resource utilization. HASHIRU addresses limitations in existing frameworks such as rigidity and resource obliviousness by using a dynamic CEO-Employee architecture with hybrid intelligence. The key methodology involves hierarchical control, dynamic agent lifecycle management based on budget constraints, and autonomous tool creation for functional extension. Experimental results show HASHIRU outperforms baselines on several tasks including outperforming Gemini 2.0 Flash on GSM8K (96% vs. 61%). The main implication for AI practitioners is a more robust and adaptable system for complex task decomposition and execution with improved resource efficiency. |
| Machine Learning | CodeContests+: High-Quality Test Case Generation for Competitive
  Programming (Read more on [arXiv](https://arxiv.org/abs/2506.05817) or [HuggingFace](https://huggingface.co/papers/2506.05817))| Kai Shen, Hongyan Li, Yang Sun, Siyao Liu, zhwang01 | This paper introduces CodeContests+, a dataset with high-quality test cases for competitive programming problems. The research aims to improve the accuracy of evaluating large language models (LLMs) in competitive programming by creating better test cases. They propose an LLM-based agent system with a Generator-Validator (G-V) approach to automatically construct more comprehensive and correct test cases. Results show that CodeContests+ achieves significantly higher accuracy with a higher True Positive Rate (TPR) than CodeContests, as verified through 1.72 million submissions. The improved test case quality provides considerable advantages for Reinforcement Learning (RL) training of LLMs, potentially enabling more effective development of reasoning and coding capabilities. |
| Multi-Modal | Truth in the Few: High-Value Data Selection for Efficient Multi-Modal
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.04755) or [HuggingFace](https://huggingface.co/papers/2506.04755))| Chong Peng, Hao Yang, Lei Wang, Kaiyuan Deng, Shenshen Li | This paper introduces a data selection paradigm for efficient multi-modal reasoning in MLLMs. It addresses the question of whether smaller high-value datasets can match or outperform full corpora. The proposed Reasoning Activation Potential (RAP) identifies cognitive samples using Causal Discrepancy Estimator (CDE) and Attention Confidence Estimator (ACE). Experiments show RAP achieves superior performance using only 9.3% of training data and reducing computational costs by over 43%. The work implies that prioritizing data quality over quantity can significantly enhance the efficiency and effectiveness of training MLLMs. |
| Natural Language Processing | GuideX: Guided Synthetic Data Generation for Zero-Shot Information
  Extraction (Read more on [arXiv](https://arxiv.org/abs/2506.00649) or [HuggingFace](https://huggingface.co/papers/2506.00649))| Eneko Agirre, Iker García-Ferrero, OSainz, neildlf | This paper introduces GUIDEX, a novel method for generating synthetic data to enhance zero-shot information extraction (IE) performance. The research aims to address the challenge of domain adaptation in IE by automatically defining schemas, inferring guidelines, and generating synthetically labeled instances. GUIDEX fine-tunes Llama 3.1 and achieves state-of-the-art results across seven zero-shot Named Entity Recognition benchmarks, gaining up to 7 F1 points over previous methods. The implication for AI practitioners is a more robust and adaptable zero-shot IE system that requires less human-labeled data and generalizes better across diverse domains. |
