

## Papers for 2025-06-03

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective
  Reinforcement Learning for LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.01939) or [HuggingFace](https://huggingface.co/papers/2506.01939))| lyq333, Zhenru, xionghuichen, chujiezheng, shenzhi-wang | This paper investigates the role of token entropy patterns in reinforcement learning with verifiable rewards (RLVR) for large language model reasoning. The study aims to understand how different tokens influence reasoning performance by analyzing token entropy patterns in Chain-of-Thought reasoning. The key methodology involves restricting policy gradient updates to forking tokens, i.e. high-entropy minority tokens. Results show significant performance gains, e.g., +11.04 on AIME'25 for the Qwen3-32B model, by utilizing only 20% of the tokens. The findings suggest that optimizing high-entropy tokens is crucial for effective RLVR, enabling AI practitioners to develop more focused and efficient LLM reasoning strategies. |
| Machine Learning | Taming LLMs by Scaling Learning Rates with Gradient Grouping (Read more on [arXiv](https://arxiv.org/abs/2506.01049) or [HuggingFace](https://huggingface.co/papers/2506.01049))| danxu, MarcusB3n, ZedongWangAI, Juanxi, Lupin1998 | This paper introduces Scaling with Gradient Grouping (SGG), a parameter-efficient wrapper for adaptive learning rate optimizers designed to improve LLM training. The research aims to enhance training stability and convergence speed by adapting learning rates based on gradient statistics within each layer. SGG groups parameters within layers based on gradient similarities and dynamically adjusts scaling factors to improve parameter-wise adaptation. Experiments show that SGG improves perplexity by up to 30.2% on C4 pre-training and achieves improvements on GLUE and other downstream tasks. The method offers a way to stabilize and enhance LLM training through adaptive learning rate scaling, providing better convergence and performance. |
| Reinforcement Learning | Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with
  Jigsaw Puzzles (Read more on [arXiv](https://arxiv.org/abs/2505.23590) or [HuggingFace](https://huggingface.co/papers/2505.23590))| Feiyu Xiong, Zhiyu Li, Bo Tang, RyanZhu, wangzifu | This paper studies rule-based visual reinforcement learning (RL) using jigsaw puzzles. It aims to understand how MLLMs perform and generalize in rule-based visual RL settings. The methodology involves fine-tuning MLLMs on jigsaw puzzle tasks and evaluating their performance on puzzle completion and downstream visual tasks. Key results include achieving near-perfect accuracy on jigsaw puzzles and generalization to complex configurations via fine-tuning, with the performance affected by task configurations; RL also exhibited better generalization than SFT. The study implies that rule-based visual RL can effectively train MLLMs for visual tasks, with task-specific configurations impacting generalization capabilities, while initial SFT may hinder RL optimization. |
| Computer Vision | Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion
  Models (Read more on [arXiv](https://arxiv.org/abs/2506.00996) or [HuggingFace](https://huggingface.co/papers/2506.00996))| Jaegul Choo, Junha Hyung, Kinam Kim | The paper introduces Temporal In-Context Fine-Tuning (TIC-FT) for versatile control of video diffusion models. It addresses the challenge of limited data and compute in conditional video generation by adapting pretrained video diffusion models without architectural changes. TIC-FT concatenates condition and target frames temporally, inserting noise-leveled buffer frames to smooth transitions, requiring only 10-30 training samples.  Experiments demonstrate TIC-FT outperforms existing baselines, achieving up to 0.8329 CLIP-I score on I2V tasks. This enables efficient adaptation of video diffusion models for diverse conditional generation tasks like image-to-video and video-to-video translation. |
| Reinforcement Learning | REASONING GYM: Reasoning Environments for Reinforcement Learning with
  Verifiable Rewards (Read more on [arXiv](https://arxiv.org/abs/2505.24760) or [HuggingFace](https://huggingface.co/papers/2505.24760))| Richard Jones, Joe Sharratt, JeanKaddour, OllieStanley, zafstojano | The paper introduces REASONING GYM (RG), a library of reasoning environments designed for reinforcement learning with verifiable rewards. It addresses the scarcity of high-quality training data by providing over 100 procedurally generated tasks spanning multiple domains and offering adjustable complexity. The key methodology involves creating algorithmically verifiable tasks with parameters to control difficulty and structural variation. Experimental results demonstrate the efficacy of RG in evaluating and training reasoning models, showcasing that curriculum RLVR accelerates training and improves final accuracy. Frontier LLMs exhibit low zero-shot performance on many RG tasks, indicating the need for specialized reasoning capabilities. |
| Multi-Modal | SmolVLA: A Vision-Language-Action Model for Affordable and Efficient
  Robotics (Read more on [arXiv](https://arxiv.org/abs/2506.01844) or [HuggingFace](https://huggingface.co/papers/2506.01844))| imstevenpmwork, pepijn223, fracapuano, danaaubakirova, mshukor | SmolVLA introduces a compact vision-language-action model for robotics, reducing training and inference costs. The research focuses on creating an accessible and efficient VLA that can be trained on a single GPU using community-collected data. Key to their approach is a streamlined architecture paired with an asynchronous inference stack decoupling perception and action. Despite its small size (450M parameters), SmolVLA achieves competitive performance, exemplified by achieving a success rate of 78.3% on real-world tasks. SmolVLA offers an affordable solution for enabling natural language-driven robotic control on consumer-grade hardware. |
| Natural Language Processing | ARIA: Training Language Agents with Intention-Driven Reward Aggregation (Read more on [arXiv](https://arxiv.org/abs/2506.00539) or [HuggingFace](https://huggingface.co/papers/2506.00539))| Siyu Yuan, Yikai Zhang, Xintao, sheep33333, rhyang2021 | The paper introduces ARIA, a method for training language agents in open-ended environments to address reward sparsity. ARIA projects actions into a low-dimensional intention space and aggregates rewards, thus densifying reward signals. The key methodology involves semantic projection and hierarchical clustering to construct intention spaces for policy optimization using REINFORCE. Experiments across four tasks demonstrated an average performance gain of 9.95% compared to RL baselines. This implies a more efficient and effective strategy for training language agents via intention-driven reward aggregation. |
| Multi-Modal | LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon
  Embodied Tasks (Read more on [arXiv](https://arxiv.org/abs/2506.00411) or [HuggingFace](https://huggingface.co/papers/2506.00411))| Zhijie Deng, Yihan Wang, Siqi Kou, Jiaxuan Sun, Yysrc | The paper introduces LoHoVLA, a unified vision-language-action model for long-horizon embodied tasks, addressing limitations in existing VLA models and hierarchical architectures. LoHoVLA leverages a large pretrained VLM to jointly generate language and action tokens, enhancing generalization across tasks through shared representations and hierarchical closed-loop control. The model is trained on LoHoSet, a new dataset of 20 long-horizon tasks within the Ravens simulator, demonstrating superior performance compared to both hierarchical and standard VLA approaches. Experimentally, LoHoVLA significantly surpasses baseline methods on Ravens tasks, showcasing improved reasoning and planning capabilities. This unified architecture presents a promising direction for developing more generalizable embodied intelligence. |
| Computer Vision | Learning Video Generation for Robotic Manipulation with Collaborative
  Trajectory Control (Read more on [arXiv](https://arxiv.org/abs/2506.01943) or [HuggingFace](https://huggingface.co/papers/2506.01943))| Runsen Xu, Jianhong Bai, Xian Liu, Xintao Wang, Xiao Fu | This paper introduces RoboMaster, a novel framework for generating robotic manipulation videos with collaborative trajectory control. The research aims to address the limitation of existing trajectory-based methods that struggle to capture multi-object interaction in complex robotic tasks. RoboMaster decomposes the interaction process into pre-interaction, interaction, and post-interaction phases, modeling inter-object dynamics through a collaborative trajectory formulation and mask-based object embeddings. Experiments on the Bridge V2 dataset demonstrate that RoboMaster outperforms existing approaches in visual quality and trajectory accuracy, achieving a trajectory error of 16.47 compared to others. The framework offers AI practitioners an effective approach for simulating realistic robotic manipulation with fine-grained control and improved user interactivity. |
| Multi-Modal | ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2506.01853) or [HuggingFace](https://huggingface.co/papers/2506.01853))| Jun Zhu, Shenghao Xie, Zhengyi Wang, Junliang Ye, zzzrw | The paper introduces ShapeLLM-Omni, a native 3D multimodal LLM for understanding and generating 3D content alongside text and images. The primary objective is to extend multimodal LLMs with native 3D capabilities, addressing the current limitation of being confined to 2D images. The method involves training a 3D VQVAE to encode 3D meshes into discrete tokens, constructing a large-scale 3D-Alpaca dataset for training, and fine-tuning the Qwen-2.5-vl-7B-Instruct model. The ShapeLLM-Omni model achieves comparable language understanding to baseline models while enabling 3D content generation; however, 3D generation performance (Frechet Distance) is slightly below that of Trellis. This work provides a pathway for future research in 3D-native AI by demonstrating the integration of 3D modalities into multimodal language models. |
| Multi-Modal | SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2506.01713) or [HuggingFace](https://huggingface.co/papers/2506.01713))| Dongfei Cui, Yu Zhang, Che Liu, Zhihao Dou, Zhongwei Wan | This paper introduces SRPO, a reflection-aware reinforcement learning framework to enhance multimodal reasoning in LLMs. The research aims to improve reasoning accuracy and reflection quality by enabling explicit self-reflection and self-correction. SRPO uses a two-stage approach: constructing a reflection-focused dataset and employing Group Relative Policy Optimization with a novel reward mechanism. Experiments using Qwen-2.5-VL models show that SRPO outperforms state-of-the-art models, achieving notable improvements; for instance, on MathVista, SRPO-7B achieved 75.8% accuracy. The framework provides a pathway for AI practitioners to improve reasoning capabilities by leveraging self-reflection during training. |
| Reinforcement Learning | AReaL: A Large-Scale Asynchronous Reinforcement Learning System for
  Language Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.24298) or [HuggingFace](https://huggingface.co/papers/2505.24298))| Zhiyu Mei, Chen Zhu, Xujie Shen, Jiaxuan Gao, Wei Fu | The paper introduces AREAL, a fully asynchronous reinforcement learning system designed for training large language models for reasoning tasks. AREAL decouples generation from training to improve GPU utilization and system efficiency, addressing the limitations of synchronous RL systems. The methodology includes continuous rollout generation, asynchronous model updates, workload balancing, and a staleness-enhanced PPO variant. Experiments on math and code reasoning benchmarks demonstrate up to a 2.57x training speedup compared to synchronous systems while maintaining or improving final performance. AREAL allows AI practitioners to train high-performing language reasoning models more efficiently by maximizing GPU utilization and overcoming system-level inefficiencies. |
| Multi-Modal | EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation
  with Large Multimodal Models (Read more on [arXiv](https://arxiv.org/abs/2506.01667) or [HuggingFace](https://huggingface.co/papers/2506.01667))| Luc Van Gool, Danda Pani Paudel, Zhitong Xiong, Bin Ren, Yan Shu | The paper introduces EarthMind, a novel vision-language framework for multi-granular and multi-sensor Earth Observation (EO) data understanding. It addresses the challenge of comprehensive EO data understanding by incorporating spatial attention prompting (SAP) and cross-modal fusion. EarthMind projects heterogeneous features into a unified semantic space, enabling effective interpretation by LLMs. The EarthMind-Bench benchmark was used for evaluation, achieving state-of-the-art performance surpassing GPT-40, with a smaller 4B parameter scale. These results demonstrate EarthMind's potential to handle multi-granular and multi-sensor challenges in a unified framework, offering a practical tool for EO applications. |
| Machine Learning | MiCRo: Mixture Modeling and Context-aware Routing for Personalized
  Preference Learning (Read more on [arXiv](https://arxiv.org/abs/2505.24846) or [HuggingFace](https://huggingface.co/papers/2505.24846))| Feng Luo, Yifan Sun, Jingyan Shen, Ray2333, FlippyDora | This paper introduces MiCRo, a two-stage framework for personalized preference learning using mixture modeling and context-aware routing. The research addresses the problem of capturing diverse and heterogeneous human preferences in reward modeling for RLHF, where standard methods assume a global reward function. MiCRo employs a context-aware mixture model to capture diverse human preferences from binary preference datasets and then integrates an online routing strategy for dynamic adaptation based on specific context. Experiments on multiple preference datasets demonstrate MiCRo effectively captures diverse human preferences, improving downstream personalization with accuracy scores of 0.7830 on HelpSteer2 and 0.8218 on RPR. The implication is that MiCRo provides a scalable and efficient approach for personalized preference adaptation in RLHF without requiring explicit fine-grained annotations. |
| Natural Language Processing | Incentivizing Reasoning for Advanced Instruction-Following of Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.01413) or [HuggingFace](https://huggingface.co/papers/2506.01413))| Yuchen Shi, Zihan Xu, Zongyi Li, Gang Li, yolay | This paper addresses the challenge of large language models (LLMs) failing to follow complex instructions with paralleling, chaining, and branching structures. The research aims to improve LLMs' instruction-following capabilities by incentivizing reasoning through reinforcement learning (RL) with verifiable rule-centric reward signals and sample-wise contrast.  Their methodology involves a reproducible data acquisition method and RL with rule-centric rewards to cultivate reasoning specifically for instruction following. Evaluations on seven benchmarks show a 1.5B LLM achieves 11.74% gains, performing comparably to an 8B LLM, demonstrating the effectiveness of the proposed method. The main implication is that incentivized reasoning can significantly enhance LLMs' ability to follow intricate instructions, with potential for improved performance in various natural language tasks. |
| Computer Vision | Cora: Correspondence-aware image editing using few step diffusion (Read more on [arXiv](https://arxiv.org/abs/2505.23907) or [HuggingFace](https://huggingface.co/papers/2505.23907))| Andrea Tagliasacchi, Negar Hassanpour, Sauradip Nag, Aryan Mikaeili, Amirhossein-Alimohammadi | The paper presents Cora, a novel diffusion-based image editing framework that supports diverse edits with few denoising steps. It addresses the limitations of existing methods by introducing correspondence-aware noise correction and interpolated attention maps to align textures and structures between source and target images. Extensive experiments demonstrate that Cora excels in maintaining structure, textures, and identity, achieving superior results compared to alternatives according to user studies. Cora enables more accurate texture transfer and content generation compared to existing few-step methods. The main implication is providing AI practitioners with a more effective method for image editing that enhances control and flexibility while preserving important image attributes. |
| Natural Language Processing | DyePack: Provably Flagging Test Set Contamination in LLMs Using
  Backdoors (Read more on [arXiv](https://arxiv.org/abs/2505.23001) or [HuggingFace](https://huggingface.co/papers/2505.23001))| Soheil Feizi, mmoayeri, wangwenxiao, yizecheng | The paper introduces DyePack, a framework leveraging backdoor attacks to detect test set contamination in LLMs. It addresses the problem of models being trained on benchmark test sets, leading to inflated performance metrics. DyePack mixes backdoor samples with test data, allowing detection when models exhibit suspicious performance on these samples. The framework enables computation of false positive rates (FPR), achieving FPRs as low as 0.000073% on MMLU-Pro using eight backdoors, suggesting its efficacy in contamination detection. DyePack provides a safeguard for future benchmark development and ensures fair model comparisons by detecting models trained on contaminated data. |
| Natural Language Processing | Reasoning Like an Economist: Post-Training on Economic Problems Induces
  Strategic Generalization in LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.00577) or [HuggingFace](https://huggingface.co/papers/2506.00577))| Yifang Chen, Xiangqi Jin, Xingyu Dong, Steven-Shaobo, MasterZhou | This paper explores whether post-training techniques can improve strategic generalization in Large Language Models (LLMs) for multi-agent systems, focusing on economic reasoning as a testbed. The study introduces Recon, a 7B-parameter LLM post-trained on a dataset of 2,100 economic reasoning problems via Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO). Evaluation on economic benchmarks shows a 14.7% absolute accuracy gain. The main implication is that domain-aligned post-training enhances both benchmark accuracy and generalization capabilities, suggesting a scalable approach to agent alignment by fostering strategic behavior. |
| Multi-Modal | VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL (Read more on [arXiv](https://arxiv.org/abs/2505.23977) or [HuggingFace](https://huggingface.co/papers/2505.23977))| Bhaskar Ramasubramanian, Yuetai Li, Fengqing Jiang, zhangchenxu, EthanSta | The paper introduces VISUALSPHINX, a large-scale synthetic dataset for training and benchmarking vision-language models (VLMs) in logical reasoning tasks. It addresses the lack of structured, large-scale datasets by proposing a rule-to-image synthesis pipeline that uses a rule-level genetic algorithm and program-based image synthesis. A QWEN2.5-VL-7B model fine-tuned with VISUALSPHINX via GRPO achieved a 55.94% accuracy on a newly created VISUALSPHINX-TEST, significantly outperforming the baseline and comparable closed-source models. This demonstrates improved logical coherence and generalizability in VLMs, benefiting other reasoning tasks such as algebraic, arithmetic, and geometry reasoning. The dataset facilitates the development of enhanced VLM reasoning capabilities for applications like diagram understanding and spatial problem solving. |
| Natural Language Processing | From Token to Action: State Machine Reasoning to Mitigate Overthinking
  in Information Retrieval (Read more on [arXiv](https://arxiv.org/abs/2505.23059) or [HuggingFace](https://huggingface.co/papers/2505.23059))| Seung-won Hwang, yeonseokjeong, waylight3 | This paper introduces State Machine Reasoning (SMR) to mitigate overthinking in information retrieval (IR) by controlling reasoning steps as transitions between queries and document rankings. The research aims to address redundant trajectories and misguided reasoning common in Chain-of-Thought (CoT) prompting. SMR uses discrete actions (REFINE, RERANK, STOP) to structure reasoning and enable early stopping, validated through experiments on BEIR and BRIGHT. Results show a 3.4% improvement in nDCG@10 on the BRIGHT benchmark, alongside a 74.4% reduction in token usage. The SMR framework provides a practical and generalizable alternative to conventional CoT, offering improved retrieval performance and efficiency. |
| Computer Vision | Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision
  Geometry Priors (Read more on [arXiv](https://arxiv.org/abs/2505.24625) or [HuggingFace](https://huggingface.co/papers/2505.24625))| Liwei Wang, Yanyang Li, Shijia Huang, zd11024 | The paper introduces Video-3D Geometry LLM (VG LLM) to enhance MLLMs' 3D scene understanding from video data without explicit 3D input. It addresses the challenge of MLLMs struggling with 3D geometry by incorporating a 3D visual geometry encoder that extracts 3D priors from video sequences. The method fuses these geometry-aware features with visual tokens before feeding them into the MLLM backbone. Experiments show that the 4B VG LLM achieves a 46.1% average score on VSI-Bench, surpassing Gemini-1.5-Pro, indicating superior spatial reasoning. This suggests that implicit 3D geometry modeling can significantly improve MLLMs' spatial understanding and reasoning abilities. |
| Machine Learning | Stepsize anything: A unified learning rate schedule for
  budgeted-iteration training (Read more on [arXiv](https://arxiv.org/abs/2505.24452) or [HuggingFace](https://huggingface.co/papers/2505.24452))| Zhouchen Lin, zhou Xun, Yiming Dong, Anda Tang, Taoer | This paper introduces a unified learning rate schedule for budgeted-iteration training, aiming to optimize model performance under limited iteration budgets. The research addresses the problem of heuristic learning rate design by constructing a training budget-aware optimization framework that accounts for landscape curvature variations. It derives a Unified Budget-Aware (UBA) schedule controlled by a single hyper-parameter and proves its convergence under different parameter values. Experimental results show that UBA consistently surpasses commonly-used schedules across vision and language tasks, achieving state-of-the-art performance on approximately half of the language benchmarks evaluated. The UBA schedule provides a reliable and easily adaptable tool for AI practitioners aiming to achieve optimal performance with limited computational resources. |
| Machine Learning | CodeV-R1: Reasoning-Enhanced Verilog Generation (Read more on [arXiv](https://arxiv.org/abs/2505.24183) or [HuggingFace](https://huggingface.co/papers/2505.24183))| Chongxiao Li, Xiaoyun Zhang, Hanqi Lyu, dihuang, zhuyaoyu | The paper introduces CodeV-R1, a reinforcement learning framework for Verilog generation from natural language specifications. It addresses the lack of automated verification, high-quality data, and prohibitive computation costs by developing a rule-based testbench generator, round-trip data synthesis, and an adaptive RLVR algorithm. The framework distills reasoning patterns and applies reinforcement learning on high-quality data. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively. The main implication is a cost-effective approach to improving reasoning and performance of LLMs for hardware description language generation. |
| Natural Language Processing | WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web
  Tasks (Read more on [arXiv](https://arxiv.org/abs/2506.01952) or [HuggingFace](https://huggingface.co/papers/2506.01952))| Tatsumi Sunada, Atsuki Sato, Kazuki Egashira, Zaiying Zhao, AtsuMiyai | The paper introduces WebChoreArena, a new benchmark for evaluating web-browsing agents on complex, labor-intensive tasks beyond general browsing. It investigates the ability of agents to handle tasks requiring massive memory, precise calculation, and long-term memory across multiple webpages. Experimental results using GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro show significant performance improvements compared to WebArena, but even Gemini 2.5 Pro achieved only 44.9% accuracy, indicating considerable room for advancement.  The benchmark enables a clearer evaluation of agent progress and reveals limitations in current models, suggesting the need for enhanced capabilities in memory utilization and reasoning for web-based tasks. |
| Computer Vision | Pro3D-Editor : A Progressive-Views Perspective for Consistent and
  Precise 3D Editing (Read more on [arXiv](https://arxiv.org/abs/2506.00512) or [HuggingFace](https://huggingface.co/papers/2506.00512))| Zhendong Mao, Mengqi Huang, Yang Zheng, CNcreator0331 | The paper introduces Pro3D-Editor, a novel framework for consistent and precise text-guided 3D editing using a progressive-views paradigm. It addresses the challenge of inconsistent multi-view editing by propagating editing semantics from salient to sparse views. The method incorporates a Primary-view Sampler, Key-view Render (with MoVE-LoRA), and Full-view Refiner to achieve accurate and spatially consistent edits. Experiments demonstrate a 47.4% improvement in LPIPS and a 9.7% improvement in DINO-I, indicating enhanced editing quality and accuracy. The framework enables AI practitioners to achieve high-quality, localized 3D edits with improved spatial consistency. |
| Natural Language Processing | OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and
  Cleaning (Read more on [arXiv](https://arxiv.org/abs/2506.00338) or [HuggingFace](https://huggingface.co/papers/2506.00338))| Jinchuan Tian, William Chen, Yui Sudo, Shakeel Muhammad, pyf98 | This paper introduces OWSM v4, an enhanced open-source speech foundation model, achieved through data scaling and cleaning of the YODAS dataset. The study addresses the challenge of noisy data in web-crawled datasets by developing a scalable data-cleaning pipeline using public toolkits for language identification and CTC confidence scoring. The resulting models, trained on the curated data, significantly outperform previous OWSM versions, achieving a 10.7% WER on MLS. OWSM-CTC v4 reaches 93.6% LID accuracy on FLEURS and 3.3% WER on the long-form English web-presentation dataset. This demonstrates that academic-scale resources can match industrial models with careful data curation. |
| Natural Language Processing | Stress-testing Machine Generated Text Detection: Shifting Language
  Models Writing Style to Fool Detectors (Read more on [arXiv](https://arxiv.org/abs/2505.24523) or [HuggingFace](https://huggingface.co/papers/2505.24523))| Giovanni Puccetti, Alessio Miaschi, Cristiano Ciaccio, Michele Papucci, andreapdr | This paper presents a method for stress-testing machine-generated text (MGT) detection systems by adversarially fine-tuning language models to mimic human writing styles. The research aims to evaluate the robustness of MGT detectors against linguistically-informed attacks. Direct Preference Optimization (DPO) is used to align LLMs' style toward human-written text by shifting their linguistic feature distributions. Results show that detectors can be easily fooled, with a significant drop in detection performance (up to 60% in some cases), even with limited examples. The main implication is that current MGT detectors rely on superficial stylistic cues, highlighting the need for more robust methods. |
| Computer Vision | VAU-R1: Advancing Video Anomaly Understanding via Reinforcement
  Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2505.23504) or [HuggingFace](https://huggingface.co/papers/2505.23504))| Xiaodong Cun, Xi Shen, Qixiang Chen, Liyun Zhu | The paper introduces VAU-R1, a data-efficient framework for video anomaly understanding that leverages reinforcement fine-tuning (RFT) of multimodal large language models (MLLMs). The research addresses the challenges of fine-grained spatio-temporal perception and robust reasoning in video anomaly scenarios. VAU-R1 uses group relative policy optimization (GRPO) and task-specific rewards to enhance anomaly reasoning, evaluated on a newly proposed VAU-Bench benchmark. Empirical results show that VAU-R1 improves question answering accuracy and temporal grounding, outperforming supervised fine-tuning (SFT); for example, Qwen2.5-VL-3B+RFT achieves 87.08% accuracy on multiple choice QA. This framework offers a foundation for interpretable and reasoning-aware video anomaly understanding, which can aid in applications like security surveillance and disaster alert systems. |
| Natural Language Processing | LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech
  Detoxification (Read more on [arXiv](https://arxiv.org/abs/2506.01484) or [HuggingFace](https://huggingface.co/papers/2506.01484))| Helmut Schmid, Ashish Yashwanth Kangen, Lukas Kouba, Ercong Nie, shuzyuan | This paper presents PARADEHATE, a new parallel dataset for hate speech detoxification created using an LLM-in-the-loop pipeline. The research investigates the potential of LLMs to automate the construction of high-quality parallel detoxification datasets by replacing human annotators. The methodology leverages GPT-40-mini to rephrase hate speech, verify content preservation, and evaluate toxicity. Experiments demonstrate that a BART model fine-tuned on PARADEHATE achieves a style accuracy of 0.95 and fluency of 0.78, outperforming existing detoxification methods. The primary implication is that LLM-generated datasets can effectively train models for hate speech detoxification, offering a scalable alternative to human annotation. |
| Natural Language Processing | zip2zip: Inference-Time Adaptive Vocabularies for Language Models via
  Token Compression (Read more on [arXiv](https://arxiv.org/abs/2506.01084) or [HuggingFace](https://huggingface.co/papers/2506.01084))| Chris Wendler, Maxime Peyrard, Yunzhen yao, Saibo Geng, nathanrchn | The paper introduces zip2zip, a framework that enables language models to dynamically adapt their token vocabulary at inference time through token compression. It addresses the challenge of static tokenizers failing to adapt to domain-specific inputs, leading to longer token sequences and higher computational costs. The proposed method uses Lempel-Ziv-Welch (LZW) compression to incrementally merge co-occurring tokens into hypertokens, an embedding layer for runtime computation of hypertokens, and a compression-aware language modeling variant for training. Fine-tuning Phi-3 models with zip2zip resulted in 20-60% reductions in sequence length and improved inference latency. This allows AI practitioners to improve LLM efficiency without retraining base models on domain-specific tasks. |
| Natural Language Processing | SATA-BENCH: Select All That Apply Benchmark for Multiple Choice
  Questions (Read more on [arXiv](https://arxiv.org/abs/2506.00643) or [HuggingFace](https://huggingface.co/papers/2506.00643))| Stephanie Eckman, Chi Xue, Xi Fang, Shixian Cui, xwjzds | SATA-BENCH is a new benchmark for evaluating large language models (LLMs) on Select All That Apply (SATA) questions. The paper investigates the ability of LLMs to identify multiple correct answers in multiple-choice settings, a capability often required in real-world applications. The authors curated a diverse dataset of 1,604 SATA questions and evaluated 27 LLMs, finding that even the strongest model achieved only 41.8% exact match accuracy. They identify selection bias and count bias as key failure modes and propose a novel decoding strategy called Choice Funnel which improved exact match accuracy by up to 29%. This work highlights limitations in current LLMs' multi-answer reasoning and provides a framework for improving their performance in realistic, multi-answer applications. |
| Computer Vision | ComposeAnything: Composite Object Priors for Text-to-Image Generation (Read more on [arXiv](https://arxiv.org/abs/2505.24086) or [HuggingFace](https://huggingface.co/papers/2505.24086))| Cordelia Schmid, Shizhe Chen, zk95 | The paper introduces ComposeAnything, a framework for text-to-image generation that utilizes composite object priors to enhance compositional image generation without retraining existing models. It leverages LLMs to produce 2.5D semantic layouts from text, which are then used to generate a coarse composite object prior guiding the denoising process via object prior reinforcement and spatial-controlled denoising. ComposeAnything outperforms state-of-the-art methods on T2I-CompBench with a 77.16 score on 3D-Spatial arrangements and NSR-1K benchmarks, enabling seamless generation of compositional objects and coherent backgrounds. The approach allows for generating high-quality images with compositions that faithfully reflect the input text, which advances interpretable and controllable image synthesis for AI practitioners. |
