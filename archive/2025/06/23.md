

## Papers for 2025-06-23

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights (Read more on [arXiv](https://arxiv.org/abs/2506.16406) or [HuggingFace](https://huggingface.co/papers/2506.16406))| Xuanlei Zhao, Yuhao Zhou, Dongwen Tang, Zhiyuan Liang, VictorKai1996NUS | The paper introduces Drag-and-Drop LLMs (DnD), a novel approach for rapidly specializing large language models (LLMs) to specific tasks without per-task training. DnD generates LoRA weight updates directly from unlabeled task prompts using a lightweight text encoder and a cascaded hyper-convolutional decoder. The method achieves up to 12,000x lower overhead compared to full fine-tuning and improves performance by up to 30% on unseen benchmarks for common-sense reasoning, math, coding, and multimodal tasks. DnD offers a training-free alternative to gradient-based adaptation, enabling rapid specialization of LLMs across diverse domains. |
| Computer Vision | PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and
  Quantized Attention in Visual Generation Models (Read more on [arXiv](https://arxiv.org/abs/2506.16054) or [HuggingFace](https://huggingface.co/papers/2506.16054))| Huixia Li, Xuefeng Xiao, Xinhao Yang, Ke Hong, A-suozhang | The paper introduces PAROAttention, a novel approach for efficient sparse and quantized attention in visual generation models by reorganizing attention patterns. It addresses the challenge of dispersed and irregular visual attention patterns hindering sparsification and quantization. PAROAttention unifies diverse attention patterns into a hardware-friendly block-wise structure through pattern-aware token reordering, benefiting both sparsification and quantization. Experiments demonstrate that PAROAttention achieves nearly identical generation results to full-precision baselines with lower density (20%-30%) and bitwidth (INT8/INT4), resulting in a 1.9-2.7x end-to-end latency speedup, and enables image generation with lossless metrics, thus simplifying hardware requirements for visual AI tasks. |
| Multi-Modal | Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal
  Document Understanding (Read more on [arXiv](https://arxiv.org/abs/2506.16035) or [HuggingFace](https://huggingface.co/papers/2506.16035))| Biddwan Ahmed, Indraneel Das, Tanmay Odapally, udayallu, vishesh-t27 | This research introduces a vision-guided document chunking approach to enhance Retrieval-Augmented Generation (RAG) systems by leveraging Large Multimodal Models (LMMs). The primary objective is to improve document processing for complex structures by maintaining semantic coherence across page boundaries. The methodology involves processing PDF documents in configurable page batches with cross-batch context preservation using Gemini-2.5-Pro. Experimental results on an internal benchmark dataset demonstrate an accuracy of 0.89 compared to 0.78 for traditional chunking methods. The implication for AI practitioners is a novel method to improve RAG performance by more effectively utilizing document structure and semantic context. |
| Reinforcement Learning | VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2506.09049) or [HuggingFace](https://huggingface.co/papers/2506.09049))| Jie Yang, Yiran Qin, Heng Zhou, Xiufeng Song, FACEONG | The paper introduces VIKI-R, a novel benchmark and two-stage framework for coordinating embodied multi-agent cooperation using reinforcement learning. The research aims to address the challenges of perception-driven reasoning and scalable cooperation strategies in dynamic environments with diverse embodiments. VIKI-R first fine-tunes a pre-trained VLM with Chain-of-Thought demonstrations and then uses reinforcement learning with multi-level reward signals for optimization. Experiments show that VIKI-R significantly outperforms baselines across all task levels, achieving performance increases in agent activation, task planning, and trajectory perception, enabling the emergence of compositional cooperation patterns. The proposed benchmark and framework offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems. |
| Computer Vision | Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with
  Hybrid History Condition (Read more on [arXiv](https://arxiv.org/abs/2506.17201) or [HuggingFace](https://huggingface.co/papers/2506.17201))| Yuan Zhou, Longhuang Wu, Zhiyong Xu, Junshu Tang, Jiaqi Li | The paper introduces Hunyuan-GameCraft, a framework for high-dynamic interactive game video generation. It addresses the challenge of creating dynamic and consistent gameplay videos by unifying action controls into a shared camera representation space and employing a hybrid history-conditioned training strategy. Key to the approach is the generation of videos using a diffusion-based model trained on a large-scale dataset of gameplay recordings and fine-tuned on synthetic data, alongside model distillation for efficiency. Experiments demonstrate a significant improvement over existing methods, with a reported 55% reduction in interaction errors in cross-domain tests compared to Matrix-Game. This framework enables AI practitioners to produce more realistic and playable interactive game video content. |
| Computer Vision | DreamCube: 3D Panorama Generation via Multi-plane Synchronization (Read more on [arXiv](https://arxiv.org/abs/2506.17206) or [HuggingFace](https://huggingface.co/papers/2506.17206))| Xihui Liu, Kaiyi Huang, Jianan Wang, Yanning Zhou, Yukun Huang | The paper introduces DreamCube, a novel approach for generating 3D panoramas by synchronizing multi-plane 2D diffusion models. It addresses the challenge of generating consistent and high-quality omnidirectional content from limited panoramic data by adapting pre-trained 2D diffusion models. The key methodology involves multi-plane synchronization to ensure seam consistency without FoV overlapping and a masked RGB-D cubemap generator. Experiments demonstrate DreamCube's effectiveness, achieving a FID score of 12.58 on Structured3D dataset for RGB panorama generation. This enables AI practitioners to leverage powerful 2D diffusion priors for high-quality 3D panorama generation and depth estimation. |
| Computer Vision | Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate
  Details (Read more on [arXiv](https://arxiv.org/abs/2506.16504) or [HuggingFace](https://huggingface.co/papers/2506.16504))| Qingxiang Lin, Zibo Zhao, Haolin Liu, Yunfei Zhao, Zeqiang Lai | Hunyuan3D 2.5 is a novel suite of 3D diffusion models for generating high-fidelity 3D assets with enhanced details. The primary objective is to improve both shape and texture generation quality in 3D assets. The methodology involves a new shape foundation model (LATTICE) and upgraded texture generation using physical-based rendering. Evaluations show Hunyuan3D 2.5 outperforms previous methods, achieving a 72% win rate in image-to-3D user studies compared to commercial models. This enables AI practitioners to create realistic and detailed 3D assets for various applications with significantly improved quality. |
| Computer Vision | InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2506.15745) or [HuggingFace](https://huggingface.co/papers/2506.15745))| Simyung Chang, Jungwook Choi, Kyuhong Shim, Minsoo Kim | The paper introduces InfiniPot-V, a training-free framework for compressing key-value (KV) caches in streaming video understanding, aiming to reduce memory footprint without sacrificing accuracy. It monitors the KV cache and applies two compression techniques: Temporal-axis Redundancy (TaR) and Value-Norm (VaN) ranking, removing redundant tokens while preserving semantically significant ones. Evaluated across four MLLMs, InfiniPot-V cuts GPU memory usage by up to 94% while sustaining real-time generation and maintaining or surpassing full-cache accuracy. This resolves the KV cache bottleneck for on-device streaming video assistants, enabling practical deployment in memory-constrained environments. |
| Computer Vision | Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with
  Production-Ready PBR Material (Read more on [arXiv](https://arxiv.org/abs/2506.15442) or [HuggingFace](https://huggingface.co/papers/2506.15442))| Xin Huang, Yifei Feng, Mingxin Yang, Shuhui Yang, Team Hunyuan3D | Hunyuan3D 2.1 is presented as a comprehensive system for generating high-fidelity 3D assets with production-ready PBR materials from single image inputs. The research focuses on creating a robust and scalable 3D asset creation pipeline accessible beyond research environments. It employs Hunyuan3D-DiT for shape generation and Hunyuan3D-Paint for texture synthesis, leveraging diffusion models and multi-view PBR diffusion techniques. Quantitative evaluations show superior performance compared to existing methods, with ULIP-I score of 0.1395 for shape generation. The open-sourced system aims to democratize advanced 3D AIGC for practitioners in gaming, virtual reality, and industrial design. |
| Multi-Modal | UniFork: Exploring Modality Alignment for Unified Multimodal
  Understanding and Generation (Read more on [arXiv](https://arxiv.org/abs/2506.17202) or [HuggingFace](https://huggingface.co/papers/2506.17202))| Xizhou Zhu, Hao Li, Lirui Zhao, Quanfeng Lu, Teng Li | The paper introduces UniFork, a novel Y-shaped Transformer architecture for unified multimodal understanding and generation. It addresses the challenge of modality alignment by analyzing the distinct alignment patterns required for image understanding versus generation tasks. UniFork shares shallow layers for cross-task representation learning and employs task-specific branches in deeper layers to avoid interference. Ablation experiments demonstrate that UniFork outperforms conventional fully shared Transformer architectures, achieving a 46% accuracy on GenEval. This architecture provides a more effective trade-off between shared semantic learning and task specialization for unified multimodal models. |
| Natural Language Processing | Reranking-based Generation for Unbiased Perspective Summarization (Read more on [arXiv](https://arxiv.org/abs/2506.15925) or [HuggingFace](https://huggingface.co/papers/2506.15925))| Kathleen McKeown, Nicholas Deas, narutatsuri | This paper addresses unbiased perspective summarization by improving evaluation and generation techniques. The research questions focus on identifying reliable metrics and enhancing LLM-based summarization beyond zero-shot methods. A test set is constructed to evaluate metric reliability, demonstrating the underperformance of traditional metrics versus language model-based metrics; reranking-based methods are then employed, with preference tuning further improving performance. Results show that reranking outperforms prompting, and preference tuning with DPO significantly boosts faithfulness. The findings contribute to more reliable evaluation and development of perspective summarization methods for AI practitioners. |
| Computer Vision | Long-term Traffic Simulation with Interleaved Autoregressive Motion and
  Scenario Generation (Read more on [arXiv](https://arxiv.org/abs/2506.17213) or [HuggingFace](https://huggingface.co/papers/2506.17213))| Philipp Krähenbühl, Shuhan Tan, Xiuyu Yang | The paper introduces InfGen, a unified next-token prediction model for long-term traffic simulation that combines motion simulation and scene generation. It addresses the challenge of maintaining realistic scene layouts in long-term simulations by interleaving closed-loop motion simulation with dynamic agent insertion and removal. InfGen employs a transformer architecture with task-specific tokenizers and mode-control tokens to switch between motion simulation and scene generation. The model outperforms prior SOTA models in 30-second long-term traffic simulation, achieving improved realism and stable rollouts with significantly lower Agent Count Error. This enables more realistic and adaptable traffic simulations for self-driving system development. |
