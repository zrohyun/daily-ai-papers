

## Papers for 2025-06-16

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | Aligned Novel View Image and Geometry Synthesis via Cross-modal
  Attention Instillation (Read more on [arXiv](https://arxiv.org/abs/2506.11924) or [HuggingFace](https://huggingface.co/papers/2506.11924))| Taekyoung Kim, Dongyoon Han, Sangdoo Yun, Junho Kim, Min-Seop Kwak | The paper introduces a diffusion-based framework for aligned novel view image and geometry synthesis. The research aims to generate novel views from sparse, unposed reference images, simultaneously predicting both image and geometry. This is achieved via cross-modal attention instillation, transferring attention maps from the image diffusion branch to the geometry diffusion branch. The method demonstrates superior performance in extrapolative settings on the RealEstate10K dataset, achieving a PSNR of 17.41, suggesting improved generative capabilities. This allows AI practitioners to synthesize high-fidelity, geometrically consistent 3D scenes from limited 2D inputs without explicit pose information. |
| Natural Language Processing | Effective Red-Teaming of Policy-Adherent Agents (Read more on [arXiv](https://arxiv.org/abs/2506.09600) or [HuggingFace](https://huggingface.co/papers/2506.09600))| Guy Uziel, Matan Vetzler, Koren Lazar, George Kour, Itay Nakash | The paper introduces CRAFT, a multi-agent red-teaming system to evaluate the robustness of policy-adherent LLM agents. It addresses the challenge of ensuring agents consistently adhere to policies while maintaining natural interactions, focusing on adversarial users exploiting agents for personal benefit. The methodology involves policy-aware persuasive strategies that outperform conventional jailbreak methods, significantly increasing the attack success rate to 70.0% compared to alternatives. The main implication is the need for stronger safeguards to protect policy-adherent agents against sophisticated adversarial attacks, as current methods are insufficient. |
| Natural Language Processing | The Diffusion Duality (Read more on [arXiv](https://arxiv.org/abs/2506.10892) or [HuggingFace](https://huggingface.co/papers/2506.10892))| Justin Chiu, Guanghan Wang, Aaron Gokaslan, Justin Deschenaux, Subham Sekhar Sahoo | The paper introduces Duo, a framework that bridges continuous Gaussian diffusion models and discrete Uniform-state diffusion models (USDMs) for natural language processing. It aims to improve USDMs, which are typically outperformed by autoregressive models, by transferring techniques from Gaussian diffusion. The key methodology involves a curriculum learning strategy guided by the Gaussian process and Discrete Consistency Distillation, adapting consistency distillation to the discrete setting.  Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks; Discrete Consistency Distillation reduces NFEs from 1024 to 8. This allows Duo to achieve faster training and few-step generation, making it more competitive with masked diffusion models and opening new avenues for improving USDMs by leveraging techniques from Gaussian diffusion. |
| Machine Learning | LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive
  Programming? (Read more on [arXiv](https://arxiv.org/abs/2506.11928) or [HuggingFace](https://huggingface.co/papers/2506.11928))| Kaiyuan Liu, Shang Zhou, Zeyu Shen, Zerui Cheng, Zihan Zheng | The paper introduces LiveCodeBench Pro, a benchmark for evaluating LLMs in competitive programming, judged by Olympiad medalists. The study investigates whether LLMs can truly reason at the level of elite human competitors in algorithmic problem-solving. The methodology involves annotating Codeforces, ICPC, and IOI problems for algorithmic categories and conducting line-by-line analysis of model-generated solutions. Results indicate that the best model achieves only 53% pass@1 on medium-difficulty problems and 0% on hard problems without external tools, revealing limitations in algorithmic reasoning. This highlights the need for improvements in code-centric LLM reasoning, suggesting implementation precision and tool augmentation drive performance more than superior reasoning. |
| Machine Learning | pLSTM: parallelizable Linear Source Transition Mark networks (Read more on [arXiv](https://arxiv.org/abs/2506.11997) or [HuggingFace](https://huggingface.co/papers/2506.11997))| Sepp Hochreiter, Wei Lin, Thomas Schmied, Richard Freinschlag, korbip | The paper introduces parallelizable Linear Source Transition Mark networks (pLSTMs) for processing data with a higher level structure, such as images and graphs, efficiently. The research aims to extend the notion of multi-dimensionality to linear RNNs by adapting ideas from Multi-Dimensional RNNs (MDRNNs) to linear architectures. pLSTMs utilize Source, Transition, and Mark gates operating on the line graph of a DAG and incorporate directed propagation (P-mode) and diffusive distribution (D-mode) for long-range dependencies. Empirical results on an arrow-pointing extrapolation task demonstrate that pLSTMs generalize well to larger image sizes (achieving significantly better performance than Transformers), and they also achieve competitive results on molecular graph and computer vision benchmarks.  This approach provides AI practitioners with a parallelizable recurrent architecture capable of handling structured data beyond sequential inputs. |
| Multi-Modal | A High-Quality Dataset and Reliable Evaluation for Interleaved
  Image-Text Generation (Read more on [arXiv](https://arxiv.org/abs/2506.09427) or [HuggingFace](https://huggingface.co/papers/2506.09427))| kpzhang, ZhangShenglin, fanrui00, cyrilli, finyorko | The paper introduces InterSyn, a large-scale multimodal dataset for instruction-following interleaved image-text generation. It addresses the lack of high-quality training data and reliable evaluation metrics for this task. The authors propose a Self-Evaluation with Iterative Refinement (SEIR) method to construct InterSyn and SynJudge, an automatic evaluation model. Experiments demonstrate that LMMs trained on InterSyn achieve significant performance gains, including a 52.1% improvement in image-text synergy. The work provides a new benchmark and evaluation tool for advancing unified multimodal systems. |
| Reinforcement Learning | SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation
  via Skill Blending (Read more on [arXiv](https://arxiv.org/abs/2506.09366) or [HuggingFace](https://huggingface.co/papers/2506.09366))| Tan-Dzung Do, Haoran Geng, jitendra1995, AmineElhafsi, yxK | The paper introduces SkillBlender, a novel hierarchical reinforcement learning framework for versatile humanoid loco-manipulation. The research aims to enable humanoids to perform diverse tasks with minimal task-specific reward engineering. SkillBlender pretrains goal-conditioned primitive skills and dynamically blends them using a high-level controller. Experiments show that SkillBlender significantly outperforms baselines, achieving more accurate and feasible movements, e.g., reducing task error in complex manipulation tasks. The framework offers a structured approach to developing versatile humanoid control policies with reduced reward engineering, facilitating easier adoption for AI practitioners. |
| Multi-Modal | Detecting Harmful Memes with Decoupled Understanding and Guided CoT
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.08477) or [HuggingFace](https://huggingface.co/papers/2506.08477))| Anh Tuan Luu, Fengjun Pan, bobxwu | The paper introduces U-CoT+, a novel framework for detecting harmful memes by decoupling understanding and reasoning. It addresses the limitations of existing approaches in resource efficiency, flexibility, and explainability by converting visual memes into detailed textual descriptions using a high-fidelity meme-to-text pipeline and employing guided zero-shot CoT prompting with general LLMs. Experiments on seven benchmark datasets validate U-CoT+'s effectiveness, achieving performance comparable to supervised methods, with Mistral-12B obtaining 72.90% accuracy and 72.87% F1 score on FHM. This allows for explainable and adaptable harmful meme detection using smaller LLMs, significantly reducing resource demands. |
| Natural Language Processing | Beyond Homogeneous Attention: Memory-Efficient LLMs via
  Fourier-Approximated KV Cache (Read more on [arXiv](https://arxiv.org/abs/2506.11886) or [HuggingFace](https://huggingface.co/papers/2506.11886))| Yuerong Song, Ruixiao Li, Qiqi Wang, Siyang He, Xiaoran Liu | This paper introduces FourierAttention, a training-free framework for memory-efficient LLMs via Fourier-approximated KV cache. It investigates the heterogeneous roles of transformer head dimensions, finding lower dimensions prioritize local context while upper dimensions capture long-range dependencies. The methodology involves projecting long-context-insensitive dimensions onto orthogonal Fourier bases to approximate their temporal evolution with fixed-length spectral coefficients. Evaluations on LLaMA models demonstrate FourierAttention achieves the best long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH) benchmarks. This approach provides AI practitioners with a novel, computationally efficient method to reduce memory demands of LLMs without significant performance compromise. |
| Computer Vision | DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware
  Regressive GRPO (Read more on [arXiv](https://arxiv.org/abs/2506.07464) or [HuggingFace](https://huggingface.co/papers/2506.07464))| Hyunwoo J. Kim, Jinyoung Kim, Jeehye Na, Jinyoung Park | The paper introduces DeepVideo-R1, a video large language model fine-tuned with reinforcement learning to improve video reasoning capabilities. It addresses limitations in applying Group Relative Policy Optimization (GRPO) to VideoLLMs, specifically reliance on safeguards and the vanishing advantage problem. DeepVideo-R1 utilizes Regressive GRPO (Reg-GRPO), reformulating the GRPO objective as a regression task, and difficulty-aware data augmentation. Experimental results demonstrate that DeepVideo-R1 significantly enhances video reasoning performance, achieving a 10.06 improvement compared to GRPO on the Qwen2.5-VL-3B benchmark. The implication is that regression-based RL objectives combined with difficulty-aware augmentation can effectively train large-scale multimodal reasoning models. |
| Natural Language Processing | Configurable Preference Tuning with Rubric-Guided Synthetic Data (Read more on [arXiv](https://arxiv.org/abs/2506.11702) or [HuggingFace](https://huggingface.co/papers/2506.11702))| vicgalle | This paper introduces Configurable Preference Tuning (CPT), a novel framework for dynamically adjusting language model behavior based on explicit, human-interpretable directives. The research aims to endow LLMs with the ability to modulate their outputs at inference time by leveraging synthetically generated preference data conditioned on structured rubrics defining attributes like writing style. Key methodology involves fine-tuning LLMs with these rubric-guided preference pairs using a DPO-style objective. Results demonstrate a significant improvement across several models and metrics after CPT fine-tuning; for example, Mistral-Nemo-12B's accuracy improved from 0.60 to 0.83. CPT enables AI practitioners to achieve more granular and controllable alignment of LLMs without retraining for each configuration. |
| Multi-Modal | ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual
  Perception in VLMs (Read more on [arXiv](https://arxiv.org/abs/2506.10128) or [HuggingFace](https://huggingface.co/papers/2506.10128))| Yuhang Zhou, Yongyuan Liang, Chao Feng, Zhengyuan Yang, Xiyao Wang | The paper introduces ViCrit, a reinforcement learning proxy task to enhance visual perception in vision-language models (VLMs) by training them to identify subtle, synthetically injected visual hallucinations in image captions. The research aims to address the scarcity of vision-centric tasks that are challenging and unambiguously verifiable, which is impeding the progress of visual perception in VLMs. ViCrit involves training VLMs to pinpoint corrupted spans in modified captions given an image, providing a binary, exact-match reward. Experiments show models trained with ViCrit exhibit substantial gains across VL benchmarks, including an improvement from 35.2% to 40.1% on MathVision for Qwen2.5-VL-72B-Instruct. The main implication is that fine-grained hallucination criticism is an effective and generalizable objective for enhancing visual perception in VLMs. |
| Natural Language Processing | A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data (Read more on [arXiv](https://arxiv.org/abs/2506.11130) or [HuggingFace](https://huggingface.co/papers/2506.11130))| Hsi-Chun Cheng, Liang-Hsuan Tseng, Ho-Lam Chung, Chan-Jan Hsu, Cheng Kang Chou | The paper introduces a self-refining framework to enhance ASR using TTS-synthesized data. It aims to improve ASR performance, especially in low-resource settings, by leveraging unlabeled speech. The methodology involves generating pseudo-labels via an ASR model, training a TTS system, and then retraining the ASR using synthetic data generated by the TTS system. Results show Twister, the refined ASR model, achieves up to a 20% error rate reduction on Mandarin and a 50% reduction on code-switching benchmarks compared to Whisper-large-v2. The framework offers a data-efficient method for ASR adaptation in resource-constrained scenarios, reducing the dependence on real speech data. |
| Reinforcement Learning | SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement
  Learning for LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2506.08989) or [HuggingFace](https://huggingface.co/papers/2506.08989))| Yang Wang, Yeyun Gong, Zhong-Zhi Li, Xiao Liang, yegong | The paper introduces a Self-aware Weakness-driven problem Synthesis (SwS) framework to enhance reinforcement learning (RL) for large language model (LLM) reasoning. It addresses the challenge of creating high-quality, targeted training data by identifying model weaknesses and generating synthetic problems to augment training. The methodology involves iterative RL training to pinpoint questions the model struggles with, then extracting key concepts to synthesize new, challenging problems. Experiments show SwS yields performance gains, with 10.0% and 7.7% improvements on 7B and 32B models, respectively, across eight reasoning benchmarks. This enables practitioners to create training data specifically tailored to the model's deficiencies. |
| Natural Language Processing | Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity
  Dilemma of Embeddings (Read more on [arXiv](https://arxiv.org/abs/2506.08592) or [HuggingFace](https://huggingface.co/papers/2506.08592))| Fandong Meng, Jiangnan Li, Mo Yu, Zhenlin Su, lxucs | This paper examines the failure of dense retrievers on simple queries due to the granularity dilemma of embeddings. The research aims to understand why text encoders struggle with fine-grained entity/event recognition. A new Chinese dataset, CapRetrieval, is introduced, featuring image captions and short-phrase queries. Zero-shot evaluation reveals shortcomings in state-of-the-art encoders, and finetuning with data generation strategies improves performance, achieving a 5+% gain over the best baseline. The results highlight the challenge of balancing fine-grained salience and overall semantic alignment for effective embeddings. |
| Computer Vision | JAFAR: Jack up Any Feature at Any Resolution (Read more on [arXiv](https://arxiv.org/abs/2506.11136) or [HuggingFace](https://huggingface.co/papers/2506.11136))| Matthieu Cord, Jean-Emmanuel Haugeard, Louis Serrano, Loick Chambon, Paul Couairon | The paper introduces JAFAR, a lightweight and flexible feature upsampler for foundation vision encoders. JAFAR aims to enhance spatial resolution of visual features for downstream tasks by using attention-based modules and Spatial Feature Transform modulation. It is trained without high-resolution supervision. Experiments demonstrate that JAFAR recovers fine-grained spatial details and outperforms existing upsampling methods, achieving, for example, a +1.63 mIoU improvement over the next best method across semantic segmentation datasets. JAFAR offers AI practitioners a versatile drop-in module that enhances performance in various downstream tasks by effectively recovering spatial details. |
| Computer Vision | Inherently Faithful Attention Maps for Vision Transformers (Read more on [arXiv](https://arxiv.org/abs/2506.08915) or [HuggingFace](https://huggingface.co/papers/2506.08915))| Diego Marcos, Dino Ienco, Cassio F. Dantas, ananthu-aniraj | This paper introduces an attention-based method to improve the faithfulness of attention maps in vision transformers. The research aims to mitigate spurious correlations in object recognition by restricting the model's receptive field to task-relevant image regions. The proposed two-stage framework involves a region selector and a masked classifier trained jointly, allowing the second stage to refine the first stage's output by filtering out potentially spurious information. Experiments demonstrate improved robustness against spurious correlations and out-of-distribution backgrounds with, for example, an improved WGA of 88.6% on MetaShift. The method offers AI practitioners a way to develop vision transformers that are more robust to contextual biases. |
