

## Papers for 2025-06-20

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Reinforcement Learning | Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain
  Perspective (Read more on [arXiv](https://arxiv.org/abs/2506.14965) or [HuggingFace](https://huggingface.co/papers/2506.14965))| Yutao Xie, Fan Zhou, Tianyang Liu, Shibo Hao, Zhoujun Cheng | This paper revisits reinforcement learning (RL) for improving large language model (LLM) reasoning across diverse domains. It addresses the challenge of lacking reliable RL reward signals by introducing GURU, a curated corpus of 92K verifiable examples across six reasoning domains.  The study employs domain-specific reward design and filtering techniques to train models and systematically re-evaluates established RL findings, observing significant variations across domains.  Results show that GURU-7B/32B achieve state-of-the-art performance among open models, outperforming baselines by 7.9% and 6.7% on a 17-task evaluation suite. The findings emphasize the domain-specific nature of RL reasoning mechanisms, suggesting the need for multi-domain training and evaluation. |
| Natural Language Processing | EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech
  Emotion Detection (Read more on [arXiv](https://arxiv.org/abs/2506.09827) or [HuggingFace](https://huggingface.co/papers/2506.09827))| Maurice Kraus, Gollam Rabby, Robert Kaczmarczyk, felfri, ChristophSchuhmann | This paper introduces EmoNet-Voice, a benchmark dataset for fine-grained speech emotion detection. The research aims to evaluate the emotional understanding capabilities of AI systems using synthetic speech. The methodology involves curating a large-scale synthetic speech dataset (EmoNet-Voice BIG) and a benchmark dataset (EmoNet-Voice BENCH) with expert annotations of 40 emotion categories and intensity levels. The evaluation reveals that the proposed EMPATHICINSIGHT-VOICE model achieves state-of-the-art performance, with a Pearson correlation of 0.421 on EmoNet-Voice BENCH. The benchmark enables AI practitioners to rigorously evaluate and improve speech emotion detection models, especially for nuanced emotional states. |
| Multi-Modal | SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning (Read more on [arXiv](https://arxiv.org/abs/2506.15154) or [HuggingFace](https://huggingface.co/papers/2506.15154))| Dorien Herremans, Abhinaba Roy, Anuradha Chopra | This paper introduces SonicVerse, a multi-task learning model for music captioning that integrates feature detection for improved descriptions. The research aims to enhance music captioning by incorporating auxiliary tasks like key and vocals detection. SonicVerse uses a projection-based architecture to transform audio into language tokens and detects music features through auxiliary heads, projecting these features into language tokens to augment the captioning input. Experiments on an extended MusicBench dataset show improved caption quality, and temporal features, as measured by BLEU, are enhanced. The results imply that incorporating music features improves caption generation, aiding in the creation of detailed and temporally informed descriptions for music pieces. |
| Multi-Modal | Improved Iterative Refinement for Chart-to-Code Generation via
  Structured Instruction (Read more on [arXiv](https://arxiv.org/abs/2506.14837) or [HuggingFace](https://huggingface.co/papers/2506.14837))| Weiran Huang, Lichao Sun, Yuyang Wang, Chengzhi Xu, WaltonFuture | The paper addresses the suboptimal performance of multimodal large language models (MLLMs) in chart-to-code generation by proposing ChartIR, an iterative refinement method based on structured instruction. The research focuses on improving both visual understanding and code translation through structured descriptions (description and difference instructions) and an iterative refinement process. ChartIR significantly outperforms direct generation and METAL, achieving a GPT-40 Score of 6.56 on the Plot2Code dataset compared to 5.61 and 6.02 for direct generation and METAL, respectively. This method enhances visual and structural fidelity in generated charts, offering a robust framework for improving MLLM performance in this domain. |
