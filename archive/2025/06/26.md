

## Papers for 2025-06-26

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image
  Generation (Read more on [arXiv](https://arxiv.org/abs/2506.18095) or [HuggingFace](https://huggingface.co/papers/2506.18095))| Ke Ji, Shunian Chen, Zhenyang Cai, Junying Chen, cppppppc | This paper presents ShareGPT-4o-Image, a dataset designed to align multimodal models with GPT-4o-level image generation capabilities. It aims to democratize access to advanced image generation by synthesizing 45K text-to-image and 46K text-and-image-to-image samples using GPT-4o's capabilities. The authors develop Janus-4o, a multimodal large language model, which significantly improves text-to-image generation and supports text-and-image-to-image generation. Janus-4o achieves impressive performance in text-and-image-to-image generation with only 91K synthetic samples and 6 hours of training, improving over Janus-Pro by 4 points on EvalGen, thereby fostering open research in photorealistic, instruction-aligned image generation. |
| Natural Language Processing | Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.19697) or [HuggingFace](https://huggingface.co/papers/2506.19697))| Jaewoo Kang, Hyeon Hwang, Chanwoong Yoon, Taewhoo Lee, affjljoo3581 | The paper introduces Outlier-Safe Pre-Training (OSP) to improve the robustness of 4-bit quantization in LLMs. It aims to mitigate activation outliers that degrade quantization performance. The OSP framework combines the Muon optimizer, Single-Scale RMSNorm, and learnable embedding projection. The OSP model achieves a 35.7 average score on 10 benchmarks under 4-bit quantization, compared to 26.5 for Adam, with a 2% training overhead. OSP enables more efficient LLM deployment by fundamentally altering quantization behavior through proactive outlier prevention. |
| Reinforcement Learning | DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware
  Planning (Read more on [arXiv](https://arxiv.org/abs/2506.16012) or [HuggingFace](https://huggingface.co/papers/2506.16012))| Hang Xu, Siyuan He, Boyu Li, WizardTY, tellarin | The paper introduces DualTHOR, a physics-based simulation platform built upon AI2-THOR for training and evaluating dual-arm humanoid robots in household environments. The research aims to address limitations of existing platforms by incorporating realistic robot morphologies, a diverse task suite, inverse kinematics solvers, and a contingency mechanism for simulating real-world uncertainties. DualTHOR employs Unity physics engine and incorporates a task history management system for diverse trajectory exploration. Evaluation reveals current VLMs struggle with dual-arm coordination and exhibit limited robustness, achieving a success rate of approximately 36% for DAG-Plan on dual-arm essential tasks. The implication is that DualTHOR provides a valuable benchmark for developing more capable and robust VLMs for embodied tasks, specifically by providing a framework to develop planning and reaction to uncertainty. |
| Reinforcement Learning | OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling (Read more on [arXiv](https://arxiv.org/abs/2506.20512) or [HuggingFace](https://huggingface.co/papers/2506.20512))| Pengfei Liu, Xuefeng Li, Fan Zhou, Zengzhi Wang | This paper investigates the impact of mid-training strategies on reinforcement learning (RL) scaling for large language models, focusing on mathematical reasoning. It examines how different mathematical corpora and QA-style data affect RL dynamics in Llama models. The study finds that MegaMath-Web-Pro improves both base model and RL performance, while incorporating instruction data further enhances RL outcomes; using a two-stage training called Stable-then-Decay, the OctoThinker family of models achieved performance on par with Qwen2.5.  The work provides insights into pre-training strategies for building RL-scalable foundation models and emphasizes the importance of high-quality data and careful data formatting. |
| Machine Learning | Use Property-Based Testing to Bridge LLM Code Generation and Validation (Read more on [arXiv](https://arxiv.org/abs/2506.18315) or [HuggingFace](https://huggingface.co/papers/2506.18315))| Jing Shao, Zhe Zhang, Lehan He, lsheng2024, zx55 | This paper presents Property-Generated Solver (PGS), a framework for enhancing LLM-based code generation using property-based testing (PBT). The research aims to improve code correctness by leveraging PBT to validate high-level program properties. PGS employs two LLM agents: a Generator for code and a Tester for managing the PBT life-cycle and feedback. Experiments on code generation benchmarks show PGS achieves significant pass@1 improvements, ranging from 23.1% to 37.3% relative gains over established TDD methods. The implication for AI practitioners is a more robust mechanism for steering LLMs toward correct and generalizable code, addressing the challenge of ensuring functional correctness in complex programming tasks. |
| Computer Vision | RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain
  Randomization for Robust Bimanual Robotic Manipulation (Read more on [arXiv](https://arxiv.org/abs/2506.18088) or [HuggingFace](https://huggingface.co/papers/2506.18088))| Yibin Liu, Zijian Cai, Baijun Chen, Zanxin Chen, TianxingChen | This paper introduces RoboTwin 2.0, a scalable simulation framework for generating diverse data for bimanual robotic manipulation. It addresses the challenge of insufficient synthetic data for robust bimanual manipulation by creating a data generation method and realistic simulation environment. The key methodology involves an MLLM-based expert code generation module with simulation-in-the-loop feedback and comprehensive domain randomization. Experiments show that a VLA model fine-tuned on RoboTwin 2.0 data achieves a 367% relative improvement on unseen scene real-world tasks. The implication is the enhanced generation of data for training robust robotic manipulation policies capable of generalization to unseen environments. |
| Natural Language Processing | Is There a Case for Conversation Optimized Tokenizers in Large Language
  Models? (Read more on [arXiv](https://arxiv.org/abs/2506.18674) or [HuggingFace](https://huggingface.co/papers/2506.18674))| Pedro Reviriego, Gonzalo Mart√≠nez, Javier Conde, Raquel Ferrando | The paper explores conversation-optimized tokenizers for Large Language Models (LLMs) to improve energy efficiency. It investigates whether optimizing tokenizers for chatbot conversations can reduce the number of tokens compared to tokenizers trained on general corpora. The study re-trains existing tokenizers using a dataset of chatbot conversations and evaluates the performance against the original tokenizers. Results show a 5% to 10% reduction in the number of tokens for chatbot dialogues using conversation-optimized tokenizers, while maintaining performance on original training corpora. The research suggests that application-specific tokenization can significantly reduce the computational cost of LLM inference in conversational settings. |
| Natural Language Processing | When Life Gives You Samples: The Benefits of Scaling up Inference
  Compute for Multilingual LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.20544) or [HuggingFace](https://huggingface.co/papers/2506.20544))| Sara Hooker, Julia Kreutzer, Ye Shen, Daniel D'souza, ammar-cohere | The paper investigates how to efficiently scale inference compute for multilingual LLMs across diverse tasks. It explores sampling and selection strategies to improve performance without retraining the model. The study introduces a hedged sampling method and novel selection strategies (CHOPS and X-MBR), showing an average +6.8 jump in win-rates on m-ArenaHard-v2.0 prompts against proprietary models with Command-A achieving +9.0 improvement in win-rates with minimal compute. Results underscore the need for language- and task-aware inference to democratize performance improvements. |
| Reinforcement Learning | ReCode: Updating Code API Knowledge with Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2506.20495) or [HuggingFace](https://huggingface.co/papers/2506.20495))| Ningyu Zhang, Huajun Chen, Wenhao Yu, Yunzhi Yao, Haoze Wu | ReCode addresses the issue of LLMs' outdated API knowledge hindering code generation reliability in dynamic environments. It investigates how reinforcement learning can enhance LLMs' ability to adapt to API updates by training models to migrate code versions based on updated information. ReCode uses a modified string similarity metric for code evaluation as a reward during reinforcement learning. Experiments demonstrate ReCode improves LLMs' code generation performance in dynamic API scenarios, achieving a higher Pass@1 score than a 32B parameter code instruction-tuned model on CodeUpdateArena. The framework provides a method for AI practitioners to address API obsolescence, a key limitation of current LLMs. |
| Computer Vision | HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based
  Diffusion Sampling (Read more on [arXiv](https://arxiv.org/abs/2506.20452) or [HuggingFace](https://huggingface.co/papers/2506.20452))| Farnood Salehi, Tobias Vontobel, RMW, msadat97 | The paper introduces HiWave, a novel training-free approach for generating high-resolution images using wavelet-based diffusion sampling. It aims to enhance the visual fidelity and structural coherence of ultra-high-resolution images synthesized by pretrained diffusion models. The method employs a two-stage pipeline, generating a base image followed by patch-wise DDIM inversion and wavelet-based detail enhancement to control low and high-frequency components, respectively. Evaluated using Stable Diffusion XL, HiWave demonstrates superior perceptual quality and mitigates common visual artifacts; a user study showed HiWave was preferred in over 80% of comparisons. HiWave provides AI practitioners with a practical, training-free solution for achieving high-quality, ultra-high-resolution image synthesis. |
| Computer Vision | Inverse-and-Edit: Effective and Fast Image Editing by Cycle Consistency
  Models (Read more on [arXiv](https://arxiv.org/abs/2506.19103) or [HuggingFace](https://huggingface.co/papers/2506.19103))| Aibek Alanov, Andrey Kuznetsov, Ilia Beletskii | The paper introduces a novel framework for efficient image editing using consistency models. It aims to improve image inversion quality, a critical aspect for high-fidelity editing in distilled diffusion models. The core methodology involves a cycle-consistency optimization strategy to enhance reconstruction accuracy while trading off editability and content preservation. The method achieves state-of-the-art performance, matching or surpassing full-step diffusion models with substantially improved efficiency, showing an improvement on the LPIPS metric. This allows AI practitioners to perform fast and effective image editing without sacrificing quality. |
| Natural Language Processing | The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.18403) or [HuggingFace](https://huggingface.co/papers/2506.18403))| Carlos C. N. Kuhn, adnaan525 | The paper introduces the Debugging Decay Index (DDI) framework for evaluating the effectiveness of iterative debugging in code-generating LLMs. It investigates how the effectiveness of debugging decays over successive attempts and proposes strategic interventions to enhance the process. The study fits an exponential decay model to the debugging effectiveness data and determines optimal intervention points. The DDI analysis of eighteen SOTA models reveals that debugging effectiveness typically follows predictable exponential decay trajectories and strategic fresh starts improve final accuracy without additional computational costs, enhancing performance by up to 10%. The DDI provides a quantitative framework for optimising iterative code generation strategies for AI practitioners. |
| Natural Language Processing | Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining
  and Extracting Rare and Hidden Content (Read more on [arXiv](https://arxiv.org/abs/2506.20331) or [HuggingFace](https://huggingface.co/papers/2506.20331))| Eric de la Clergerie, Nathan Godey, rntc | The paper introduces Biomed-Enriched, a novel biomedical text dataset designed to improve language model pretraining for rare and hidden content extraction. The research aims to enhance the capabilities of language models in the biomedical domain by curating a dataset with specific annotations derived from LLM scoring. This is achieved through a two-stage annotation process involving initial annotation with Llama-3.1-70B-Instruct followed by fine-tuning a smaller XLM-ROBERTa model for corpus-wide application. Experiments show that targeted clinical upsampling improves performance by 5% on MMLU ProfMed, and data efficiency is significantly enhanced, achieving similar scores with only a third of the training tokens, offering a more computationally efficient pretraining strategy for biomedical language models. |
| Multi-Modal | MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility
  Applications (Read more on [arXiv](https://arxiv.org/abs/2506.19502) or [HuggingFace](https://huggingface.co/papers/2506.19502))| Paul Laban, Matt Laing, AleksandrAlgazinov | The paper introduces MATE, a multimodal accessibility multi-agent system (MAS) for modality conversions tailored to user needs. It addresses the lack of customization in existing MAS by developing an open-source, lightweight framework that performs modality conversions, assisting people with disabilities. The key methodology involves a novel model, ModCon-Task-Identifier, for extracting modality conversion tasks from user inputs. Experiments show that ModCon-Task-Identifier outperforms other LLMs and statistical models, achieving an accuracy of 0.917 on a custom dataset. MATE provides a flexible architecture that AI practitioners can adapt for real-time multimodal accessibility applications across various domains. |
