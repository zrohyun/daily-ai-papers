

## Papers for 2025-09-01

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs
  via Bi-Mode Annealing and Reinforce Learning (Read more on [arXiv](https://arxiv.org/abs/2508.21113) or [HuggingFace](https://huggingface.co/papers/2508.21113))| Han Hu, Shiming Xiang, Bolin Ni, Qi Yang, Jie Jiang | The paper introduces R-4B, an auto-thinking MLLM that adaptively decides when to think based on problem complexity. It aims to equip the model with both thinking and non-thinking capabilities using bi-mode annealing and reinforce learning. The methodology involves training the model on a curated dataset with both thinking and non-thinking samples, followed by bi-mode policy optimization. R-4B achieves state-of-the-art performance across 25 benchmarks, outperforming Qwen2.5-VL-7B and achieving performance comparable to Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks. R-4B provides a more efficient approach to MLLMs by reducing computational costs through adaptive thinking. |
| Reinforcement Learning | EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for
  General Robot Control (Read more on [arXiv](https://arxiv.org/abs/2508.21112) or [HuggingFace](https://huggingface.co/papers/2508.21112))| Zhaoqing Chen, Qizhi Chen, Haoming Song, sundrops, delinqu | This paper introduces EO-1, a unified embodied foundation model for superior multimodal reasoning and robot control through interleaved vision-text-action pre-training. The research aims to enable flexible and powerful multimodal embodied reasoning and action generation. EO-1 leverages synergies between auto-regressive decoding and flow matching denoising on a new dataset, EO-Data1.5M, containing over 1.5 million samples.  Evaluations on RoboVQA show a BLEU-4 score of 58.5, outperforming GPT-40, demonstrating effectiveness in open-world understanding and generalization. The main implication is a path toward advanced embodied foundation models by emphasizing interleaved learning and high-quality multimodal embodied reasoning datasets. |
| Machine Learning | A.S.E: A Repository-Level Benchmark for Evaluating Security in
  AI-Generated Code (Read more on [arXiv](https://arxiv.org/abs/2508.18106) or [HuggingFace](https://huggingface.co/papers/2508.18106))| Libo Chen, Lei Zhang, Bin Wang, wanng, KekeLian | The paper introduces A.S.E, a repository-level benchmark to evaluate the security of AI-generated code. It aims to address the limitations of existing benchmarks by incorporating real-world repository context and reproducible evaluation. A.S.E uses a Dockerized environment with expert-defined rules for security, build quality, and stability assessments. Experiments on leading LLMs revealed Claude-3.7-Sonnet achieves the best overall performance (63.01), while Qwen3-235B-A22B-Instruct tops Security (48.06). The findings suggest concise decoding strategies outperform slow-thinking reasoning for security patching, implying a need to reassess reasoning paradigms in secure code generation. |
| Computer Vision | Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation (Read more on [arXiv](https://arxiv.org/abs/2508.20470) or [HuggingFace](https://huggingface.co/papers/2508.20470))| Qi Jia, Liang Jin, Runze Zhang, Guoguang Du, lixiaochuan | The paper introduces Droplet3D, a framework that leverages commonsense priors from videos to facilitate 3D content generation. The research aims to address data scarcity in 3D by utilizing videos as a supervisory signal for spatial consistency and semantic knowledge. The methodology involves training a generative model, Droplet3D, on a newly created large-scale video dataset with multi-view level annotations called Droplet3D-4M. Quantitative results include an improved CLIP-S score of 0.866 compared to a baseline of 0.737. This work implies that leveraging video data and associated commonsense priors can significantly enhance the quality and generalization capabilities of 3D generative models for AI practitioners. |
| Computer Vision | TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head
  Synthesis (Read more on [arXiv](https://arxiv.org/abs/2508.13618) or [HuggingFace](https://huggingface.co/papers/2508.13618))| Pengcheng Chen, Zihan Ye, Yexin Liu, Hejin Huang, Shunian Chen | The paper introduces TalkVid, a new large-scale dataset for audio-driven talking head synthesis designed to address limitations in diversity and quality of existing datasets. The main objective is to improve the generalization capabilities of talking head models across different ethnicities, languages, and age groups. TalkVid was created using a multi-stage automated pipeline to filter for motion stability, aesthetic quality, and facial detail and contains 1244 hours of video from 7729 unique speakers. Experiments demonstrate that models trained on TalkVid outperform those trained on previous datasets, and the introduction of TalkVid-Bench, a stratified evaluation set, reveals performance disparities across subgroups that were previously obscured; for example, TalkVid trained models achieve lower FID/FVD scores, implying better visual quality. The dataset and benchmark aim to provide AI practitioners with resources for developing more robust and equitable talking-head synthesis models. |
| Computer Vision | UItron: Foundational GUI Agent with Advanced Perception and Planning (Read more on [arXiv](https://arxiv.org/abs/2508.21767) or [HuggingFace](https://huggingface.co/papers/2508.21767))| Yufeng Zhong, Wenkang Han, Liming Zheng, Jing Huang, Zhixiong Zeng | The paper introduces Ultron, a foundational GUI agent designed for advanced perception and planning in mobile and PC environments. The primary objective is to develop a GUI agent capable of understanding and interacting with complex interfaces, addressing limitations in existing solutions related to data scarcity and initial capabilities. Ultron employs a systemic data engineering approach and curriculum reinforcement learning to enhance perception, grounding, and planning skills. Experiments demonstrate Ultron achieves superior performance in GUI perception and grounding benchmarks, showing significant progress with Chinese mobile applications, and achieves a 47.4% task completion rate on offline Chinese App evaluations. The implementation of Ultron facilitates more effective and practical deployment of GUI agents in real-world applications. |
| Reinforcement Learning | Think in Games: Learning to Reason in Games via Reinforcement Learning
  with Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.21365) or [HuggingFace](https://huggingface.co/papers/2508.21365))| Yifan Lu, Zining Zhu, Yuan Sui, Yu Gu, Yi Liao | This paper introduces Think-In Games (TiG), a novel framework to enable Large Language Models (LLMs) to develop procedural knowledge through direct interaction with game environments. The research aims to bridge the gap between LLMs' declarative knowledge and the procedural knowledge acquired through reinforcement learning (RL). TiG reformulates RL-based decision-making as a language modeling task, where LLMs generate language-guided policies iteratively refined through online RL based on environmental feedback. Experiments demonstrate that TiG achieves competitive performance with significantly reduced data and computational demands, achieving 90.91% accuracy with Qwen-3-14B on an action prediction task. This approach enables more transparent and interpretable AI agents capable of both effective action and reasoning in dynamic settings. |
| Natural Language Processing | TiKMiX: Take Data Influence into Dynamic Mixture for Language Model
  Pre-training (Read more on [arXiv](https://arxiv.org/abs/2508.17677) or [HuggingFace](https://huggingface.co/papers/2508.17677))| Jiyao Deng, Yuanfan Guo, Fengze Liu, Binbin Liu, Yifan Wang | The paper introduces TiKMiX, a novel method for dynamically adjusting the data mixture in language model pre-training based on data influence. It aims to address the suboptimal nature of static data mixing strategies by observing and adapting to the model’s evolving data preferences. The methodology involves a Group Influence metric to evaluate the impact of different data domains and optimization via direct optimization (TiKMiX-D) or a regression model (TiKMiX-M) for mixture prediction. Experiments on up to 1 trillion tokens show TiKMiX-D exceeds REGMIX performance using only 20% of resources, and TiKMiX-M achieves a 2% average gain across downstream benchmarks. TiKMiX provides AI practitioners with a computationally efficient and dynamically adaptive data mixing strategy that promotes better model performance by mitigating data under-digestion. |
| Natural Language Processing | Efficient Code Embeddings from Code Generation Models (Read more on [arXiv](https://arxiv.org/abs/2508.21290) or [HuggingFace](https://huggingface.co/papers/2508.21290))| Han Xiao, Scott Martens, Michael Günther, Saba Sturua, dariakryvosheieva | The paper introduces jina-code-embeddings, a suite of code embedding models for retrieving code from natural language queries and identifying semantically similar code snippets. The research aims to improve code embedding by leveraging autoregressive backbones pre-trained on both text and code and fine-tuning them with task-specific instruction prefixes. The models use last-token pooling to generate embeddings and are trained using a contrastive objective with the InfoNCE loss function. Results show significant performance improvements over comparable models, achieving competitive benchmark performance, such as a 98.41% accuracy on the HumanEval benchmark. The models offer AI practitioners an efficient solution for code retrieval and understanding in RAG architectures. |
| Natural Language Processing | A Survey of Scientific Large Language Models: From Data Foundations to
  Agent Frontiers (Read more on [arXiv](https://arxiv.org/abs/2508.21148) or [HuggingFace](https://huggingface.co/papers/2508.21148))| Jiamin Wu, Wanghan Xu, Wei Li, Chenglong Ma, Ming Hu | This paper presents a data-centric survey of scientific large language models (Sci-LLMs) and their co-evolution with scientific data. It reframes Sci-LLM development as models co-evolving with data, focusing on challenges like multimodality and domain specificity. The authors systematically review recent Sci-LLMs and analyze over 270 pre-/post-training datasets, highlighting heterogeneous, multi-scale corpora and distinct demands.  On 190 benchmark datasets, it traces a shift towards process- and discovery-oriented evaluations and semi-automated annotation solutions. This work offers a roadmap for building trustworthy and evolving AI systems for accelerating scientific discovery. |
| Multi-Modal | Morae: Proactively Pausing UI Agents for User Choices (Read more on [arXiv](https://arxiv.org/abs/2508.21456) or [HuggingFace](https://huggingface.co/papers/2508.21456))| Amy Pavel, Jeffrey P. Bigham, Dingzeyu Li, Yi-Hao Peng | This paper introduces Morae, a UI agent that proactively pauses automation to allow blind and low-vision users to make informed choices. The research aims to improve UI accessibility by enabling user agency during task automation. Morae leverages multimodal models to interpret user queries, UI code, and screenshots, prompting users for clarification and presenting relevant options. A user study showed Morae achieved significantly higher usefulness ratings (6.50) compared to baseline agents and enabled more preference-aligned choices. The key implication is a mixed-initiative approach enhancing UI accessibility by balancing automation with user control in UI agents. |
| Multi-Modal | AHELM: A Holistic Evaluation of Audio-Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.21376) or [HuggingFace](https://huggingface.co/papers/2508.21376))| Siwei Yang, Zijun Wang, Chi Heem Wong, Haoqin Tu, Tony Lee | The paper introduces AHELM, a benchmark for evaluating audio-language models (ALMs) across various capabilities and limitations. It aims to address the lack of standardized benchmarks by aggregating datasets and standardizing evaluation procedures. AHELM holistically measures ALM performance across ten aspects, including audio perception, reasoning, fairness, and safety, using 14 datasets including two novel ones. The evaluations of 14 ALMs and 3 baseline systems reveal that Gemini 2.5 Pro ranks top in several aspects but exhibits group unfairness on ASR tasks, and a baseline system performs reasonably well. AHELM facilitates a more equitable comparison of ALMs and identifies specific areas for improvement to smart assistants. |
| Reinforcement Learning | HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data
  for Mobile Dexterous Manipulation (Read more on [arXiv](https://arxiv.org/abs/2508.20085) or [HuggingFace](https://huggingface.co/papers/2508.20085))| Tianhai Liang, Pu Hua, Langzhe Gu, Tianming Wei, Zhecheng Yuan | The paper introduces HERMES, a framework for transferring human motion into robot manipulation skills, particularly for mobile bimanual dexterous manipulation. It addresses the challenge of translating diverse human hand motions into feasible robot behaviors adaptable to various environments. HERMES uses a unified reinforcement learning approach to transform heterogeneous human motion data from multiple sources into physically plausible robotic behaviors and an end-to-end depth image-based sim2real transfer method for improved generalization. Experiments demonstrate that HERMES exhibits generalizable behaviors across diverse scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks with a +54.5% average performance gain compared to raw depth input baseline. The framework provides AI practitioners with a method to leverage human motion data for training robots in complex manipulation tasks, especially in unstructured real-world environments. |
| Computer Vision | CLIPSym: Delving into Symmetry Detection with CLIP (Read more on [arXiv](https://arxiv.org/abs/2508.14197) or [HuggingFace](https://huggingface.co/papers/2508.14197))| Raymond A. Yeh, Md Ashiqur Rahman, Tinghan Yang | CLIPSym is a novel framework for symmetry detection that leverages pre-trained CLIP models. The paper investigates whether CLIP's image and language encoders can aid symmetry detection by utilizing additional symmetry cues from natural image descriptions. CLIPSym introduces a rotation-equivariant decoder with a hybrid Transformer and G-Convolution, along with a Semantic-Aware Prompt Grouping (SAPG) technique to enhance CLIP's language encoder. Empirically, CLIPSym achieves state-of-the-art performance on standard symmetry detection datasets, outperforming previous methods on DENDI with a 2.0% improvement in F1 score. This demonstrates the potential of vision-language models to improve geometric understanding tasks and provides AI practitioners with a new approach for symmetry detection. |
