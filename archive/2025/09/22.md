

## Papers for 2025-09-22

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Machine Learning | RPG: A Repository Planning Graph for Unified and Scalable Codebase
  Generation (Read more on [arXiv](https://arxiv.org/abs/2509.16198) or [HuggingFace](https://huggingface.co/papers/2509.16198))| Steven Liu, Xin Zhang, Kyleraha, Cipherxzc, Luo2003 | This paper introduces the Repository Planning Graph (RPG) for scalable codebase generation. The research aims to address the challenge of generating complete repositories from scratch by providing a coherent plan across proposal- and implementation-level stages. The key methodology involves encoding capabilities, file structures, data flows, and functions in a single graph to replace ambiguous natural language with an explicit blueprint.  ZeroRepo, built on RPG, achieves 81.5% functional coverage and a 69.7% pass rate on the RepoCraft benchmark, surpassing Claude Code. This suggests that structured representations like RPGs can enhance long-horizon planning and improve LLM understanding for repository generation, thus accelerating agent localization. |
| Multi-Modal | MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid
  Vision Tokenizer (Read more on [arXiv](https://arxiv.org/abs/2509.16197) or [HuggingFace](https://huggingface.co/papers/2509.16197))| jialingt, haosoul122, haotiz, bpan, FrozzZen | Manzano is a simple and scalable unified multimodal model for understanding and generating visual content. It addresses the performance trade-off between understanding and generation by coupling a hybrid image tokenizer with a unified training recipe. The model employs a shared vision encoder with lightweight adapters for image-to-text and text-to-image tasks, along with an autoregressive LLM and a diffusion decoder. Manzano achieves state-of-the-art results among unified models and is competitive with specialist models, particularly on text-rich evaluation. The hybrid tokenizer design validates minimal task conflicts and consistent gains from model scaling. |
| Machine Learning | Latent Zoning Network: A Unified Principle for Generative Modeling,
  Representation Learning, and Classification (Read more on [arXiv](https://arxiv.org/abs/2509.15591) or [HuggingFace](https://huggingface.co/papers/2509.15591))| Wenyu Wang, Junyi Zhu, Xuefei Ning, Enshu Liu, fjxmlzn | The paper introduces the Latent Zoning Network (LZN), a unified framework for generative modeling, representation learning, and classification. It explores whether a single principle can address these core ML tasks, aiming to simplify pipelines and foster synergy. LZN creates a shared Gaussian latent space partitioned into disjoint zones by encoders and decoders for various data types. Experiments show LZN improves FID scores on CIFAR10 from 2.76 to 2.59 when integrated with Rectified Flow and achieves competitive results in unsupervised representation learning and joint generation/classification. The framework offers AI practitioners a potentially simplified and unified approach to handling diverse machine learning tasks. |
| Multi-Modal | BaseReward: A Strong Baseline for Multimodal Reward Model (Read more on [arXiv](https://arxiv.org/abs/2509.16127) or [HuggingFace](https://huggingface.co/papers/2509.16127))| jianfeipan, xuwang, KaiWu123, achernarcursa, yifanzhang114 | The paper introduces BaseReward, a strong baseline for multimodal reward modeling to align Multimodal Large Language Models (MLLMs) with human preferences. It addresses the lack of a systematic guide for constructing high-performance MRMs by investigating various components of the MRM development pipeline. BaseReward, built upon a Qwen2.5-VL backbone, establishes a new state-of-the-art on benchmarks such as MM-RLHF-Reward Bench, improving accuracy by approximately 11%. Furthermore, it enhances an MLLM's performance across perception, reasoning, and conversational tasks when integrated into a real-world reinforcement learning pipeline. The work provides an empirically-backed guide for developing robust reward models for the next generation of MLLMs. |
| Computer Vision | SPATIALGEN: Layout-guided 3D Indoor Scene Generation (Read more on [arXiv](https://arxiv.org/abs/2509.14981) or [HuggingFace](https://huggingface.co/papers/2509.14981))| Yongsen Mao, Yixun Liang, Heng Li, Chuan Fang, bertjiazheng | SPATIALGEN is a layout-guided 3D indoor scene generation framework. It addresses the challenge of creating high-fidelity 3D indoor environments by introducing a multi-view multi-modal diffusion model conditioned on a 3D semantic layout. The method synthesizes appearance, geometry, and semantic information from arbitrary viewpoints, preserving spatial consistency across modalities. The framework is trained on a new comprehensive synthetic dataset featuring 12,328 annotated scenes and demonstrates improved performance over existing methods, achieving a CLIP similarity score of 26.84% on their dataset. SPATIALGEN provides AI practitioners with a tool to generate realistic and controllable 3D indoor scenes, advancing applications in design, virtual reality, and robotics. |
| Computer Vision | BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent (Read more on [arXiv](https://arxiv.org/abs/2509.15566) or [HuggingFace](https://huggingface.co/papers/2509.15566))| Jiahui Yang, Shaokang Wang, Pei Fu, Ruoceng Zhang, Shaojie Zhang | The paper introduces BTL-UI, a brain-inspired framework for human-GUI interaction, aimed at mimicking human cognitive processes. It addresses the gap between current AI agents' interaction logic and natural human-GUI communication patterns by decomposing interactions into Blink, Think, and Link phases. The methodology includes a novel blink data generation pipeline and a process-outcome integrated reward mechanism for reinforcement learning. BTL-UI achieves state-of-the-art performance on GUI understanding and interaction tasks, achieving 89.1% GUI grounding accuracy on ScreenSpot-V2. The implication is a more human-like and efficient approach to GUI agent development, potentially enabling advanced digital assistants. |
| Computer Vision | Lynx: Towards High-Fidelity Personalized Video Generation (Read more on [arXiv](https://arxiv.org/abs/2509.15496) or [HuggingFace](https://huggingface.co/papers/2509.15496))| Linjie Luo, Jing Liu, gutianpei, tzhi-bytedance, shensang | Lynx is a high-fidelity model for personalized video synthesis from a single image. The research aims to achieve robust identity preservation in generated videos while maintaining temporal coherence and visual realism. The model utilizes two lightweight adapters, ID-adapter and Ref-adapter, within a Diffusion Transformer architecture to inject identity and reference features. Evaluated on a benchmark of 40 subjects and 20 prompts, Lynx achieves a face resemblance score of 0.779 and an overall video quality score of 0.956, outperforming existing methods. This advancement offers AI practitioners an effective way to create personalized videos with greater identity fidelity and overall quality. |
| Reinforcement Learning | A Vision-Language-Action-Critic Model for Robotic Real-World
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.15937) or [HuggingFace](https://huggingface.co/papers/2509.15937))| Jiangmiao, simonlin123, andyzsz123, haoranzhang, fuxian | The paper introduces VLAC, a Vision-Language-Action-Critic model for real-world robotic reinforcement learning to address sparse rewards and inefficient exploration. The research aims to improve real-world RL by developing a general process reward model trained on large-scale heterogeneous datasets, unifying the critic and policy within a single autoregressive architecture. VLAC is trained on vision-language and robot trajectory datasets and is enhanced to reject irrelevant prompts and detect regression/stagnation. The model achieves success rates from 30% to 90% within 200 real-world interaction episodes, and, with human-in-the-loop interventions, final success rates up to 100% are achieved. This allows for improved sample efficiency and performance in complex robotic manipulation tasks. |
| Computer Vision | RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes (Read more on [arXiv](https://arxiv.org/abs/2509.15123) or [HuggingFace](https://huggingface.co/papers/2509.15123))| Narendra Ahuja, Hao Zhang, fangli3 | The paper introduces ROS-Cam, a novel RGB-only supervised method for optimizing camera parameters in dynamic scenes. It aims to accurately and efficiently estimate camera parameters without relying on ground truth motion masks or other unavailable priors. The method employs patch-wise tracking filters, outlier-aware joint optimization using a Cauchy loss, and a two-stage optimization strategy. Experiments on five datasets demonstrate superior performance, achieving, for example, an ATE of 0.065 on MPI-Sintel which is better than casualSAM. ROS-Cam offers AI practitioners a more efficient and accurate approach to camera parameter optimization in dynamic scenes using only RGB video, improving reconstruction quality and reducing reliance on extensive ground truth data. |
| Natural Language Processing | Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in
  Instruction-Guided Expressive Text-To-Speech Systems (Read more on [arXiv](https://arxiv.org/abs/2509.13989) or [HuggingFace](https://huggingface.co/papers/2509.13989))| Hung-yi Lee, Kuan-Yu Chen, Tzu-Chieh Wei, Huang-Cheng Chou, Yi-Cheng Lin | This paper investigates the alignment between user instructions and listener perception in instruction-guided text-to-speech (ITTS) systems. It aims to quantify the instruction-perception gap, particularly regarding adverbs of degree, graded emotion intensity, speaker age, and word-level emphasis. The authors collect a large-scale human evaluation dataset, E-VOC, and analyze the controllability of five ITTS models based on perceptual ratings and objective acoustic measures. The results show that gpt-40-mini-tts demonstrates the best alignment between instructions and generated utterances, while fine-grained control and speaker age remain challenging across systems. The study highlights the need for improved ITTS models that can consistently translate subtle instruction differences into perceptible acoustic variations. |
| Multi-Modal | Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided
  Role-playing Agents (Read more on [arXiv](https://arxiv.org/abs/2509.15233) or [HuggingFace](https://huggingface.co/papers/2509.15233))| Chao Zhang, Xueqiao Zhang, RoyalVane, YifanZhu, raul678 | The paper introduces Video2Roleplay, a multimodal dataset and framework for video-guided role-playing agents (RPAs) to incorporate dynamic perceptual abilities. It addresses the lack of dynamic role profiles in existing RPAs by integrating video modality. The framework combines adaptive temporal sampling with dynamic and static role profile representations. Evaluation results demonstrate the effectiveness of the framework, achieving improved performance on RPAs, including a human-likeness metric. The study highlights the importance of dynamic role profiles in developing RPAs for more immersive and interactive experiences. |
| Natural Language Processing | WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained
  Speech Recognition Transformers (Read more on [arXiv](https://arxiv.org/abs/2509.10452) or [HuggingFace](https://huggingface.co/papers/2509.10452))| Karun Kumar, Akshat Pandey, tetrisd | The paper introduces WhisTLE, a novel text-only domain adaptation method for pretrained encoder-decoder ASR transformers. It addresses the challenge of adapting ASR models to new domains where speech data is limited by training a VAE to model encoder outputs from text and fine-tuning the decoder. The method utilizes deep supervision to guide the adaptation of the ASR model's latent state, complementing TTS adaptation. Experiments across four out-of-domain datasets demonstrate that WhisTLE with TTS reduces word error rate (WER) by 12.3% relative to TTS-only adaptation. This implies AI practitioners can effectively adapt pretrained ASR models to new domains using only text data with latent space supervision. |
| Multi-Modal | Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn
  Dialogue (Read more on [arXiv](https://arxiv.org/abs/2509.15061) or [HuggingFace](https://huggingface.co/papers/2509.15061))| Hui Zhang, Sicheng Xie, Tianyi Lu, Xinghao Zhu, leolin9248 | This paper introduces the Ask-to-Clarify framework for embodied agents to resolve instruction ambiguity through multi-turn dialogue and generate low-level actions. The research objective is to enable agents to actively interact with humans for clarification before execution, moving beyond passive instruction following. The methodology involves a Vision-Language Model (VLM) for collaboration, a diffusion model for action generation, and a connection module to generate conditions for the diffusion model, trained with a two-stage knowledge-insulation strategy. Evaluated on 8 real-world tasks, the framework achieves a significantly higher success rate than existing state-of-the-art VLAs (e.g., 95.0% average success rate on "Put the Object on the plate" tasks, versus existing methods that perform much lower). This approach provides AI practitioners with a new method for building truly collaborative embodied agents capable of addressing real-world ambiguities. |
