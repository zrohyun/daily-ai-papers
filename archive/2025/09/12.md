

## Papers for 2025-09-12

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | HuMo: Human-Centric Video Generation via Collaborative Multi-Modal
  Conditioning (Read more on [arXiv](https://arxiv.org/abs/2509.08519) or [HuggingFace](https://huggingface.co/papers/2509.08519))| Zhuowei Chen, Bingchuan Li, Jiawei Liu, Tianxiang Ma, Liyang Chen | HuMo introduces a novel framework for human-centric video generation via collaborative multimodal conditioning. The research addresses the challenge of coordinating heterogeneous modalities by constructing a high-quality dataset and employing a two-stage progressive multimodal training paradigm. HuMo adopts a minimal-invasive image injection strategy for subject preservation and a focus-by-predicting strategy for audio-visual sync. Experimental results demonstrate that HuMo surpasses state-of-the-art methods in subject preservation with an AES score of 0.731 and audio-visual sync sub-tasks with a Sync-C score of 6.252, establishing a unified framework for collaborative multimodal-conditioned HCVG. The proposed framework offers AI practitioners a new method to achieve high-quality, controllable human video synthesis. |
| Reinforcement Learning | SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.09674) or [HuggingFace](https://huggingface.co/papers/2509.09674))| Zhaohui Yang, Yuhao Zhang, Jiale Yu, Yuxin Zuo, Haozhan Li | SimpleVLA-RL is an efficient reinforcement learning framework for Vision-Language-Action (VLA) models. The research investigates whether reinforcement learning can improve the step-by-step action planning of VLAs. The framework employs VLA-specific trajectory sampling, scalable parallelization, and optimized loss computation, implemented on veRL. Experiments demonstrate that SimpleVLA-RL achieves SoTA performance on LIBERO, outperforming SFT and even exceeds πο on RoboTwin 1.0&2.0; with one-trajectory SFT it boosts LIBERO-Long success rates from 17.1% to 91.7% implying robust generalization and reduced data dependency. |
| Natural Language Processing | EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for
  Speech-to-Speech LLMs (Read more on [arXiv](https://arxiv.org/abs/2509.09174) or [HuggingFace](https://huggingface.co/papers/2509.09174))| Kaiqi Kou, Xiangnan Ma, Zhanchen Dai, Yuhao Du, Yuhao Zhang | The paper introduces EchoX, a novel approach to mitigate the acoustic-semantic gap in speech-to-speech large language models (SLLMs). The primary objective is to bridge the gap in feature representation space that causes degradation in knowledge and reasoning capabilities in SLLMs. EchoX leverages semantic representations to dynamically generate speech training targets, integrating both acoustic and semantic learning. Experiments demonstrate that EchoX achieves advanced performance on knowledge-based question-answering benchmarks, achieving comparable performance with around six thousand hours of training data. This approach offers a more efficient way to learn unified speech and semantic representations for building high-performing speech-based language models. |
| Computer Vision | Kling-Avatar: Grounding Multimodal Instructions for Cascaded
  Long-Duration Avatar Animation Synthesis (Read more on [arXiv](https://arxiv.org/abs/2509.09595) or [HuggingFace](https://huggingface.co/papers/2509.09595))| Wentao Hu, Zekun Wang, Wenyuan Zhang, Jiwen Liu, Yikang Ding | Kling-Avatar is a cascaded framework for long-duration avatar animation synthesis conditioned on multimodal instructions. The paper addresses the challenge of creating realistic and controllable avatars by grounding instructions in a unified global plan using a multimodal large language model (MLLM) director, followed by parallel sub-clip generation guided by blueprint keyframes. Their method achieves superior performance in lip synchronization accuracy and instruction controllability over existing methods, as shown by improved GSB metrics. This facilitates the generation of vivid and fluent videos with strong generalization to open scenarios. Kling-Avatar provides a new benchmark and approach for semantically grounded, high-fidelity avatar synthesis, which could benefit applications such as digital human livestreaming. |
| Reinforcement Learning | Harnessing Uncertainty: Entropy-Modulated Policy Gradients for
  Long-Horizon LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2509.09265) or [HuggingFace](https://huggingface.co/papers/2509.09265))| Xintao Wang, Yingru Li, Yuqian Fu, Jiacai Liu, Jiawei Wang | This paper addresses the challenge of sparse rewards in long-horizon LLM agent tasks. The research aims to improve credit assignment by leveraging the agent's intrinsic uncertainty. The proposed Entropy-Modulated Policy Gradients (EMPG) framework re-calibrates learning signals based on step-wise uncertainty and introduces a bonus for future clarity. Experiments on WebShop, ALFWorld, and Deep Search demonstrate substantial performance gains, achieving a +3.3 point improvement on the Deep Search benchmark. EMPG offers AI practitioners a method for more efficient and stable training of LLM agents in complex tasks by adapting the learning signal to the model's confidence. |
| Multi-Modal | VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action
  Model (Read more on [arXiv](https://arxiv.org/abs/2509.09372) or [HuggingFace](https://huggingface.co/papers/2509.09372))| Zirui Ge, Can Cui, Lingxiao Li, Pengxiang Ding, Yihao Wang | This paper introduces VLA-Adapter, a novel paradigm for Vision-Language-Action models that bridges vision-language representations to actions without extensive pre-training. The research aims to reduce reliance on large-scale VLMs and pre-training by effectively bridging perception and action spaces. The proposed method employs a lightweight Policy module with Bridge Attention to inject optimal conditions into the action space, achieving high performance with only a 0.5B-parameter backbone. Experiments demonstrate state-of-the-art performance, achieving 97.3% on the LIBERO benchmark, and fast inference speeds. VLA-Adapter enables training a powerful VLA model on a single GPU in 8 hours, significantly lowering the barrier to VLA model deployment. |
| Multi-Modal | FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning
  Dataset and Comprehensive Benchmark (Read more on [arXiv](https://arxiv.org/abs/2509.09680) or [HuggingFace](https://huggingface.co/papers/2509.09680))| Shuai Bai, Linjiang Huang, Chengqi Duan, Aldrich Yu, Rongyao Fang | The paper introduces FLUX-Reason-6M and PRISM-Bench to address the lack of large-scale reasoning datasets and comprehensive benchmarks in text-to-image (T2I) generation. It aims to bridge the performance gap between open-source and closed-source T2I models by providing resources for complex reasoning. The methodology involves creating a 6-million-image dataset with 20 million bilingual descriptions, organized around six key characteristics and Generation Chain-of-Thought (GCoT), along with a novel evaluation benchmark with seven distinct tracks. Evaluation of 19 leading models on PRISM-Bench reveals performance gaps and highlights areas needing improvement, for example a GPT-Image-1 achieves the highest total score of 86.3. The released dataset and benchmark catalyze future research in reasoning-oriented T2I generation, offering valuable tools for training and evaluation. |
| Multi-Modal | Can Understanding and Generation Truly Benefit Together -- or Just
  Coexist? (Read more on [arXiv](https://arxiv.org/abs/2509.09666) or [HuggingFace](https://huggingface.co/papers/2509.09666))| Hui Han, Junyan Ye, Zongjian Li, Kaiqing Lin, Zhiyuan Yan | This paper introduces a novel framework, UAE, for unified multimodal learning, addressing the schism between understanding and generation. The research aims to intrinsically link understanding (I2T) and generation (T2I) through a reconstruction objective. The methodology involves pre-training a decoder with large-scale image captions and a Unified-GRPO reinforcement learning approach covering cold-start, generation for understanding, and understanding for generation. Empirical results on Unified-Bench show UAE achieves a unified score of 86.09, indicating improved mutual benefits. The research provides AI practitioners with a verifiable objective and optimization strategy towards genuine multimodal unification and intelligence. |
| Computer Vision | SpatialVID: A Large-Scale Video Dataset with Spatial Annotations (Read more on [arXiv](https://arxiv.org/abs/2509.09676) or [HuggingFace](https://huggingface.co/papers/2509.09676))| Jian Gao, Youtian Lin, Rujie Zheng, Yufeng Yuan, Jiahao Wang | The paper introduces SpatialVID, a large-scale video dataset with rich spatial and semantic annotations for advancing 3D vision research. It addresses the scarcity of high-quality training data for spatial intelligence by providing per-frame camera poses, depth maps, structured captions, and serialized motion instructions. The methodology involves collecting and processing over 21,000 hours of raw video into 2.7 million clips, followed by a detailed annotation pipeline. The dataset comprises 7,089 hours of dynamic content and a high-quality subset, SpatialVID-HQ, consisting of 1,146 hours. SpatialVID provides a valuable resource for improving model generalization and performance in video and 3D vision tasks, facilitating more accurate and controllable video simulations. |
| Computer Vision | Visual Programmability: A Guide for Code-as-Thought in Chart
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2509.09286) or [HuggingFace](https://huggingface.co/papers/2509.09286))| Ethan Chern, Jiadi Su, Fei Zhang, Yan Ma, Bohao Tang | This paper introduces Visual Programmability (VP), a learnable property indicating whether a chart-question pair is better solved through programmatic reasoning or direct visual analysis. The research aims to enable Vision-Language Models (VLMs) to autonomously choose their reasoning pathway for chart understanding. The proposed adaptive framework trains a VLM with reinforcement learning, employing a dual-reward system that combines data accuracy and decision rewards. Experiments on diverse benchmarks demonstrate strong performance, with the adaptive model achieving 62.8% average accuracy, outperforming fixed strategies by intelligently switching reasoning modes. The work implies that VLMs can be taught not only to reason but also how to reason, dynamically selecting the optimal reasoning pathway for each task. |
| Computer Vision | Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust
  Text-based Person Retrieval (Read more on [arXiv](https://arxiv.org/abs/2509.09118) or [HuggingFace](https://huggingface.co/papers/2509.09118))| Kaicheng Yang, Ziyong Feng, Xiang An, Yifan Zhang, Tianlu Zheng | This paper introduces a framework for robust text-based person retrieval using CLIP. It addresses the challenges of limited data and noisy text tokens in person-centric image-text data. The proposed GA-DMS framework adaptively masks noisy text tokens and predicts informative ones, improving cross-modal alignment. The WebPerson dataset is introduced, comprising 5M high-quality image-text pairs to provide more training data. Experiments show GA-DMS achieves state-of-the-art Rank-1 accuracy of 77.6% on CUHK-PEDES, demonstrating its effectiveness in fine-grained person retrieval. |
| Computer Vision | 2D Gaussian Splatting with Semantic Alignment for Image Inpainting (Read more on [arXiv](https://arxiv.org/abs/2509.01964) or [HuggingFace](https://huggingface.co/papers/2509.01964))| Guangming Lu, Xiaoming Li, Chaofeng Chen, learn12138 | The paper introduces a novel image inpainting framework using 2D Gaussian Splatting with semantic alignment. It addresses the challenge of coherent image completion by encoding incomplete images into a continuous Gaussian feature space and reconstructing them through a differentiable rasterization process. The methodology includes patch-level rasterization for efficiency and DINO feature adaptation for semantic guidance. Experiments on CelebA-HQ show competitive performance with an FID score of 6.38, suggesting a viable approach for high-quality image restoration. The framework offers a lightweight and semantically aware alternative for image inpainting, potentially influencing future research on continuous representations in image processing. |
| Natural Language Processing | LoCoBench: A Benchmark for Long-Context Large Language Models in Complex
  Software Engineering (Read more on [arXiv](https://arxiv.org/abs/2509.09614) or [HuggingFace](https://huggingface.co/papers/2509.09614))| Jianguo Zhang, Rithesh Murthy, Zhiwei Liu, Zuxin Liu, Jielin Qiu | LoCoBench is introduced as a comprehensive benchmark for evaluating long-context LLMs in complex software engineering tasks. The benchmark addresses the evaluation gap for models requiring whole-codebase understanding and architectural consistency. LoCoBench provides 8,000 evaluation scenarios across 10 programming languages and 36 domain categories, scaling systematically from 10K to 1M tokens. Evaluation of state-of-the-art models using 17 metrics revealed significant performance degradation, with a performance drop from 29% to 3% for Claude 3.5 Sonnet when increasing context length in LongCodeBench. The results highlight the existing challenges for long-context understanding and architectural coherence in complex software development. |
| Multi-Modal | OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and
  Embodiment-aware Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.09332) or [HuggingFace](https://huggingface.co/papers/2509.09332))| Yuzheng Zhuang, Zhanguang Zhang, Shiguang Wu, Dafeng Chi, Yuecheng Liu | The paper introduces OmniEVA, an embodied versatile planner addressing limitations in MLLM-based embodied systems related to geometric adaptability and embodiment awareness. It aims to improve embodied reasoning and task planning by adaptively grounding 3D information and incorporating embodiment constraints. OmniEVA uses a task-adaptive 3D grounding mechanism and an embodiment-aware reasoning framework, achieving state-of-the-art performance on 7 out of 8 embodied reasoning benchmarks. The primary implication is the potential for improved versatility and executability in embodied AI systems, enabling robust task execution in real-world scenarios. The framework effectively merges 2D and 3D inputs via task-conditioned feature selection, resulting in versatile and executable plans. |
| Reinforcement Learning | The Choice of Divergence: A Neglected Key to Mitigating Diversity
  Collapse in Reinforcement Learning with Verifiable Reward (Read more on [arXiv](https://arxiv.org/abs/2509.07430) or [HuggingFace](https://huggingface.co/papers/2509.07430))| Xiaoyu Tan, Zhijian Zhou, Jason Klein Liu, Jiaran Hao, Long Li | The paper addresses the diversity collapse in Reinforcement Learning with Verifiable Reward (RLVR) for fine-tuning large language models. It investigates how the choice of divergence term in RLVR objectives affects knowledge retention and solution diversity. The proposed Diversity-Preserving Hybrid RL (DPH-RL) framework employs mass-covering f-divergences as a rehearsal mechanism to maintain broad solution coverage. Experiments on math and SQL generation demonstrate that DPH-RL improves both Pass@1 and Pass@k, with DPH-JS achieving a Pass@8 score 4.3% higher than GRPO on the Bird dataset. This work highlights the importance of divergence measure selection as a tool for building more general and diverse reasoning models. |
| Multi-Modal | Modality Alignment with Multi-scale Bilateral Attention for Multimodal
  Recommendation (Read more on [arXiv](https://arxiv.org/abs/2509.09114) or [HuggingFace](https://huggingface.co/papers/2509.09114))| Dong-Ho Lee, Chan-Yang Ju, renkelin | The paper introduces MambaRec, a novel framework for multimodal recommendation, aiming to improve fusion quality and reduce representational bias. It addresses this by integrating a Dilated Refinement Attention Module (DREAM) for local feature alignment and Maximum Mean Discrepancy (MMD) with contrastive loss for global distribution regularization. Experiments on e-commerce datasets demonstrate that MambaRec outperforms existing methods, achieving improvements in fusion quality, generalization, and efficiency. For instance, MambaRec showed improvements on the Baby dataset, as measured by a Recall@10 of 0.0660 and NDCG@10 of 0.0363. The proposed framework can improve recommendation systems by enabling more effective fusion of multimodal features, enhancing personalization and content understanding. |
| Machine Learning | Reasoning Introduces New Poisoning Attacks Yet Makes Them More
  Complicated (Read more on [arXiv](https://arxiv.org/abs/2509.05739) or [HuggingFace](https://huggingface.co/papers/2509.05739))| Jamie Hayes, Harsh Chaudhari, Yiren Zhao, Ilia Shumailov, Hanna Foerster | The paper introduces a novel "decomposed reasoning poison" attack against Large Language Models (LLMs) with step-by-step reasoning. It investigates whether attackers can manipulate the reasoning path (Chain-of-Thought, CoT) to influence final answers by poisoning the intermediate reasoning steps with triggers split across multiple components. The attack modifies only the reasoning path, leaving prompts and final answers clean. The research shows that reliably activating these decomposed poisons to change final answers is surprisingly difficult, suggesting an emergent form of backdoor robustness originating from the models' reasoning capabilities and the separation between reasoning and answer generation; attack success ranges from 3.25% – 19.25% for poisoned answers. The findings imply that practitioners should consider the resilience to CoT-based poisoning when evaluating LLM security. |
