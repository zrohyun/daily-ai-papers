

## Papers for 2025-09-16

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling (Read more on [arXiv](https://arxiv.org/abs/2509.12201) or [HuggingFace](https://huggingface.co/papers/2509.12201))| Yang Zhou, MingyuLiu, Xxxy13, lizizun, ZhouTimeMachine | The paper introduces OmniWorld, a large-scale multi-domain and multi-modal dataset for advancing 4D world modeling. It aims to address the lack of diversity and high-quality annotations in existing datasets for tasks like 4D reconstruction and video generation. The methodology involves creating a new synthetic dataset (OmniWorld-Game) and curating existing public datasets, incorporating diverse modalities like depth maps, camera poses, and text captions. Experiments demonstrate that fine-tuning existing state-of-the-art models on OmniWorld leads to significant performance gains; for example, CUT3R's performance notably improved in camera pose estimation tasks. This dataset offers AI practitioners a valuable resource for training and evaluating general-purpose 4D world models with enhanced spatial-temporal consistency. |
| Reinforcement Learning | UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.11543) or [HuggingFace](https://huggingface.co/papers/2509.11543))| Yongliang Shen, Fei Tang, xhyandwyy, Mizukiluke, LZXzju | The paper introduces Semi-online Reinforcement Learning (RL) for GUI automation, aiming to bridge the gap between stable offline training and effective online interaction. It addresses the limitations of offline RL by simulating online dynamics on pre-collected trajectories, incorporating a Patch Module to recover from action mismatches and dual-level advantages for optimization. The UI-S1-7B model achieves state-of-the-art performance among 7B models, improving success rates by +12.0% on AndroidWorld and +23.8% on AITW, demonstrating enhanced multi-turn reasoning capabilities. This work provides AI practitioners with a more efficient and scalable framework for GUI agents, reducing the need for extensive environment interaction during training. |
| Computer Vision | InternScenes: A Large-scale Simulatable Indoor Scene Dataset with
  Realistic Layouts (Read more on [arXiv](https://arxiv.org/abs/2509.10813) or [HuggingFace](https://huggingface.co/papers/2509.10813))| Wenzhe Cai, Li Luo, Yichen Jin, Peizhou Cao, Weipeng Zhong | InternScenes is a large-scale, simulatable indoor scene dataset designed to address limitations in existing datasets regarding scene diversity and realistic layouts. The paper introduces a dataset comprising approximately 40,000 diverse scenes integrated from real-world scans, procedurally generated scenes, and designer-created scenes. The key methodology involves creating real-to-sim replicas, enhancing interactivity with interactive objects, and resolving object collisions using physical simulations. The dataset achieves an average of 41.5 objects per region, demonstrating complex layouts, and benchmark results show challenges in scene layout generation and point-goal navigation. InternScenes enables scaling up model training and facilitates generation and navigation tasks in complex, realistic environments, providing AI practitioners with a valuable resource for embodied AI research. |
| Computer Vision | LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion
  Transformers via Explicit Correspondence (Read more on [arXiv](https://arxiv.org/abs/2509.12203) or [HuggingFace](https://huggingface.co/papers/2509.12203))| Lionel M. Ni, Xianfang Zeng, Xili Dai, Zixin Yin, dorni | The paper introduces LazyDrag, a novel drag-based image editing method for Multi-Modal Diffusion Transformers (MM-DiTs) that leverages explicit correspondence to enhance stability and fidelity. It addresses the limitations of implicit attention-based point matching by generating an explicit correspondence map from user drag inputs, driving attention control. The primary objective is to enable stable full-strength inversion without test-time optimization (TTO), thus unlocking the generative capabilities of diffusion models. LazyDrag outperforms existing methods on Drag-Bench, achieving a mean distance (MD) of 21.49, improving drag accuracy and perceptual quality. The approach enables more precise geometric control and text guidance in image editing tasks for AI practitioners. |
| Computer Vision | Locality in Image Diffusion Models Emerges from Data Statistics (Read more on [arXiv](https://arxiv.org/abs/2509.09672) or [HuggingFace](https://huggingface.co/papers/2509.09672))| Vincent Sitzmann, Justin Solomon, Chenyang Yuan, Artem Lukoianov | This paper investigates the emergence of locality in image diffusion models, challenging the view that it solely arises from convolutional neural network inductive biases. It demonstrates that locality is a statistical property inherent in image datasets, observable even in optimal linear denoisers. The research shows that sensitivities closely approximate projections onto principal components with high signal-to-noise ratios (SNR). The authors craft an analytical denoiser leveraging these insights, achieving better performance than expert-crafted alternatives based on r² coefficient of determination and mean-squared error metrics between the predictions of the analytical model and a trained DDPM. These findings suggest that practitioners should consider data statistics to better model and interpret diffusion models. |
| Multi-Modal | Measuring Epistemic Humility in Multimodal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.09658) or [HuggingFace](https://huggingface.co/papers/2509.09658))| Kaiyang Zhou, Sifeng Shang, Bingkui Tong, JiaerX | This paper introduces HumbleBench, a new benchmark to evaluate epistemic humility in multimodal large language models (MLLMs). The research investigates the ability of MLLMs to reject plausible but incorrect answers, reflecting awareness of their knowledge limitations. HumbleBench contains 22,831 multiple-choice questions with a "None of the above" option, focusing on object, relation, and attribute hallucination types. Experiments on state-of-the-art MLLMs show the best models achieve around 70% accuracy, indicating a significant challenge in false-option rejection. The main implication is that current MLLMs struggle with uncertainty, highlighting the need for better training strategies to improve their reliability in safety-critical applications. |
| Multi-Modal | Lost in Embeddings: Information Loss in Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.11986) or [HuggingFace](https://huggingface.co/papers/2509.11986))| Ivan Vulić, Caiqi Zhang, Chengzu Li, Raphael Tang, lyan62 | This paper investigates information loss in Vision-Language Models (VLMs) due to the connector module which projects visual features into the language model's embedding space. The study quantifies this loss by analyzing changes in k-nearest neighbor relationships (KNOR) and performing patch-level embedding reconstruction. Experiments reveal that connectors distort the local geometry of visual representations, with k-nearest neighbors diverging by 40-60% post-projection, correlating with degradation in retrieval performance. Patch-level reconstruction highlights areas of high information loss related to poor performance on visually-grounded question answering. The findings suggest that connector design should prioritize preserving semantic image representation and task-relevant visual information. |
| Natural Language Processing | CognitiveSky: Scalable Sentiment and Narrative Analysis for
  Decentralized Social Media (Read more on [arXiv](https://arxiv.org/abs/2509.11444) or [HuggingFace](https://huggingface.co/papers/2509.11444))| Subasish Das, Anandi Dutta, gauravfs-14 | CognitiveSky is an open-source, scalable framework for real-time sentiment, emotion, and narrative analysis on decentralized social media platforms like Bluesky. The research aims to develop a modular pipeline that ingests, labels, and summarizes Bluesky discourse for mental health monitoring. The system utilizes transformer-based models (ROBERTa and DistilRoBERTa) for sentiment and emotion detection and MiniBatch NMF for topic modeling on TF-IDF vectors. CognitiveSky analyzed 58,567 social media posts and identified Fear as the top emotion present in 31.3% of all detected emotions, providing a dynamic dashboard for visualization. The framework offers AI practitioners a transparent and extensible tool for computational social science in evolving digital ecosystems, adaptable to various domains beyond mental health. |
| Multi-Modal | Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2509.12132) or [HuggingFace](https://huggingface.co/papers/2509.12132))| Shuo Ren, Chen Wang, Wei Sun, Junhong Wu, Pu Jian | This paper introduces Reflection-V, a vision-language model (VLM) designed to enhance visual reflection capabilities. The research aims to address the limited visual reflection observed in current VLMs, where attention to visual information diminishes with longer generated responses. Reflection-V employs a two-stage training approach: vision-centered reasoning data construction using a multi-modal agent and reinforcement learning with a visual attention-based reward model. The model demonstrates significant improvements, achieving up to 71.1% accuracy on general reasoning benchmarks. The enhanced visual reflection enables more robust and consistent reliance on visual information during reasoning, offering AI practitioners a method for improving VLM grounding and reducing textual hallucinations. |
| Natural Language Processing | EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI (Read more on [arXiv](https://arxiv.org/abs/2509.11648) or [HuggingFace](https://huggingface.co/papers/2509.11648))| UVSKKR | The paper introduces EthicsMH, a pilot dataset for evaluating ethical reasoning in mental health AI systems. It aims to address the gap in existing benchmarks that do not adequately capture the nuanced ethical dilemmas in mental health. The dataset comprises 125 ethically challenging scenarios structured with multiple decision options, expert-aligned reasoning, and multi-stakeholder viewpoints. By releasing EthicsMH, the authors aim to provide a seed resource for developing AI systems capable of responsibly handling some of society's most delicate decisions in mental health contexts, facilitating research on fairness, ethical alignment, and sensitivity to patient-centered concerns. |
| Reinforcement Learning | Learning to Optimize Multi-Objective Alignment Through Dynamic Reward
  Weighting (Read more on [arXiv](https://arxiv.org/abs/2509.11452) or [HuggingFace](https://huggingface.co/papers/2509.11452))| Changlong Yu, Xin Liu, Shiyang Li, Zilong Wang, ylu610 | The paper introduces dynamic reward weighting for multi-objective alignment in reinforcement learning, addressing the limitations of fixed-weight scalarization. It aims to improve the exploration of Pareto fronts by adaptively adjusting reward weights during training. The methodology involves hypervolume-guided weight adaptation and gradient-based weight optimization, which are integrated into existing RL algorithms. Experiments on the Math500 dataset using the Qwen3-8B model with GRPO demonstrate improved Pareto fronts, showing a reduction in required training steps and superior performance across accuracy, conciseness, and clarity; specifically achieving 0.850 accuracy compared to a 0.832 baseline. This approach offers a more efficient and versatile toolkit for aligning LLMs with multiple objectives, enabling practitioners to achieve better trade-offs. |
| Multi-Modal | PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits (Read more on [arXiv](https://arxiv.org/abs/2509.11362) or [HuggingFace](https://huggingface.co/papers/2509.11362))| Zhenhao Chen, Guangyi Chen, Minghao Fu, Wong Yu Kang, Loka Li | The paper introduces PersonaX, a multimodal dataset for analyzing human behavior traits inferred by LLMs. It aims to enable comprehensive analysis of public traits by combining LLM-inferred behavioral assessments with facial imagery and biographical features. The methodology involves abstracting trait scores and applying statistical independence tests, alongside a novel causal representation learning framework tailored to multimodal data. Experiments on synthetic and real-world data demonstrate the approach's effectiveness, but specific quantitative results are not given. PersonaX provides a foundation for studying LLM-inferred traits in conjunction with visual and biographical attributes, advancing multimodal trait analysis and causal reasoning for AI practitioners. |
| Natural Language Processing | GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings (Read more on [arXiv](https://arxiv.org/abs/2509.10844) or [HuggingFace](https://huggingface.co/papers/2509.10844))| Yixuan Tang, yixuantt | The paper introduces GAPrune, a novel pruning framework for domain-aware embedding models. It addresses the challenge of balancing general linguistic understanding with domain-specific knowledge during pruning. The method uses Fisher Information and gradient alignment to identify and preserve important parameters, resulting in a Domain Alignment Importance (DAI) score. Experiments on FinMTEB and ChemTEB show that GAPrune maintains performance within 2.5% of dense models at 50% sparsity in one-shot pruning and achieves +4.51% improvement on FinMTEB after retraining. GAPrune enables practitioners to develop efficient, specialized embedding models with enhanced domain-specific capabilities. |
