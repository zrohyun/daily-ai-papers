

## Papers for 2025-09-09

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Reverse-Engineered Reasoning for Open-Ended Generation (Read more on [arXiv](https://arxiv.org/abs/2509.06160) or [HuggingFace](https://huggingface.co/papers/2509.06160))| Wangchunshu Zhou, Minghao Liu, Qixin Xu, Haoran Que, Haozhe Wang | The paper introduces Reverse-Engineered Reasoning (REER), a novel paradigm for instilling deep reasoning in LLMs for open-ended generation by working backwards from known good outputs. It addresses the challenge of applying deep reasoning to non-verifiable tasks like creative writing where reinforcement learning and instruction distillation struggle. REER computationally discovers latent, step-by-step reasoning processes using a gradient-free local search algorithm, creating a dataset of 20,000 deep reasoning trajectories. The resulting DeepWriter-8B model achieves performance competitive with proprietary models like GPT-4 and Claude 3.5 and a score of 91.28 on LongBench-Write, suggesting a promising scalable alternative to RL and distillation. |
| Natural Language Processing | WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents (Read more on [arXiv](https://arxiv.org/abs/2509.06501) or [HuggingFace](https://huggingface.co/papers/2509.06501))| Aili Chen, Jingyang Li, Chi Zhang, Yunji Li, Junteng Liu | The paper introduces WebExplorer, a novel approach for training long-horizon web agents by addressing the scarcity of challenging information-seeking data. It proposes a systematic data generation method using model-based exploration and iterative, long-to-short query evolution to create challenging query-answer pairs.  The research focuses on developing a web agent capable of multi-step reasoning and complex web navigation.  WebExplorer-8B, fine-tuned with the generated data, achieves state-of-the-art performance at its scale, reaching 15.7% accuracy on BrowseComp-en, outperforming WebSailor-72B.  The work suggests a practical path towards developing more capable long-horizon web agents via curated high-quality datasets and advanced training techniques. |
| Reinforcement Learning | Revolutionizing Reinforcement Learning Framework for Diffusion Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.06949) or [HuggingFace](https://huggingface.co/papers/2509.06949))| Ke Shen, Ye Tian, Bowen Li, Ling Yang, Yinjie Wang | The paper introduces TraceRL, a novel trajectory-aware reinforcement learning framework tailored for diffusion language models (DLMs). It aims to improve reasoning performance in DLMs by incorporating preferred inference trajectories and a diffusion-based value model. The methodology involves aligning the training objective with the sampling trajectory, enabling faster optimization across different DLM architectures. TraDo-8B-Instruct, derived using TraceRL, achieves a 6.1% relative accuracy improvement over Qwen2.5-7B-Instruct on math reasoning benchmarks. TraceRL facilitates building, training, and deploying diffusion LLMs, enhancing both reasoning and coding capabilities for AI practitioners. |
| Computer Vision | Does DINOv3 Set a New Medical Vision Standard? (Read more on [arXiv](https://arxiv.org/abs/2509.06467) or [HuggingFace](https://huggingface.co/papers/2509.06467))| Bailiang Jian, Jinpeng Lu, Haoyuan Shi, Yinda Chen, Che Liu | This paper benchmarks the DINOv3 vision transformer as a unified encoder for medical vision tasks. It investigates whether DINOv3, pre-trained on natural images, can directly serve as a powerful encoder for medical imaging without domain-specific pre-training. The study systematically evaluates DINOv3's performance on 2D/3D classification and segmentation across various medical imaging modalities, analyzing its scalability by varying model sizes and input resolutions. Results show DINOv3 achieves impressive performance, outperforming medical-specific models like BiomedCLIP on some tasks, though performance degrades in scenarios requiring deep domain specialization. This establishes DINOv3 as a strong baseline, suggesting the potential to leverage its features as a robust prior for complex medical tasks, but highlights limitations in tasks requiring domain adaptation and consistent scaling behavior. |
| Computer Vision | Reinforced Visual Perception with Tools (Read more on [arXiv](https://arxiv.org/abs/2509.01656) or [HuggingFace](https://huggingface.co/papers/2509.01656))| Mingyang Fu, Zhihan Hu, Zixian Ma, Dongping Chen, Zetong Zhou | The paper introduces REVPT, a reinforcement learning framework to enhance multi-modal LLMs' visual reasoning abilities through strategic tool usage. REVPT uses a GRPO-based RL algorithm to train models to reason with visual tools. Experiments demonstrate state-of-the-art performance on perception-heavy benchmarks, with REVPT-3B and 7B outperforming instruct models by 9.03% and 9.44% on CV-Bench, respectively. REVPT provides insights on visual tool usage through extensive ablations and contributes open-source code and datasets, offering AI practitioners a valuable resource for visual tool usage. |
| Computer Vision | Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.06461) or [HuggingFace](https://huggingface.co/papers/2509.06461))| Baolong Bi, Lingrui Mei, Yiwei Wang, Shenghua Liu, Yuyao Ge | This paper introduces Contrastive Attention Refinement for Visual Enhancement (CARVE), a training-free method to improve VLMs' visual reasoning by addressing the negative impact of visual complexity on attention mechanisms. The research investigates the correlation between visual complexity, attention entropy, and reasoning performance. CARVE extracts task-relevant visual signals through attention contrasting at the pixel level, effectively filtering visual noise. Experiments show CARVE enhances performance, achieving up to 75% improvement on open-source models. This offers AI practitioners an efficient way to improve visual reasoning by contrasting attention without additional training. |
| Reinforcement Learning | Reinforcement Learning Foundations for Deep Research Systems: A Survey (Read more on [arXiv](https://arxiv.org/abs/2509.06733) or [HuggingFace](https://huggingface.co/papers/2509.06733))| Wei Han, Hannan Cao, Jingru Lin, Zhi Chen, Wenjun Li | This survey paper provides a comprehensive overview of reinforcement learning (RL) foundations for training deep research systems, specifically focusing on agentic AI models. It addresses the limitations of supervised fine-tuning (SFT) and preference-based methods (DPO) and advocates for RL to optimize trajectory-level policies. The survey systematizes research along three axes: data synthesis, RL methods (stability, sample efficiency, long context), and agentic RL training systems. It covers agent architecture and coordination, as well as evaluation benchmarks in QA, VQA, and tool-interaction tasks. The paper distills recurring patterns and infrastructure bottlenecks, offering practical guidance for training robust, transparent deep research agents with RL, but provides no direct quantitative metrics. |
| Computer Vision | UniVerse-1: Unified Audio-Video Generation via Stitching of Experts (Read more on [arXiv](https://arxiv.org/abs/2509.06155) or [HuggingFace](https://huggingface.co/papers/2509.06155))| Xinyao Liao, Ling-Hao Chen, Aojie Li, Wei Zuo, Duomin Wang | UniVerse-1 is a unified, open-source model for simultaneous audio-video generation. The research aims to address the lack of publicly available models capable of synchronous audio-video generation by presenting a Veo-3-like model. The methodology employs a stitching of experts (SoE) technique, fusing pre-trained video and music generation models, along with an online annotation pipeline to ensure data alignment and independent noise sampling. After finetuning on 7,600 hours of audio-video data, UniVerse-1 achieves high coherence demonstrated by an ID consistency score of 0.89 on Verse-Bench. The model and code are released to facilitate further research in multimodal generation, closing the gap with closed-source systems. |
| Computer Vision | Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,
  but Not Direct the Play? (Read more on [arXiv](https://arxiv.org/abs/2509.03516) or [HuggingFace](https://huggingface.co/papers/2509.03516))| Rui Chen, Huijuan Huang, Xinting Hu, Yuan Wang, lioooox | This paper introduces T2I-COREBENCH, a comprehensive benchmark for evaluating text-to-image (T2I) models on composition and reasoning. The main objective is to address limitations in existing benchmarks regarding comprehensive evaluations across and within both composition and reasoning capabilities. The methodology involves structuring composition around scene graph elements (instance, attribute, relation) and reasoning around deductive, inductive, and abductive inference, formulating a 12-dimensional taxonomy. Experiments across 27 T2I models reveal composition capability remains limited in complex scenarios, while reasoning capability lags further behind, with all models struggling to infer implicit elements; for instance, Qwen-Image achieved 78.0 in composition but only 49.3 in reasoning. The benchmark highlights the need for enhanced reasoning capabilities in T2I models to faithfully generate images from complex prompts. |
| Computer Vision | Interleaving Reasoning for Better Text-to-Image Generation (Read more on [arXiv](https://arxiv.org/abs/2509.06945) or [HuggingFace](https://huggingface.co/papers/2509.06945))| Shixiang Tang, Shaosheng Cao, Zheyong Xie, Shuang Chen, Wenxuan Huang | This paper introduces Interleaving Reasoning Generation (IRG) for improved text-to-image generation. The research aims to enhance image generation fidelity by incorporating multi-turn text-based reasoning during the generation process. IRG alternates between text-based thinking and image synthesis, refining details through iterative reflection, which is facilitated by Interleaving Reasoning Generation Learning (IRGL). Experiments show state-of-the-art performance with 5-10 point gains on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN benchmarks. The implication is that interleaving reasoning provides a powerful paradigm for advancing text-to-image synthesis by improving visual quality and fine-grained fidelity. |
| Machine Learning | Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI
  Agents (Read more on [arXiv](https://arxiv.org/abs/2509.06917) or [HuggingFace](https://huggingface.co/papers/2509.06917))| James Zou, Jonathan K. Pritchard, Joe R. Davis, Jiacheng Miao | Paper2Agent introduces a framework that transforms research papers into interactive AI agents to enhance knowledge dissemination and reuse. The research aims to overcome the barriers of understanding and adapting research papers' code and methods by automating the conversion process. The key methodology involves constructing a Model Context Protocol (MCP) server using multiple agents to analyze papers and codebases, followed by iterative testing and refinement. Evaluations demonstrate the creation of reliable paper agents, reproducing original results and correctly executing user queries with 100% accuracy in AlphaGenome's case; this offers AI practitioners a new paradigm for collaborative research and democratized access to complex methods. |
| Reinforcement Learning | Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM
  Step-Provers (Read more on [arXiv](https://arxiv.org/abs/2509.06493) or [HuggingFace](https://huggingface.co/papers/2509.06493))| Xia Xiao, Kun Yuan, Yanchen Nie, Zeyu Zheng, Ran Xin | This paper introduces BFS-Prover-V2, a system that scales both training and inference for LLM step-provers. The work addresses the dual scaling problem by utilizing a multi-turn off-policy RL framework and a planner-enhanced multi-agent search architecture. The system leverages adaptive tactic-level data filtering and periodic retraining to improve LLM step-provers during training. BFS-Prover-V2 achieves 95.08% and 41.4% on the MiniF2F and ProofNet test sets, respectively, showcasing state-of-the-art results on formal mathematics benchmarks. The methods presented are applicable to other domains requiring long-horizon reasoning and complex search, potentially aiding the development of more capable reasoning agents. |
| Machine Learning | R^textbf{2AI}: Towards Resistant and Resilient AI in an
  Evolving World (Read more on [arXiv](https://arxiv.org/abs/2509.06786) or [HuggingFace](https://huggingface.co/papers/2509.06786))| Bowen Zhou, Chaochao Lu, Jie Fu, Xiang Wang, Youbang Sun | This paper introduces R²AI, a framework for developing resistant and resilient AI systems capable of adapting to evolving threats. The main objective is to bridge the gap between rapidly advancing AI capabilities and lagging safety progress by proposing a "safe-by-coevolution" approach. R²AI integrates fast and slow safe models, adversarial simulation via a safety wind tunnel, and continual feedback loops to guide the coevolution of safety and capability. The authors argue that this framework offers a scalable and proactive approach to maintaining continual safety in dynamic environments. The key implication is a shift from static AI safety constraints to dynamically evolving capabilities through continual learning. |
| Natural Language Processing | Llama-GENBA-10B: A Trilingual Large Language Model for German, English
  and Bavarian (Read more on [arXiv](https://arxiv.org/abs/2509.05668) or [HuggingFace](https://huggingface.co/papers/2509.05668))| Hoi-Fong Mak, Gokul Ramakrishnan, Stefan Schweter, Jophin John, Michael Hoffmann | Llama-GENBA-10B is a trilingual large language model developed to address the English-centric bias in current LLMs by balancing resources for German, English, and Bavarian. The paper investigates the challenges of creating a trilingual model including corpus curation, tokenizer development, and cross-lingual transfer optimization. The model, based on Llama 3.1-8B and scaled to 10B parameters, was continuously pretrained on 164B tokens and evaluated using a newly created trilingual benchmark suite. Results demonstrate strong cross-lingual performance, with a fine-tuned variant achieving state-of-the-art performance in Bavarian. The work demonstrates an efficient methodology for developing inclusive foundation models for low-resource languages. |
| Natural Language Processing | Test-Time Scaling in Reasoning Models Is Not Effective for
  Knowledge-Intensive Tasks Yet (Read more on [arXiv](https://arxiv.org/abs/2509.06861) or [HuggingFace](https://huggingface.co/papers/2509.06861))| See-Kiong Ng, Bryan Hooi, James Xu Zhao | The paper investigates the effectiveness of test-time scaling in reasoning models for knowledge-intensive tasks, finding it does not consistently improve accuracy and can increase hallucinations. The authors evaluate 12 reasoning models on two knowledge-intensive benchmarks by increasing test-time computation. Results show that increased computation does not reliably improve accuracy and often increases hallucination rates; for example, few models consistently improve accuracy while some even increased hallucination. The study implies that increasing test-time computation is not a reliable strategy for enhancing factual accuracy in current language models, suggesting new methods are needed. Analysis suggests hallucination changes are linked to the model's willingness to abstain or attempt previously unanswered questions. |
| Machine Learning | MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI
  Agents (Read more on [arXiv](https://arxiv.org/abs/2509.06477) or [HuggingFace](https://huggingface.co/papers/2509.06477))| Zhengxi Lu, Weiqing He, Yaozhen Liang, Guangyi Liu, Pengxiang Zhao | The paper introduces MAS-Bench, a benchmark for evaluating GUI-shortcut hybrid mobile agents. The research focuses on evaluating agents' ability to autonomously generate and utilize shortcuts to improve task efficiency in mobile environments. The methodology involves a benchmark comprising 139 tasks across 11 real-world applications, a knowledge base of 88 predefined shortcuts, and 7 evaluation metrics. Experiments demonstrate that hybrid agents achieve significantly higher success rates (up to 64.1%) compared to GUI-only agents. MAS-Bench provides a platform for developing more efficient and robust intelligent agents by addressing the evaluation gap in GUI-shortcut hybrid mobile agents. |
| Natural Language Processing | Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in
  the TPTP Ecosystem (Read more on [arXiv](https://arxiv.org/abs/2509.06809) or [HuggingFace](https://huggingface.co/papers/2509.06809))| Damien Sileo, Valentin Quesnel | The paper introduces a method for generating high-quality datasets for training Large Language Models (LLMs) in mathematical reasoning. It addresses the scarcity of logically sound mathematical data by leveraging E-prover's saturation capabilities on the TPTP axiom library. The methodology involves saturating axioms, filtering interesting theorems, and generating tasks like entailment verification, premise selection, and proof reconstruction. Experiments on frontier models reveal performance collapses on tasks requiring deep structural reasoning. The framework provides a diagnostic tool and a data source to improve LLM mathematical reasoning capabilities. |
| Multi-Modal | D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.06771) or [HuggingFace](https://huggingface.co/papers/2509.06771))| Dhanvin Sanjay Namboodiri, Rishi Bharat Junghare, Shahid Shafi Dar, Mohammad Zia Ur Rehman, Sai Kartheek Reddy Kasu | This paper introduces D-HUMOR, a novel dataset and framework for understanding dark humor in multimodal memes. The research aims to improve dark humor detection, target identification, and intensity prediction by leveraging reasoning. The proposed approach, TCRNet, combines visual, textual, and reasoning features using a tri-stream cross-reasoning network and a role-reversal self-loop for explanation refinement. The TCRNet achieves 75.00% accuracy on dark humor detection. The work implies that reasoning-aware multimodal fusion is crucial for nuanced understanding and categorization of dark humor memes. |
