

## Papers for 2025-09-24

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR (Read more on [arXiv](https://arxiv.org/abs/2509.18174) or [HuggingFace](https://huggingface.co/papers/2509.18174))| Zeina Aldallal, Ahmad Bastati, Mohamed Motasim Hamed, Muhammad Hreden, Khalil Hennara | The paper introduces Baseer, a vision-language model fine-tuned for Arabic document OCR, to address limitations of existing models in handling the complexities of the Arabic script. The research aims to improve the accuracy and efficiency of Arabic OCR by leveraging a large-scale dataset and a decoder-only fine-tuning strategy, while retaining general visual features of the Qwen2.5-VL-3B model. Baseer is trained on a hybrid dataset of synthetic and real-world Arabic documents, and evaluated using a newly created Misraj-DocOCR benchmark. Results demonstrate that Baseer achieves a state-of-the-art performance with a WER of 0.25, outperforming existing open-source and commercial solutions. The implication is that domain-specific adaptation of general-purpose MLLMs significantly enhances OCR accuracy for morphologically rich languages like Arabic. |
| Reinforcement Learning | Reinforcement Learning on Pre-Training Data (Read more on [arXiv](https://arxiv.org/abs/2509.19249) or [HuggingFace](https://huggingface.co/papers/2509.19249))| Evander Yang, Guanhua Huang, Zenan Xu, Kejiao Li, Siheng Li | The paper introduces Reinforcement Learning on Pre-Training Data (RLPT), a training-time scaling paradigm for optimizing Large Language Models (LLMs) by leveraging pre-training data. It addresses the challenge of scaling LLMs beyond data scarcity by formulating a next-segment reasoning objective, deriving reward signals directly from pre-training data without human annotation. The key methodology involves training the policy to accurately predict subsequent text segments conditioned on the preceding context. Experiments on Qwen3-4B-Base show absolute improvements on benchmarks like MMLU (3.0) and GPQA-Diamond (8.1). This approach enhances LLM reasoning capabilities and provides a foundation for scaling RL without relying on human-annotated rewards. |
| Reinforcement Learning | Do You Need Proprioceptive States in Visuomotor Policies? (Read more on [arXiv](https://arxiv.org/abs/2509.18644) or [HuggingFace](https://huggingface.co/papers/2509.18644))| Yushen Liang, Yufeng Liu, Di Zhang, Wenbo Lu, Juntu Zhao | This paper introduces State-free Policies for visuomotor control, aiming to enhance spatial generalization by removing proprioceptive state inputs. The research investigates whether proprioceptive states are necessary in visuomotor policies, hypothesizing that their inclusion leads to overfitting. The proposed State-free Policy relies solely on visual observations from dual wide-angle wrist cameras and relative end-effector action space. Empirical results on real-world tasks show an average success rate improvement from 0% to 85% in height generalization and from 6% to 64% in horizontal generalization compared to state-based policies. This suggests that removing proprioceptive states, when coupled with sufficient visual information, can significantly improve spatial generalization and data efficiency for visuomotor policies. |
| Multi-Modal | MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and
  Training Recipe (Read more on [arXiv](https://arxiv.org/abs/2509.18154) or [HuggingFace](https://huggingface.co/papers/2509.18154))| Wenshuo Ma, Fuwei Huang, Chongyi Wang, Zefan Wang, Tianyu Yu | The paper introduces MiniCPM-V 4.5, an 8B parameter multimodal large language model (MLLM) designed for efficiency and strong performance. It aims to address training and inference bottlenecks in MLLMs through architectural improvements, data strategy, and training methods. The key methodology involves a unified 3D-Resampler for compact image and video encoding, a unified learning paradigm for document knowledge and OCR, and a hybrid reinforcement learning strategy for short and long reasoning. MiniCPM-V 4.5 achieves state-of-the-art performance on VideoMME with only 9.9% of the inference time of prior MLLMs. This research provides AI practitioners with a highly efficient MLLM architecture and training recipe, enabling accessible and scalable multimodal AI systems. |
| Reinforcement Learning | MAPO: Mixed Advantage Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2509.18849) or [HuggingFace](https://huggingface.co/papers/2509.18849))| Xuankun Rong, Jian Liang, Yiyang Fang, Quan Zhang, Wenke Huang | The paper introduces Mixed Advantage Policy Optimization (MAPO), a novel GRPO strategy for improved foundation model post-training. MAPO aims to address advantage reversion and advantage mirror problems in existing GRPO methods by dynamically reweighting the advantage function based on trajectory certainty. The key methodology involves using advantage percent deviation for high-certainty trajectories and trajectory certainty reweighting to adaptively configure the advantage function. Experimental results on diverse reasoning tasks demonstrate that MAPO produces more stable and accurate reasoning performance. MAPO offers AI practitioners a method for training foundation models that mitigates advantage function shortcomings, resulting in improved reasoning capabilities. |
| Computer Vision | VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with
  Voxel-Aligned Prediction (Read more on [arXiv](https://arxiv.org/abs/2509.19297) or [HuggingFace](https://huggingface.co/papers/2509.19297))| Haoxiao Wang, Hengyu Liu, Zeyu Zhang, Yeqing Chen, Weijie Wang | The paper introduces VolSplat, a novel feed-forward 3D Gaussian Splatting (3DGS) framework using voxel-aligned prediction for improved novel view synthesis. It addresses limitations in pixel-aligned 3DGS by predicting Gaussians from a 3D voxel grid, enhancing multi-view consistency and adaptive density control. The methodology involves constructing 3D feature grids, refining them with a sparse 3D U-Net, and predicting voxel-aligned Gaussians, replacing error-prone 2D feature matching. Experiments on RealEstate10K and ScanNet demonstrate state-of-the-art performance, achieving a PSNR of 31.30 on RealEstate10K. VolSplat provides a more scalable and robust framework for feed-forward 3D reconstruction with better geometric consistency and enhanced rendering quality. |
| Multi-Modal | Hyper-Bagel: A Unified Acceleration Framework for Multimodal
  Understanding and Generation (Read more on [arXiv](https://arxiv.org/abs/2509.18824) or [HuggingFace](https://huggingface.co/papers/2509.18824))| Jianbin Zheng, Huafeng Kuang, Manlin Zhang, Xin Xia, Yanzuo Lu | The paper introduces Hyper-Bagel, a unified acceleration framework to improve the efficiency of multimodal understanding and generation tasks. It primarily aims to reduce the computational overhead of iterative diffusion denoising and autoregressive decoding in models like BAGEL. The approach employs speculative decoding for next-token prediction and multi-stage distillation for diffusion denoising. Hyper-Bagel achieves over a 2x speedup in multimodal understanding and a 16.67x speedup in text-to-image generation with a 6-NFE model, preserving original model output quality. This framework offers AI practitioners a means to deploy complex multimodal models with significantly reduced computational costs and faster inference times. |
| Computer Vision | Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model
  Self-Distillation (Read more on [arXiv](https://arxiv.org/abs/2509.19296) or [HuggingFace](https://huggingface.co/papers/2509.19296))| Yifeng Jiang, Jiahui Huang, Jiawei Ren, Tianchang Shen, Sherwin Bahmani | The paper introduces Lyra, a novel feed-forward framework for generating 3D scenes from single images or videos by distilling knowledge from video diffusion models. Lyra aims to generate explicit 3D Gaussian Splatting representations without multi-view training data by augmenting a typical RGB decoder with a 3DGS decoder, supervised by the RGB decoder's output. The method trains the 3DGS decoder using synthetic data generated by the video diffusion model, enabling viewpoint coverage extension via multiple camera trajectories. Experiments demonstrate state-of-the-art performance, achieving a PSNR of 21.79 on the RealEstate10K dataset for single-image 3D scene generation. This allows AI practitioners to generate high-quality 3D scenes from monocular input suitable for simulation and downstream tasks without real-world multi-view data. |
| Machine Learning | What Characterizes Effective Reasoning? Revisiting Length, Review, and
  Structure of CoT (Read more on [arXiv](https://arxiv.org/abs/2509.19284) or [HuggingFace](https://huggingface.co/papers/2509.19284))| Anthony Hartshorn, Parag Jain, Cheng Zhang, Julia Kempe, Yunzhen Feng | This paper investigates the characteristics of effective chain-of-thought (CoT) reasoning in large language models. It challenges the notion that longer CoTs and increased review always lead to better performance by examining the relationships between length, review ratio, structural properties, and accuracy across ten models. The study introduces the Failed-Step Fraction (FSF), a graph-based metric, which shows stronger predictive power for correctness compared to length and review ratio. Interventions involving CoT editing and FSF-based selection demonstrate that minimizing FSF causally improves accuracy, achieving up to a 10% accuracy improvement on AIME. The findings suggest that focusing on structure-aware test-time scaling and reducing failed reasoning branches are more effective than indiscriminately generating longer CoTs for AI practitioners. |
| Natural Language Processing | Large Language Models Discriminate Against Speakers of German Dialects (Read more on [arXiv](https://arxiv.org/abs/2509.13835) or [HuggingFace](https://huggingface.co/papers/2509.13835))| Katharina von der Wense, Anne Lauscher, Valentin Hofmann, Carolin Holtermann, Minh Duc Bui | This paper investigates dialect bias in large language models (LLMs) against speakers of German dialects. The study examines whether LLMs exhibit stereotypes associated with dialect speakers using association and decision tasks. The methodology includes a novel parallel dataset of dialectical and standard German texts to assess dialect usage bias, finding that LLMs consistently associate negative attributes with dialect speakers. For example, Llama-3.1 70B significantly associates adjectives related to being uneducated with dialect speakers. The results suggest that current LLMs display discriminatory behavior towards German dialect speakers, implying a need to address dialect bias in NLP systems. |
| Computer Vision | OpenGVL - Benchmarking Visual Temporal Progress for Data Curation (Read more on [arXiv](https://arxiv.org/abs/2509.17321) or [HuggingFace](https://huggingface.co/papers/2509.17321))| Viktor Petrenko, Igor Kulakov, Gracjan Góral, Emilia Wiśnios, Paweł Budzianowski | The paper introduces OpenGVL, a benchmark for evaluating visual temporal progress prediction in robotics, addressing data scarcity in the field. It aims to provide a standardized assessment of vision-language models (VLMs) for predicting task completion from visual observations. The methodology involves evaluating open-source and closed-source VLMs on diverse manipulation tasks and demonstrating OpenGVL's utility for automated data curation.  Evaluations reveal a performance gap, with open-source models achieving approximately 70% of the accuracy of closed-source counterparts. The benchmark facilitates efficient quality assessment of large-scale robotics datasets, enabling practitioners to curate and filter data for improved training and performance. |
| Computer Vision | CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target
  for Better Flow Matching (Read more on [arXiv](https://arxiv.org/abs/2509.19300) or [HuggingFace](https://huggingface.co/papers/2509.19300))| Rui Qian, Jiasen Lu, Liangchen Song, Pengsheng Guo, Chen Chen | The paper introduces CAR-Flow, a condition-aware reparameterization technique to improve conditional generative modeling with flow matching. It aims to alleviate the burden on the flow model by conditioning the source and target distributions, shortening the probability path. CAR-Flow employs lightweight, learned shifts to relocate distributions, leading to faster training. On ImageNet-256, CAR-Flow equipping SiT-XL/2 reduces FID from 2.07 to 1.68, using a joint shift strategy. The method implies practitioners can improve flow-based generative models by explicitly conditioning the source and target distributions with minimal additional parameters. |
| Computer Vision | HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel
  View Synthesis (Read more on [arXiv](https://arxiv.org/abs/2509.17083) or [HuggingFace](https://huggingface.co/papers/2509.17083))| Dan Xu, ZipW | The paper introduces Hybrid Radiance Fields (HyRF) for memory-efficient novel view synthesis. It addresses the memory overhead of 3D Gaussian Splatting (3DGS) by decomposing a scene into explicit Gaussians for high-frequency details and grid-based neural fields for low-frequency properties. The key methodology involves a decoupled neural field architecture and a hybrid rendering scheme. Experiments demonstrate state-of-the-art rendering quality while reducing model size by over 20x compared to 3DGS. HyRF offers AI practitioners a method for achieving high-quality, real-time novel view synthesis with significantly reduced memory requirements. |
| Computer Vision | Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal
  Gemini 2.5 Model for Remote Sensing Applications (Read more on [arXiv](https://arxiv.org/abs/2509.19087) or [HuggingFace](https://huggingface.co/papers/2509.19087))| Genady Beryozkin, Maxim Neumann, Dahun Kim, Yotam Gigi, Ganesh Mallya | This paper introduces a zero-shot approach for adapting generalist multimodal models to multi-spectral remote sensing data. The research explores whether generalist models can understand new, unfamiliar spectral inputs without training by leveraging their visual understanding and descriptive text prompts. The methodology involves transforming multi-spectral data into pseudo-images with detailed textual descriptions of band combinations and their physical interpretations. Results on the BigEarthNet dataset showed a significant F1 score gain of +0.04 with multi-spectral inputs over RGB-only, reaching 0.429. The main implication is that geospatial professionals can readily leverage powerful multimodal models like Gemini 2.5 with non-standard inputs, benefiting from rich reasoning and contextual capabilities. |
| Multi-Modal | VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via
  Travel Video Itinerary Reconstruction (Read more on [arXiv](https://arxiv.org/abs/2509.19002) or [HuggingFace](https://huggingface.co/papers/2509.19002))| So Fukuda, Ayako Sato, Lingfang Zhang, Eiki Murata, Hao Wang | The paper introduces VIR-Bench, a new benchmark for evaluating geospatial and temporal understanding of multimodal large language models (MLLMs) using travel video itinerary reconstruction. The research aims to address the lack of benchmarks focused on long-distance travel scenarios and extended geospatial-temporal trajectories. The methodology involves reconstructing a visiting order graph from 200 travel videos, assessing node and edge prediction performance. Experiments show that state-of-the-art MLLMs, including proprietary models, struggle to achieve high scores, with Qwen2.5-VL-72B achieving an overall F1 score of 38.1. The benchmark's ability to translate to concrete performance gains is verified by developing a prototype travel-planning agent with improved itinerary recommendations, implying its potential in grounding MLLMs for real-world travel scenarios. |
