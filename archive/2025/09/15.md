

## Papers for 2025-09-15

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | IntrEx: A Dataset for Modeling Engagement in Educational Conversations (Read more on [arXiv](https://arxiv.org/abs/2509.06652) or [HuggingFace](https://huggingface.co/papers/2509.06652))| Gabriele Pergola, Chiara Gambi, Mahathi Parvatham, XingweiT | The paper introduces IntrEx, a novel dataset for modeling engagement in educational conversations. It investigates linguistic features that drive engagement in teacher-student interactions using the Teacher-Student Chatroom Corpus (TSCC). The methodology involves sequence-level annotations of interestingness and expected interestingness, coupled with a comparison-based rating approach. Results show that fine-tuned LLMs, specifically Llama3-8B-Instruct, achieve AC2 scores exceeding 0.51, surpassing larger proprietary models in predicting engagement. This highlights the potential of specialized datasets for enhancing engagement modeling in educational settings. |
| Natural Language Processing | The Illusion of Diminishing Returns: Measuring Long Horizon Execution in
  LLMs (Read more on [arXiv](https://arxiv.org/abs/2509.09677) or [HuggingFace](https://huggingface.co/papers/2509.09677))| Jonas Geiping, Steffen Staab, Shashwat Goel, arvindh75, viciousa3gis | This paper investigates the long-horizon execution capabilities of Large Language Models (LLMs), arguing that scaling model size and sequential test-time compute significantly improve task completion length. It questions whether diminishing returns in single-step accuracy are economically justifiable for LLMs. The methodology involves isolating execution capability by providing LLMs with explicit knowledge and plans for a simple task. Experiments show that larger models can correctly execute significantly more turns even when smaller models achieve 100% single-turn accuracy, with GPT-5 (codename “Horizon”) executing over 1000 steps. The finding implies that focusing on execution ability and scaling can greatly improve the viability of LLMs for automating complex, multi-step tasks, despite seemingly marginal gains in single-step metrics. |
| Computer Vision | X-Part: high fidelity and structure coherent shape decomposition (Read more on [arXiv](https://arxiv.org/abs/2509.08643) or [HuggingFace](https://huggingface.co/papers/2509.08643))| Yunhan Yang, Changfeng Ma, Yang Li, Jiachen Xu, HowieYan | The paper introduces X-Part, a controllable generative model for decomposing 3D shapes into semantically meaningful and structurally coherent parts. It addresses the challenge of generating part-level 3D shapes with high fidelity and controllability. X-Part uses bounding box prompts for part generation and injects point-wise semantic features for meaningful decomposition within a synchronized multi-part diffusion process. Experiments demonstrate state-of-the-art performance with an F-score of 0.80 using an intersection of 0.1. The framework facilitates the creation of production-ready, editable 3D assets and provides a new paradigm for part-level shape generation and interactive part editing. |
| Computer Vision | InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis (Read more on [arXiv](https://arxiv.org/abs/2509.10441) or [HuggingFace](https://huggingface.co/papers/2509.10441))| Song Guo, Xiaoyu Yue, Junchao Gong, Wanghan Xu, Tao Han | The paper introduces InfGen, a novel paradigm for arbitrary resolution image generation. It addresses the computational inefficiencies of current diffusion models when generating high-resolution images. InfGen employs a fixed-size latent representation generated by diffusion models and decodes it into arbitrary resolutions using a compact, one-step generator, replacing the VAE decoder. Experiments demonstrate that InfGen improves the resolution of models such as SDXL while reducing 4K image generation time to under 10 seconds. InfGen provides a plug-and-play solution for upgrading existing generative models to arbitrary high-resolution capabilities with reduced computational complexity. |
| Natural Language Processing | HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented
  Generation for Multi-hop Question Answering (Read more on [arXiv](https://arxiv.org/abs/2509.09713) or [HuggingFace](https://huggingface.co/papers/2509.09713))| Zhehao Tan, Yihan Jiao, Yue Shen, Dan Yang, Duolin Sun | The paper introduces HANRAG, a novel retrieval-augmented generation (RAG) framework designed to enhance multi-hop question answering by improving noise resistance and adaptability. It addresses challenges in existing RAG methods, such as excessive dependence on iterative retrieval, irrational querying, and noise accumulation through a heuristic-based query routing and decomposition approach. The core methodology involves a "Revelator" agent that routes queries, decomposes complex problems, and filters noise, enhancing the system's capacity to handle diverse queries. Experimental results demonstrate superior performance on both single-hop and multi-hop question-answering tasks, achieving improvements over Adaptive-RAG in accuracy. The main implication for AI practitioners is a potentially more efficient and accurate solution for multi-hop QA, particularly in noisy or complex information environments. |
| Natural Language Processing | VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions (Read more on [arXiv](https://arxiv.org/abs/2509.09716) or [HuggingFace](https://huggingface.co/papers/2509.09716))| Dong Zhang, Chen Wang, Yuxuan Xie, Mingyang Han, Jun Zhan | The paper introduces VStyle, a bilingual benchmark for voice style adaptation in spoken language models (SLMs) using spoken instructions. It aims to evaluate an SLM's ability to modify its speaking style based on natural language instructions spanning acoustic attributes, natural language, role-play, and empathy. The authors employ a Large Audio Language Model (LALM) as a judge to assess content faithfulness, style consistency, and naturalness, and benchmark against commercial and open-source SLMs. Results reveal a significant performance gap between open-source and commercial models, with GPT-40 achieving the best English scores (4.05 MOS) and Doubao leading in Chinese (4.10 MOS). VStyle provides a foundation for advancing human-centered spoken interaction by enabling more expressive and controllable speech generation. |
| Machine Learning | FLOWER: Democratizing Generalist Robot Policies with Efficient
  Vision-Language-Action Flow Policies (Read more on [arXiv](https://arxiv.org/abs/2509.04996) or [HuggingFace](https://huggingface.co/papers/2509.04996))| Fabian Otto, Ömer Erdinç Yağmurlu, Marcel Rühle, Hongyi Zhou, Moritz Reuss | The paper introduces FLOWER, a Vision-Language-Action (VLA) policy designed to reduce computational costs in robotics deployment. It addresses the challenge of resource-intensive VLA policies through intermediate-modality fusion and action-specific Global-AdaLN conditioning. FLOWER, a 950M-parameter model, achieves competitive performance with larger VLAs on 190 tasks and attains a SoTA score of 4.53 on the CALVIN ABC benchmark, all while pretrained in 200 H100 GPU hours. This work implies a more accessible and efficient pathway for developing generalist robot policies, reducing the barrier for AI practitioners. |
| Natural Language Processing | Inpainting-Guided Policy Optimization for Diffusion Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2509.10396) or [HuggingFace](https://huggingface.co/papers/2509.10396))| Chenyu Wang, Miao Liu, Jing Huang, Mengchen Liu, Siyan Zhao | This paper introduces Inpainting-Guided Policy Optimization (IGPO) for aligning masked diffusion large language models (dLLMs). The research aims to improve exploration in reinforcement learning for dLLMs by leveraging their inpainting capabilities. IGPO strategically inserts partial ground-truth reasoning traces during online sampling, guiding exploration toward correct solutions while preserving self-generated reasoning. The method achieves state-of-the-art results on mathematical benchmarks, including a 4.9% improvement on GSM8K and reduces all-wrong groups occurrences by ~60%. IGPO provides AI practitioners with a more efficient and stable method for aligning dLLMs, overcoming challenges associated with sparse rewards and exploration in reinforcement learning. |
| Other | Virtual Agent Economies (Read more on [arXiv](https://arxiv.org/abs/2509.10147) or [HuggingFace](https://huggingface.co/papers/2509.10147))| William A. Cunningham, Julian Jacobs, Joel Z. Leibo, Matija Franklin, Nenad Tomasev | This paper proposes a framework for analyzing emergent AI agent economies, termed "sandbox economies," along intentionality and separateness dimensions. The research explores design choices for steerable agent markets to ensure alignment with human flourishing. It discusses auction mechanisms for fair resource allocation, mission economies for collective goals, and socio-technical infrastructure for trust and accountability. The authors argue that without proactive design, a vast and highly permeable AI agent economy will emerge spontaneously. This necessitates mechanisms that will avoid potential systemic economic risks and exacerbated inequality, requiring careful architecture for safety and alignment. |
| Multi-Modal | QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading (Read more on [arXiv](https://arxiv.org/abs/2509.09995) or [HuggingFace](https://huggingface.co/papers/2509.09995))| Chenyu You, Siqi Sun, Aosong Feng, Xiang Zhang, Fei Xiong | The paper introduces QuantAgent, a novel multi-agent LLM framework for high-frequency trading. It addresses the need for rapid, risk-aware decisions in HFT by decomposing trading into specialized agents using structured reasoning over price data. QuantAgent outperforms neural and rule-based baselines, achieving up to 80% directional accuracy in price movement prediction. This suggests the potential of combining structured financial priors with language-native reasoning for traceable, real-time decision systems in HFT. The framework's explainability and risk-awareness also offer a departure from opaque trading systems. |
| Natural Language Processing | MCP-AgentBench: Evaluating Real-World Language Agent Performance with
  MCP-Mediated Tools (Read more on [arXiv](https://arxiv.org/abs/2509.09734) or [HuggingFace](https://huggingface.co/papers/2509.09734))| Xiaorui Wang, Wentao Hong, Chiwei Zhu, Benfeng Xu, Zikang Guo | This paper introduces MCP-AgentBench, a benchmark for evaluating language agent performance in real-world scenarios using Model Context Protocol (MCP)-mediated tools. It addresses the lack of benchmarks capturing real-world agent performance in the MCP paradigm. The methodology includes a MCP testbed comprising 33 operational servers with 188 distinct tools and 600 queries distributed across 6 categories. Evaluation using MCP-Eval shows varied performance of leading language agents, with Qwen3-235B-A22B achieving the highest overall score using the ReAct framework. The benchmark provides a standardized framework for advancing capable and interoperable AI systems leveraging the MCP standard. |
| Machine Learning | LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised
  Learning in Open-World Scenarios (Read more on [arXiv](https://arxiv.org/abs/2509.09926) or [HuggingFace](https://huggingface.co/papers/2509.09926))| Bing Su, Yurou Liu, Zhiyuan Huang, Jiahao Chen | This paper introduces LoFT, a parameter-efficient fine-tuning framework for long-tailed semi-supervised learning in open-world scenarios. It addresses the challenges of overconfidence and low-quality pseudo-labels in existing LTSSL methods by leveraging pre-trained foundation models. The key methodology involves fine-tuning models using Parameter-Efficient Fine-Tuning (PEFT) to generate more reliable pseudo-labels, along with an OOD detection mechanism for open-world scenarios. Experiments on CIFAR-LT and ImageNet127 show that LoFT achieves superior performance and competitive results even when using only 1% of unlabeled data compared with previous works. The LoFT framework enhances the reliability and generalization of LTSSL models, providing a more effective approach for handling imbalanced data and open-world conditions. |
| Natural Language Processing | CMHG: A Dataset and Benchmark for Headline Generation of Minority
  Languages in China (Read more on [arXiv](https://arxiv.org/abs/2509.09990) or [HuggingFace](https://huggingface.co/papers/2509.09990))| XU Han, Jianing Liu, Ziyin Zhang, Zeli Su, Guixian Xu | The paper introduces CMHG, a new dataset for headline generation in Chinese minority languages like Tibetan, Uyghur, and Mongolian. The research aims to address the lack of relevant corpora for supervised NLP tasks in these languages. The methodology involves web crawling, data cleaning, and native speaker annotation to create a high-quality dataset and benchmark. Experiments using fine-tuned models like cino-cum achieve competitive ROUGE-L scores, indicating the effectiveness of CMHG for training headline generation models. The CMHG dataset enables further research and development of NLP systems for underrepresented languages. |
