

## Papers for 2025-09-10

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Reinforcement Learning | Parallel-R1: Towards Parallel Thinking via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.07980) or [HuggingFace](https://huggingface.co/papers/2509.07980))| Xinyu Yang, Xiaoyang Wang, Wenhao Yu, Hongming Zhang, Tong Zheng | The paper introduces Parallel-R1, a novel reinforcement learning framework designed to instill parallel thinking capabilities in large language models for complex reasoning tasks. It addresses cold-start issues with a progressive curriculum, using supervised fine-tuning on simpler tasks before transitioning to RL on harder problems. Experiments on math benchmarks like MATH, AMC23, and AIME show Parallel-R1 achieves significant improvements, with an 8.4% accuracy increase over sequential models trained directly on challenging tasks. The model's thinking evolves from exploration to multi-perspective verification during training, suggesting parallel thinking as a mid-training exploration scaffold for improved performance, yielding a 42.9% improvement on AIME25. The approach offers AI practitioners a method to equip LLMs with parallel reasoning via scalable RL. |
| Computer Vision | Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual
  Search (Read more on [arXiv](https://arxiv.org/abs/2509.07969) or [HuggingFace](https://huggingface.co/papers/2509.07969))| Tianjian Li, Tao Liu, Wei Li, Junyi Li, Xin Lai | This paper introduces Mini-o3, a system for deep, multi-turn reasoning in visual search tasks. The research aims to improve tool-based interactions in VLMs, enabling trial-and-error exploration for complex visual search. Mini-o3 employs a Visual Probe Dataset, iterative data collection for diverse reasoning patterns, and an over-turn masking strategy during reinforcement learning. Experiments demonstrate that Mini-o3 achieves state-of-the-art performance on challenging visual search problems, with accuracy improving as the number of interaction turns increases (e.g., significantly outperforming DeepEyes on the VisualProbe-Hard dataset). The main implication is a recipe for scaling interaction depth and diversity in VLMs, expanding the solvable frontier of difficult, visually-grounded reasoning tasks. |
| Multi-Modal | Visual Representation Alignment for Multimodal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.07979) or [HuggingFace](https://huggingface.co/papers/2509.07979))| Heeseong Shin, Hyungyu Choi, Junwan Kim, Jaewoo Jung, Heeji Yoon | The paper introduces Visual Representation Alignment (VIRAL) to improve the visual reasoning capabilities of multimodal large language models (MLLMs). It addresses the gap in vision-centric tasks by aligning internal visual representations of MLLMs with those of pre-trained vision foundation models (VFMs) using a cosine similarity-based alignment loss. The key methodology involves a regularization strategy applied during training to retain and complement visual knowledge, enhancing the model's ability to reason over complex inputs. Experiments demonstrate consistent improvements across multimodal benchmarks, with, for example, CV-Bench2D increasing from 56.82% to 59.67% using CLIP and the proposed method. This approach allows for more effective integration of visual information in training MLLMs, thereby resulting in significant performance improvement across diverse tasks. |
| Multi-Modal | Reconstruction Alignment Improves Unified Multimodal Models (Read more on [arXiv](https://arxiv.org/abs/2509.07295) or [HuggingFace](https://huggingface.co/papers/2509.07295))| XuDong Wang, Luke Zettlemoyer, Trevor Darrell, Ji Xie | This paper introduces Reconstruction Alignment (RecA), a post-training method for improving unified multimodal models (UMMs). RecA aims to enhance generation and editing fidelity by leveraging visual understanding encoder embeddings as dense "text prompts" in a self-supervised reconstruction task. The method conditions the UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image, realigning understanding and generation. Experiments show that post-training with RecA improves image generation performance, achieving 0.90 on GenEval and 88.15 on DPGBench with only 27 GPU-hours. RecA provides a resource-efficient and generalizable strategy to improve the performance of existing UMM architectures without relying on extensive image-text pairs. |
| Computer Vision | UMO: Scaling Multi-Identity Consistency for Image Customization via
  Matching Reward (Read more on [arXiv](https://arxiv.org/abs/2509.06818) or [HuggingFace](https://huggingface.co/papers/2509.06818))| Fei Ding, Mengqi Huang, Wenxu Wu, fenfan, cb1cyf | The paper introduces UMO, a framework to improve multi-identity consistency in image customization. It addresses the challenge of preserving individual identities while avoiding confusion in multi-reference image customization scenarios. UMO employs a multi-to-multi matching paradigm, optimizing a global assignment problem through Reference Reward Feedback Learning on diffusion models. Experiments show UMO achieves state-of-the-art results, significantly improving identity consistency and reducing identity confusion, demonstrated by achieving the highest ID-Sim and ID-Conf scores. UMO offers AI practitioners a scalable solution for high-fidelity identity preservation in image customization tasks. |
| Multi-Modal | Curia: A Multi-Modal Foundation Model for Radiology (Read more on [arXiv](https://arxiv.org/abs/2509.06830) or [HuggingFace](https://huggingface.co/papers/2509.06830))| Elodie Ferreres, Helene Philippe, Antoine Saporta, Julien Khlaut, Corentin Dancette | The paper introduces Curia, a multi-modal foundation model for radiology trained on 200M CT and MRI images. The study aims to develop a generalized model capable of various radiological tasks, surpassing existing single-task models. Curia utilizes self-supervised learning with DINOv2 and lightweight classifiers for downstream tasks. On a 19-task benchmark, Curia achieves performance comparable to or surpassing radiologists and other foundation models, with a mean CT organ recognition accuracy of 98.40%. The results suggest a clinically viable, versatile AI tool for medical imaging analysis. |
| Multi-Modal | F1: A Vision-Language-Action Model Bridging Understanding and Generation
  to Actions (Read more on [arXiv](https://arxiv.org/abs/2509.06951) or [HuggingFace](https://huggingface.co/papers/2509.06951))| Zherui Qiu, Jia Zeng, Hao Li, Weijie Kong, aopolin-lv | The paper introduces F1, a vision-language-action (VLA) model that bridges understanding and generation to actions for embodied AI by integrating visual foresight into the decision-making pipeline. F1 addresses the limitations of reactive state-to-action mappings by employing a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. A three-stage training recipe on a dataset of over 330k trajectories is used to enhance modular reasoning and transferable visual foresight. Evaluation demonstrates that F1 consistently outperforms existing approaches, achieving substantial gains in task success rate (average success rate of 82.2% on real-world tasks) and generalization ability. This shows that integrating visual foresight is a viable direction for advancing robust visuomotor control. |
| Reinforcement Learning | Staying in the Sweet Spot: Responsive Reasoning Evolution via
  Capability-Adaptive Hint Scaffolding (Read more on [arXiv](https://arxiv.org/abs/2509.06923) or [HuggingFace](https://huggingface.co/papers/2509.06923))| Yongcheng Zeng, Erxue Min, Zexu Sun, zhaojinm, ChillingDream | This paper introduces SEELE, a novel supervision-aided RLVR framework designed to improve the reasoning capabilities of LLMs by dynamically adjusting problem difficulty. SEELE adaptively adjusts hint length during training to maintain optimal difficulty, using a multi-round rollout sampling strategy and item response theory to predict accuracy. Experiments show SEELE outperforms GRPO and SFT, achieving a +11.8 points improvement over GRPO on math reasoning. The approach enhances exploration efficiency by aligning problem difficulty with the evolving model capability. This implies a more efficient approach to training reasoning models by dynamically adapting the training data's difficulty to the model's current capabilities. |
| Reinforcement Learning | Language Self-Play For Data-Free Training (Read more on [arXiv](https://arxiv.org/abs/2509.07414) or [HuggingFace](https://huggingface.co/papers/2509.07414))| Vijai Mohan, Yuandong Tian, Qi Ma, Mengting Gu, Jakub Grudzien Kuba | This paper introduces Language Self-Play (LSP), a data-free reinforcement learning approach for enhancing language models by enabling them to improve without additional data.  The research aims to overcome the data dependency bottleneck in language model training through a self-play framework.  LSP casts a model's capabilities as performance in a competitive game against itself, generating increasingly challenging queries and corresponding responses.  Experiments with Llama-3.2-3B-Instruct show that LSP enhances performance on instruction-following benchmarks, outperforming data-driven baselines by achieving comparable results without external data. The main implication is that self-play offers a viable alternative to data-intensive methods for improving language model performance, especially in challenging tasks. |
| Natural Language Processing | Causal Attention with Lookahead Keys (Read more on [arXiv](https://arxiv.org/abs/2509.07301) or [HuggingFace](https://huggingface.co/papers/2509.07301))| Quanquan Gu, Huizhuo Yuan, Peng Sun, Zhuoqing Song | This paper introduces CAuSal aTtention with Lookahead kEys (CASTLE), a novel attention mechanism designed to enhance sequence modeling. The research aims to improve token efficiency in language models by enabling tokens to access information from subsequent tokens while preserving autoregressive properties. CASTLE updates token keys with lookahead information, addressing limitations in capturing global context with standard causal attention. Evaluations on language modeling benchmarks demonstrate that CASTLE consistently outperforms standard causal attention, reducing validation perplexity by 0.0348 on the XL model and improving performance on downstream tasks. The method offers AI practitioners a more token-efficient attention mechanism applicable to various model scales. |
| Reinforcement Learning | Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human
  Preference (Read more on [arXiv](https://arxiv.org/abs/2509.06942) or [HuggingFace](https://huggingface.co/papers/2509.06942))| Yingfang Zhang, Shiyi Zhang, Zhantao Yang, Zhimin Li, Xiangwei Shen | The paper introduces a novel online reinforcement learning framework, SRPO, for aligning diffusion models with fine-grained human preferences. It aims to address limitations in existing methods, particularly the reliance on multi-step denoising and offline reward adaptation. SRPO predefines a noise prior and uses text-conditioned signals for online reward adjustment, which optimizes denoising and mitigates reward hacking. Finetuning FLUX.1-dev model with SRPO improves human-evaluated realism and aesthetic quality by over 3x. The approach achieves convergence within 10 minutes using 32 NVIDIA H20 GPUs. |
| Natural Language Processing | SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric
  Knowledge (Read more on [arXiv](https://arxiv.org/abs/2509.07968) or [HuggingFace](https://huggingface.co/papers/2509.07968))| Dipanjan Das, Sasha Goldshtein, Giovanni D'Antonio, Gal Yona, Lukas Haas | The paper introduces SimpleQA Verified, a 1,000-prompt benchmark for evaluating the factuality of large language models. It addresses limitations in the original SimpleQA benchmark by filtering noisy labels, topical biases, and question redundancy. The methodology involves multi-stage filtering including de-duplication, topic balancing, and source reconciliation.  On this new benchmark, Gemini 2.5 Pro achieves a state-of-the-art F1-score of 55.6, outperforming other frontier models. The benchmark dataset and evaluation code are publicly available to improve the assessment of LLM factuality. |
| Computer Vision | Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with
  Quantization-Aware Scheduling (Read more on [arXiv](https://arxiv.org/abs/2509.01624) or [HuggingFace](https://huggingface.co/papers/2509.01624))| Diana Marculescu, Natalia Frumkin | This paper introduces Q-Sched, a novel quantization-aware noise scheduler for few-step diffusion models to improve image fidelity. The research aims to mitigate performance degradation in highly compressed few-step diffusion models by optimizing the sampling trajectory rather than directly modifying model weights. Q-Sched learns pre-conditioning coefficients using a joint alignment-quality loss (JAQ) function, balancing text-image alignment and image detail. Experiments demonstrate a 15.5% FID improvement over a full-precision 4-step Latent Consistency Model and comparable or better user preference. Q-Sched offers AI practitioners a method for achieving high-fidelity image generation with significantly reduced model size through optimized quantization and few-step diffusion. |
| Reinforcement Learning | Î”L Normalization: Rethink Loss Aggregation in RLVR (Read more on [arXiv](https://arxiv.org/abs/2509.07558) or [HuggingFace](https://huggingface.co/papers/2509.07558))| Lili Qiu, Yuqing Yang, Yike Zhang, Xufang Luo, Zhiyuan He | This paper introduces AL Normalization, a novel loss aggregation method for Reinforcement Learning with Verifiable Rewards (RLVR) to address the challenge of dynamic response lengths. The research aims to mitigate high gradient variance and unstable optimization in RLVR by reformulating the aggregation as a minimum-variance unbiased estimation problem. AL Normalization provides an unbiased estimate of the policy loss and minimizes gradient variance, leading to more stable and efficient training. Experiments across various model sizes and tasks demonstrate that AL Normalization consistently outperforms baselines, achieving higher accuracy and stable training dynamics (e.g., higher Avg@8 on CountDown and weighted Avg@8 on Math datasets). The implication is a simplified and improved approach to loss aggregation in RLVR, leading to better model performance and stability. |
