

## Papers for 2025-09-23

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | LIMI: Less is More for Agency (Read more on [arXiv](https://arxiv.org/abs/2509.17567) or [HuggingFace](https://huggingface.co/papers/2509.17567))| happyZYM, evanlin2570, weizhihao1, mhjiang0408, YangXiao-nlp | The paper introduces LIMI, a novel approach for developing agentic AI that challenges the traditional scaling laws by emphasizing strategic data curation over data quantity. LIMI aims to demonstrate that sophisticated agentic intelligence can emerge from minimal but strategically curated demonstrations of autonomous behavior. The methodology involves strategically focusing on collaborative software development and scientific research workflows. LIMI achieves 73.5% on AgencyBench using only 78 training samples, outperforming state-of-the-art models like GLM-4.5 (45.1%). The study suggests that mastering agency requires understanding its essence, not simply scaling training data, offering a more sustainable paradigm for cultivating truly agentic intelligence. |
| Computer Vision | OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion
  Transformer Models (Read more on [arXiv](https://arxiv.org/abs/2509.17627) or [HuggingFace](https://huggingface.co/papers/2509.17627))| Pengze Zhang, Tianxiang Ma, Xu Bai, Xinghui Li, Jinshu Chen | OmniInsert introduces a novel framework for mask-free video insertion using diffusion transformer models. The paper addresses data scarcity, subject-scene equilibrium, and insertion harmonization in video insertion tasks. It proposes InsertPipe for data generation and a Condition-Specific Feature Injection mechanism to improve subject consistency and scene integration. Evaluated on a newly introduced benchmark, InsertBench, OmniInsert outperforms existing commercial solutions, achieving higher subject consistency and video quality scores. The framework offers a more effective and unified approach to video insertion compared to existing methods that rely on complex control signals and struggle with subject consistency. |
| Multi-Modal | Qwen3-Omni Technical Report (Read more on [arXiv](https://arxiv.org/abs/2509.17765) or [HuggingFace](https://huggingface.co/papers/2509.17765))| Lhma-aslp, Cyanbox, jinzheng-he, faychu, ZhifangGuo | Qwen3-Omni is a single multi-modal model achieving state-of-the-art performance across text, image, audio, and video without modality-specific degradation. The research aims to develop a unified architecture for perception and generation across diverse modalities. This is achieved through a Thinker-Talker Mixture-of-Experts (MoE) architecture and multi-codebook speech generation. The model attains open-source SOTA on 32 out of 36 audio and audio-visual benchmarks and sets new SOTA on 22 benchmarks, outperforming Gemini-2.5-Pro and GPT-40-Transcribe.  This suggests that joint multimodal training can achieve parity across modalities and enhance cross-modal capabilities, impacting the design of future large multimodal models. |
| Machine Learning | OnePiece: Bringing Context Engineering and Reasoning to Industrial
  Cascade Ranking System (Read more on [arXiv](https://arxiv.org/abs/2509.18091) or [HuggingFace](https://huggingface.co/papers/2509.18091))| Jiahua Wu, Ethan7, vicowang, TangJiakai5704, KID-22 | The paper introduces OnePiece, a unified framework integrating context engineering and multi-step reasoning into industrial cascade ranking systems to improve retrieval and ranking performance. The research aims to enhance ranking models by incorporating LLM-style context and reasoning capabilities. OnePiece employs structured context engineering, block-wise latent reasoning, and progressive multi-task training, achieving over +2% GMV/UU and a +2.90% increase in advertising revenue in Shopee's personalized search. The primary implication is a more efficient and effective method for integrating complex reasoning into large-scale ranking systems, potentially leading to significant business gains. |
| Computer Vision | TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning
  for Video LLMs (Read more on [arXiv](https://arxiv.org/abs/2509.18056) or [HuggingFace](https://huggingface.co/papers/2509.18056))| Shaohui Jiao, Hangyi Kuang, Shaoyong Jia, Jing Cheng, lyhisme | TempSamp-R1 is a new reinforcement fine-tuning framework to improve adapting multimodal large language models (MLLMs) for video temporal grounding tasks. The paper aims to address the limitations of existing reinforcement learning methods in tasks with large temporal search spaces by leveraging ground-truth annotations as off-policy supervision and stabilizing training with a non-linear soft advantage computation. TempSamp-R1 optimizes a single unified model to support both Chain-of-Thought (CoT) and non-CoT inference modes. Experiments demonstrate TempSamp-R1 outperforms GRPO-based baselines, achieving a Charades-STA R1@0.7 of 52.9% (+2.7%). The main implication for AI practitioners is a more effective and stable approach for adapting MLLMs to video temporal grounding tasks, enabling improved performance and generalization capabilities under limited data. |
| Multi-Modal | GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.17437) or [HuggingFace](https://huggingface.co/papers/2509.17437))| Hou Pong Chan, Weiwen Xu, Swrooy, 26hzhang, Guizhen | This paper investigates the impact of visual perception limitations on geometric reasoning in multimodal large language models (MLLMs). The research aims to quantify and mitigate the perceptual bottleneck affecting the reasoning capabilities of MLLMs on geometric tasks. They propose a two-stage reinforcement learning framework that first enhances visual perception using a curated dataset of geometric question-answering pairs and then fosters reasoning capabilities. Applied to Qwen2.5-VL-3B-Instruct, the two-stage training improves geometric reasoning by 9.7% and geometric problem-solving by 9.1% compared to direct reasoning training. The findings suggest that strong perceptual foundations are critical for effective high-level reasoning in MLLMs, offering a potential direction for improvement in vision-intensive tasks. |
| Natural Language Processing | EpiCache: Episodic KV Cache Management for Long Conversational Question
  Answering (Read more on [arXiv](https://arxiv.org/abs/2509.17396) or [HuggingFace](https://huggingface.co/papers/2509.17396))| Minsik Cho, Richa Dixit, Han-Byul Kim, Arnav Kundu, minsoo2333 | The paper introduces EPICACHE, a novel KV cache management framework for long conversational question answering (LongConvQA). It addresses the challenge of unbounded memory usage in extended dialogues by proposing a training-free method that combines block-wise prefill with episodic KV compression to preserve topic-relevant context. The methodology clusters conversation history into coherent episodes and applies episode-specific KV cache eviction, further employing an adaptive layer-wise budget allocation strategy. Across three LongConvQA benchmarks, EPICACHE improves accuracy by up to 40% over recent baselines. This enables efficient multi-turn interaction under strict resource constraints by reducing latency and memory. |
| Machine Learning | SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering
  Tasks? (Read more on [arXiv](https://arxiv.org/abs/2509.16941) or [HuggingFace](https://huggingface.co/papers/2509.16941))| Yannis Yiming He, Edwin Pan, Jeff Da, Xiang Deng, nlauffer | The paper introduces SWE-Bench Pro, a benchmark for evaluating AI agents on realistic software engineering tasks. The primary objective is to assess the capability of AI agents to solve complex, enterprise-level coding problems. The methodology involves curating a dataset of 1,865 problems from diverse repositories, emphasizing long-horizon tasks and contamination resistance. Results show that the best models, such as GPT-5, achieve below 25% Pass@1, indicating limitations in current AI agent performance. This suggests significant room for improvement in AI agents' ability to autonomously handle professional-level software development challenges. |
| Reinforcement Learning | DiffusionNFT: Online Diffusion Reinforcement with Forward Process (Read more on [arXiv](https://arxiv.org/abs/2509.16117) or [HuggingFace](https://huggingface.co/papers/2509.16117))| Qinsheng Zhang, Haoxiang Wang, Haotian Ye, Huayu Chen, Kaiwen Zheng | The paper introduces DiffusionNFT, a novel online reinforcement learning paradigm for diffusion models that optimizes the forward process via flow matching. It aims to overcome limitations of existing methods by directly optimizing the forward process using a contrastive objective between positive and negative generations. The methodology contrasts positive and negative generation subsets to implicitly improve policy by incorporating reinforcement signals into a supervised learning objective using flow matching. Results demonstrate DiffusionNFT's superior efficiency, achieving a GenEval score of 0.98 within 1k steps compared to FlowGRPO, which only reached 0.95 with over 5k steps. This efficient framework enables AI practitioners to perform RL with diffusion models using black-box solvers and avoids the complexities of likelihood estimation. |
| Robotics | ByteWrist: A Parallel Robotic Wrist Enabling Flexible and
  Anthropomorphic Motion for Confined Spaces (Read more on [arXiv](https://arxiv.org/abs/2509.18084) or [HuggingFace](https://huggingface.co/papers/2509.18084))| Jiafeng Xu, Jingchao Qiao, Liqun Huang, Jiawen Tian, cuizhongren | The paper introduces ByteWrist, a novel parallel robotic wrist designed for flexible and anthropomorphic motion in confined spaces. The research focuses on overcoming the limitations of existing wrists by developing a compact three-stage parallel drive mechanism integrated with arc-shaped end linkages for RPY motion. The methodology involves kinematic modeling, Jacobian matrix numerical solution, and experimental validation. Results show ByteWrist outperforms Kinova-based systems in narrow-space maneuverability, achieving approximately twice the grasping speed in glove box experiments. The design offers AI practitioners a promising solution for next-generation robotic manipulation in constrained environments requiring dexterity and compactness. |
| Multi-Modal | FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning
  Models on Automatically Verifiable Textual and Visual Questions (Read more on [arXiv](https://arxiv.org/abs/2509.17177) or [HuggingFace](https://huggingface.co/papers/2509.17177))| tengdai722, stephaniezhou, xuanricheng, miguelhuchen, lilaczheng | The paper presents a preliminary evaluation of large reasoning models (LRMs) on automatically verifiable textual and visual questions. The study investigates the behavior of LRMs, including misaligned thinking, hallucination of tool use, and vulnerability to harmful content.  The methodology involves creating a moderate-scale, contamination-free evaluation dataset and using LLM-assisted behavioral analysis. Results show that GPT-5 series exhibits superiority in textual problem solving, while Gemini 2.5 Pro marginally outperforms on visual questions, with an overall accuracy for Gemini-2.5 Pro for visual tasks of 61.2%. The findings highlight the need for improved transparency and consistency in LRM reasoning and greater caution in their deployment. |
| Computer Vision | VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video
  Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2509.17985) or [HuggingFace](https://huggingface.co/papers/2509.17985))| Sunghyun Cho, Janghyeok Han, Geonung Kim | VideoFrom3D is a novel framework for synthesizing high-quality 3D scene videos from coarse geometry, camera trajectories, and a reference image. The research aims to address limitations in existing video diffusion models by leveraging the complementary strengths of image and video diffusion models for high-fidelity video generation. The framework consists of a Sparse Anchor-view Generation (SAG) module using an image diffusion model and a Geometry-guided Generative Inbetweening (GGI) module using a video diffusion model conditioned on structural guidance. Experiments demonstrate the method's effectiveness, achieving a MUSIQ score of 68.615 compared to a depth-conditioned I2V model's score of 65.240. This approach offers AI practitioners a faster and more flexible alternative to traditional 3D graphic design workflows, reducing the need for detailed modeling and texturing. |
| Reinforcement Learning | ARE: Scaling Up Agent Environments and Evaluations (Read more on [arXiv](https://arxiv.org/abs/2509.17158) or [HuggingFace](https://huggingface.co/papers/2509.17158))| Matteo Bettini, Gerard Moreno-Torres Bertran, Amine Benhalloum, Pierre Andrews, HugoLaurencon | The paper introduces Meta Agents Research Environments (ARE), a research platform for scalable creation and evaluation of agentic environments. ARE facilitates the integration of synthetic and real applications and the execution of agentic orchestrations. The core objective is to bridge the gap between model development and real-world deployment. A benchmark called Gaia2 is built within ARE to measure general agent capabilities in asynchronous and dynamic settings. Experiments show no system dominates across the intelligence spectrum, with budget scaling curves plateauing, with top model scoring 42.1 overall; ARE abstractions enable continuous benchmark extension and robust evaluations. |
| Natural Language Processing | Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from
  Token and Parameter Levels (Read more on [arXiv](https://arxiv.org/abs/2509.16596) or [HuggingFace](https://huggingface.co/papers/2509.16596))| Qi Zhang, Shuo Li, Yang Nan, Umean, Junjie-Ye | This paper analyzes the impact of Supervised Fine-Tuning (SFT) on the factual knowledge of Large Language Models (LLMs) during closed-book question answering (CBQA). The study examines LLMs from the LLaMA-2 and LLaMA-3 families, evaluating performance across different scales and knowledge mastery levels of fine-tuning data using token and parameter-level analysis. Results showed that models fine-tuned with 1,920 samples performed up to 14% worse than those fine-tuned with only 240 samples, and up to 90% of parameter updates do not contribute to knowledge enhancement. Restoring these updates can improve performance, providing insights for developing effective fine-tuning strategies. |
| Natural Language Processing | Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG
  Applications (Read more on [arXiv](https://arxiv.org/abs/2509.17671) or [HuggingFace](https://huggingface.co/papers/2509.17671))| Fatma Betül Terzioğlu, Reyhan Bayraktar, ozayezerceli, MElHuseyni, selvatas | The paper introduces Turk-LettuceDetect, a suite of hallucination detection models specifically designed for Turkish Retrieval-Augmented Generation (RAG) applications. The research aims to address the challenge of hallucination in morphologically complex, low-resource languages like Turkish within RAG systems. The methodology involves fine-tuning three distinct encoder architectures—ModernBERT, TurkEmbed4STS, and multilingual EuroBERT—on a machine-translated version of the RAGTruth benchmark dataset. The ModernBERT-based model achieves an F1-score of 0.7266 on the complete test set, demonstrating strong performance, particularly on structured tasks. This work provides AI practitioners with a specialized detection mechanism necessary for Turkish RAG applications, addressing the issue of over-generation in LLMs. |
| Natural Language Processing | QWHA: Quantization-Aware Walsh-Hadamard Adaptation for
  Parameter-Efficient Fine-Tuning on Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.17428) or [HuggingFace](https://huggingface.co/papers/2509.17428))| Jae-Joon Kim, Yulhwa Kim, Beomseok Kang, Seojune Lee, Hyesung Jeon | The paper introduces QWHA, a quantization-aware PEFT method using Walsh-Hadamard Transform (WHT) adapters for efficient fine-tuning of quantized LLMs. It addresses the limitations of low-rank adaptation by employing FT-based adapters with a novel initialization scheme involving adaptive parameter selection and value refinement. QWHA mitigates quantization errors while reducing computational cost compared to existing FT-based adapters. Experiments demonstrate that QWHA consistently outperforms baselines in low-bit quantization accuracy and achieves training speedups over existing FT-based adapters; the paper reports an average layer output error that is significantly lower when using QWHA with refinement. The work provides a practical approach for deploying accurate and efficient quantized LLMs via parameter-efficient fine-tuning. |
| Reinforcement Learning | Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.18083) or [HuggingFace](https://huggingface.co/papers/2509.18083))| Damien Sileo, Valentin Quesnel, Valentin Lacombe | This paper introduces Reasoning Core, a scalable RL environment for training LLMs in symbolic reasoning. The research aims to provide a diverse and challenging benchmark beyond games and puzzles, focusing on core formal domains like PDDL planning and first-order logic. Reasoning Core leverages procedural generation with a continuous difficulty knob and external verification tools to create a virtually infinite supply of novel training instances. Zero-shot evaluations with GPT-5 reveal the difficulty of the tasks, suggesting its potential for improving LLMs' reasoning abilities. The environment facilitates RLVR training, enabling more robust and generalizable symbolic reasoning skills in LLMs. |
| Machine Learning | Understanding Embedding Scaling in Collaborative Filtering (Read more on [arXiv](https://arxiv.org/abs/2509.15709) or [HuggingFace](https://huggingface.co/papers/2509.15709))| Yonghui Yang, Fengbin Zhu, Haoyue Bai, Zhou Kaiyu, Zhuangzhuang He | This paper investigates the scaling behavior of embedding dimensions in collaborative filtering models. It aims to understand why increasing embedding dimensions sometimes fails to improve performance and discovers two novel phenomena: double-peak and logarithmic scaling. Through extensive experiments across 10 datasets and theoretical analysis, the paper identifies interaction noise as a key factor. The authors demonstrate that SGL exhibits logarithmic behavior, indicating noise robustness, while BPR often shows the double-peak phenomenon. The findings suggest that designing collaborative filtering models with noise-resistant structures is crucial for effective scaling. |
| Natural Language Processing | Synthetic bootstrapped pretraining (Read more on [arXiv](https://arxiv.org/abs/2509.15248) or [HuggingFace](https://huggingface.co/papers/2509.15248))| Emmanuel Candès, Tatsunori Hashimoto, Hong Liu, Aonan Zhang, Zitong Yang | This paper introduces Synthetic Bootstrapped Pretraining (SBP), a novel language model pretraining procedure. SBP aims to improve LM performance by modeling inter-document correlations. It trains a conditional synthesizer on document pairs from the pretraining dataset and then generates synthetic data for joint training. Experiments on a 3B-parameter model pre-trained on 1T tokens show SBP improves QA accuracy by 2.17% over a repetition baseline on average, capturing 42% of the improvement achievable by an oracle with 20x more data access. SBP provides a method for better utilization of existing pretraining data, especially in data-constrained scenarios, and introduces a way to incorporate relationships between documents into LMs. |
| Multi-Modal | MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late
  Interaction (Read more on [arXiv](https://arxiv.org/abs/2509.18095) or [HuggingFace](https://huggingface.co/papers/2509.18095))| Xintao Chen, Chun-cheng Jason Chen, Mengting Gu, Qi Ma, MrZilinXiao | This paper introduces METAEMBED, a scalable framework for multimodal retrieval that employs flexible late interaction using learnable Meta Tokens. The research aims to improve multimodal retrieval performance and scalability by rethinking how embeddings are constructed and interacted with at scale. METAEMBED appends Meta Tokens to input sequences, using their contextualized representations as multi-vector embeddings and trains through Matryoshka Multi-Vector Retrieval to organize information by granularity. Evaluated on the Massive Multimodal Embedding Benchmark (MMEB), METAEMBED achieves state-of-the-art retrieval performance, reaching 78.7% Precision@1, while maintaining scalability with 32B parameter models.  METAEMBED allows practitioners to balance retrieval quality against efficiency by adjusting the number of Meta Tokens used, enabling flexible deployment in resource-constrained environments. |
| Computer Vision | ContextFlow: Training-Free Video Object Editing via Adaptive Context
  Enrichment (Read more on [arXiv](https://arxiv.org/abs/2509.17818) or [HuggingFace](https://huggingface.co/papers/2509.17818))| Yue Ma, Xiujun Ma, Xuanhua He, Yiyang Chen | The paper introduces ContextFlow, a training-free framework for video object editing using Diffusion Transformers (DiTs). It aims to achieve precise object manipulation while maintaining fidelity and temporal consistency, addressing limitations in prior methods that suffer from inaccurate inversion and contextual conflicts. ContextFlow employs a high-order Rectified Flow solver for robust inversion and an Adaptive Context Enrichment mechanism that concatenates Key-Value pairs from reconstruction and editing paths, dynamically fusing information based on vital layer analysis. Experiments show that ContextFlow significantly outperforms existing training-free methods and surpasses some state-of-the-art training-based approaches, delivering temporally coherent, high-fidelity results. This provides AI practitioners with a novel and effective means for video object manipulation without the need for task-specific training. |
| Natural Language Processing | AuditoryBench++: Can Language Models Understand Auditory Knowledge
  without Hearing? (Read more on [arXiv](https://arxiv.org/abs/2509.17641) or [HuggingFace](https://huggingface.co/papers/2509.17641))| Jaeho Lee, Hyeonjun Kim, Hyunjong Ok, suhoyoo | The paper introduces AuditoryBench++, a benchmark for evaluating auditory knowledge in language models without audio input. It addresses the research question of whether LLMs can understand auditory properties such as pitch and loudness without direct sound input. The study proposes AIR-CoT, an auditory imagination reasoning method involving span detection and knowledge injection via special tokens. Experiments show that AIR-CoT outperforms existing models on auditory reasoning tasks, achieving 83.89% accuracy on pitch comparison. This work provides a foundation for building LLMs capable of auditory imagination, enabling more human-like multimodal reasoning. |
| Multi-Modal | Mano Report (Read more on [arXiv](https://arxiv.org/abs/2509.17336) or [HuggingFace](https://huggingface.co/papers/2509.17336))| Minghui Wu, Hanning Wang, Chenxu Zhao, Anyang Su, Tianyu Fu | The paper introduces Mano, a robust GUI agent built upon a multi-modal foundation model for automating GUI interactions. It addresses the challenges of visual complexity, dynamic environments, and multi-step reasoning in GUI automation by integrating a simulated environment for data generation, a three-stage training pipeline, and a verification module for error recovery.  The primary objective is to improve the success rate and operational accuracy of GUI agents by integrating reinforcement learning with VLMs. The methodology involves supervised fine-tuning, offline reinforcement learning, and online reinforcement learning, using domain-specific data and iterative training.  Mano demonstrates state-of-the-art performance on multiple GUI benchmarks, including Mind2Web and OSWorld, achieving significant improvements in success rate (e.g., improving OSWorld-Verified average score by 7.9 to reach 41.6). The main implication is a pathway to practical GUI agent deployment through iterative training and holistic reward design. |
| Natural Language Processing | Cross-Attention is Half Explanation in Speech-to-Text Models (Read more on [arXiv](https://arxiv.org/abs/2509.18010) or [HuggingFace](https://huggingface.co/papers/2509.18010))| Luisa Bentivogli, Matteo Negri, Marco Gaido, Dennis Fucci, Sara Papi | This paper investigates the explanatory power of cross-attention mechanisms in speech-to-text (S2T) models. It aims to determine if cross-attention scores reliably reflect the model's focus on input signals during output generation by comparing them to input saliency maps derived from feature attribution. The methodology involves analyzing cross-attention scores and SPES-derived saliency maps across various S2T models (monolingual, multilingual, multitask) at different scales. Results indicate that cross-attention scores moderately align with saliency-based explanations but only capture approximately 50% of input relevance, reaching 52-75% of encoder output saliency, with Pearson correlation values peaking around 0.55. The findings suggest that cross-attention offers an informative but incomplete view of factors driving predictions in S2T, and should be treated as a complementary rather than a stand-alone XAI tool. |
| Natural Language Processing | DIWALI - Diversity and Inclusivity aWare cuLture specific Items for
  India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian
  Context (Read more on [arXiv](https://arxiv.org/abs/2509.17399) or [HuggingFace](https://huggingface.co/papers/2509.17399))| Maunendra Sankar Desarkar, mrajbrahma, pramitsahoo | This paper introduces DIWALI, a new dataset for Indian cultural text adaptation. It addresses the lack of cultural knowledge in LLMs when adapting text for Indian contexts. The authors curate ~8k cultural concepts from 36 Indian sub-regions and evaluate several LLMs on cultural text adaptation using both automatic CSI-based and human evaluation metrics. Results show that LLMs demonstrate selective sub-regional coverage and surface-level adaptations, with models achieving average adaptation scores, for instance, a fuzzy match score of 0.615 and an exact match score of 0.855 for Llama-2-7b-chat-hf with DIWALI using an English prompt on GSM8k dataset. The study implies that LLMs require more in-depth training methods to grasp the nuances of cultural context for effective adaptation. |
| Multi-Modal | When Big Models Train Small Ones: Label-Free Model Parity Alignment for
  Efficient Visual Question Answering using Small VLMs (Read more on [arXiv](https://arxiv.org/abs/2509.16633) or [HuggingFace](https://huggingface.co/papers/2509.16633))| Anand Mishra, Piyush Arora, Navlika Singh, abhiram4572 | The paper introduces Model Parity Aligner (MPA) to enhance small vision-language models (S-VLMs) for VQA tasks. The research aims to improve S-VLMs by knowledge transfer from large VLMs without labeled training data, focusing on parity alignment. MPA leverages pseudo-labeling with quality assessment to identify and address knowledge gaps between S-VLMs and L-VLMs, targeting only disparities. Experiments on TextVQA, ST-VQA, ChartQA, and OKVQA demonstrate MPA's ability to consistently enhance S-VLM performance, achieving up to 15.2% gain (TinyLLaVA-2B on ChartQA). MPA enables S-VLMs to benefit from closed-source L-VLMs, improving core capabilities like text recognition, reducing the dependency on labeled data and computationally expensive large models. |
| Reinforcement Learning | From Uniform to Heterogeneous: Tailoring Policy Optimization to Every
  Token's Nature (Read more on [arXiv](https://arxiv.org/abs/2509.16591) or [HuggingFace](https://huggingface.co/papers/2509.16591))| Bin Cui, Mengzhang Cai, Siwei Wen, Mengjie Liu, starriver030515 | The paper introduces Heterogeneous Adaptive Policy Optimization (HAPO), a token-aware reinforcement learning algorithm for LLMs. It addresses the limitation of uniform token optimization by dynamically adapting the optimization process based on token entropy. HAPO incorporates adaptive temperature sampling, token-level group averaging for advantage calculation, differential advantage redistribution, and asymmetric adaptive clipping. Experiments on mathematical reasoning benchmarks demonstrate that HAPO consistently outperforms DAPO, achieving a 2.86 point increase on AIME'24 with Qwen2.5-Math-7B. HAPO provides a method for fine-grained control in RLHF, enabling more effective optimization of LLMs by accounting for the heterogeneous nature of tokens. |
| Machine Learning | CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End
  Code Review Evaluation in Python Projects (Read more on [arXiv](https://arxiv.org/abs/2509.14856) or [HuggingFace](https://huggingface.co/papers/2509.14856))| Hang Yu, Zihan Liao, Xunjin Zheng, Hanyang Guo, Geralt-Targaryen | This paper introduces CodeFuse-CR-Bench, a novel benchmark for evaluating end-to-end automated code review (CR) in Python projects. The benchmark aims to address the 'reality gap' by providing rich, repository-level context to better reflect real-world CR scenarios. The authors propose a multi-faceted evaluation framework combining rule-based precision with model-based quality assessment using reward models and LLMs-as-judges. Results demonstrate that no single LLM dominates all aspects of CR, with Gemini 2.5 Pro achieving the highest comprehensive performance. This benchmark facilitates holistic, multi-dimensional CR evaluation, advancing the development of practical CR assistants. |
| Machine Learning | From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI
  Ecosystem (Read more on [arXiv](https://arxiv.org/abs/2509.09873) or [HuggingFace](https://huggingface.co/papers/2509.09873))| Ahmed E. Hassan, Gopi Krishnan Rajbahadur, Bram Adams, James Jewitt, hao-li | This paper audits license compliance in the open-source AI ecosystem. It investigates the propagation and potential drift of licenses from Hugging Face datasets and models to GitHub applications. The methodology involves tracing license lineage across 364K datasets, 1.6M models, and 140K GitHub projects, coupled with a license conflict detection engine called LicenseRec. Results indicate systemic non-compliance, with 35.5% of model-to-application transitions eliminating restrictive license clauses. The study implies that AI practitioners must address license compliance as a critical governance challenge and employ automated tools for AI-aware license management. |
| Computer Vision | VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery (Read more on [arXiv](https://arxiv.org/abs/2509.17191) or [HuggingFace](https://huggingface.co/papers/2509.17191))| Shiya Huang, Zeyu Zhang, Biao Wu, Tengfei Cheng, Jinchao Ge | The paper introduces VaseVQA, a multimodal benchmark and agent for analyzing ancient Greek pottery. It addresses the challenge of equipping MLLMs with expert-level reasoning for cultural heritage artifacts. The approach uses an SFT-then-RL system with diagnosis-guided, taxonomy-conditioned rewards and GRPO to improve performance on specific question types. Experiments show state-of-the-art results on style classification and historical attribution with gains in compositional robustness, achieving an overall accuracy of 75.71%. VaseVQA provides a reusable resource for future research in specialized-domain multimodal understanding by turning evaluation into supervision. |
| Machine Learning | SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward
  Learning (Read more on [arXiv](https://arxiv.org/abs/2509.16548) or [HuggingFace](https://huggingface.co/papers/2509.16548))| Zhaopeng Tu, Xiaobo Liang, Juntao Li, Xinyu Shi, dyyyyyyyy | This paper introduces SCAN, a self-denoising Monte Carlo annotation method for robust process reward learning. It addresses the challenge of noisy synthetic data in process reward model (PRM) training by proposing a self-denoising strategy and a self-confidence metric to improve annotation quality. SCAN achieves a 39.2 F1 score improvement in ProcessBench (from 19.9 to 59.1) using synthetic data and lightweight models. The primary implication is that SCAN offers a scalable and cost-efficient approach for PRM training, reducing reliance on expensive human annotations and external strong supervisions. Performance continues to improve as the synthetic dataset size scales up. |
