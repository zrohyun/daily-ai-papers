

## Papers for 2025-09-08

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Why Language Models Hallucinate (Read more on [arXiv](https://arxiv.org/abs/2509.04664) or [HuggingFace](https://huggingface.co/papers/2509.04664))| Edwin Zhang, Santosh S. Vempala, Ofir Nachum, Adam Tauman Kalai | This paper analyzes the statistical causes of hallucinations in language models, arguing that they arise from training and evaluation procedures that reward guessing over acknowledging uncertainty. The research investigates how pretraining objectives and post-training evaluations contribute to the problem. The key methodology involves analyzing errors as binary classification problems. It demonstrates how even error-free training data can lead to errors due to optimization objectives. The study implies that modifying evaluation benchmarks to penalize guessing, rather than introducing new hallucination evaluations, can steer the field towards more trustworthy AI systems, though the quantitative metrics demonstrating this claim are unclear from the provided text. |
| Multi-Modal | Symbolic Graphics Programming with Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.05208) or [HuggingFace](https://huggingface.co/papers/2509.05208))| Kaipeng Zhang, Zeju Qiu, Haoquan Zhang, Yamei Chen, YangyiH | This paper introduces symbolic graphics programming (SGP) for LLMs to generate images from natural language descriptions via SVG code. The research aims to improve LLMs' visual understanding and generation capabilities, particularly in creating precise visual content. They propose a reinforcement learning approach with verifiable rewards, aligning text and rendered images using strong vision encoders, and evaluate on SGP-GenBench. Results show significant improvements in SVG generation quality and semantics, with the RL-trained Qwen-2.5-7B achieving a VQA score of 0.596, comparable to frontier systems. The study demonstrates SGP as a lens for cross-modal grounding and RL as a scalable method to inject visual knowledge. |
| Natural Language Processing | Set Block Decoding is a Language Model Inference Accelerator (Read more on [arXiv](https://arxiv.org/abs/2509.04185) or [HuggingFace](https://huggingface.co/papers/2509.04185))| Jeremy Reizenstein, Daniel Haziza, Marton Havasi, Heli Ben-Hamu, Itai Gat | The paper introduces Set Block Decoding (SBD), a novel inference acceleration technique for autoregressive language models. The research aims to reduce the computational cost of inference by enabling parallel decoding of multiple, non-consecutive tokens within a single architecture. SBD integrates next token prediction (NTP) and masked token prediction (MATP) and utilizes solvers from the discrete diffusion literature to sample future tokens. Fine-tuning Llama-3.1 8B and Qwen-3 8B models shows SBD achieves a 3-5x reduction in forward passes while maintaining equivalent performance to NTP training. SBD offers AI practitioners a flexible and computationally efficient method to accelerate language model inference without architectural changes or extra training hyperparameters. |
| Multi-Modal | WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.04744) or [HuggingFace](https://huggingface.co/papers/2509.04744))| Amit Namburi, Yash Vishe, Gagan Mundada, ZacharyNovack, XinXuNLPer | This paper introduces WildScore, a benchmark for evaluating multimodal large language models (MLLMs) in symbolic music reasoning. It aims to assess the ability of MLLMs to interpret real-world music scores and answer complex musicological queries. The methodology involves curating a dataset from genuine musical compositions with user-generated questions, paired with a systematic taxonomy for evaluation; MLLMs are evaluated through multiple-choice questions. Empirical benchmarking of state-of-the-art MLLMs on WildScore reveals that even widely used models show inconsistent accuracy across musical reasoning tasks, achieving a best average performance of 68.31% accuracy using GPT-4.1-mini. WildScore provides AI practitioners with a standardized tool and dataset for developing and evaluating MLLMs in the specialized domain of symbolic music analysis. |
| Multi-Modal | LatticeWorld: A Multimodal Large Language Model-Empowered Framework for
  Interactive Complex World Generation (Read more on [arXiv](https://arxiv.org/abs/2509.05263) or [HuggingFace](https://huggingface.co/papers/2509.05263))| Zhan Zhao, Wei Jia, Tongwei Gu, Zhengxia Zou, Yinglin Duan | LatticeWorld is a multimodal framework that generates interactive 3D virtual environments. The paper aims to streamline the production pipeline of 3D worlds using LLMs and industry-grade rendering engines like Unreal Engine. It uses multimodal inputs (text and visual instructions) to generate large-scale 3D interactive environments with dynamic agents and high-fidelity physics. Experiments show LatticeWorld achieves superior accuracy in scene layout generation and visual fidelity. The framework increases industrial production efficiency by over 90x while maintaining high creative quality, offering AI practitioners a faster method for generating complex interactive virtual environments. |
| Computer Vision | LuxDiT: Lighting Estimation with Video Diffusion Transformer (Read more on [arXiv](https://arxiv.org/abs/2509.03680) or [HuggingFace](https://huggingface.co/papers/2509.03680))| Sanja Fidler, Igor Gilitschenski, Zan Gojcic, Kai He, Ruofan Liang | LuxDiT is a novel data-driven approach for estimating scene lighting from a single image or video by generating HDR environment maps. The research aims to address the challenges of limited ground-truth HDR environment maps and the need for global context in lighting estimation. The method fine-tunes a video diffusion transformer (DiT) trained on a large synthetic dataset and applies low-rank adaptation (LoRA) using real HDR panoramas. The method reduces lighting estimation error by 45% on Laval Outdoor sunlight direction. LuxDiT provides AI practitioners with a generative model for accurate and scene-consistent lighting prediction, enabling realistic virtual object insertion and relighting. |
| Computer Vision | WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool (Read more on [arXiv](https://arxiv.org/abs/2509.05296) or [HuggingFace](https://huggingface.co/papers/2509.05296))| Wenzheng Chang, Yifan Wang, Jianjun Zhou, Zizun Li, ghy0324 | The paper introduces WinT3R, a feed-forward model for online 3D reconstruction and camera pose estimation. It aims to achieve high reconstruction quality and real-time performance by addressing the trade-off between them. WinT3R uses a sliding window mechanism for local information exchange and a compact camera token pool to maintain global context. The model achieves state-of-the-art online reconstruction quality and camera pose estimation, with a reconstruction speed of 17 FPS. This enables AI practitioners to achieve high-quality geometry reconstruction while maintaining real-time performance. |
| Natural Language Processing | On Robustness and Reliability of Benchmark-Based Evaluation of LLMs (Read more on [arXiv](https://arxiv.org/abs/2509.04013) or [HuggingFace](https://huggingface.co/papers/2509.04013))| Kevin Roitero, Stefano Mizzaro, Vincenzo Della Mea, Riccardo Lunardi | This paper investigates the robustness and reliability of benchmark-based evaluations of Large Language Models (LLMs) when questions are paraphrased. The study aims to determine if LLM evaluations are stable and if LLMs generalize well across different question phrasings. They systematically paraphrase benchmark questions and assess the impact on 34 LLMs' performance. The results show that while LLM rankings remain relatively stable, absolute performance drops significantly when questions are paraphrased; accuracy decreases for state-of-the-art LLMs despite consistent rankings. This suggests that current benchmarks may overestimate LLMs' generalization capabilities, highlighting the need for robustness-aware evaluation methodologies. |
| Multi-Modal | MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in
  3D CT Disease Detection, Understanding and Reporting (Read more on [arXiv](https://arxiv.org/abs/2509.03800) or [HuggingFace](https://huggingface.co/papers/2509.03800))| Vanessa Wildman, Jike Zhong, Yuxiang Lai, Yenho Chen, Yuheng Li | MedVista3D is a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis designed to reduce diagnostic errors. It addresses the challenge of combining localized disease detection with global understanding and natural language reporting in 3D CT analysis. The methodology involves a multi-scale loss function aligning CT volumes and organ-level features with text descriptions, enhanced semantic supervision using LLM-rewritten reports, and a Radiology Semantic Matching Bank (RSMB) for semantics-aware alignment. MedVista3D achieves state-of-the-art performance on zero-shot disease classification, report retrieval, and medical visual question answering. The model facilitates more effective integration of visual and textual data for enhanced reasoning in medical imaging applications. |
| Robotics | U-ARM : Ultra low-cost general teleoperation interface for robot
  manipulation (Read more on [arXiv](https://arxiv.org/abs/2509.02437) or [HuggingFace](https://huggingface.co/papers/2509.02437))| Junda Huang, Zewei Ye, Chenyang Shi, Zhaoye Zhou, Yanwen Zou | This paper introduces U-Arm, a low-cost, adaptable teleoperation framework for robot manipulation. The research aims to create a more accessible and versatile interface for controlling various commercial robotic arms, focusing on reducing cost and improving usability compared to existing systems. The methodology involves designing three structurally distinct 3D-printed leader arms with optimized mechanical design and servo selection, achieving a BOM cost of around $50. Experimental results demonstrate that U-Arm achieves a 39% higher data collection efficiency than Joycon. The development of U-Arm offers AI practitioners a more cost-effective tool for generating high-quality, real-world robotic manipulation data, potentially accelerating research in robot learning and control. |
| Natural Language Processing | Behavioral Fingerprinting of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.04504) or [HuggingFace](https://huggingface.co/papers/2509.04504))| Xing Li, Zhiyuan Yang, Ying Zhang, Hui-Ling Zhen, Zehua Pei | The paper introduces "Behavioral Fingerprinting," a framework to characterize Large Language Models (LLMs) beyond traditional performance metrics. It aims to create multi-faceted profiles of LLMs' intrinsic cognitive and interactive styles. The methodology involves a diagnostic prompt suite and an automated evaluation pipeline using an LLM as a judge. The results reveal divergence in alignment-related behaviors, with sycophancy varying significantly across models despite converging core capabilities (e.g., abstract reasoning). The implication is that alignment is a design choice rather than an emergent property of intelligence, requiring specific, highly variable developer strategies. |
| Reinforcement Learning | Bootstrapping Task Spaces for Self-Improvement (Read more on [arXiv](https://arxiv.org/abs/2509.04575) or [HuggingFace](https://huggingface.co/papers/2509.04575))| Yoram Bachrach, Andrei Lupu, Minqi Jiang | This paper introduces Exploratory Iteration (EXIT), a family of autocurriculum RL methods for training LLMs to perform multi-step self-improvement at inference time. The research aims to address the limitations of fixed iteration depth in self-improvement tasks by dynamically growing the task space. EXIT selectively samples informative intermediate histories during episodes, treating these as new task instances for training. Experiments across competition math, multi-turn tool-use, and machine learning engineering domains show EXIT policies exhibit strong inference-time self-improvement on held-out tasks, with improvements exceeding average iteration depth; for example, on the held-out math splits, IMPROVE, DIVERGE, and the full EXIT method all lead to a higher net number of successfully corrected solutions accumulating over 170 successful corrections. EXIT offers a method for improving model performance and task diversity in self-improvement scenarios. |
