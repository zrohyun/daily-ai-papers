

## Papers for 2025-09-18

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Hala Technical Report: Building Arabic-Centric Instruction & Translation
  Models at Scale (Read more on [arXiv](https://arxiv.org/abs/2509.14008) or [HuggingFace](https://huggingface.co/papers/2509.14008))| Bernard Ghanem, Mohammad Zbeeb, Hasan Abed Al Kader Hammoud | The paper presents HALA, a family of Arabic-centric instruction and translation models. It aims to address the scarcity of high-quality Arabic instruction data by developing an efficient translate-and-tune pipeline. The methodology involves compressing a multilingual translator to FP8, constructing a million-scale bilingual corpus, and fine-tuning models at various scales using the translated instruction data and slerp merging. HALA-9B achieves a 69.9% average score on Arabic-centric benchmarks, outperforming its base model FANAR-1-9B-Instruct by 0.7%. The work enables practitioners to leverage efficient, scalable Arabic instruction tuning and accelerates research in Arabic NLP. |
| Multi-Modal | SAIL-VL2 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2509.14033) or [HuggingFace](https://huggingface.co/papers/2509.14033))| Zijian Kang, Yue Liao, Fangxun Shu, Yongjie Ye, Weijie Yin | The paper introduces SAIL-VL2, a vision-language foundation model for multimodal understanding. It aims to improve performance and efficiency using large-scale data curation, progressive training, and efficient MoE architectures. The methodology involves training SAIL-VL2 using a large dataset with specific scoring/filtering, progressive training from vision encoder to SFT-RL, and MoE designs. Results show competitive performance across 106 datasets, achieving state-of-the-art results on MMMU and Math-Vista. SAIL-VL2 provides an efficient and extensible open-source multimodal foundation, enabling further research and development in the field. |
| Computer Vision | PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era (Read more on [arXiv](https://arxiv.org/abs/2509.12989) or [HuggingFace](https://huggingface.co/papers/2509.12989))| Zihao Dongfang, Kaiyu Lei, Ziqiao Weng, Chenfei Liao, Xu Zheng | This paper reviews the emerging trend of omnidirectional vision (360Â° vision) in embodied AI, driven by increased industrial and academic interest. The research addresses the challenges in omnidirectional generation, perception, understanding, and datasets, proposing an ideal panoramic system architecture called PANORAMA. The paper categorizes the challenges into data bottlenecks, model capabilities, and application blanks, suggesting solutions for each. It establishes a PANORAMA system architecture and a six-stage roadmap for future research in this field. This study implies the potential development of robust, general-purpose omnidirectional AI systems integrated into the embodied AI paradigm. |
| Multi-Modal | GenExam: A Multidisciplinary Text-to-Image Exam (Read more on [arXiv](https://arxiv.org/abs/2509.14232) or [HuggingFace](https://huggingface.co/papers/2509.14232))| Yu Qiao, Changyao Tian, Xiangyu Zhao, Penghao Yin, Zhaokai Wang | The paper introduces GenExam, a new multidisciplinary benchmark designed to evaluate text-to-image generation models on tasks requiring integrated understanding, reasoning, and generation skills. GenExam comprises 1,000 exam-style prompts across 10 subjects, each equipped with a ground truth image and fine-grained scoring points. Experiments evaluating state-of-the-art models like GPT-Image-1 and Gemini-2.5-Flash-Image on GenExam reveal strict scores below 15%, indicating a significant challenge. GenExam offers a rigorous assessment of models' abilities and insights into building general AGI. |
| Machine Learning | Scrub It Out! Erasing Sensitive Memorization in Code Language Models via
  Machine Unlearning (Read more on [arXiv](https://arxiv.org/abs/2509.13755) or [HuggingFace](https://huggingface.co/papers/2509.13755))| Zhou Yang, Di Wang, Zhikun Zhang, Yao Wan, Zhaoyang Chu | This paper addresses the privacy vulnerability of code language models (CLMs) unintentionally memorizing sensitive training data. It investigates whether sensitive information memorized by CLMs can be effectively and efficiently erased through machine unlearning. The authors introduce CODEERASER, a selective gradient ascent approach, which unlearns sensitive code segments while preserving structural integrity and functionality. Experiments on CodeParrot, CodeGen-Mono, and Qwen2.5-Coder demonstrate that CODEERASER reduces memorization by up to 93.89% while retaining 99.00% model utility. The work implies that machine unlearning offers a practical way to mitigate sensitive memorization in CLMs without full retraining. |
| Reinforcement Learning | THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.13761) or [HuggingFace](https://huggingface.co/papers/2509.13761))| Yicheng Pan, Jiefeng Ma, Pengfei Hu, Zhenrong Zhang, Qikai Chang | This paper introduces THOR, a tool-integrated hierarchical optimization framework via reinforcement learning for mathematical reasoning. The research aims to improve LLMs' mathematical reasoning by addressing challenges in data construction, optimization granularity, and inference. THOR utilizes a multi-agent actor-critic pipeline, TIRGen, for data construction and an RL strategy for joint trajectory and step-level code optimization. Experiments demonstrate state-of-the-art performance on mathematical benchmarks, achieving 79.8 average accuracy on reasoning models. The implication for AI practitioners is a robust and generalizable method for enhancing LLMs' reasoning and code generation abilities through hierarchical optimization and tool integration. |
| Computer Vision | Wan-Animate: Unified Character Animation and Replacement with Holistic
  Replication (Read more on [arXiv](https://arxiv.org/abs/2509.14055) or [HuggingFace](https://huggingface.co/papers/2509.14055))| Mingyang Huang, Siqi Hu, Li Hu, Xin Gao, Gang Cheng | Wan-Animate is a unified framework for character animation and replacement using holistic replication. The paper addresses the challenge of simultaneously controlling motion, expression, and environmental integration in character animation. It employs a modified input paradigm in Wan-I2V, spatially-aligned skeleton signals for body motion, and implicit facial features for expression reenactment, along with a Relighting LoRA for environmental consistency. Experimental results show state-of-the-art performance, including an SSIM of 0.813 and FVD of 118.65, compared to other open-source models. The framework provides AI practitioners with a robust, open-source model for creating high-fidelity character animations and replacements, facilitating diverse applications and novel product paradigms. |
| Machine Learning | SteeringControl: Holistic Evaluation of Alignment Steering in LLMs (Read more on [arXiv](https://arxiv.org/abs/2509.13450) or [HuggingFace](https://huggingface.co/papers/2509.13450))| Zhun Wang, Nathan W. Henry, David Park, Nicholas Crispino, Vincent Siu | The paper introduces STEERINGCONTROL, a benchmark for evaluating representation steering methods in LLMs across core alignment objectives.  It aims to address limitations in existing alignment evaluations by systematically assessing the effectiveness of steering methods and their behavioral entanglement on primary and secondary tasks. The study employs a modular steering framework and evaluates five popular steering methods on Qwen-2.5-7B and Llama-3.1-8B, finding performance dependent on method-model-behavior combinations.  Results demonstrate significant concept entanglement, highlighting the need for careful selection and combination of steering methods; for example, a 72.7% refusal rate was achieved with DIM on Qwen-2.5-7B. This benchmark provides a comprehensive evaluation suite for AI practitioners developing and applying representation steering techniques. |
| Natural Language Processing | Improving Context Fidelity via Native Retrieval-Augmented Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.13683) or [HuggingFace](https://huggingface.co/papers/2509.13683))| Xiangru Tang, Shiqi Li, Xinyu Wang, Jinlin Wang, Suyuchen Wang | The paper introduces CARE, a native retrieval-augmented reasoning framework, to improve context fidelity in large language models (LLMs). It aims to enhance LLMs' ability to dynamically identify and integrate in-context evidence without extensive labeled data or external tools. The key methodology involves a two-phase training process: supervised fine-tuning (SFT) to establish evidence integration and reinforcement learning (RL) to refine self-retrieval. Experiments show CARE outperforms vanilla SFT and traditional RAG methods, achieving a +15.29% average F1 improvement on real-world QA tasks with LLaMA-3.1 8B. This improvement in context fidelity and accuracy makes LLMs more reliable and efficient for knowledge-intensive tasks. |
| Multi-Modal | MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,
  Results, Discussion, and Outlook (Read more on [arXiv](https://arxiv.org/abs/2509.14142) or [HuggingFace](https://huggingface.co/papers/2509.14142))| Bowen Zhou, Yaxiong Chen, Jiajun Zhang, Shengwu Xiong, Peng Xu | This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning, aiming to advance research in multimodal machine learning and LLMs. The challenge focused on real-world and specialized scenarios, using two new datasets (Lens and AdsQA) to evaluate reasoning capabilities. The challenge evaluated 40+ baselines, open-sourcing datasets, code, and rankings for reproducibility, and included 76 participating teams.  Results include detailed performance metrics for various models on the Lens and AdsQA datasets, but specific quantitative gains are dependent on the model. The challenge provides a comprehensive benchmark and open-source resources to facilitate future research in multimodal reasoning and benefit AI practitioners in addressing the complexities of real-world applications. |
