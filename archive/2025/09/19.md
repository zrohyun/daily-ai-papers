

## Papers for 2025-09-19

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform
  Data (Read more on [arXiv](https://arxiv.org/abs/2509.15221) or [HuggingFace](https://huggingface.co/papers/2509.15221))| Zehao Li, QiushiSun, heroding77, ownerEli, zyliu | The paper introduces ScaleCUA, a dataset and model family for scaling computer use agents (CUAs) across multiple platforms. It aims to address the limited data scale and transferability of existing CUAs by creating a large-scale, cross-platform GUI-centric dataset. ScaleCUA uses a closed-loop pipeline uniting automated agents with human experts to collect data across 6 operating systems and 3 task domains.  The ScaleCUA-7B model achieves strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2), suggesting data-driven scaling is effective for general-purpose, cross-platform CUAs. |
| Reinforcement Learning | FlowRL: Matching Reward Distributions for LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.15207) or [HuggingFace](https://huggingface.co/papers/2509.15207))| Hengli Li, Dinghuai Zhang, jayyoung0802, daixuancheng, xuekai | FlowRL is introduced to match the full reward distribution in LLM reinforcement learning, addressing limitations of reward-maximizing methods. The research aims to promote diverse exploration and generalizable reasoning trajectories by minimizing the reverse KL divergence between policy and a normalized target distribution derived from a learnable partition function. FlowRL achieves a 10.0% average improvement over GRPO and 5.1% over PPO on math benchmarks and performs better on code reasoning. The results suggest reward distribution matching is key for efficient exploration and diverse reasoning, offering AI practitioners a method to enhance exploration by matching the target distribution rather than maximizing rewards. |
| Natural Language Processing | Reasoning over Boundaries: Enhancing Specification Alignment via
  Test-time Delibration (Read more on [arXiv](https://arxiv.org/abs/2509.14760) or [HuggingFace](https://huggingface.co/papers/2509.14760))| Zhilin Wang, Dongrui Liu, Xuyang Hu, Yafu Li, zzzhr97 | The paper introduces ALIGN3, a test-time deliberation method for enhancing specification alignment in large language models (LLMs). The research aims to improve LLMs' ability to follow dynamic, scenario-specific specifications (spec) from behavioral and safety perspectives. ALIGN3 employs hierarchical reflection and revision to reason over specification boundaries at test-time. Experiments using SPECBENCH, a new benchmark covering 5 scenarios, 103 spec, and 1,500 prompts, show ALIGN3 improves specification alignment, achieving up to 11.89% improvement in Specification Alignment Rate (SAR). The main implication is that test-time deliberation is an effective strategy for reasoning over real-world specification boundaries with minimal overhead. |
| Machine Learning | Evolving Language Models without Labels: Majority Drives Selection,
  Novelty Promotes Variation (Read more on [arXiv](https://arxiv.org/abs/2509.15194) or [HuggingFace](https://huggingface.co/papers/2509.15194))| Kishan Panaganti, Wenhao Yu, Haolin Liu, invokerliang, yujunzhou | This paper introduces EVOL-RL, a novel label-free reinforcement learning framework for evolving language models. The research aims to address the entropy collapse observed in existing label-free methods by coupling stability with variation. EVOL-RL combines majority-voted answers for selection and a novelty-aware reward favoring semantically distinct reasoning.  Experiments show EVOL-RL boosts pass@1 accuracy from 4.6% to 16.4% and pass@16 from 18.5% to 37.9% on AIME25 when training Qwen3-4B-Base. The approach enables LLMs to self-improve without external labels, maintaining exploration capacity and generalization. |
| Computer Vision | Understand Before You Generate: Self-Guided Training for Autoregressive
  Image Generation (Read more on [arXiv](https://arxiv.org/abs/2509.15185) or [HuggingFace](https://huggingface.co/papers/2509.15185))| Xihui Liu, Wenlong Zhang, Yuqing Wang, GoodEnough, YueXY233 | This paper introduces Self-guided Training for Autoregressive models (ST-AR) to enhance image understanding and generation in autoregressive models. The research aims to address limitations in local dependence, semantic inconsistency, and spatial invariance in autoregressive image generation. ST-AR integrates masked image modeling and contrastive learning to expand attention and ensure feature alignment. Experiments show ST-AR improves FID by approximately 42% for LlamaGen-L and 49% for LlamaGen-XL. The framework offers a method to improve visual representation learning in autoregressive models without pre-trained representations, potentially leading to more efficient and unified generative models. |
| Machine Learning | FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial
  Search and Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.13160) or [HuggingFace](https://huggingface.co/papers/2509.13160))| Jiashuo Liu, Jianpeng Jiao, Liang Hu, WenhaoHuang, zhangysk | This paper introduces FinSearchComp, a new benchmark for evaluating financial search and reasoning in LLM agents. It aims to assess the ability of agents to perform complex, multi-step searches and knowledge-grounded reasoning using time-sensitive, domain-specific data. The benchmark consists of 635 expert-curated questions spanning global and Greater China markets, divided into three tasks mimicking financial analyst workflows. Evaluations of 21 models showed that Grok 4 (web) achieved the highest overall score (68.9%) on the global subset. The primary implication is that despite improvements with web search and financial plugins, significant gaps remain in freshness awareness and multi-source reconciliation for expert-level financial reasoning. |
| Computer Vision | RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation (Read more on [arXiv](https://arxiv.org/abs/2509.15212) or [HuggingFace](https://huggingface.co/papers/2509.15212))| SpaceProduct, Sicong, yaniii, huangsiteng, yumingj | The paper presents RynnVLA-001, a vision-language-action (VLA) model for robot manipulation. It investigates improving VLA model initialization using video generative pretraining from human demonstrations. RynnVLA-001 employs a two-stage pretraining methodology: ego-centric video generation followed by human-centric trajectory-aware modeling and action representation with ActionVAE. When fine-tuned on downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, such as 90.6% average success rate over three tasks, indicating a more effective initialization strategy for VLA models. The pretraining strategy provides a method to transfer manipulation skills learned from human demonstrations to robot manipulation. |
| Computer Vision | AToken: A Unified Tokenizer for Vision (Read more on [arXiv](https://arxiv.org/abs/2509.14476) or [HuggingFace](https://huggingface.co/papers/2509.14476))| Mingze Xu, Liangchen Song, afshin525, byeongjooahn, Jiasenlu | The paper presents ATOKEN, a unified visual tokenizer for images, videos, and 3D assets capable of high-fidelity reconstruction and semantic understanding. It addresses the challenge of fragmented visual representations by encoding diverse inputs into a shared 4D latent space using a pure transformer architecture with 4D rotary position embeddings. ATOKEN employs an adversarial-free training objective with perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality, such as 0.21 rFID with 82.2% ImageNet accuracy for images. ATOKEN enables various downstream applications, including visual generation and multimodal LLMs, demonstrating competitive performance across benchmarks and unified visual tokenization. |
| Computer Vision | WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model
  via Training-Free Guidance (Read more on [arXiv](https://arxiv.org/abs/2509.15130) or [HuggingFace](https://huggingface.co/papers/2509.15130))| Ruibo Li, Tong Zhao, ChiZhang, 2hiTee, ChenxiSong | The paper introduces WorldForge, a training-free framework for enhancing 3D/4D generation in video diffusion models by improving controllability and geometric consistency. It addresses the challenge of limited control in video diffusion models by introducing three modules: Intra-Step Recursive Refinement (IRR), Flow-Gated Latent Fusion (FLF), and Dual-Path Self-Corrective Guidance (DSG). The methodology leverages a warping-and-repainting pipeline combined with the modules to enable precise trajectory injection and disentangle motion from appearance in the latent space. Experiments demonstrate that WorldForge achieves state-of-the-art controllability and visual quality, surpassing existing methods in tasks like 3D scene generation, where the method attains a FID score of 96.08. The framework offers AI practitioners a plug-and-play solution for controllable video synthesis without requiring retraining. |
| Computer Vision | MultiEdit: Advancing Instruction-based Image Editing on Diverse and
  Challenging Tasks (Read more on [arXiv](https://arxiv.org/abs/2509.14638) or [HuggingFace](https://huggingface.co/papers/2509.14638))| Xijun Gu, Lin Liu, HaoxingChen, dreamzz5, Mingsong07 | The paper introduces MultiEdit, a new large-scale dataset for instruction-based image editing (IBIE) targeting diverse and challenging tasks. It addresses the limitation of existing datasets by focusing on complex editing scenarios and utilizing multi-modal large language models (MLLMs) for data generation. The key methodology involves a novel dataset construction pipeline with visual-adaptive editing instructions and high-fidelity image generation. Experiments show that fine-tuning foundational models with MultiEdit-Train improves performance on sophisticated editing tasks, achieving a CLIPimg score of 0.8174 on the proposed MultiEdit-Test benchmark. MultiEdit provides a valuable resource for advancing research into more diverse and challenging IBIE capabilities. |
| Computer Vision | Unleashing the Potential of Multimodal LLMs for Zero-Shot
  Spatio-Temporal Video Grounding (Read more on [arXiv](https://arxiv.org/abs/2509.15178) or [HuggingFace](https://huggingface.co/papers/2509.15178))| Rynson W. H. Lau, Gerhard Hancke, yuhaoliu, zaiquan | This paper presents a zero-shot spatio-temporal video grounding (STVG) framework leveraging multimodal large language models (MLLMs). It addresses suboptimal grounding in MLLMs by proposing decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies. DSTH decouples queries into attribute and action sub-queries, using logit-guided re-attention to learn spatial and temporal prompts. TAS enhances temporal consistency by assembling predictions from original and temporally-augmented frames; on the HC-STVGv1 dataset, the method achieves 24.8% m_vIoU using LLaVA-OneVision-7B, outperforming existing zero-shot methods. The framework provides a training-free approach to improve MLLM performance in STVG by better exploiting attribute and action cues. |
| Natural Language Processing | Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question
  Answering with LLMs (Read more on [arXiv](https://arxiv.org/abs/2509.15020) or [HuggingFace](https://huggingface.co/papers/2509.15020))| Katharina von der Wense, MinhDucBui, mario-sanz | This paper investigates the impact of tokenization strategies on the accuracy and calibration of LLMs in multiple-choice question answering (MCQA). The study specifically examines the tokenization of the space preceding the answer letter and its effect on model performance. Through experiments on various LLMs and datasets, tokenizing the space together with the answer letter yields statistically significant gains, with accuracy improvements of up to 11% on MMLU. This approach improves model calibration and impacts model rankings, underscoring the importance of consistent tokenization strategies for reliable LLM evaluation and comparison. |
| Multi-Modal | EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal
  Ultrasound Intelligence (Read more on [arXiv](https://arxiv.org/abs/2509.14977) or [HuggingFace](https://huggingface.co/papers/2509.14977))| Qinghua Huang, WeiWang, lidachen, Ruimed, chaoyinshe | The paper introduces EchoVLM, a vision-language model (VLM) for universal ultrasound intelligence. It aims to address the limitations of existing general-purpose VLMs in ultrasound medical tasks, such as poor generalization in multi-organ lesion recognition. EchoVLM uses a Mixture of Experts (MoE) architecture trained on data spanning seven anatomical regions and incorporates a Dual-path MoE module with dynamic routing. Experimental results demonstrate that EchoVLM achieves significant improvements, with a 10.15-point increase in BLEU-1 scores compared to Qwen2-VL on ultrasound report generation. The model enhances diagnostic accuracy in ultrasound imaging, potentially providing a viable technical solution for future clinical applications. |
| Computer Vision | FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution
  Remote Sensing Change Detection (Read more on [arXiv](https://arxiv.org/abs/2509.06482) or [HuggingFace](https://huggingface.co/papers/2509.06482))| Zhewei Zhang, Yuhan Jiang, Shuangxi Miao, pedramghamisi, zx-Xie | The paper introduces FSG-Net, a novel deep learning architecture for high-resolution remote sensing change detection. It aims to disentangle semantic changes from nuisance variations and improve boundary delineation. The methodology involves a frequency-spatial synergistic pipeline with Discrepancy-Aware Wavelet Interaction Module (DAWIM), Synergistic Temporal-Spatial Attention Module (STSAM), and Lightweight Gated Fusion Unit (LGFU). FSG-Net achieves state-of-the-art performance, with an F1-score of 94.16% on the CDD dataset. The proposed network offers AI practitioners an effective framework for enhancing change detection accuracy in remote sensing applications by mitigating pseudo-changes and refining feature fusion. |
