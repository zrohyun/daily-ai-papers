

## Papers for 2025-09-29

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | LongLive: Real-time Interactive Long Video Generation (Read more on [arXiv](https://arxiv.org/abs/2509.22622) or [HuggingFace](https://huggingface.co/papers/2509.22622))|  | The paper introduces LongLive, a frame-level autoregressive framework enabling real-time interactive long video generation. The research aims to address efficiency and quality challenges in generating long videos while maintaining interactivity. LongLive integrates KV-recache, streaming long tuning, and short window attention with a frame-level attention sink for efficient and coherent video generation. The framework achieves 20.7 FPS on a single NVIDIA H100 and strong VBench performance. LongLive provides AI practitioners with a system capable of interactive long video generation, enhancing creative applications by facilitating dynamic narrative control and visual consistency. |
| Reinforcement Learning | Quantile Advantage Estimation for Entropy-Safe Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.22611) or [HuggingFace](https://huggingface.co/papers/2509.22611))| An Zhang, Jiancan Wu, xiangwang1223, 737443h, junkang0909 | This paper introduces Quantile Advantage Estimation (QAE) to stabilize training in Reinforcement Learning with Verifiable Rewards (RLVR) for LLMs, addressing oscillations between entropy collapse and explosion. The research investigates the impact of the mean-baseline in value-free RL on negative-advantage samples, and introduces a K-quantile baseline. QAE induces a response-level gate, reinforcing rare successes on hard queries and targeting failures on easy queries, which leads to a two-sided entropy safety. Experiments on Qwen3-8B/14B-Base across AIME'24/'25 and AMC'23 show sustained pass@1 gains using QAE. The work implies that baseline design, rather than token-level heuristics, is the primary mechanism for scaling RLVR. |
| Computer Vision | MinerU2.5: A Decoupled Vision-Language Model for Efficient
  High-Resolution Document Parsing (Read more on [arXiv](https://arxiv.org/abs/2509.22186) or [HuggingFace](https://huggingface.co/papers/2509.22186))| SunYuefeng, hotelll, ouyanglinke, wanderkid, starriver030515 | The paper introduces MinerU2.5, a 1.2B-parameter vision-language model for efficient high-resolution document parsing. It aims to improve recognition accuracy and computational efficiency in document analysis. MinerU2.5 employs a two-stage parsing strategy, decoupling global layout analysis from local content recognition and leverages a comprehensive data engine for training. The model achieves state-of-the-art performance on multiple benchmarks, including OmniDocBench with an overall score of 90.67, surpassing both general-purpose and domain-specific models. MinerU2.5 offers AI practitioners a computationally efficient solution for high-resolution document parsing tasks, enabling improved accuracy in downstream applications. |
| Reinforcement Learning | EPO: Entropy-regularized Policy Optimization for LLM Agents
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.22576) or [HuggingFace](https://huggingface.co/papers/2509.22576))| Li Yu-Jhe, Wentian Zhao, timecuriosity, ztwang, Iscarrot | This paper introduces Entropy-regularized Policy Optimization (EPO) to improve reinforcement learning for LLM agents in multi-turn environments. EPO addresses the exploration-exploitation cascade failure unique to sparse-reward, multi-turn settings. The method combines entropy regularization, entropy smoothing, and adaptive phase-based weighting to stabilize training. EPO achieves up to 152% performance improvement on ScienceWorld and 19.8% on ALFWorld compared to baselines. This indicates that specialized entropy control is crucial for training LLM agents in complex, sparse-reward environments, enabling more stable and effective learning. |
| Natural Language Processing | Variational Reasoning for Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.22637) or [HuggingFace](https://huggingface.co/papers/2509.22637))|  | This paper introduces a variational reasoning framework for improving the reasoning abilities of language models. The research aims to optimize thinking traces as latent variables using variational inference, deriving multi-trace objectives and a forward-KL formulation to stabilize training. The methodology interprets rejection sampling and binary-reward RL as local forward-KL objectives, implicitly weighted by model accuracy. Empirically, the method is validated on Qwen 2.5 and Qwen 3 models, demonstrating improved performance across various reasoning tasks; for instance, achieving a higher Avg@2 score on MATH500. This work provides a principled probabilistic perspective for training reasoning models, unifying variational inference with RL-style methods. |
| Natural Language Processing | Language Models Can Learn from Verbal Feedback Without Scalar Rewards (Read more on [arXiv](https://arxiv.org/abs/2509.22638) or [HuggingFace](https://huggingface.co/papers/2509.22638))|  | This paper introduces feedback-conditional policy (FCP), a method for training language models directly from verbal feedback without relying on scalar rewards. The research aims to overcome limitations of scalar reward-based reinforcement learning, such as information loss and reward imbalance. FCP approximates the feedback-conditional posterior through maximum likelihood training on response-feedback pairs and uses an online bootstrapping stage for refinement. Results show that FCP matches or surpasses scalar-based baselines on math and reasoning tasks, achieving 38.7% average accuracy on the math suite, comparable to GRPO. The main implication is that verbal feedback can serve as a first-class training signal, enabling more expressive and data-efficient learning. |
| Natural Language Processing | ReviewScore: Misinformed Peer Review Detection with Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2509.21679) or [HuggingFace](https://huggingface.co/papers/2509.21679))|  | The paper introduces ReviewScore, a novel metric for detecting misinformed peer review points in AI conference submissions using large language models. The research aims to address the degradation of peer review quality by identifying unanswerable questions and factually incorrect weaknesses in reviews. The methodology involves an automated engine to reconstruct premises, and a human-expert annotated dataset to evaluate LLMs. Experiments using eight LLMs demonstrate moderate agreement with human experts, achieving F1 scores between 0.4-0.5 and Kappa scores between 0.3-0.4 for ReviewScore evaluation, indicating the potential for automated review quality assessment. The framework enables AI practitioners to filter out misinformed reviews or provide reviewers feedback. |
| Computer Vision | CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2509.22647) or [HuggingFace](https://huggingface.co/papers/2509.22647))|  | The paper introduces Captioning Reinforcement Learning (CapRL), a novel framework for training image captioning models using Reinforcement Learning with Verifiable Rewards (RLVR). The research aims to overcome limitations of Supervised Fine-Tuning (SFT) by designing an objective reward function based on the utility of a caption in enabling a non-visual language model to answer questions about the image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and reward is derived from the accuracy of a separate LLM answering multiple-choice questions based solely on that caption. Results on 12 benchmarks show CapRL achieves performance comparable to Qwen2.5-VL-72B, exceeding the baseline by an average of 8.4% in the Prism framework. CapRL offers AI practitioners a more general and accurate approach to image captioning, moving beyond SFT-based limitations. |
| Computer Vision | MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.22281) or [HuggingFace](https://huggingface.co/papers/2509.22281))| Weipeng Zhong, Xudong Xu, Zhen Luo, nfliang, wuzhi-hao | The paper introduces MesaTask, a novel task and dataset for task-oriented tabletop scene generation using 3D spatial reasoning. It addresses the challenge of generating realistic and task-relevant tabletop scenes from high-level human instructions. The core methodology involves a Spatial Reasoning Chain that decomposes scene generation into object inference, spatial interrelation reasoning, and scene graph construction. Experimental results show MesaTask outperforms baselines, achieving a FID score of 40.3 and a user study rating of 6.12. The research offers a framework for generating plausible and interactive tabletop scenes, facilitating robust policy learning for robotic manipulation. |
| Reinforcement Learning | No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM
  Reinforcement Learning via Entropy-Guided Advantage Shaping (Read more on [arXiv](https://arxiv.org/abs/2509.21880) or [HuggingFace](https://huggingface.co/papers/2509.21880))|  | This paper introduces RL-ZVP, a reinforcement learning algorithm designed to improve LLM reasoning by leveraging zero-variance prompts. It addresses the limitation of existing methods like GRPO by extracting learning signals from prompts where all responses receive the same reward, modulating feedback with token-level entropy. RL-ZVP outperforms GRPO by up to 8.61 points in accuracy on math reasoning benchmarks, indicating the potential of zero-variance prompts for RLVR. The results suggest that learning from these prompts can significantly enhance reasoning capabilities and improve training stability, challenging the common practice of filtering them out. This offers AI practitioners a new strategy for more efficient LLM training. |
| Multi-Modal | VoiceAssistant-Eval: Benchmarking AI Assistants across Listening,
  Speaking, and Viewing (Read more on [arXiv](https://arxiv.org/abs/2509.22651) or [HuggingFace](https://huggingface.co/papers/2509.22651))|  | VoiceAssistant-Eval is introduced as a comprehensive benchmark for evaluating AI assistants across listening, speaking, and viewing abilities. The research aims to address the limitations of existing benchmarks in assessing the full range of AI assistant capabilities, including voice personalization, hands-free interaction, and multi-modal integration. The methodology involves a curated dataset of 10,497 examples spanning 13 task categories and evaluates 21 open-source models and GPT-4o-Audio. Key results show that proprietary models do not universally outperform open-source ones, with the Step-Audio-2-mini (7B) achieving more than double the listening accuracy of LLaMA-Omni2-32B-Bilingual. The benchmark highlights areas needing improvement, including multimodal input and role-play voice imitation, providing a framework for guiding the development of next-generation AI assistants. |
| Reinforcement Learning | UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon
  Scenarios (Read more on [arXiv](https://arxiv.org/abs/2509.21766) or [HuggingFace](https://huggingface.co/papers/2509.21766))| Zeyu Qin, Haoyu Wang, Xuelin Zhang, Huaisong Zhang, Haotian Luo | The paper introduces UltraHorizon, a new benchmark designed to evaluate AI agent capabilities in ultra-long-horizon, partially observable environments, addressing a gap in existing benchmarks. The primary research objective is to measure foundational skills like sustained reasoning, planning, memory management, and tool use in complex, real-world scenarios. UltraHorizon uses exploration tasks across three distinct environments, requiring agents to iteratively uncover hidden rules through interaction, exhibiting trajectories that average 200k+ tokens and 400+ tool calls in the heaviest setting. Experiments reveal LLM agents consistently underperform compared to humans, indicating a capability gap; furthermore, simply scaling up agents does not resolve the challenges. This research implies the need for novel architectural approaches that go beyond scaling, emphasizing principled memory integration, adaptive reasoning, and robust exploration strategies for long-horizon tasks. |
| Computer Vision | LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale
  Diffusion Transformer (Read more on [arXiv](https://arxiv.org/abs/2509.22414) or [HuggingFace](https://huggingface.co/papers/2509.22414))|  | The paper presents LucidFlux, a novel caption-free universal image restoration (UIR) framework utilizing a large-scale diffusion transformer. It addresses the challenge of recovering images with unknown degradations while preserving semantic consistency by introducing a dual-branch conditioner and timestep- and layer-adaptive modulation. LucidFlux enforces semantic alignment through SigLIP features, avoiding the latency and instability of text prompts. The framework outperforms state-of-the-art diffusion-based models across diverse degradations, achieving a CLIP-IQA+ score of 0.7406 on RealLQ250. LucidFlux offers AI practitioners a robust and efficient method for UIR without relying on textual captions, improving performance in scenarios with mixed and unknown degradations. |
| Natural Language Processing | WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level
  Feedback and Step-Level Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.22644) or [HuggingFace](https://huggingface.co/papers/2509.22644))| Zhuofan Zong, Yunqiao Yang, Houxing Ren, Zimu Lu, scikkk | The paper introduces WebGen-Agent, a novel approach to interactive website generation using multi-level feedback and step-level reinforcement learning. It aims to enhance website generation by incorporating visual aesthetics and user interaction feedback, which are often neglected by current code-generation agents. WebGen-Agent leverages screenshots and GUI-agent testing for iterative refinement, alongside Step-GRPO with screenshot and GUI-agent feedback for training. Experiments show that WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9% and its appearance score from 3.0 to 3.9 on the WebGen-Bench dataset. This framework provides AI practitioners a more comprehensive way to generate high-quality websites by integrating visual and functional assessments during the codebase generation. |
| Computer Vision | SPARK: Synergistic Policy And Reward Co-Evolving Framework (Read more on [arXiv](https://arxiv.org/abs/2509.22624) or [HuggingFace](https://huggingface.co/papers/2509.22624))|  | The paper introduces SPARK, a synergistic policy and reward co-evolving framework for improving Large Vision-Language Models (LVLMs) by internalizing the reward model. It aims to address the limitations of existing Reinforcement Learning (RL) approaches, such as high costs of human feedback and reward-policy mismatch. SPARK recycles valuable rollouts and correctness data to simultaneously train the model as a generative reward model using pointwise reward, pairwise comparison, and evaluation conditioned on reflection responses. SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks, 12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks. The framework allows test-time scaling via self-reflection without external reward models, reducing dependence on costly human preference data and external judge models. |
| Multi-Modal | See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned
  Aerial Navigation (Read more on [arXiv](https://arxiv.org/abs/2509.22653) or [HuggingFace](https://huggingface.co/papers/2509.22653))| Chih-Hai Su, Yang-Sen Lin, Chih Yao Hu, jayinnn, yuna0x0 | The paper introduces See, Point, Fly (SPF), a training-free aerial vision-and-language navigation framework for UAVs. It addresses the challenge of enabling UAVs to navigate based on free-form instructions in diverse environments. The key methodology involves using VLMs to decompose instructions into 2D waypoints on the input image, transforming these into 3D displacement vectors for UAV control, and adaptively adjusting travel distance. SPF achieves a 63% absolute margin improvement over previous state-of-the-art methods in a DRL simulation benchmark, suggesting a significant advance in zero-shot UAV navigation capabilities. |
| Computer Vision | Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in
  Subject-Driven Generation (Read more on [arXiv](https://arxiv.org/abs/2509.21989) or [HuggingFace](https://huggingface.co/papers/2509.21989))| Peter Wonka, Bernard Ghanem, Aleksandar Cvejic, abdo-eldesokey | The paper introduces Mind-the-Glitch, a pipeline for detecting visual inconsistencies in subject-driven image generation by computing visual correspondences using features from pre-trained diffusion models. It addresses the challenge of evaluating subject consistency by disentangling semantic and visual features and proposes a Visual Semantic Matching (VSM) metric. The key methodology involves an automated dataset generation pipeline with annotated correspondences and a contrastive architecture for feature separation. Empirical results demonstrate that VSM outperforms existing metrics like CLIP and DINO, achieving higher correlation with an oracle (0.582 Spearman). This offers AI practitioners a tool for quantifying and localizing inconsistencies, advancing subject-driven image generation. |
| Natural Language Processing | Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on
  Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval (Read more on [arXiv](https://arxiv.org/abs/2509.21710) or [HuggingFace](https://huggingface.co/papers/2509.21710))|  | This paper introduces Think-on-Graph 3.0 (ToG-3), a novel framework for enhancing Large Language Models (LLMs) with external knowledge via a Multi-Agent Dual-Evolving Context Retrieval (MACER) mechanism. It addresses the trade-off between graph-based RAG's dependence on high-quality graphs and the limitations of LLM extractors, particularly in resource-constrained scenarios. The methodology dynamically constructs a Chunk-Triplets-Community heterogeneous graph index incorporating dual evolution of Evolving Query and Evolving Sub-Graph for evidence retrieval, managed by Constructor, Retriever, Reflector, and Responser agents. Experiments demonstrate ToG-3 outperforms baselines on deep and broad reasoning benchmarks, achieving higher Exact Match and F1 scores on HotpotQA, 2WikiMultihopQA, and Musique; ablation studies confirmed the efficacy of the MACER framework. The main implication is an efficient and adaptive approach to graph-based RAG enabling deep reasoning even with lightweight LLMs. |
| Natural Language Processing | PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.19894) or [HuggingFace](https://huggingface.co/papers/2509.19894))| Lingpeng Kong, Zhuocheng Gong, Jian Guan, Wei Wu, xl-zhao | PromptCoT 2.0 is a framework for scalable prompt synthesis to enhance reasoning in LLMs. It addresses the lack of high-quality training data by using an EM loop to iteratively refine rationales for prompt construction. The key methodology involves generating synthetic prompts guided by iteratively refined rationales, enabling self-play and supervised fine-tuning.  Experiments show that PromptCoT 2.0 improves performance on benchmarks like AIME 24, achieving 92.1% accuracy in a self-play setting. This synthetic data generation provides a new avenue for scaling reasoning capabilities in open-source LLMs. |
| Multi-Modal | D-Artemis: A Deliberative Cognitive Framework for Mobile GUI
  Multi-Agents (Read more on [arXiv](https://arxiv.org/abs/2509.21799) or [HuggingFace](https://huggingface.co/papers/2509.21799))| Jinyuan Li, Yuqi Wang, Wenjie Lu, Yibo Feng, Hongze Mi | The paper introduces D-Artemis, a novel deliberative cognitive framework for mobile GUI multi-agents, designed to emulate human-like interaction and enhance automation. It addresses the data bottleneck, error detection costs, and contradictory guidance issues in current approaches. D-Artemis leverages fine-grained tip retrieval, proactive pre-execution alignment via a Thought-Action Consistency Check, and post-execution status reflection for strategic learning. Experimental results demonstrate state-of-the-art performance on AndroidWorld, achieving a 75.8% success rate, and 96.8% on ScreenSpot-V2. The framework enhances general-purpose MLLMs for GUI tasks without training on complex trajectory datasets, demonstrating strong generalization capabilities. |
| Computer Vision | UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models (Read more on [arXiv](https://arxiv.org/abs/2509.21760) or [HuggingFace](https://huggingface.co/papers/2509.21760))| Yuchao Gu, Lan Chen, HelenMao | This paper introduces UniVid, a framework for unifying diverse vision tasks using a pre-trained video generation model. It explores whether a pre-trained video generation model can adapt to various image and video tasks through supervised fine-tuning. UniVid fine-tunes a video diffusion transformer to address generation and pixel-level understanding tasks by representing them as visual sentences. The model achieves strong cross-modal and cross-source generalization despite being trained solely on natural video data, reaching 53.13% pixel accuracy on semantic segmentation. This suggests the potential for pre-trained video generation models to serve as a scalable foundation for general-purpose vision modeling, reducing reliance on task-specific pre-training. |
| Reinforcement Learning | Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive
  Exploration for Agentic Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.22601) or [HuggingFace](https://huggingface.co/papers/2509.22601))| Gang Li, Zhengbao He, Xiaoyu Tan, Yulei Qin, tedsun | The paper introduces SPEAR, a curriculum-based self-imitation learning recipe for training agentic LLMs. It aims to balance exploration-exploitation by managing policy entropy through intrinsic rewards and self-imitation, steering policy evolution across stages. The method utilizes auxiliary tool call rewards and strengthens self-imitation for comparative action-level exploration, while advantage recalibration and covariance-based clipping stabilize training. SPEAR demonstrates increased success rates in ALFWorld (up to 16.1%) and WebShop (up to 20.7%) compared to baselines. This approach offers a scalable technique for enhancing LLM agent performance in tasks requiring tool use and long-term planning. |
| Multi-Modal | X-Streamer: Unified Human World Modeling with Audiovisual Interaction (Read more on [arXiv](https://arxiv.org/abs/2509.21574) or [HuggingFace](https://huggingface.co/papers/2509.21574))| Guoxian Song, Chenxu Zhang, Zenan Li, You Xie, gutianpei | X-Streamer is a framework for building infinitely streamable digital humans capable of real-time audiovisual interaction. The paper explores the challenge of creating consistent and context-aware responses in digital humans across text, speech, and video modalities. The core methodology employs a Thinker-Actor dual-transformer architecture, unifying multimodal understanding and generation via chunk-wise autoregressive diffusion and cross-attention mechanisms. The framework achieves real-time performance on two A100 GPUs while maintaining long-range conversational coherence. X-Streamer offers AI practitioners a pathway toward unified world modeling of interactive digital humans, enabling new applications in entertainment, education, and agent technology. |
| Computer Vision | TUN3D: Towards Real-World Scene Understanding from Unposed Images (Read more on [arXiv](https://arxiv.org/abs/2509.21388) or [HuggingFace](https://huggingface.co/papers/2509.21388))| Anna Vorontsova, Alexey Zakharov, Bulat Gabdullin, Nikita Drozdov, Anton Konushin | The paper introduces TUN3D, a novel method for joint layout estimation and 3D object detection from multi-view images without requiring ground-truth camera poses or depth supervision. The research aims to relax data requirements in 3D scene understanding by processing images with and without camera poses. TUN3D employs a lightweight sparse-convolutional backbone with dedicated heads for 3D object detection and layout estimation, leveraging a parametric wall representation. Experiments show that TUN3D achieves state-of-the-art performance across three challenging scenarios, obtaining an F1 score of 66.6 on ScanNet for layout estimation. The method's ability to perform scene understanding from unposed images provides a practical solution for applications using consumer-grade cameras and prerecorded videos. |
| Natural Language Processing | Chasing the Tail: Effective Rubric-based Reward Modeling for Large
  Language Model Post-Training (Read more on [arXiv](https://arxiv.org/abs/2509.21500) or [HuggingFace](https://huggingface.co/papers/2509.21500))|  | This paper introduces a rubric-based reward modeling approach to address reward over-optimization in LLM post-training. The main objective is to improve reward model accuracy, particularly in the high-reward region, to better align LLMs. They propose an iterative workflow (Refinement-through-Differentiation) that leverages off-policy examples and rubric refinement. Experiments show that rubric-based rewards mitigate over-optimization and achieve significant improvements, with a top win-rate of 39.7% on the generalist domain using the "4 Great & Diverse Pairs" rubric. The implication for AI practitioners is a more effective strategy for reward modeling that reduces reward hacking and improves LLM alignment. |
| Computer Vision | RefAM: Attention Magnets for Zero-Shot Referral Segmentation (Read more on [arXiv](https://arxiv.org/abs/2509.22650) or [HuggingFace](https://huggingface.co/papers/2509.22650))| Federico Tombari, Muhammad Ferjad Naeem, Alessio Tonioni, Anna Kukleva, enisimsar | The paper introduces REFAM, a training-free framework for zero-shot referral segmentation using diffusion transformer models. It aims to improve grounding accuracy by addressing attention sinks and noise in cross-attention maps. The key method involves appending and filtering stop words as "attention magnets" to redistribute attention and refine heatmaps. REFAM achieves state-of-the-art zero-shot performance on both image and video segmentation benchmarks, such as improving mIoU by 2.5 on RefCOCOg test compared to HybridGL. The results suggest a novel and efficient approach to leverage pre-trained diffusion models for downstream vision-language tasks without fine-tuning or additional components. |
| Reinforcement Learning | WoW: Towards a World omniscient World model Through Embodied Interaction (Read more on [arXiv](https://arxiv.org/abs/2509.22642) or [HuggingFace](https://huggingface.co/papers/2509.22642))| Weishi Mi, Xiaozhu Ju, Chun-Kai Fan, Peidong Jia, Xiaowei Chi | The paper introduces WoW, a generative world model trained through embodied interaction to enhance physical intuition in AI. It investigates the hypothesis that authentic physical intuition requires causally-rich real-world interactions. The key methodology involves training a 14B-parameter model on 2 million robot interaction trajectories, refined through a vision-language model agent called SOPHIA. Results on WoWBench, a new benchmark for physical consistency, demonstrate state-of-the-art performance with 96.53% instruction understanding and 80.16% physical law adherence. WoW provides evidence that large-scale, real-world interaction is vital for developing physical intuition in AI models, with potential applications in robotics, VLM reasoning, and physical simulation. |
| Computer Vision | FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image
  Editing (Read more on [arXiv](https://arxiv.org/abs/2509.22244) or [HuggingFace](https://huggingface.co/papers/2509.22244))| Linghe Kong, Xiaohong Liu, Haotong Qin, Zhiteng Li, Junyi Wu | The paper introduces FlashEdit, a novel framework for real-time text-guided image editing. It aims to reduce latency and semantic entanglement while maintaining high-fidelity edits. The methodology includes a One-Step Inversion-and-Editing (OSIE) pipeline, Background Shield (BG-Shield), and Sparsified Spatial Cross-Attention (SSCA). Experiments demonstrate an over 150x speedup compared to DDIM+P2P while preserving background consistency and structural integrity. FlashEdit enables interactive and expressive creative tools for diffusion-based editing. |
| Computer Vision | ERGO: Efficient High-Resolution Visual Understanding for Vision-Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2509.21991) or [HuggingFace](https://huggingface.co/papers/2509.21991))| Ki-Ung Song, Seungmin Yang, Wooksu Shin, Jewon Lee, bokyeong1015 | The paper introduces ERGO, an efficient two-stage coarse-to-fine reasoning pipeline for high-resolution visual understanding in vision-language models. It aims to reduce computational overhead by first identifying task-relevant regions in downsampled images and then processing only these regions at full resolution. ERGO uses a reinforcement learning framework with a novel reward to guide the model to focus on informative regions, even with ambiguous visual cues. Experiments show that ERGO surpasses Qwen2.5-VL-7B on the V* benchmark by 4.7 points while using only 23% of the vision tokens, achieving a 3x inference speedup. This allows practitioners to achieve higher accuracy with greater efficiency in real-world vision-language applications. |
| Multi-Modal | Where MLLMs Attend and What They Rely On: Explaining Autoregressive
  Token Generation (Read more on [arXiv](https://arxiv.org/abs/2509.22496) or [HuggingFace](https://huggingface.co/papers/2509.22496))| Shiming Liu, Siyuan Liang, Kangwei Liu, Xiaoqing Guo, Ruoyu Chen | The paper introduces EAGLE, a black-box framework for explaining autoregressive token generation in Multimodal Large Language Models (MLLMs). It aims to determine the perceptual regions MLLMs attend to and quantify the reliance on language priors versus perceptual evidence. EAGLE uses an objective function unifying sufficiency and indispensability, optimized via greedy search over sparsified image regions. Experiments across open-source MLLMs show that EAGLE outperforms existing methods with an average improvement of 20.0% in insertion and 13.4% in deletion for image captioning, while requiring less GPU memory. The work enables AI practitioners to improve the interpretability and reliability of MLLMs by disentangling the influence of different modalities on token generation. |
| Computer Vision | HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion
  Models (Read more on [arXiv](https://arxiv.org/abs/2509.22300) or [HuggingFace](https://huggingface.co/papers/2509.22300))| Romann M. Weber, Farnood Salehi, msadat97 | The paper introduces HiGS, a novel history-guided sampling method to enhance image quality and efficiency in diffusion models. It addresses the issue of unrealistic outputs and lack of fine details in diffusion models by integrating recent model predictions into each inference step. HiGS leverages the difference between current and past predictions to steer the sampling process, improving details and structure without extra training or fine-tuning. Experiments demonstrate HiGS consistently improves image quality across diverse models and architectures, achieving a new state-of-the-art FID of 1.61 for unguided ImageNet generation at 256x256 with only 30 sampling steps. HiGS provides a plug-and-play enhancement for diffusion sampling, enabling faster generation with higher fidelity, thus providing immediate benefits to AI practitioners using diffusion models. |
| Natural Language Processing | StateX: Enhancing RNN Recall via Post-training State Expansion (Read more on [arXiv](https://arxiv.org/abs/2509.22630) or [HuggingFace](https://huggingface.co/papers/2509.22630))| Zhiyuan Liu, Xu Han, Zhen Leng Thai, Xingyu Shen, chen-yingfa | The paper introduces StateX, a training pipeline to efficiently expand the state size of pre-trained Recurrent Neural Networks (RNNs) through post-training to enhance recall ability. The primary research objective is to improve the recall and in-context learning capabilities of RNNs without significant increase in training costs or parameter count. They propose architectural modifications for linear attention and state space models to scale up the state size with minimal parameter overhead, and demonstrate improved performance on models up to 1.3B parameters. Experiments show a relative accuracy gain in recall-intensive tasks of 3.36% for GLA and 1.1% for Mamba2. StateX provides AI practitioners with a cost-effective way to improve long-context recall in RNNs, potentially bridging the performance gap with Transformers. |
| Multi-Modal | X-CoT: Explainable Text-to-Video Retrieval via LLM-based
  Chain-of-Thought Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.21559) or [HuggingFace](https://huggingface.co/papers/2509.21559))| Raghuveer Rao, Sohail Dianat, Majid Rabbani, Jiamian Wang, prasannareddyp | The paper introduces X-CoT, an explainable text-to-video retrieval framework using LLM-based chain-of-thought reasoning. It addresses the limitations of embedding model-based systems, which lack interpretability and are sensitive to data quality. The methodology involves augmenting benchmark datasets with video annotations and devising a retrieval CoT for pairwise comparisons. X-CoT improves retrieval performance, achieving a +5.6% increase in R@1 on MSVD using CLIP. This framework enables users to assess retrieval models, examine data quality, and interpret ranking results, enhancing the trustworthiness and transparency of text-to-video retrieval. |
| Computer Vision | Real-Time Object Detection Meets DINOv3 (Read more on [arXiv](https://arxiv.org/abs/2509.20787) or [HuggingFace](https://huggingface.co/papers/2509.20787))| Xi Shen, Xuanlong Yu, Longfei Liu, Yongjie Hou, Shihua Huang | The paper introduces DEIMv2, a real-time object detection framework leveraging DINOv3 features. It aims to improve the performance-cost trade-off for object detection across various deployment scenarios. The methodology involves a Spatial Tuning Adapter (STA) to convert DINOv3's single-scale output into multi-scale features, a simplified decoder, and enhanced Dense O2O augmentation. DEIMv2-X achieves 57.8 AP on COCO with 50.3M parameters. The work provides AI practitioners with a versatile and scalable object detection solution spanning from ultra-lightweight to high-performance models. |
| Natural Language Processing | CHURRO: Making History Readable with an Open-Weight Large
  Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition (Read more on [arXiv](https://arxiv.org/abs/2509.19768) or [HuggingFace](https://huggingface.co/papers/2509.19768))|  | This paper introduces CHURRO, a 3B-parameter open-weight VLM, for high-accuracy, low-cost historical text recognition. The research aims to address the limitations of existing VLMs in reading diverse and degraded historical documents. CHURRO is trained on CHURRO-DS, a unified dataset of 155 historical corpora, and fine-tuned to perform page-level OCR and HTR. The model achieves 82.3% and 70.1% normalized Levenshtein similarity on printed and handwritten CHURRO-DS test sets, respectively, outperforming other VLMs. CHURRO provides AI practitioners with a specialized and efficient tool for historical text digitization and analysis, enabling community-driven research. |
| Computer Vision | Finding 3D Positions of Distant Objects from Noisy Camera Movement and
  Semantic Segmentation Sequences (Read more on [arXiv](https://arxiv.org/abs/2509.20906) or [HuggingFace](https://huggingface.co/papers/2509.20906))| Eija Honkavaara, Arno Solin, Julppe1 | This paper addresses the problem of localizing distant objects from noisy camera movement and semantic segmentation sequences. The objective is to accurately estimate the 3D position of objects detected in camera images, even with noisy camera pose estimates and imperfect segmentation. The method employs a particle filter that iteratively refines the object's location and uncertainty based on GNSS-estimated camera poses and image segments. Experiments using both simulated and drone-captured data demonstrate the method's effectiveness; in real-world data, it achieved a mean RMSE of 92.00 meters. This approach enables practical localization tasks in scenarios with limited computational resources or poor telecommunication, particularly for applications like drone-based wildfire monitoring. |
| Natural Language Processing | Instruction-Following Evaluation in Function Calling for Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2509.18420) or [HuggingFace](https://huggingface.co/papers/2509.18420))| NikolaiSkripko | This paper introduces IFEval-FC, a benchmark to evaluate instruction following in function calling for Large Language Models. It assesses whether LLMs adhere to format instructions embedded within JSON schema descriptions. The key methodology involves 750 test cases with embedded formats for input parameters and corresponding user queries. Results show that even state-of-the-art models like GPT-5 and Claude Opus 4.1 frequently fail to follow basic formatting rules, with best model reaching 79.87% average accuracy. This highlights a limitation for practical applications in real-world agent systems. |
