

## Papers for 2025-09-30

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Machine Learning | SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable
  Sparse-Linear Attention (Read more on [arXiv](https://arxiv.org/abs/2509.24006) or [HuggingFace](https://huggingface.co/papers/2509.24006))|  | The paper introduces Sparse-Linear Attention (SLA), a novel attention mechanism for diffusion transformers that addresses the quadratic complexity bottleneck. SLA decomposes attention weights into sparse high-rank and dense low-rank components, applying sparse and linear attention respectively to reduce computational cost. This method achieves a 20x reduction in attention computation in DiT models without sacrificing generation quality, demonstrated by reducing attention computation by 95% without degrading end-to-end generation quality. The efficient GPU kernel implementation of SLA yields a 13.7x speedup in attention computation and 2.2x end-to-end acceleration on video generation tasks. SLA enables faster and more efficient video generation by strategically combining sparse and linear attention mechanisms. |
| Machine Learning | Multiplayer Nash Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2509.23102) or [HuggingFace](https://huggingface.co/papers/2509.23102))|  | The paper introduces Multiplayer Nash Preference Optimization (MNPO), a novel framework to align LLMs with complex human preferences by generalizing Nash learning from human feedback to the multiplayer regime. It addresses the limitations of two-player Nash learning by formulating alignment as an n-player game, enhancing competitive dynamics and covering diverse preference structures. The key methodology involves developing an algorithm where each policy competes against a population while being regularized toward a reference model, and establishing well-defined Nash equilibria. Empirical evaluations on instruction-following benchmarks demonstrate that MNPO consistently outperforms existing NLHF baselines, achieving superior alignment quality, with MNPO achieving a score of 57.27 on AlpacaEval 2.0. MNPO provides a more scalable foundation for aligning LLMs with complex, non-transitive human preferences, improving robustness in multi-agent alignment scenarios. |
| Multi-Modal | RealUnify: Do Unified Models Truly Benefit from Unification? A
  Comprehensive Benchmark (Read more on [arXiv](https://arxiv.org/abs/2509.24897) or [HuggingFace](https://huggingface.co/papers/2509.24897))| Yuran Wang, Yue Ding, zooblastlbz, THUdyh, DogNeverSleep | This paper introduces RealUnify, a novel benchmark for evaluating bidirectional capability synergy in unified multimodal models. It aims to determine if architectural unification enables synergetic interaction between visual understanding and generation. The benchmark comprises 1,000 human-annotated instances across 10 categories and 32 subtasks, using a dual-evaluation protocol (direct and stepwise). Evaluations of 12 leading unified models revealed that they struggle to achieve effective synergy, with a best open-source model achieving only 37.5% accuracy on UEG tasks. The results highlight the need for improved training strategies and inductive biases to fully realize the potential of unified multimodal modeling. |
| Computer Vision | OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation
  and Editing (Read more on [arXiv](https://arxiv.org/abs/2509.24900) or [HuggingFace](https://huggingface.co/papers/2509.24900))| Huanyu Zhang, Chaoyou Fu, Xuehai Bai, Zhihong Chen, DogNeverSleep | The paper introduces OpenGPT-4o-Image, a large-scale dataset for advancing image generation and editing in multimodal AI. It addresses the gap in comprehensive training data by providing 80,000 instruction-image pairs across 11 domains and 51 subtasks. The methodology combines a hierarchical task taxonomy with automated data generation using structured resource pools and GPT-4o. Fine-tuning leading models on the dataset achieved up to 18% improvement on editing tasks (UniWorld-V1 on ImgEdit-Bench). The dataset and methodology facilitate the development of more capable and robust multimodal systems by enabling systematic data construction. |
| Multi-Modal | Visual Jigsaw Post-Training Improves MLLMs (Read more on [arXiv](https://arxiv.org/abs/2509.25190) or [HuggingFace](https://huggingface.co/papers/2509.25190))| Lewei Lu, Yushan Zhang, Penghao Wu, luodian, Paranioar | This paper introduces Visual Jigsaw, a self-supervised post-training framework for improving visual understanding in multimodal large language models (MLLMs). The research aims to enhance MLLMs' perception capabilities without altering their architecture by formulating a general ordering task across visual modalities. Visual Jigsaw involves partitioning, shuffling, and reconstructing visual inputs, deriving supervisory signals automatically. Experiments demonstrate substantial improvements in fine-grained perception; for example, the Image Jigsaw increased performance on DA-2K by 17.11. The framework offers a generic and effective way to strengthen visual understanding in MLLMs, potentially inspiring further research on vision-centric pretext tasks. |
| Computer Vision | SANA-Video: Efficient Video Generation with Block Linear Diffusion
  Transformer (Read more on [arXiv](https://arxiv.org/abs/2509.24695) or [HuggingFace](https://huggingface.co/papers/2509.24695))|  | The paper presents SANA-Video, an efficient diffusion model for high-resolution video generation. It addresses the challenge of computationally expensive video generation by employing a block linear diffusion transformer architecture. The method utilizes linear attention and a constant-memory KV cache to synthesize videos up to 720x1280 resolution with significantly reduced memory requirements and latency. SANA-Video achieves competitive performance compared to other small diffusion models, showcasing a 16x speedup in generation latency, and can generate a 5-second 720p video in 29 seconds on an RTX 5090 GPU using NVFP4 precision. This enables faster and more accessible high-quality video generation for AI practitioners. |
| Machine Learning | Democratizing AI scientists using ToolUniverse (Read more on [arXiv](https://arxiv.org/abs/2509.23426) or [HuggingFace](https://huggingface.co/papers/2509.23426))|  | The paper introduces ToolUniverse, an ecosystem designed to democratize AI scientist development by providing standardized tools and environments. It addresses the challenge of building bespoke AI systems by unifying tools, data, and analyses into a common ecosystem. ToolUniverse integrates over 600 machine learning models, datasets, and APIs, automatically refining tool interfaces and composing tools into agentic workflows. A case study on hypercholesterolemia demonstrates the platform's utility, identifying a potent drug analog with favorable properties, and a general ecosystem for constructing AI scientists is introduced. The ToolUniverse ecosystem benefits AI practitioners, as it combines large language models with other AI reasoning models to scale the building of AI scientists. |
| Natural Language Processing | When Does Reasoning Matter? A Controlled Study of Reasoning's
  Contribution to Model Performance (Read more on [arXiv](https://arxiv.org/abs/2509.22193) or [HuggingFace](https://huggingface.co/papers/2509.22193))|  | The paper investigates the effectiveness of reasoning in large language models (LLMs) across various tasks and model scales. It explores how reasoning impacts performance by comparing instruction fine-tuning (IFT) models against reasoning models, controlling for factors like data and model capacity through a synthetic data distillation framework. Results demonstrate that reasoning consistently improves performance, often matching or exceeding larger IFT systems, especially on reasoning-intensive and open-ended tasks; reasoning models become valuable as model size scales, overcoming IFT limits. The study suggests that practitioners can leverage reasoning models for tasks that benefit from complex problem-solving, particularly as model size increases, despite the higher training and inference costs. |
| Multi-Modal | GSM8K-V: Can Vision Language Models Solve Grade School Math Word
  Problems in Visual Contexts (Read more on [arXiv](https://arxiv.org/abs/2509.25160) or [HuggingFace](https://huggingface.co/papers/2509.25160))|  | The paper introduces GSM8K-V, a purely visual multi-image mathematical reasoning benchmark derived from GSM8K. It aims to assess vision language models' (VLMs) ability to solve math word problems presented visually rather than textually. The methodology involves an automated image-generation pipeline combined with human annotation to convert GSM8K samples into visual form, resulting in 1,319 high-quality samples. Evaluations show that the best-performing model, Gemini-2.5-Pro, achieves 46.93% accuracy on GSM8K-V, compared to 95.22% on GSM8K. This highlights a significant gap in VLMs' visual mathematical reasoning capabilities, implying a need for more robust and generalizable models in understanding and reasoning over visual contexts. |
| Natural Language Processing | EasySteer: A Unified Framework for High-Performance and Extensible LLM
  Steering (Read more on [arXiv](https://arxiv.org/abs/2509.25175) or [HuggingFace](https://huggingface.co/papers/2509.25175))|  | The paper introduces EasySteer, a unified framework for high-performance and extensible large language model (LLM) steering. The research aims to address the limitations of existing steering frameworks related to computational efficiency and extensibility. EasySteer leverages vLLM's optimized inference engine and provides a modular architecture for both analysis-based and learning-based steering methods. Experiments demonstrate a 5.5-11.4x speedup over existing frameworks, with applications such as overthinking mitigation achieving a 40% reduction in tokens. EasySteer provides a production-ready capability for deploying controllable language models, improving efficiency and enabling greater control over LLM behavior. |
| Computer Vision | EditScore: Unlocking Online RL for Image Editing via High-Fidelity
  Reward Modeling (Read more on [arXiv](https://arxiv.org/abs/2509.23909) or [HuggingFace](https://huggingface.co/papers/2509.23909))|  | This paper introduces EditScore, a high-fidelity reward model for unlocking online reinforcement learning in image editing. The main objective is to overcome the lack of efficient and accurate reward signals that hinder the application of RL in image editing. The authors develop EditReward-Bench, a benchmark to evaluate reward models, and subsequently create EditScore, a series of reward models that match proprietary VLMs in performance. Experimentally, using EditScore enables efficient policy optimization and improves image editing quality, resulting in substantial performance uplifts. The development of domain-specialized reward models like EditScore is presented as key to realizing the full potential of RL in image editing. |
| Natural Language Processing | SparseD: Sparse Attention for Diffusion Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.24014) or [HuggingFace](https://huggingface.co/papers/2509.24014))| Xinchao Wang, Xinyin Ma, Gongfan Fang, adamdad, INV-WZQ | This paper introduces SparseD, a sparse attention mechanism for accelerating diffusion language models (DLMs). It aims to reduce the quadratic complexity of attention in DLMs by identifying and leveraging sparse attention patterns, specifically varying attention across heads, consistent patterns across denoising steps, and the criticality of early denoising stages. SparseD pre-computes head-specific sparse patterns and reuses them across denoising steps, using full attention initially before switching to sparse attention to maintain quality. Experiments show that SparseD achieves up to 1.50x speedup over FlashAttention at a 64k context length with 1,024 steps while preserving accuracy. The method offers a practical solution for deploying DLMs in long-context applications by improving inference efficiency. |
| Natural Language Processing | Towards Personalized Deep Research: Benchmarks and Evaluations (Read more on [arXiv](https://arxiv.org/abs/2509.25106) or [HuggingFace](https://huggingface.co/papers/2509.25106))|  | This paper introduces Personalized Deep Research Bench, a new benchmark for evaluating personalization in Deep Research Agents (DRAs). The research aims to bridge the gap in existing evaluations by pairing 50 research tasks with 25 authentic user profiles, creating 250 realistic user-task queries. The study proposes the PQR Evaluation Framework, measuring Personalization Alignment, Content Quality, and Factual Reliability. Experiments across various systems revealed capabilities and limitations, with a best overall performance of 6.64 achieved by OAgents under Task w/Persona. This work establishes a foundation for personalized AI research assistants. |
| Natural Language Processing | Sequential Diffusion Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.24007) or [HuggingFace](https://huggingface.co/papers/2509.24007))|  | The paper introduces Sequential Diffusion Language Models (SDLMs) to address limitations of fixed-length decoding and KV-cache incompatibility in diffusion language models. It proposes Next Sequence Prediction (NSP) to unify next-token and next-block prediction, enabling adaptive generation length. SDLM performs diffusion inference within fixed-size blocks but dynamically decodes subsequences based on model confidence, preserving KV-cache compatibility. Experiments demonstrate that SDLM matches or surpasses autoregressive baselines with only 3.5M training samples, achieving a 2.1x throughput improvement over Qwen-2.5. The key implication is a new method for retrofitting pre-trained autoregressive models to improve inference efficiency with minimal training cost and strong scalability potential. |
| Computer Vision | VideoScore2: Think before You Score in Generative Video Evaluation (Read more on [arXiv](https://arxiv.org/abs/2509.22799) or [HuggingFace](https://huggingface.co/papers/2509.22799))|  | This paper introduces VIDEOSCORE2, a multi-dimensional framework for evaluating AI-generated videos, focusing on visual quality, text alignment, and physical consistency. The research aims to provide a more comprehensive and interpretable video evaluation compared to existing methods. VIDEOSCORE2 is trained using a two-stage pipeline of supervised fine-tuning followed by reinforcement learning with Group Relative Policy Optimization on a large-scale human-annotated dataset.  Experiments demonstrate superior performance with 44.35 accuracy on the in-domain VIDEOSCORE-BENCH-v2 and 50.37 average performance across four out-of-domain benchmarks. This framework allows for more effective reward modeling and Best-of-N sampling, facilitating the development of better video generation models. |
| Reinforcement Learning | Random Policy Valuation is Enough for LLM Reasoning with Verifiable
  Rewards (Read more on [arXiv](https://arxiv.org/abs/2509.24981) or [HuggingFace](https://huggingface.co/papers/2509.24981))| Binxing Jiao, Chen Hu, Qingpeng Cai, Yuxiao Ye, Haoran He | The paper introduces Random Policy Valuation for Diverse Reasoning (ROVER), a novel RL algorithm for improving reasoning in LLMs. It addresses training instability and diversity collapse in current RL methods by proving that the optimal action can be recovered from the Q-function of a fixed, uniformly random policy. ROVER samples actions from a softmax over uniform-policy Q-values, preserving diversity throughout training. Across multiple benchmarks, ROVER demonstrates superior performance in both quality (+8.2 on pass@1, +16.8 on pass@256) and diversity (+17.6%). This simplified approach allows practitioners to achieve improved reasoning and generalization capabilities with minimal complexity compared to existing methods. |
| Computer Vision | Euclid's Gift: Enhancing Spatial Perception and Reasoning in
  Vision-Language Models via Geometric Surrogate Tasks (Read more on [arXiv](https://arxiv.org/abs/2509.24473) or [HuggingFace](https://huggingface.co/papers/2509.24473))|  | This paper introduces a method to enhance spatial perception and reasoning in vision-language models (MLLMs) using geometric surrogate tasks. The research aims to improve spatial intelligence in MLLMs by fine-tuning them on a new dataset, Euclid30K, comprising plane and solid geometry problems. The methodology involves using Group Relative Policy Optimization (GRPO) to train models such as Qwen2.5VL and RoboBrain2.0. Results show a VSI-Bench accuracy increase of 5.5 percentage points across evaluated models after training on Euclid30K, with RoboBrain2.0-Euclid-7B achieving 49.6% accuracy. This indicates that geometry-centric fine-tuning can confer broadly transferable spatial skills to vision-language models. |
| Reinforcement Learning | Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach
  for LLM Reasoning in RLVR (Read more on [arXiv](https://arxiv.org/abs/2509.23808) or [HuggingFace](https://huggingface.co/papers/2509.23808))| Cevaaa, MasterVito, AnikiFan, Gambel, Niugan | This paper introduces Velocity-Exploiting Rank-Learning (VERL) to enhance LLM reasoning in RLVR by decoupling exploration and exploitation. It investigates hidden-state dynamics using Effective Rank (ER) and its derivatives, Effective Rank Velocity (ERV) and Effective Rank Acceleration (ERA), to create a synergistic incentive structure. VERL shapes the RL advantage function based on ERA to prospectively amplify rewards for exploration and reinforce exploitative gains. The method achieves up to 21.4% absolute accuracy improvement on the Gaokao 2024 dataset. This suggests a new direction for RLVR by shaping the advantage function. |
| Reinforcement Learning | From f(x) and g(x) to f(g(x)): LLMs Learn New Skills in RL by
  Composing Old Ones (Read more on [arXiv](https://arxiv.org/abs/2509.25123) or [HuggingFace](https://huggingface.co/papers/2509.25123))| Hanbin Wang, Ganqu Cui, Yuchen Zhang, Weize Chen, Lifan Yuan | This paper demonstrates that Large Language Models (LLMs) can acquire genuinely new skills in reinforcement learning (RL) by composing existing ones, mirroring human cognitive skill acquisition. The research investigates whether RL teaches LLMs new skills or merely activates existing ones using a synthetic framework based on string transformation functions. Key methodology involves training LLMs on atomic skills and then using RL to enable them to learn unseen compositions of these skills. Results show that RL enables LLMs to generalize to more difficult problems, improving performance on unseen Level-3 tasks to 30% compared to near-zero without RL. This implies that building base models with basic skills, followed by RL with appropriate incentivization, can lead to more advanced and generalizable skills. |
| Reinforcement Learning | Critique-Coder: Enhancing Coder Models by Critique Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2509.22824) or [HuggingFace](https://huggingface.co/papers/2509.22824))|  | This paper introduces Critique Reinforcement Learning (CRL), a new paradigm that enhances coder models by incorporating critique as part of the reinforcement learning process. The main objective is to address the lack of explicit critique and reflection mechanisms in existing RL methods for LLMs. CRL trains models to generate critiques for question-solution pairs, with rewards based on the alignment between the critique's judgment and the ground truth. The proposed CRITIQUE-CODER, trained with a hybrid RL and CRL approach, achieves improved performance on coding benchmarks, notably reaching 60.8% on LiveCodeBench (v5), outperforming other reasoning models. CRL enhances general reasoning abilities transferable across tasks, implying it can complement RL for LLM reasoning. |
| Computer Vision | VGGT-X: When VGGT Meets Dense Novel View Synthesis (Read more on [arXiv](https://arxiv.org/abs/2509.25191) or [HuggingFace](https://huggingface.co/papers/2509.25191))| Zhaoxiang Zhang, Junran Peng, Zimo Tang, Chuanchen Luo, Yang Liu | The paper introduces VGGT-X, an improved 3D foundation model for dense novel view synthesis (NVS). It addresses the challenges of high VRAM usage and noisy outputs when scaling 3D foundation models to dense multi-view settings. VGGT-X incorporates memory-efficient implementation, adaptive global alignment, and robust 3D Gaussian Splatting (3DGS) training. The proposed method achieves state-of-the-art results in dense COLMAP-free NVS and pose estimation, significantly closing the fidelity gap compared to COLMAP-initialized pipelines, achieving 0.992 AUC@30 on MipNeRF360 dataset. VGGT-X enables more scalable and robust NVS, providing insights for future 3D foundation model development. |
| Computer Vision | BRIDGE - Building Reinforcement-Learning Depth-to-Image Data Generation
  Engine for Monocular Depth Estimation (Read more on [arXiv](https://arxiv.org/abs/2509.25077) or [HuggingFace](https://huggingface.co/papers/2509.25077))|  | This paper introduces BRIDGE, a reinforcement-learning-optimized depth-to-image (D2I) data generation engine for monocular depth estimation (MDE). The research aims to overcome limitations in data scarcity and quality for MDE by generating a large-scale, high-quality RGB-D dataset from source depth maps. BRIDGE synthesizes over 20 million realistic and geometrically accurate RGB images using an RL-optimized D2I model and trains a depth estimation model with a hybrid supervision strategy. The method achieves state-of-the-art performance on several benchmarks, including a 97.3% accuracy on the DA2K dataset, demonstrating superior detail capture and robustness. The innovation of BRIDGE enables the creation of massive, high-quality datasets for training more effective and generalizable MDE models, improving depth prediction in complex scenes. |
| Computer Vision | Rolling Forcing: Autoregressive Long Video Diffusion in Real Time (Read more on [arXiv](https://arxiv.org/abs/2509.25161) or [HuggingFace](https://huggingface.co/papers/2509.25161))|  | The paper introduces Rolling Forcing, a novel autoregressive technique for real-time, long-horizon video diffusion. It addresses the challenge of error accumulation in streaming video generation while maintaining temporal consistency and real-time performance. The methodology incorporates a rolling-window joint denoising scheme, attention sink mechanism for global context anchoring, and an efficient training algorithm to mitigate exposure bias. Results show real-time performance at 16 fps on a single GPU and a substantially lower drift as measured by \(\Delta_{\text{Quality}}\)=0.01, outperforming existing methods.  Rolling Forcing offers AI practitioners a method for generating extended, coherent video streams with minimal error accumulation in real-time interactive applications. |
| Multi-Modal | MMPB: It's Time for Multi-Modal Personalization (Read more on [arXiv](https://arxiv.org/abs/2509.22820) or [HuggingFace](https://huggingface.co/papers/2509.22820))|  | The paper introduces MMPB, a novel benchmark for evaluating Vision-Language Model (VLM) personalization. It aims to address the gap in evaluating VLMs' ability to adapt to individual users' visual concepts and preferences. The benchmark includes 10k image-query pairs across 111 personalizable concepts, categorized into humans, animals, objects, and characters, and structures personalization into three task types: concept injection, multi-turn dialogue, and personalized querying. Evaluations of 23 VLMs reveal limitations in maintaining consistency, handling user preferences, and adapting to visual cues; for example, even top VLMs on general benchmarks struggle with preference-grounded tasks.  MMPB offers insights and a foundation for truly personalized multi-modal AI. |
| Natural Language Processing | InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long
  Adaptation (Read more on [arXiv](https://arxiv.org/abs/2509.24663) or [HuggingFace](https://huggingface.co/papers/2509.24663))| Yuxuan Li, Chaojun Xiao, Zhou Su, Zihan Zhou, Weilin Zhao | The paper introduces InfLLM-V2, a dense-sparse switchable attention mechanism to improve long-sequence processing in large language models. It addresses the computational and memory bottlenecks of self-attention by reusing dense attention parameters and smoothly transitioning to sparse attention for longer sequences. The method achieves 4x faster performance than dense attention while retaining 98.1% of performance on long-context understanding. This seamless short-to-long adaptation maintains consistency between processing sequences and reduces training instability.  InfLLM-V2 provides an efficient implementation for the research community. |
| Natural Language Processing | Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference
  Learning (Read more on [arXiv](https://arxiv.org/abs/2509.23285) or [HuggingFace](https://huggingface.co/papers/2509.23285))|  | This paper addresses the challenge of inefficient tool usage in Tool-Integrated Reasoning (TIR) for large language models. It explores the impact of tool calls on information entropy and proposes Tool-Light, a framework incorporating continuous self-evolved sampling and a two-stage fine-tuning approach (SFT and self-evolved DPO). The key methodology uses a hybrid sampling technique combining vanilla and entropy-guided approaches to construct a high-quality dataset. Experimental results on 10 datasets show Tool-Light improves the model's efficiency in executing TIR tasks and achieves better performance in Efficiency and Necessity metrics. The main implication is that optimizing for entropy during dataset construction and using a self-evolved training process can significantly improve the effectiveness and accuracy of tool-integrated reasoning in LLMs. |
| Multi-Modal | MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech (Read more on [arXiv](https://arxiv.org/abs/2509.25131) or [HuggingFace](https://huggingface.co/papers/2509.25131))|  | MGM-Omni is a unified Omni LLM for omni-modal understanding and expressive, long-horizon personalized speech generation. The research aims to overcome limitations in existing systems by enabling efficient cross-modal interaction and low-latency, streaming speech generation. It adopts a dual-track, token-based architecture and a chunk-based parallel decoding scheme. Experiments demonstrate that MGM-Omni outperforms existing open-source models in preserving timbre identity, producing natural speech, and achieving superior long-form audio understanding. The model achieves competitive ASR results with 4.0 CER on CommonVoice (ZH) and 1.8 CER on AISHELL, offering AI practitioners an efficient paradigm for omnimodal understanding and controllable speech generation. |
| Multi-Modal | HunyuanImage 3.0 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2509.23951) or [HuggingFace](https://huggingface.co/papers/2509.23951))|  | HunyuanImage 3.0 is a native multimodal model unifying understanding and generation across text and images within an autoregressive framework. The paper aims to develop an open-source image generation model rivaling closed-source alternatives by extending a pre-trained Mixture-of-Experts large language model (LLM). The approach involves meticulous data curation, advanced architecture design with a Chain-of-Thoughts schema, and progressive model training. Results demonstrate HunyuanImage 3.0 achieves comparable performance to state-of-the-art models, with a relative win rate of 14.10% compared to HunyuanImage 2.1 in GSB evaluation. This offers AI practitioners a powerful and publicly accessible multimodal foundation for exploring new ideas and fostering a vibrant multimodal ecosystem. |
| Natural Language Processing | Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs
  at Test Time (Read more on [arXiv](https://arxiv.org/abs/2509.22572) or [HuggingFace](https://huggingface.co/papers/2509.22572))| Yi Yang, Ruijie Quan, Fan Ma, yixuan7878 | The paper introduces Dynamic Experts Search (DES), a test-time scaling strategy to enhance reasoning in Mixture-of-Experts (MoE) LLMs. DES addresses the limitation of output-level sampling in existing methods by controlling expert activation during inference. The methodology integrates Dynamic MoE for direct expert count control and Expert Configuration Inheritance for consistent expert count within reasoning paths. Experiments demonstrate that DES outperforms TTS baselines, achieving higher accuracy and stability without additional cost (e.g., improving accuracy on MATH benchmarks). DES provides a practical and scalable architecture-aware TTS approach by harnessing structural flexibility in LLMs. |
| Reinforcement Learning | SIRI: Scaling Iterative Reinforcement Learning with Interleaved
  Compression (Read more on [arXiv](https://arxiv.org/abs/2509.25176) or [HuggingFace](https://huggingface.co/papers/2509.25176))|  | The paper introduces SIRI, a reinforcement learning approach for large reasoning models that improves efficiency and accuracy by interleaving compression and expansion of the reasoning budget. The research aims to overcome the performance trade-off when reducing repetitive thinking in large reasoning models. SIRI dynamically adjusts the maximum rollout length during training, alternating between compressing and expanding reasoning, using a cosine scheduler. Results show SIRI-low improves AIME24 performance by 43.2% with a 46.9% token reduction after three iterations, while SIRI-high achieves the highest accuracy among compared methods. The method demonstrates potential for dynamically balancing exploration and efficiency in LRM reasoning, converging toward an optimal point. |
| Natural Language Processing | Scaling Generalist Data-Analytic Agents (Read more on [arXiv](https://arxiv.org/abs/2509.25084) or [HuggingFace](https://huggingface.co/papers/2509.25084))|  | The paper introduces DATAMIND, a scalable data synthesis and agent training recipe for constructing generalist data-analytic agents. It addresses challenges in building open-source data-analytic agents, including insufficient data and unstable rollout. The methodology involves a fine-grained task taxonomy, knowledge-augmented trajectory sampling, and a dynamically adjustable training objective. Trained on DATAMIND-12K, DATAMIND-14B achieves a state-of-the-art average score of 71.16% on multiple data analysis benchmarks. This work offers actionable insights about agent training for the community and provides open-source resources. |
| Natural Language Processing | From Harm to Help: Turning Reasoning In-Context Demos into Assets for
  Reasoning LMs (Read more on [arXiv](https://arxiv.org/abs/2509.23196) or [HuggingFace](https://huggingface.co/papers/2509.23196))| Nie Zheng, Zihang Fu, Weida Liang, Haonan Wang, tyzhu | This paper addresses the issue of reasoning LLMs (RLMs) performing worse with few-shot Chain-of-Thought (CoT) prompting compared to direct answering. It investigates the mechanisms behind this decline, identifying semantic misguidance and strategy transfer failure. The authors introduce Insight-to-Solve (I2S), a sequential test-time procedure that transforms demonstrations into reusable insights and generates target-specific reasoning traces. Experiments demonstrate that I2S and I2S+ consistently outperform direct answering, with GPT-4.1 achieving a +14.0% improvement on AIME'25. The results suggest that in-context demonstrations can be effectively leveraged via an insight-refine-solve framework to improve RLM performance. |
| Natural Language Processing | Taming Masked Diffusion Language Models via Consistency Trajectory
  Reinforcement Learning with Fewer Decoding Step (Read more on [arXiv](https://arxiv.org/abs/2509.23924) or [HuggingFace](https://huggingface.co/papers/2509.23924))|  | This paper introduces methods to improve Masked Diffusion Language Models (MDLMs) by addressing limitations in decoding strategies and reinforcement learning. The research aims to enhance MDLMs with fewer decoding steps. Key methodologies include EOS Early Rejection (EOSER), Ascending Step-Size (ASS) decoding, and Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO). Experiments on reasoning tasks show that the proposed methods, particularly CJ-GRPO combined with EOSER and ASS, achieve competitive performance with fewer decoding steps, as evidenced by results on Sudoku puzzles with CJ-GRPO+EOSER outperforming other methods. The main implication is a potential for more efficient MDLMs for AI practitioners. |
| Reinforcement Learning | Efficient Multi-turn RL for GUI Agents via Decoupled Training and
  Adaptive Data Curation (Read more on [arXiv](https://arxiv.org/abs/2509.23866) or [HuggingFace](https://huggingface.co/papers/2509.23866))|  | This paper introduces DART, a decoupled agentic reinforcement learning framework for GUI agents designed to address the challenges of slow interactions and insufficient high-quality training data. The research focuses on improving training efficiency and learning effectiveness in GUI automation tasks. DART separates the training system into four asynchronous modules: environment cluster, rollout service, data manager, and trainer, coupled with an adaptive data curation scheme involving pre-collecting trajectories, dynamic adjustment of rollout parameters, high-entropy step training, and truncated importance sampling. Evaluated on the OS-World benchmark, DART-GUI-7B achieves a 42.13% task success rate, a 14.61% absolute gain over the base model, and 7.34% higher than open-source SOTA. The framework enhances training efficiency and performance of RL-based GUI agents by decoupling training components and curating high-quality training data. |
| Reinforcement Learning | Rethinking Large Language Model Distillation: A Constrained Markov
  Decision Process Perspective (Read more on [arXiv](https://arxiv.org/abs/2509.22921) or [HuggingFace](https://huggingface.co/papers/2509.22921))|  | This paper presents a novel LLM distillation approach formulated as a constrained reinforcement learning problem. The research aims to maximize task-specific rewards while constraining the divergence from the teacher model below a defined threshold. The methodology adapts constrained state augmented reinforcement learning by modifying the reward function to maintain theoretical guarantees without state augmentation, teacher access during deployment, or dual Lagrangian overhead. Experiments on mathematical reasoning tasks demonstrate improved constraint satisfaction and reasoning compared to soft Lagrangian relaxation baselines. The approach achieves a superior balance balancing divergence minimization, reward maximization, and reasoning quality. |
| Computer Vision | Hyperspherical Latents Improve Continuous-Token Autoregressive
  Generation (Read more on [arXiv](https://arxiv.org/abs/2509.24335) or [HuggingFace](https://huggingface.co/papers/2509.24335))| Hui Xue, guolinke | The paper introduces SphereAR, a novel autoregressive model that improves image generation quality by leveraging hyperspherical latent spaces. The research aims to address the variance collapse issue in continuous-token autoregressive models by constraining AR inputs and outputs to lie on a fixed-radius hypersphere. SphereAR couples a hyperspherical VAE with a causal Transformer and token-level diffusion head, enforcing scale invariance. Empirically, SphereAR-H (943M) achieves state-of-the-art results for AR models on ImageNet 256x256 class-conditional generation, obtaining an FID score of 1.34. This suggests that hyperspherical constraints can stabilize AR decoding and enhance image generation performance. |
| Natural Language Processing | AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced
  Self-Play (Read more on [arXiv](https://arxiv.org/abs/2509.24193) or [HuggingFace](https://huggingface.co/papers/2509.24193))| Yue Yu, Jonathan Wang, Zihan Dong, Yuchen Zhuang, Ran Xu | The paper introduces AceSearcher, a cooperative self-play framework designed to enhance LLMs' search and reasoning abilities for complex tasks. It aims to improve multi-hop retrieval and reasoning by training a single LLM in two alternating roles: decomposer and solver. AceSearcher employs supervised fine-tuning followed by reinforcement fine-tuning optimized for final answer accuracy without intermediate supervision. Experiments across 10 datasets demonstrate an average exact match improvement of 7.6% over state-of-the-art baselines. AceSearcher-32B achieves performance comparable to DeepSeek-V3 using less than 5% of its parameters, offering an efficient solution for complex reasoning-intensive RAG tasks. |
| Computer Vision | LOVE-R1: Advancing Long Video Understanding with an Adaptive Zoom-in
  Mechanism via Multi-Step Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.24786) or [HuggingFace](https://huggingface.co/papers/2509.24786))|  | The paper introduces LOVE-R1, a model for advancing long video understanding using an adaptive zoom-in mechanism via multi-step reasoning. It aims to address the conflict between long-form temporal understanding and detailed spatial perception in large video-language models. LOVE-R1 employs a slow-fast dynamic frame processing mechanism where it initially samples the entire video at a low resolution and then adaptively zooms in on relevant clips at a higher resolution based on multi-step reasoning, finetuned using decoupled reinforcement learning. Experimental results show that LOVE-R1 achieves 48.2% on LVBench and outperforms the baseline Qwen2.5-VL by an average of 3.1% across benchmarks. The adaptive zoom-in technique offers a way to balance temporal and spatial details in long videos, which can potentially increase the long video understanding abilities of video-language models. |
| Computer Vision | MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially
  Disentangled Attention and Identity-Aware Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.21953) or [HuggingFace](https://huggingface.co/papers/2509.21953))| Zeyi Huang, Zhizhong Wang, Yehao Lu, Yibo Jiang, SugerWu | The paper introduces MultiCrafter, a framework for high-fidelity multi-subject image generation. It addresses attribute leakage and preference alignment by disentangling attention and using identity-aware reinforcement learning. The method uses explicit positional supervision and Mixture-of-Experts to improve subject fidelity and aligns with human preferences using a novel reinforcement learning framework. Experiments demonstrate MultiCrafter significantly improves subject fidelity, achieving a 28.3% relative improvement in Face-Sim over the next best method, while better aligning with human preferences. This framework enables more realistic and controllable multi-subject image synthesis for AI practitioners. |
| Computer Vision | PixelCraft: A Multi-Agent System for High-Fidelity Visual Reasoning on
  Structured Images (Read more on [arXiv](https://arxiv.org/abs/2509.25185) or [HuggingFace](https://huggingface.co/papers/2509.25185))|  | PixelCraft is a multi-agent system for high-fidelity visual reasoning on structured images. The paper addresses the challenge of MLLMs struggling with structured images due to perceptual errors. PixelCraft uses a fine-tuned MLLM for pixel-level grounding integrated with CV algorithms in tool agents, enabling a dynamic workflow with agent discussion and self-criticism. Experiments on chart and geometry benchmarks show PixelCraft significantly improves performance, achieving a 70.24% accuracy on EvoChart with GPT-4o. The system offers AI practitioners a new standard for structured image reasoning by facilitating high-fidelity processing and flexible visual analysis. |
| Natural Language Processing | Pretraining Large Language Models with NVFP4 (Read more on [arXiv](https://arxiv.org/abs/2509.25149) or [HuggingFace](https://huggingface.co/papers/2509.25149))|  | This paper explores efficient pretraining of large language models (LLMs) using the NVFP4 numerical format. It investigates the stability and accuracy challenges of training LLMs with 4-bit floating point precision. The proposed method integrates random Hadamard transforms, two-dimensional quantization, stochastic rounding, and selective high-precision layers. The approach is validated by pretraining a 12-billion-parameter model on 10 trillion tokens achieving comparable MMLU-pro accuracy of 62.58% to an FP8 baseline. The results demonstrate NVFP4 as a viable technique for more efficient LLM pretraining, reducing compute and memory requirements. |
| Natural Language Processing | SCI-Verifier: Scientific Verifier with Thinking (Read more on [arXiv](https://arxiv.org/abs/2509.24285) or [HuggingFace](https://huggingface.co/papers/2509.24285))| Jingqi Ye, Junchi Yao, Fangchen Yu, Chenyu Huang, desimfj | This paper introduces SCI-Verifier, a reasoning-augmented verifier for scientific domains, to improve the reliability of LLMs in scientific reasoning tasks. The study addresses the limitations of current verification methods by proposing SCI-VerifyBench, a cross-disciplinary benchmark, and SCI-Verifier, a unified verifier using post-training to enhance logical reasoning. SCI-Verifier demonstrates superior performance in equivalence judgment and multi-step reasoning while maintaining concise outputs. Experimental results show that SCI-Verifier achieves an accuracy of 86.28% on SCI-VerifyBench and par or superior performance to other models. This work offers a framework to systematically evaluate and improve LLM capabilities in scientific reasoning domains. |
| Natural Language Processing | Alignment through Meta-Weighted Online Sampling: Bridging the Gap
  between Data Generation and Preference Optimization (Read more on [arXiv](https://arxiv.org/abs/2509.23371) or [HuggingFace](https://huggingface.co/papers/2509.23371))| Xin Geng, Shiqi Qiao, Biao Liu, Ning Xu, jmyang | The paper addresses the distribution mismatch between offline preference data and the evolving policy in aligning large language models (LLMs). It proposes Meta-Weighted Adaptive Preference Optimization (MetaAPO), a framework that dynamically couples data generation with model training using a meta-learner. MetaAPO employs a meta-learner to guide online generation and assign sample-wise meta-weights, balancing online and offline data. Experiments show MetaAPO outperforms existing preference optimization methods, achieving improved alignment while reducing online annotation costs by 42%. This method allows for more efficient and effective LLM alignment with human preferences. |
| Reinforcement Learning | WirelessMathLM: Teaching Mathematical Reasoning for LLMs in Wireless
  Communications with Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.23219) or [HuggingFace](https://huggingface.co/papers/2509.23219))| Li Wei, Wenhe Zhang, Yiyang Zhu, Mengbing Liu, XINLI1997 | This paper introduces WirelessMathLM, a reinforcement learning approach for teaching mathematical reasoning to LLMs in the specialized domain of wireless communications. The research aims to improve LLM performance on technical mathematics problems by leveraging verifiable correctness as a reward signal. The methodology employs Group Relative Policy Optimization (GRPO) to train compact models directly from base checkpoints without supervised warm-start, using binary verification rewards. The 7B model achieves 39.5% accuracy on WirelessMathBench-XL, approaching GPT-40 performance while using significantly fewer parameters. The implication is that verifiable rewards enable efficient domain specialization, potentially transforming AI development for technical domains. |
| Natural Language Processing | StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient
  SpeechLLMs (Read more on [arXiv](https://arxiv.org/abs/2509.22220) or [HuggingFace](https://huggingface.co/papers/2509.22220))| Wei Jia, Aiwei Liu, Chuhan Wu, Linhao Zhang, QbethQ | The paper introduces StableToken, a novel semantic speech tokenizer designed to improve robustness in SpeechLLMs. It addresses the instability of existing tokenizers under acoustic noise by employing a multi-branch architecture and bit-wise voting mechanism. The key methodology involves parallel audio processing with representations merged via a consensus-driven bit-wise voting. StableToken achieves state-of-the-art token stability, reducing Unit Edit Distance (UED) by over 60% (from 26.17% to 10.17%) relative to existing tokenizers under diverse noise conditions. This robustness translates to improved downstream SpeechLLM performance on tasks like speech understanding and generation. |
| Natural Language Processing | AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety
  Alignment of Large Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2509.24269) or [HuggingFace](https://huggingface.co/papers/2509.24269))|  | The paper introduces AdvChain, an adversarial chain-of-thought tuning method to improve the safety alignment of large reasoning models (LRMs). It addresses the "snowball effect," where minor reasoning deviations amplify, leading to harmful or overly cautious responses. AdvChain uses a dataset of Temptation-Correction and Hesitation-Correction samples to train models for dynamic self-correction. Experiments demonstrate AdvChain significantly enhances robustness against jailbreak attacks, reducing attack success rate from 51% to 5.0% (DeepSeek-R1-7B). This work provides AI practitioners with a method to improve the safety-utility balance of LRMs without compromising reasoning capabilities. |
| Multi-Modal | UniVid: The Open-Source Unified Video Model (Read more on [arXiv](https://arxiv.org/abs/2509.24200) or [HuggingFace](https://huggingface.co/papers/2509.24200))| Meng Fang, Biao Wu, Junhui Lin, Jiabin Luo, SteveZeyuZhang | The paper introduces UniVid, an open-source unified video model for both understanding and generation tasks. It addresses semantic faithfulness and efficient extension of image-centric MLLMs to video through a lightweight adapter coupled with a diffusion decoder. The methodology includes Temperature Modality Alignment for prompt adherence and Pyramid Reflection for efficient temporal reasoning using dynamic keyframe selection. Experiments demonstrate state-of-the-art performance, achieving a 2.2% improvement on VBench-Long total score compared to previous SOTA methods. UniVid offers AI practitioners a unified framework for video intelligence with competitive performance and efficiency. |
| Machine Learning | PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation (Read more on [arXiv](https://arxiv.org/abs/2509.23338) or [HuggingFace](https://huggingface.co/papers/2509.23338))|  | The paper introduces PARROT, a benchmark for evaluating LLMs in cross-system SQL translation, an underexplored task with practical importance. It addresses the limitations of existing SQL benchmarks by providing a diverse and realistic corpus of 598 translation pairs from 38 open-source benchmarks and real-world services, covering 22 database systems.  The methodology includes a rigorous curation pipeline to maximize dialect diversity and real-world relevance, augmented with a specialized challenge set and a unified evaluation protocol.  Experiments show that LLMs achieve lower than 38.53% accuracy on average, indicating substantial room for improvement in system-specific SQL understanding.  PARROT provides AI practitioners with a valuable resource for developing and evaluating more robust and accurate cross-system SQL translation models. |
| Natural Language Processing | ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2509.22830) or [HuggingFace](https://huggingface.co/papers/2509.22830))|  | The paper introduces ChatInject, a novel prompt injection attack exploiting LLM chat templates in agent systems. It investigates the vulnerability of LLMs to contextual manipulation through crafted chat templates and persuasive multi-turn dialogues. The methodology involves formatting malicious payloads to mimic native chat templates, thereby bypassing instruction hierarchies. Experiments show ChatInject achieves significantly higher average attack success rates, improving from 5.18% to 32.05% on AgentDojo and from 15.13% to 45.90% on InjecAgent. The findings highlight vulnerabilities in current agent systems and the need for more sophisticated defenses against template-based attacks. |
| Multi-Modal | UniMIC: Token-Based Multimodal Interactive Coding for Human-AI
  Collaboration (Read more on [arXiv](https://arxiv.org/abs/2509.22570) or [HuggingFace](https://huggingface.co/papers/2509.22570))|  | The paper introduces UniMIC, a unified token-based multimodal interactive coding framework to enhance human-AI collaboration by addressing the limitations of existing codecs optimized for unimodal communication. The research aims to develop an efficient communication protocol between edge devices and cloud AI agents using compact tokenized representations and lightweight Transformer-based entropy models to minimize inter-token redundancy. UniMIC employs generic, masked, and text-conditioned models to effectively compress multimodal data. Experiments on tasks like text-to-image generation, inpainting, outpainting, and VQA demonstrate significant bitrate savings at ultra-low bitrates (e.g., < 0.05 bpp) without compromising task performance. The work suggests a practical paradigm shift towards AI-native communication protocols for future multimodal interactive systems, although details on the impact of individual tokenizers is omitted. |
| Reinforcement Learning | Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and
  Planning (Read more on [arXiv](https://arxiv.org/abs/2509.25052) or [HuggingFace](https://huggingface.co/papers/2509.25052))|  | The paper introduces Cogito, ergo ludo (CEL), a novel reinforcement learning agent architecture that learns to play by reasoning and planning. It aims to develop an agent that builds an explicit, language-based understanding of its environment's mechanics and its own strategy through interaction and reflection. CEL utilizes a Large Language Model (LLM) to perform rule induction and strategy summarization after each episode, resulting in a strategic playbook. In grid-world tasks like Minesweeper, Frozen Lake, and Sokoban, CEL autonomously discovers rules and develops effective policies from sparse rewards, achieving a peak success rate of 54% in Minesweeper. The CEL agent is a step towards more general and interpretable agents that build a transparent model of their world, potentially coupling explicit understanding with traditional architecture efficiency. |
| Computer Vision | Learning Goal-Oriented Language-Guided Navigation with Self-Improving
  Demonstrations at Scale (Read more on [arXiv](https://arxiv.org/abs/2509.24910) or [HuggingFace](https://huggingface.co/papers/2509.24910))|  | This paper introduces Self-Improving Demonstrations (SID) for goal-oriented language-guided navigation, enabling agents to learn exploration strategies at scale. The research aims to improve navigation agent performance by iteratively training on self-generated, successful trajectories. SID initializes an agent with shortest-path data and uses it to generate novel trajectories, retraining the agent on these demonstrations, thus improving exploration capabilities. The resulting agent achieves a 50.9% success rate on the unseen validation splits of SOON. The approach provides a scalable and transferable method for learning exploration strategies in goal-oriented navigation tasks. |
| Natural Language Processing | MathBode: Frequency-Domain Fingerprints of LLM Mathematical Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.23143) or [HuggingFace](https://huggingface.co/papers/2509.23143))|  | MathBode introduces a dynamic diagnostic tool for evaluating mathematical reasoning in large language models (LLMs) using frequency-domain analysis. The research aims to assess reasoning fidelity and consistency by driving problem parameters sinusoidally and analyzing the model's gain and phase response. The methodology involves fitting first-harmonic responses to model outputs across five closed-form families, generating Bode-style fingerprints. Results demonstrate systematic low-pass behavior and phase lag, differentiating between LLM tiers, where mid-tier and frontier models present R2 values, suggesting accuracy. The implication is that MathBode provides AI practitioners with actionable metrics for benchmarking mathematical reasoning beyond static accuracy. |
| Machine Learning | Local Success Does Not Compose: Benchmarking Large Language Models for
  Compositional Formal Verification (Read more on [arXiv](https://arxiv.org/abs/2509.23061) or [HuggingFace](https://huggingface.co/papers/2509.23061))| Binhang Yuan, Jie Fu, Xingwei Qu, Xu Xu, XINLI1997 | This paper introduces DAFNYCOMP, a benchmark for evaluating compositional reasoning in large language models (LLMs) for formal verification. The research aims to quantify the ability of LLMs to generate correct specifications for multi-function programs in Dafny. DAFNYCOMP consists of 300 synthesized programs requiring LLMs to reason across function boundaries to ensure correctness. Results show a significant performance drop: LLMs achieve >99% syntax correctness but only 3.69% verification on DAFNYCOMP, compared to >58% verification in single-function benchmarks. This highlights the limitations of current LLMs in compositional reasoning and suggests that local success does not guarantee end-to-end formal verification capabilities. |
| Natural Language Processing | REMA: A Unified Reasoning Manifold Framework for Interpreting Large
  Language Model (Read more on [arXiv](https://arxiv.org/abs/2509.22518) or [HuggingFace](https://huggingface.co/papers/2509.22518))| Shuo Zhang, Junrong Yue, Ronghao Chen, Guanzhi Deng, liboaccn | This paper introduces REMA, a framework for interpreting LLM reasoning failures by analyzing the geometry of internal representations. It aims to provide a measurable geometric analysis perspective on complex reasoning. REMA quantifies the deviation of erroneous representations from the correct reasoning manifold using k-nearest neighbor distance and identifies divergence points across model layers. Experiments on diverse LLMs show high separability between correct and erroneous reasoning representations and reveal model- and task-dependent patterns of failure origins. The framework offers a new avenue for in-depth understanding and diagnosis of internal computational processes, quantified by the statistically significant geometric deviation from correct reasoning manifolds. |
| Natural Language Processing | BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal
  Decrees and Notifications (Read more on [arXiv](https://arxiv.org/abs/2509.24908) or [HuggingFace](https://huggingface.co/papers/2509.24908))|  | This paper introduces BOE-XSUM, a new dataset for extreme summarization of Spanish legal documents into clear language. The research aims to evaluate the capability of generative language models in producing concise and plain-language summaries of complex legal texts. The study fine-tunes medium-sized large language models (LLMs) on BOE-XSUM and compares their performance against general-purpose generative models in a zero-shot setting. The best-performing model, BERTIN GPT-J 6B, achieves a 24% performance gain over the top zero-shot model, DeepSeek-R1 (41.6% vs. 33.5% accuracy). This work provides a valuable resource for Spanish NLP and demonstrates the effectiveness of fine-tuning for domain-specific summarization tasks, while also highlighting remaining challenges in creating high-quality extreme summaries. |
| Multi-Modal | IWR-Bench: Can LVLMs reconstruct interactive webpage from a user
  interaction video? (Read more on [arXiv](https://arxiv.org/abs/2509.24709) or [HuggingFace](https://huggingface.co/papers/2509.24709))| Tianyuan Huang, Yunwen Li, Yufan Shen, Minghao Liu, Yang Chen | The paper introduces IWR-Bench, a new benchmark for evaluating Large Vision-Language Models (LVLMs) in interactive webpage reconstruction from user interaction videos. It investigates whether LVLMs can reconstruct the dynamic functionalities of webpages from video observation. The methodology involves a dataset of 113 tasks from real-world websites and an agent-as-a-judge evaluation framework with two metrics: Interactive Functionality Score (IFS) and Visual Fidelity Score (VFS). Experiments on 28 LVLMs reveal the best model achieved only a 36.35% overall score, with IFS (24.39%) lagging significantly behind VFS (64.25%). This result underscores the limitations of current models in reasoning about temporal dynamics and synthesizing event-driven logic for interactive webpage reconstruction. |
| Natural Language Processing | BPMN Assistant: An LLM-Based Approach to Business Process Modeling (Read more on [arXiv](https://arxiv.org/abs/2509.24592) or [HuggingFace](https://huggingface.co/papers/2509.24592))| Darko Etinger, Nikola Tankovic, jtlicardo | This paper introduces BPMN Assistant, a tool for natural language-based creation and editing of Business Process Model and Notation (BPMN) diagrams. The primary objective is to enhance process modification accuracy using Large Language Models (LLMs). The method involves a specialized JSON-based representation of BPMN diagrams, and process generation quality is evaluated using Graph Edit Distance (GED) and Relative Graph Edit Distance (RGED). Results demonstrate that JSON offers greater reliability and significantly higher editing success rates compared to XML manipulation. The tool allows AI practitioners to generate BPMN diagrams through natural language, bridging the gap between informal descriptions and formal models. |
| Natural Language Processing | Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.23233) or [HuggingFace](https://huggingface.co/papers/2509.23233))|  | This paper introduces the task of corpus-level inconsistency detection in Wikipedia using Large Language Models (LLMs). The research aims to improve Wikipedia's accuracy by identifying contradictory facts within its extensive corpus. The proposed methodology, CLAIRE, leverages LLM reasoning with retrieval to surface potentially inconsistent claims and contextual evidence for human review, where editors reported higher confidence and identified more inconsistencies when using CLAIRE. Experiments on a newly constructed dataset, WIKICOLLIDE, reveal that at least 3.3% of Wikipedia facts contradict each other, while the best automated system achieves an AUROC of 75.1%. The work implies that automated tools can practically assist human editors in improving knowledge consistency at scale, but current systems have substantial room for improvement. |
| Machine Learning | RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human
  Mobility (Read more on [arXiv](https://arxiv.org/abs/2509.23115) or [HuggingFace](https://huggingface.co/papers/2509.23115))|  | RHYTHM is a novel human mobility foundation model designed to improve trajectory prediction by integrating temporal tokenization and LLM reasoning. It addresses the challenge of modeling complex spatio-temporal dependencies by partitioning trajectories into daily segments and encoding them as discrete tokens with hierarchical attention. The model is augmented with prompt embeddings derived from a frozen LLM to enrich token representations. Evaluated on three real-world datasets, RHYTHM achieves a 2.4% improvement in overall accuracy and reduces training time by 24.6%. This framework offers AI practitioners a more scalable and accessible approach for accurate trajectory prediction with reduced computational overhead. |
| Natural Language Processing | Charting a Decade of Computational Linguistics in Italy: The CLiC-it
  Corpus (Read more on [arXiv](https://arxiv.org/abs/2509.19033) or [HuggingFace](https://huggingface.co/papers/2509.19033))| Luca Dini, Alessandro Bondielli, Serena Auriemma, Chiara Alzetta, alemiaschi | This paper analyzes the evolution of Computational Linguistics (CL) and Natural Language Processing (NLP) research trends in Italy using the CLiC-it corpus. The study aims to track research trends within the Italian CL and NLP community from 2014-2024. It compiles the proceedings of the CLiC-it conference, analyzing metadata (author provenance, gender, affiliations) and paper content using topic modeling and network analysis. The analysis revealed a growing research landscape with a diversification of research topics, including the rise of large language models and socially impactful applications with approximately 51.78% new contributors each year. The CLiC-it Corpus provides valuable insights into emerging trends and developments, supporting informed decisions and future directions in the field for both Italian and international research communities. |
| Multi-Modal | Advancing Reference-free Evaluation of Video Captions with Factual
  Analysis (Read more on [arXiv](https://arxiv.org/abs/2509.16538) or [HuggingFace](https://huggingface.co/papers/2509.16538))| Subarna Tripathi, Tz-Ying Wu, dipta007 | The paper introduces a reference-free framework for evaluating video captions by focusing on factual grounding. The primary research objective is to address the limitations of reference-based evaluation protocols in assessing video caption quality across diverse domains. The methodology involves training a multimodal model (Qwen2.5-VL) using pseudo captions of varying quality generated by an LLM based on supervised data, enabling the model to evaluate caption accuracy based on alignment with video content. The VC-Inspector achieves superior alignment with human judgments on the VATEX-Eval dataset, outperforming existing methods, with a top Kendall's correlation of 42.58 for the 7B model.  This offers AI practitioners a scalable and generalizable solution for evaluating video caption factual accuracy, improving objective assessments in diverse video domains. |
