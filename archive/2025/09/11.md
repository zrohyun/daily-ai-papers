

## Papers for 2025-09-11

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Reinforcement Learning | A Survey of Reinforcement Learning for Large Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2509.08827) or [HuggingFace](https://huggingface.co/papers/2509.08827))| Runze Liu, Youbang Sun, Bingxiang He, Yuxin Zuo, Kaiyan Zhang | This survey paper investigates recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs), highlighting the transformation of LLMs into Large Reasoning Models (LRMs). It aims to revisit the development of this domain and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). The methodology includes examining research applying RL to LLMs and LRMs for reasoning abilities, including foundational components, core problems, training resources, and downstream applications. The survey identifies future opportunities and directions for this rapidly evolving area, and hopes it will promote research on RL for broader reasoning models. It also includes a comparison of representative open-source models trained with RL and a discussion of limitations in both applying RL and limitations of current LLMs. |
| Computer Vision | RewardDance: Reward Scaling in Visual Generation (Read more on [arXiv](https://arxiv.org/abs/2509.08826) or [HuggingFace](https://huggingface.co/papers/2509.08826))| Liang Li, Ming Li, Zilyu Ye, Yu Gao, Jie Wu | The paper introduces RewardDance, a scalable reward modeling framework for improving visual generation models via reinforcement learning. It addresses the limitations of existing reward models by reformulating reward score as the model's probability of predicting a "yes" token, aligning reward objectives with VLM architectures. RewardDance achieves scaling across model size (up to 26B parameters) and context by incorporating task-specific instructions and chain-of-thought reasoning. Experiments demonstrate RewardDance surpasses state-of-the-art methods, achieving alignment improvement validated by GSB in text-to-video generation tasks. The framework mitigates reward hacking and enhances diversity, which enables AI practitioners to develop more robust and high-quality visual generation models. |
| Computer Vision | 3D and 4D World Modeling: A Survey (Read more on [arXiv](https://arxiv.org/abs/2509.07996) or [HuggingFace](https://huggingface.co/papers/2509.07996))| Ao Liang, Youquan Liu, Jianbiao Mei, Wesley Yang, Lingdong Kong | This paper surveys 3D and 4D world modeling and generation, a rapidly growing area often overshadowed by 2D approaches. It aims to provide a comprehensive review of methodologies utilizing native 3D/4D representations. The paper establishes precise definitions and proposes a structured taxonomy spanning VideoGen, OccGen, and LiDARGen approaches. It systematically summarizes datasets and evaluation metrics tailored to 3D/4D settings, including a benchmark of recent methods revealing key trade-offs between realism, geometric accuracy, temporal stability, and controllability. By highlighting open challenges and future directions, this survey serves as a foundational reference for researchers aiming to advance 3D/4D generative models for embodied AI. |
| Reinforcement Learning | AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making
  through Multi-Turn Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.08755) or [HuggingFace](https://huggingface.co/papers/2509.08755))| Honglin Guo, Baodai Huang, Chenyang Liao, Jixuan Huang, Zhiheng Xi | The paper introduces AgentGym-RL, a framework for training LLM agents for multi-turn interactive decision-making through reinforcement learning. The research aims to provide a unified RL framework for training agents from scratch across diverse environments. They propose ScalingInter-RL, a training approach for exploration-exploitation balance, emphasizing exploitation initially and gradually shifting towards exploration. AgentGym-RL agents match or surpass commercial models on 27 tasks across diverse environments. The main implication is a new framework and training method for developing intelligent agents via RL. |
| Computer Vision | P3-SAM: Native 3D Part Segmentation (Read more on [arXiv](https://arxiv.org/abs/2509.06784) or [HuggingFace](https://huggingface.co/papers/2509.06784))| Yunhan Yang, Jiachen Xu, Xinhao Yan, Yang Li, murcherful | The paper introduces P3-SAM, a native 3D point-promptable part segmentation model. The research aims to automate the segmentation of 3D assets into constituent parts with improved robustness. P3-SAM employs a feature extractor, multiple segmentation heads, and an IoU predictor, trained on a dataset of nearly 3.7 million models with part segmentation labels, and a mask merging algorithm. Results demonstrate precise segmentation and strong robustness, achieving state-of-the-art performance. This approach offers AI practitioners a robust method for automated 3D part segmentation, facilitating advanced 3D understanding and model reuse. |
| Natural Language Processing | Hunyuan-MT Technical Report (Read more on [arXiv](https://arxiv.org/abs/2509.05209) or [HuggingFace](https://huggingface.co/papers/2509.05209))| Yang Du, Mingyang Song, Bingxin Qu, Zheng Li, Mao Zheng | The paper introduces Hunyuan-MT-7B, an open-source multilingual translation model supporting 33 languages with a focus on Mandarin and ethnic minority languages. It aims to improve translation quality, especially for low-resource languages and dialects. The model employs a holistic training process involving general pre-training, MT-oriented pre-training, supervised fine-tuning (SFT), and reinforcement learning (RL). Hunyuan-MT-7B achieves state-of-the-art performance in the WMT2025 shared task, ranking first in 30 out of 31 language pairs. This provides AI practitioners with an accessible, high-performance foundation model for advancing multilingual translation research and applications. |
| Natural Language Processing | <think> So let's replace this phrase with insult... </think> Lessons
  learned from generation of toxic texts with LLMs (Read more on [arXiv](https://arxiv.org/abs/2509.08358) or [HuggingFace](https://huggingface.co/papers/2509.08358))| Alexander Panchenko, Daniil Moskovskiy, Sergey Pletenev | This paper investigates the use of LLM-generated synthetic toxic data for training text detoxification models. The research questions center around whether LLMs can fully replace human annotators in creating parallel datasets for detoxification. The methodology involves generating toxic text from neutral sources using Llama 3 and Qwen activation-patched models and evaluating BART models fine-tuned on this data against human-annotated baselines. The results show that models trained on synthetic data underperform, with performance drops of up to 30% in joint metrics, due to a critical lexical diversity gap. This highlights the importance of diverse, human-annotated data for robust detoxification systems, indicating that LLMs currently cannot fully substitute for human annotation. |
| Multi-Modal | EnvX: Agentize Everything with Agentic AI (Read more on [arXiv](https://arxiv.org/abs/2509.08088) or [HuggingFace](https://huggingface.co/papers/2509.08088))| Wenzheng Tom Tang, Yikun Wang, Yingxuan Yang, Zimian Peng, Linyao Chen | The paper introduces EnvX, a framework leveraging Agentic AI to transform GitHub repositories into intelligent agents capable of natural language interaction and inter-agent collaboration. The main objective is to address the limitations of manual software reuse by agentizing repositories for autonomous task execution and collaboration. EnvX employs a three-phase process involving environment initialization, agentic automation, and an Agent-to-Agent protocol. Evaluated on GitTaskBench, EnvX achieves a 74.07% execution completion rate and 51.85% task pass rate, outperforming existing frameworks. This work enables greater accessibility and collaboration within the open-source ecosystem by treating repositories as interactive agents rather than static code resources. |
| Natural Language Processing | HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI
  Assistants (Read more on [arXiv](https://arxiv.org/abs/2509.08494) or [HuggingFace](https://huggingface.co/papers/2509.08494))| Jacy Reese Anthis, Jacob Haimes, Daniel Samuelson, Benjamin Sturgeon | The paper introduces HumanAgencyBench (HAB) for evaluating human agency support in AI assistants. It assesses how AI assistants handle user queries across six dimensions of agency. The methodology involves using LLMs to simulate user queries and evaluate AI responses. Results show low-to-moderate agency support in contemporary LLM-based assistants with notable variations across developers, with Claude models generally showing more support but exhibiting weaker performance in avoiding value manipulation. The main implication is a need for more robust safety and alignment targets to improve AI's support for human agency. |
