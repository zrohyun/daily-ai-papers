

## Papers for 2025-09-25

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | SIM-CoT: Supervised Implicit Chain-of-Thought (Read more on [arXiv](https://arxiv.org/abs/2509.20317) or [HuggingFace](https://huggingface.co/papers/2509.20317))| Yuhang Cao, Xiaoyi Dong, Yuhang Zang, LiuXR, Wiselnn | The paper introduces SIM-CoT, a novel approach to enhance implicit chain-of-thought reasoning in Large Language Models. It addresses latent instability issues arising from insufficient step-level supervision. SIM-CoT employs an auxiliary decoder during training to align each implicit token with corresponding explicit reasoning steps. Experiments show SIM-CoT boosts baseline methods like Coconut by +8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. This improves token efficiency and model performance by enriching the latent reasoning space with more meaningful data. |
| Computer Vision | Video models are zero-shot learners and reasoners (Read more on [arXiv](https://arxiv.org/abs/2509.20328) or [HuggingFace](https://huggingface.co/papers/2509.20328))| rgeirhos, kswersky, nmatares, yuxuanli, ThaddaeusWiedemer | The paper investigates zero-shot learning and reasoning capabilities of video models. The research explores whether video models can achieve general-purpose vision understanding similar to LLMs. Veo 3 is prompted across a diverse range of tasks, and its performance is evaluated. Veo 3 demonstrates zero-shot abilities, including object segmentation, physical reasoning, and problem-solving, with significant performance gains compared to Veo 2; for example, edge detection achieves 0.77 pass@10. The findings suggest that video models are emerging as unified, generalist vision foundation models, impacting how AI practitioners approach diverse vision tasks. |
| Natural Language Processing | Advancing Speech Understanding in Speech-Aware Language Models with GRPO (Read more on [arXiv](https://arxiv.org/abs/2509.16990) or [HuggingFace](https://huggingface.co/papers/2509.16990))| Avihu, rhoory, NimrodShabtay1986, hagaia, avishai-elmakies | This paper introduces a Group Relative Policy Optimization (GRPO)-based method for training Speech-Aware Large Language Models (SALLMs) on open-format speech understanding tasks. The research aims to improve SALLMs' performance on tasks like Spoken Question Answering and Automatic Speech Translation using GRPO with BLEU as a reward signal. The key methodology involves leveraging GRPO to optimize SALLMs, surpassing standard supervised fine-tuning (SFT). Empirical results demonstrate that the GRPO approach outperforms standard SFT on SQA and AST across multiple relevant metrics, achieving BLEU improvements of up to 9.8% on SQA tasks. The findings suggest that GRPO is an effective technique for enhancing the generative abilities of SALLMs, providing AI practitioners with a robust approach for speech-related tasks. |
| Natural Language Processing | LLMs4All: A Review on Large Language Models for Research and
  Applications in Academic Disciplines (Read more on [arXiv](https://arxiv.org/abs/2509.19580) or [HuggingFace](https://huggingface.co/papers/2509.19580))| Yanfang, lalor, Sweson, ZehongWang, mtybilly | This paper reviews Large Language Models (LLMs) and their applications across diverse academic disciplines. The objective is to explore how LLMs are shaping research and practice in various fields and to discuss limitations and future directions. The methodology involves surveying state-of-the-art LLMs and their integration into arts, economics, science, and engineering. The paper overviews LLMs' engagement across disciplines and discusses key observations and insights. This review helps researchers and practitioners interested in exploiting LLMs to advance their works in diverse real-world applications, though quantitative results and performance are not explicitly detailed. |
| Computer Vision | EditVerse: Unifying Image and Video Editing and Generation with
  In-Context Learning (Read more on [arXiv](https://arxiv.org/abs/2509.20360) or [HuggingFace](https://huggingface.co/papers/2509.20360))| Tianyu Wang, sooyek, Shaldon, CaiYuanhao, juxuan27 | EditVerse is a unified framework for image and video editing and generation, leveraging in-context learning. It addresses the fragmentation in video editing by representing text, images, and videos as unified token sequences. The key methodology involves full self-attention and a four-dimensional Rotary Positional Embedding to handle diverse modalities and resolutions. Experiments on EditVerseBench demonstrate state-of-the-art performance, surpassing existing methods in editing faithfulness with a 7.65 VLM evaluation score, compared to open-source research models. This framework enables natural cross-modal knowledge transfer and emergent editing abilities, offering AI practitioners a more generalized and scalable approach. |
| Natural Language Processing | EmbeddingGemma: Powerful and Lightweight Text Representations (Read more on [arXiv](https://arxiv.org/abs/2509.20354) or [HuggingFace](https://huggingface.co/papers/2509.20354))| Marksherwood, osanseviero, ssmoot, SindhuRaghuram97, hschechter | The paper introduces EmbeddingGemma, a novel lightweight text embedding model based on the Gemma 3 language model family. It aims to create powerful yet resource-efficient text representations. The methodology involves encoder-decoder initialization, geometric embedding distillation, spread-out regularization, and checkpoint merging from varied mixtures. EmbeddingGemma (300M) achieves state-of-the-art results on the MTEB benchmark, outperforming models with fewer than 500M parameters, achieving performance comparable to models double its size. EmbeddingGemma offers AI practitioners a high performance-to-cost ratio text embedding solution suitable for low-latency and on-device applications. |
| Computer Vision | PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2509.20358) or [HuggingFace](https://huggingface.co/papers/2509.20358))| Yiming Huang, thomagram, frankzydou, MorPhLingXD, chenwang | The paper introduces PhysCtrl, a framework for physics-grounded image-to-video generation with control over physical parameters and external forces. It aims to generate physically plausible video by learning the distribution of physical dynamics across materials. PhysCtrl employs a diffusion model conditioned on physics parameters, enhanced with a spatiotemporal attention mechanism and physics-based constraints, trained on a 550K synthetic dataset. The model achieves 77.03% vIoU on trajectory generation, outperforming existing methods. This enables AI practitioners to synthesize controllable, high-fidelity videos grounded in physical principles, improving visual quality and physical plausibility. |
| Computer Vision | Logics-Parsing Technical Report (Read more on [arXiv](https://arxiv.org/abs/2509.19760) or [HuggingFace](https://huggingface.co/papers/2509.19760))| Fan Yang, Shuzhao Li, Xiangyang Chen, ZjuCv, xiuwenzhu | The paper introduces Logics-Parsing, an end-to-end LVLM framework enhanced with reinforcement learning, for improved document parsing. It aims to address limitations in handling complex document layouts and reading orders by incorporating layout analysis and reading order inference. The methodology involves a two-stage SFT-then-RL training strategy with meticulously designed reward mechanisms. Evaluated on the LogicsParsingBench, Logics-Parsing achieves state-of-the-art performance, attaining the lowest aggregate edit distance of 0.124 on English documents. This demonstrates improved structural understanding and content ordering, enabling more accurate document analysis for AI practitioners. |
| Multi-Modal | Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal
  Understanding and Generation (Read more on [arXiv](https://arxiv.org/abs/2509.19244) or [HuggingFace](https://huggingface.co/papers/2509.19244))| Zhe Lin, xternalz, kl3141, JoshuaGu, jacklishufan | Lavida-O is a unified masked diffusion model (MDM) for multi-modal understanding and generation. The paper aims to develop a framework that enables image-level understanding, object grounding, image editing, and high-resolution text-to-image synthesis within a single model. It introduces an Elastic Mixture-of-Transformers architecture, token compression, universal text conditioning, and stratified sampling for efficient and high-quality generation, further incorporating planning and iterative self-reflection for improved performance. Lavida-O achieves state-of-the-art performance on benchmarks like RefCOCO object grounding, GenEval text-to-image generation, and ImgEdit image editing, with up to a 6.8x speedup compared to other models. This unified approach with explicit mechanisms leveraging understanding capabilities offers a new paradigm for scalable multimodal reasoning and generation. |
| Other | On the Use of Agentic Coding: An Empirical Study of Pull Requests on
  GitHub (Read more on [arXiv](https://arxiv.org/abs/2509.14745) or [HuggingFace](https://huggingface.co/papers/2509.14745))| Hajimu Iida, Brittany Reid, Yutaro Kashiwa, Miku Watanabe, hao-li | This paper presents an empirical study on the impact of agentic coding tools, specifically Claude Code, on open-source projects. The study investigates how AI-generated pull requests (PRs) are received by developers, aiming to understand their usefulness and acceptance in real-world projects. The methodology involves analyzing 567 pull requests across 157 open-source projects, assessing acceptance rates and revision requirements. The results indicate that 83.8% of Agentic-PRs are accepted, with 54.9% merged without further modification; however, acceptance is lower than Human-PRs (91.0%). The key implication is that while AI provides a strong starting point for code contributions, human oversight is essential for correctness, maintainability, and alignment with project standards. |
