

## Papers for 2025-09-03

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Reinforcement Learning | The Landscape of Agentic Reinforcement Learning for LLMs: A Survey (Read more on [arXiv](https://arxiv.org/abs/2509.02547) or [HuggingFace](https://huggingface.co/papers/2509.02547))| Hejia Geng, Guibin Zhang, henggg, Artemis0430, JeremyYin | The survey provides a comprehensive overview of agentic reinforcement learning for large language models (LLMs). It addresses the paradigm shift from LLM-RL as sequence generation to agentic RL as decision-making processes. The survey contrasts LLM-RL's single-step MDPs with Agentic RL's temporally extended POMDPs, proposing a twofold taxonomy around agentic capabilities and task domains. Synthesizing over 500 works, the survey highlights reinforcement learning's critical role in transforming static modules into adaptive agentic behavior. The study provides a compendium of open-source environments, benchmarks, and frameworks, implying pathways for AI practitioners to scale and generalize AI agents. |
| Reinforcement Learning | UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2509.02544) or [HuggingFace](https://huggingface.co/papers/2509.02544))| Haoyang Zou, zhwang4ai, JoeYing, jzfeng, MingComplex | The paper introduces UI-TARS-2, a GUI-centered agent model, for enhanced interaction in graphical user interfaces using multi-turn reinforcement learning. It addresses challenges in data scalability, multi-turn RL, GUI-only operation, and environment stability by employing a data flywheel, stabilized multi-turn RL framework, and hybrid GUI environment. The primary methodology involves scalable data generation, asynchronous agent rollout, and parameter interpolation. Empirical results show UI-TARS-2 achieves significant improvements, reaching 88.2 on Online-Mind2Web and a normalized score of 59.8 across 15 game environments. This suggests that UI-TARS-2 advances the state of GUI agents and generalizes effectively to real-world interactive scenarios. |
| Reinforcement Learning | SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn
  Tool-Integrated Reasoning (Read more on [arXiv](https://arxiv.org/abs/2509.02479) or [HuggingFace](https://huggingface.co/papers/2509.02479))| Qian Liu, Longtao Zheng, Zhenghai Xue, xszheng2020, R1ch0rd | The paper presents SimpleTIR, an algorithm for stabilizing multi-turn Tool-Integrated Reasoning (TIR) training in Large Language Models (LLMs).  It aims to address training instability and performance collapse in multi-turn TIR caused by distributional drift from external tool feedback. The core methodology involves filtering trajectories containing "void turns" (turns yielding neither code nor a final answer) to block harmful gradients. Experiments on math reasoning benchmarks show SimpleTIR achieves state-of-the-art performance, elevating AIME24 score to 50.5 from a 22.1 baseline using the Qwen2.5-7B model. SimpleTIR offers AI practitioners a way to train robust LLMs with external tools without relying on supervised fine-tuning, promoting diverse reasoning strategies. |
| Computer Vision | ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long
  Video Understanding (Read more on [arXiv](https://arxiv.org/abs/2508.21496) or [HuggingFace](https://huggingface.co/papers/2508.21496))| Xuanyu Zheng, Ruohui Wang, Mercury7353, datamonkey, HLSv | This paper introduces ELV-Halluc, a benchmark for evaluating semantic aggregation hallucinations (SAH) in long video understanding. The study aims to identify and address the issue of models misattributing semantics across events in longer videos. The authors use an adversarial triplet question pair design to quantify a model's sensitivity to semantic misalignment and positional encoding for SAH mitigation. Experiments on 16 models reveal a positive correlation between SAH and semantic complexity; positional encoding and DPO strategy helped alleviate SAH. An 8K QA pair dataset was curated, achieving a 27.7% reduction in SAH ratio and a 0.9% improvement on VideoMME. |
| Multi-Modal | LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model (Read more on [arXiv](https://arxiv.org/abs/2509.00676) or [HuggingFace](https://huggingface.co/papers/2509.00676))| Jianwei Yang, Chunyuan Li, Benjamin-eecs, drogozhang, russwang | The paper introduces LLaVA-Critic-R1, a multimodal critic model trained via reinforcement learning to optimize preference judgments while retaining generation ability. It explores whether critic models can serve as effective policy models. The methodology involves reorganizing preference-labeled critic datasets and performing reinforcement learning directly on a base generative model. Results demonstrate that LLaVA-Critic-R1 matches or surpasses specialized reasoning VLMs on 26 benchmarks, achieving a 5.7% average gain over its base model and reaching 71.9 on MMMU at the 7B scale. This implies that RL training on critic data can create unified models excelling in both evaluation and generation, thus simplifying multimodal systems. |
| Multi-Modal | POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models
  for Document Conversion (Read more on [arXiv](https://arxiv.org/abs/2509.01215) or [HuggingFace](https://huggingface.co/papers/2509.01215))| Haicheng Wang, Le Tian, Zhongyin Zhao, YxxxB, YuanLiuuuuuu | The paper introduces POINTS-Reader, a distillation-free framework for document conversion that enhances vision-language models. It addresses the challenge of creating high-quality document extraction datasets without relying on external models by proposing a two-stage pipeline of synthetic data generation and iterative self-improvement. The key methodology involves generating diverse synthetic data with unified output formats followed by iteratively refining the model on real-world documents using self-generated annotations and targeted filtering strategies. POINTS-Reader achieves state-of-the-art performance on document conversion tasks, surpassing existing models like Qwen2.5-VL-72B on the table metric of OmniDocBench. This approach offers AI practitioners a pathway to develop more robust and generalizable document understanding capabilities without the limitations of distillation. |
| Reinforcement Learning | VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use (Read more on [arXiv](https://arxiv.org/abs/2509.01055) or [HuggingFace](https://huggingface.co/papers/2509.01055))| Zhiheng Lyu, Zhuofeng Li, Yi Lu, JasperHaozhe, DongfuJiang | The paper introduces VERLTOOL, a unified and modular framework for Agentic Reinforcement Learning with Tool use (ARLT) designed to address limitations in existing task-specific codebases. It aims to improve efficiency, extensibility, and community adoption in ARLT research. VERLTOOL standardizes tool management via APIs, achieves asynchronous rollout execution for near 2x speedup, and demonstrates competitive performance across six ARLT domains. Evaluated on tasks like math, SQL, and visual reasoning, VERLTOOL achieves comparable results to specialized systems while offering a unified infrastructure. VERLTOOL provides a scalable foundation for tool-augmented RL research through a modular plugin architecture, enabling rapid tool integration. |
| Machine Learning | Baichuan-M2: Scaling Medical Capability with Large Verifier System (Read more on [arXiv](https://arxiv.org/abs/2509.02208) or [HuggingFace](https://huggingface.co/papers/2509.02208))| Jayok6, yuanshuai, sdujq, anselcmy, fairyang | This paper introduces Baichuan-M2, a medical AI model scaling medical capabilities through a large verifier system. The research addresses the gap between static benchmark performance and real-world clinical decision-making by creating a dynamic verification framework involving a Patient Simulator and Clinical Rubrics Generator. Baichuan-M2, a 32B-parameter model, is trained using multi-stage reinforcement learning with an improved GRPO algorithm. Evaluated on HealthBench, Baichuan-M2 outperforms open-source models, achieving a score above 32 on the HealthBench Hard benchmark. The dynamic verifier system is essential for aligning LLM capabilities with practical clinical applications, establishing a new Pareto front for medical AI deployment. |
| Multi-Modal | Kwai Keye-VL 1.5 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2509.01563) or [HuggingFace](https://huggingface.co/papers/2509.01563))| SXxtyz, Chengru, bhsc24, dingboyang, biaoYang | The paper introduces Keye-VL-1.5, an 8-billion parameter multimodal model that enhances video understanding while retaining general vision-language capabilities. It addresses challenges in video comprehension by introducing a Slow-Fast video encoding strategy, progressive pre-training to extend context length, and a comprehensive post-training pipeline for reasoning and alignment. The model demonstrates significant improvements, achieving state-of-the-art performance in video-centric benchmarks, such as a 6.5% improvement on Video-MMMU. Keye-VL-1.5 offers practical solutions for building more capable multimodal models, advancing video understanding and reasoning capabilities. |
| Reinforcement Learning | Implicit Actor Critic Coupling via a Supervised Learning Framework for
  RLVR (Read more on [arXiv](https://arxiv.org/abs/2509.02522) or [HuggingFace](https://huggingface.co/papers/2509.02522))| Lu Wang, Yukun Chen, Ze Gong, Longze Chen, Geaming | This paper introduces PACS, a novel Reinforcement Learning with Verifiable Rewards (RLVR) framework. It addresses the challenges of sparse rewards and unstable policy gradients in RLVR by reformulating the problem as a supervised learning task with cross-entropy loss. PACS implicitly couples actor and critic roles through shared parameterization, leading to stable and efficient training. Evaluated on mathematical reasoning tasks, PACS outperforms PPO and GRPO, achieving a 59.78% pass@256 on AIME 2025. The framework offers a promising avenue for LLMs post-training with verifiable rewards by improving reasoning performance. |
| Reinforcement Learning | DCPO: Dynamic Clipping Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2509.02333) or [HuggingFace](https://huggingface.co/papers/2509.02333))| Kai Lu, Chengfeng Dou, sdujq, GuoPD, yangshui | This paper introduces Dynamic Clipping Policy Optimization (DCPO), a novel reinforcement learning algorithm to enhance reasoning capabilities in large language models. The research aims to address zero gradient issues in existing methods like GRPO by dynamically adjusting clipping bounds based on token probabilities and standardizing rewards across training steps. DCPO achieves state-of-the-art performance on mathematical reasoning benchmarks, surpassing GRPO with an Avg@1 of 46.7 on AIME24 using the Qwen2.5-Math-7B model. DCPO's efficient data utilization enables AI practitioners to leverage generated data more effectively for reinforcement learning in LLMs, potentially reducing training costs and improving performance in complex reasoning tasks. The paper reports an average 28% improvement in non-zero advantages over GRPO. |
| Natural Language Processing | Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task
  Arithmetic (Read more on [arXiv](https://arxiv.org/abs/2509.01363) or [HuggingFace](https://huggingface.co/papers/2509.01363))| Bernard Ghanem, Mohammad Zbeeb, hammh0a | This paper introduces "reasoning vectors" for transferring chain-of-thought capabilities between large language models via task arithmetic. The research aims to extract and transfer reasoning abilities acquired through resource-intensive reinforcement learning. A reasoning vector is extracted from SFT and GRPO fine-tuned QWEN2.5 models on GSM8K and added to other models. Adding this vector improves performance on benchmarks like GSM8K (+4.9%) and BigBenchHard (+12.3% for the 1.5B model). The method enables practitioners to enhance models by reusing existing open-source models without further expensive training. |
| Computer Vision | GenCompositor: Generative Video Compositing with Diffusion Transformer (Read more on [arXiv](https://arxiv.org/abs/2509.02460) or [HuggingFace](https://huggingface.co/papers/2509.02460))| Lingen Li, Guangzhi Wang, Xiaodong Cun, Xiaoyu521, Ysz2022 | The paper introduces GenCompositor, a novel generative video compositing method using a Diffusion Transformer (DiT). It addresses the task of adaptively injecting foreground video into a target background video with user-specified trajectories and scales. The method employs a DiT pipeline with a background preservation branch, a DiT fusion block for dynamic element integration, and Extended Rotary Position Embedding (EROPE) for layout alignment. Experiments show that GenCompositor outperforms existing solutions in fidelity and consistency, achieving a 94.87% SSIM score. This enables AI practitioners to automate and customize video compositing with enhanced control and realism. |
| Natural Language Processing | Jointly Reinforcing Diversity and Quality in Language Model Generations (Read more on [arXiv](https://arxiv.org/abs/2509.02534) or [HuggingFace](https://huggingface.co/papers/2509.02534))| Tianlu, jcklcn, spermwhale, danyaljj, dogtooth | The paper introduces DARLING, a framework to jointly optimize for response quality and semantic diversity in language model generations. DARLING addresses the issue of reduced diversity during post-training by introducing a learned partition function to measure diversity at a semantic level. The method combines this diversity signal with a quality reward during online reinforcement learning. Experiments show DARLING outperforms quality-only RL baselines, producing outputs with higher quality and novelty, achieving higher pass@1 for solution quality in verifiable tasks. DARLING prevents diversity collapse and improves exploration in creative and instruction-following tasks. |
| Computer Vision | OpenVision 2: A Family of Generative Pretrained Visual Encoders for
  Multimodal Learning (Read more on [arXiv](https://arxiv.org/abs/2509.01644) or [HuggingFace](https://huggingface.co/papers/2509.01644))| Zirui Wang, Letian Zhang, Xianhang Li, Yanqing Liu, cihangxie | The paper introduces OpenVision 2, a simplified and more efficient generative visual encoder for multimodal learning. It aims to enhance training efficiency by removing the text encoder and contrastive loss from the original OpenVision architecture, retaining only the captioning loss. The methodology involves training solely with a caption loss and masking two-thirds of visual tokens during pretraining. Results show comparable performance to OpenVision on multimodal benchmarks while reducing training time by ~1.5x and memory usage by ~1.8x with ViT-L/14. This implies that a purely generative objective can rival contrastive methods, lowering computational costs and enabling greater scalability for vision encoders. |
| Computer Vision | M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via
  Self-Supervision (Read more on [arXiv](https://arxiv.org/abs/2509.01360) or [HuggingFace](https://huggingface.co/papers/2509.01360))| Yan-Jie Zhou, Heng Guo, Chengyu Fang, Zheng Jiang, Che Liu | The paper introduces M³Ret, a unified framework for zero-shot multimodal medical image retrieval via self-supervision. It addresses the need for modality-agnostic representations in medical imaging by training a single encoder on a large-scale dataset of 2D X-rays/ultrasounds, RGB endoscopy videos, and 3D CT scans. The method employs both generative (MAE) and contrastive (SimDINO) SSL paradigms to learn transferable features. M³Ret achieves state-of-the-art zero-shot image retrieval performance, surpassing DINOv3 and BMC-CLIP, achieving a Recall@5 of 0.345 on ChestXray14. This framework enables cross-modal retrieval, even generalizing to unseen modalities like MRI, thus paving the way for foundation models in medical image understanding. |
| Natural Language Processing | Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm
  Simulators for Conditional Synthetic Data Generation (Read more on [arXiv](https://arxiv.org/abs/2509.02040) or [HuggingFace](https://huggingface.co/papers/2509.02040))| Xiaolei Huang, Weisi Liu, kwangju | This paper introduces Genetic Prompt, a framework combining genetic algorithms with LLMs to augment synthetic data generation for NLP tasks. The research aims to improve the quality and diversity of synthetic data by treating textual attributes as genes and simulating genetic operations. Genetic Prompt outperforms state-of-the-art baselines, achieving a micro-F1 score of 86.7% on AGNews using a GPT-4o generator, and shows robust performance across various LLM sizes. Fusing synthetic data with real data significantly boosts downstream model performance, especially in class-imbalanced scenarios, indicating its practical utility for improving model generalization. |
| Machine Learning | Benchmarking Optimizers for Large Language Model Pretraining (Read more on [arXiv](https://arxiv.org/abs/2509.01440) or [HuggingFace](https://huggingface.co/papers/2509.01440))| mjaggi, MatPag, Andron00e | This paper benchmarks optimizers for pretraining large language models (LLMs). It aims to provide a comprehensive evaluation of optimization techniques by standardizing LLM pretraining scenarios and varying model size, batch size, and training duration. Through careful tuning, the study offers guidance on selecting optimizers, reporting that AdEMAMix and MARS perform well at larger scales, outperforming AdamW. The study also reveals that weight decay and warmup influence performance and require careful tuning, and provides code for reproducible research and benchmarking of future methods. The results offer practitioners insights into up-to-date optimizer selection and best practices for LLM pretraining. |
| Natural Language Processing | The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in
  LLMs with Camlang (Read more on [arXiv](https://arxiv.org/abs/2509.00425) or [HuggingFace](https://huggingface.co/papers/2509.00425))| Solomon Tsai, Zhujun Jin, Yixuan Liu, Fenghua Liu, yulongchen | This paper introduces Camlang, a novel constructed language designed to diagnose metalinguistic reasoning in LLMs. It aims to evaluate whether LLMs can master a new language through explicit grammar rules and lexical mappings, similar to human second-language acquisition. The methodology involves adapting CommonsenseQA into Camlang (Camlang-CSQA-v0) and evaluating LLM performance against human benchmarks. Results show that GPT-5 achieves 47% EM accuracy in Camlang, significantly lower than its 98% in English and below human performance at 87%, indicating a gap in metalinguistic competence. The study implies that current LLMs struggle to integrate explicit grammar rules and lexical lookups for reasoning in unfamiliar languages, relying more on shallow alignment. |
| Machine Learning | Fantastic Pretraining Optimizers and Where to Find Them (Read more on [arXiv](https://arxiv.org/abs/2509.02046) or [HuggingFace](https://huggingface.co/papers/2509.02046))| Percy Liang, Tengyu Ma, David Hall, Kaiyue Wen | This paper investigates the performance of various deep learning optimizers in language model pretraining, addressing concerns about previously claimed speedups over AdamW. The study conducts a systematic benchmark of ten optimizers across different model scales and data-to-model ratios, employing rigorous hyperparameter tuning and end-of-training evaluations. Results indicate that the actual speedups are generally lower than reported, decreasing to 1.1x for 1.2B parameter models, and matrix-based optimizers outperform scalar-based ones. The study highlights the importance of comprehensive hyperparameter optimization and evaluations at the end of training for fair comparisons, suggesting that matrix-based optimizer speedups are inversely proportional to model scale. |
| Machine Learning | Universal Deep Research: Bring Your Own Model and Strategy (Read more on [arXiv](https://arxiv.org/abs/2509.00244) or [HuggingFace](https://huggingface.co/papers/2509.00244))| Pavlo Molchanov, Peter Belcak | The paper introduces Universal Deep Research (UDR), a generalist agentic system for customizable deep research strategies. The primary objective is to enable users to create and refine their own research workflows around any language model without additional training. UDR converts user-defined research queries into executable code snippets, allowing for complex procedures and control over research processes. The system demonstrated improved reliability by generating code to follow user-defined strategies in a single pass. UDR offers AI practitioners a framework for building customizable and auditable research agents, decoupling control logic from computationally expensive language model reasoning. |
| Machine Learning | FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in
  Diverse Adventure Games (Read more on [arXiv](https://arxiv.org/abs/2509.01052) or [HuggingFace](https://huggingface.co/papers/2509.01052))| Dongmin Park, Jaehyeon Son, Heeseung Yun, Junseo Kim, ahnpersie | The paper introduces FlashAdventure, a benchmark for GUI agents evaluating full story arc completion in diverse Flash-based adventure games. The research aims to address the limitations of existing game benchmarks by focusing on complete storylines and the observation-behavior gap. They propose CUA-as-a-Judge for automatic evaluation and COAST, an agentic framework leveraging long-term clue memory, resulting in a milestone completion rate improvement. Experiments reveal current GUI agents struggle with full story arcs, while COAST improves milestone completion by bridging the observation-behavior gap, though a gap with human performance remains. This suggests further research is needed to improve time-dependent observation association in GUI agents. |
| Computer Vision | Discrete Noise Inversion for Next-scale Autoregressive Text-based Image
  Editing (Read more on [arXiv](https://arxiv.org/abs/2509.01984) or [HuggingFace](https://huggingface.co/papers/2509.01984))| Amin Heyrani Nobar, Ngan Hoai Nguyen, Ligong Han, Xiaoxiao He, quandao10 | This paper introduces VARIN, a noise inversion-based editing technique for text-to-image editing using visual autoregressive (VAR) models. The research aims to enable prompt-guided image editing in VAR models without additional training while preserving original image details. VARIN employs a novel Location-aware Argmax Inversion (LAI) to generate inverse Gumbel noises for precise image reconstruction and controllable edits. Experiments demonstrate that VARIN effectively modifies source images according to prompts while maintaining background and structure, achieving a PSNR of 26.54 compared to other methods. This allows practitioners to effectively edit images with VAR models using textual prompts while retaining image integrity. |
| Machine Learning | MobiAgent: A Systematic Framework for Customizable Mobile Agents (Read more on [arXiv](https://arxiv.org/abs/2509.00531) or [HuggingFace](https://huggingface.co/papers/2509.00531))| Wangbo Gong, Yisheng Zhao, Xi Zhao, fengerhu, sjtuzc | MobiAgent is presented as a comprehensive mobile agent system designed to enhance accuracy and efficiency in real-world task execution. The research addresses limitations in existing agent models by introducing the MobiMind-series agent models, the AgentRR acceleration framework, and the MobiFlow benchmarking suite. A key methodology involves an AI-assisted agile data collection pipeline to reduce manual annotation costs. Compared to general-purpose LLMs and specialized GUI agent models, MobiAgent achieves state-of-the-art performance in mobile scenarios. This work provides AI practitioners with a customizable framework for developing more robust and efficient mobile agents. |
| Computer Vision | MedDINOv3: How to adapt vision foundation models for medical image
  segmentation? (Read more on [arXiv](https://arxiv.org/abs/2509.02379) or [HuggingFace](https://huggingface.co/papers/2509.02379))| Xiaofeng Yang, Yuheng Li, wy20030128, yuxianglai117, mcl0222 | The paper introduces MedDINOv3, a framework for adapting vision foundation models to medical image segmentation. It addresses the challenge of transferring representations learned from natural images to medical imaging, while overcoming the suboptimal performance of ViT backbones. MedDINOv3 employs a multi-scale token aggregation architecture and domain-adaptive pretraining on a large CT dataset (CT-3M) using a multi-stage DINOv3 recipe. The resulting model achieves state-of-the-art performance on four segmentation benchmarks, surpassing nnU-Net on OAR segmentation by +2.57% DSC on AMOS22 and +5.49% DSC on BTCV. This demonstrates the potential of vision foundation models as unified backbones for medical image segmentation. |
| Natural Language Processing | AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with
  Knowledge Augmentation for Robust Constitutional Alignment of Language Models (Read more on [arXiv](https://arxiv.org/abs/2509.02133) or [HuggingFace](https://huggingface.co/papers/2509.02133))| Rahul Karthikeyan, Shivam Dubey, Aryan Kasat, Snehasis Mukhopadhyay, amanchadha | The paper introduces AMBEDKAR, a framework for multi-level bias elimination in Large Language Models (LLMs) with a focus on caste and religion in the Indian context. The research aims to mitigate societal biases reflected in LLMs, guiding outputs toward fairness and neutrality. AMBEDKAR incorporates a constitution-aware decoding layer with a speculative decoding algorithm that reduces bias during generation using a small language model (SLM) and a constitutionally-guided LLM verifier. The approach yields an absolute reduction of bias up to 26.41% compared to baseline and suggests the reinterpretation of speculative decoding not merely as an efficiency tool but as a mechanism for fairness. The framework empowers AI practitioners with a constitution-grounded approach, providing a means to steer LLM outputs towards alignment with fairness principles derived from the Indian constitution. |
| Multi-Modal | Improving Large Vision and Language Models by Learning from a Panel of
  Peers (Read more on [arXiv](https://arxiv.org/abs/2509.01610) or [HuggingFace](https://huggingface.co/papers/2509.01610))| Simon Jenni, Jing Shi, Jefferson Hernandez, kushalkafle, vicenteor | This paper introduces Panel-of-Peers (PoP), a novel self-improvement learning framework for enhancing Large Vision and Language Models (LVLMs). The research aims to overcome limitations of human-curated, machine-generated, and self-supervised preference data by using a panel of LVLMs to evaluate and learn from each other's collective outputs through an iterative process. The key methodology involves generating, assessing, and refining outputs in response to curated prompts. Experiments show significant improvement across multiple benchmarks; notably, PoP increases the average score on fifteen benchmarks from 48% to 57%. PoP provides a scalable alternative to self-supervised alignment by leveraging peer evaluations for improved model performance without extensive human-labeled datasets. |
| Computer Vision | ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association (Read more on [arXiv](https://arxiv.org/abs/2509.01584) or [HuggingFace](https://huggingface.co/papers/2509.01584))| Daniel Cremers, Xi Wang, Shenhan Qian, zhangganlin | The paper introduces ViSTA-SLAM, a real-time monocular visual SLAM system. The research aims to achieve high-quality 3D reconstruction and accurate trajectory estimation without requiring camera intrinsics. It employs a symmetric two-view association (STA) model as the frontend, estimating relative poses and regressing local pointmaps, combined with Sim(3) pose graph optimization. The system achieves state-of-the-art performance on the 7-Scenes dataset, improving ATE RMSE to 0.055. ViSTA-SLAM offers a lightweight and accurate alternative for visual SLAM, especially in scenarios with diverse camera setups and limited computational resources. |
| Computer Vision | Towards More Diverse and Challenging Pre-training for Point Cloud
  Learning: Self-Supervised Cross Reconstruction with Decoupled Views (Read more on [arXiv](https://arxiv.org/abs/2509.01250) or [HuggingFace](https://huggingface.co/papers/2509.01250))| Junchi Yan, Shaofeng Zhang, Xiangdong Zhang | This paper introduces Point-PQAE, a novel cross-reconstruction generative framework for self-supervised point cloud learning. It addresses the challenge of informative pre-training by reconstructing one decoupled view of a point cloud from another. The method involves a crop mechanism for view generation and a positional encoding to represent 3D relative positions, enhancing the pre-training difficulty. Point-PQAE outperforms Point-MAE by 6.5%, 7.0%, and 6.7% on ScanObjectNN with the MLP-LINEAR protocol. The proposed method offers AI practitioners a more diverse and challenging pre-training strategy for improved 3D representation learning. |
| Natural Language Processing | SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction (Read more on [arXiv](https://arxiv.org/abs/2509.00581) or [HuggingFace](https://huggingface.co/papers/2509.00581))| bindsch, amanchadha, shollercoaster | The paper introduces SQL-of-Thought, a novel multi-agent framework designed to improve text-to-SQL conversion accuracy. It aims to enhance SQL query generation through schema linking, subproblem identification, guided query planning, and an error correction loop. The method employs taxonomy-guided dynamic error modification informed by in-context learning. Experiments on the Spider dataset show SQL-of-Thought achieves state-of-the-art results, with an execution accuracy of 91.59%. This implies that incorporating structured reasoning and error feedback leads to more reliable SQL querying systems. |
| Computer Vision | C-DiffDet+: Fusing Global Scene Context with Generative Denoising for
  High-Fidelity Object Detection (Read more on [arXiv](https://arxiv.org/abs/2509.00578) or [HuggingFace](https://huggingface.co/papers/2509.00578))| Vito Renó, Abdenour Hadid, Bekhouche, xkruvox, ldb0071 | C-DiffDet+ improves object detection by fusing global scene context with generative denoising. The paper aims to enhance object detection fidelity, particularly in fine-grained tasks. It introduces a Context-Aware Fusion (CAF) module and a Global Context Encoder (GCE) to integrate scene-level understanding with local proposal features. On the CarDD dataset, C-DiffDet+ achieves 64.8% AP, improving upon prior state-of-the-art. The approach provides AI practitioners with a method for more robust object detection, especially in scenarios requiring contextual understanding. |
| Machine Learning | Metis: Training Large Language Models with Advanced Low-Bit Quantization (Read more on [arXiv](https://arxiv.org/abs/2509.00404) or [HuggingFace](https://huggingface.co/papers/2509.00404))| Hengjie Cao, wenzi001, ZhouJixian, cnyangyifeng, ChenMengyi | The paper presents Metis, a training framework for large language models with advanced low-bit quantization that addresses anisotropy in parameter distributions. It aims to mitigate the conflict between limited numerical precision and wide data distributions through spectral decomposition and adaptive learning rates. The methodology involves spectral decomposition with random embedding, adaptive spectral learning rate, and a dual-range regularizer to ensure stable low-bit training. Experiments demonstrate that FP8 training matches or surpasses FP32 baselines, and FP4 training achieves accuracy comparable to FP32. Metis enables more robust and scalable LLM training under advanced low-bit quantization, potentially broadening access to efficient LLMs. |
| Computer Vision | FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable
  Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2508.20586) or [HuggingFace](https://huggingface.co/papers/2508.20586))| Zhen Wang, Zhuandi He, Shiyue Zhang, Yanwei Lei, zhengchong | The paper presents FastFit, a novel framework for high-speed multi-reference virtual try-on, addressing the limitations of existing methods in handling multi-item outfits and computational inefficiency. It accelerates the try-on process by decoupling reference feature encoding from the denoising process via a cacheable diffusion architecture, using Reference Class Embedding and Semi-Attention mechanisms to enable a Reference KV Cache. FastFit achieves an average 3.5x speedup compared to comparable methods while maintaining high fidelity. The authors also introduce DressCode-MR, a large-scale dataset for multi-reference virtual try-on, to facilitate further research. Experiments on multiple datasets, including DressCode-MR, VITON-HD, and DressCode, demonstrate that FastFit improves image fidelity while offering significant inference efficiency, potentially leading to more practical virtual try-on applications. |
