

## Papers for 2025-09-04

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Robix: A Unified Model for Robot Interaction, Reasoning and Planning (Read more on [arXiv](https://arxiv.org/abs/2509.01106) or [HuggingFace](https://huggingface.co/papers/2509.01106))| Zixuan Wang, Wei Li, Heng Dong, Mengxi Zhang, Huang Fang | The paper introduces Robix, a unified vision-language model for robot interaction, reasoning, and planning. It aims to integrate robot reasoning, task planning, and natural language interaction within a single architecture. Robix leverages chain-of-thought reasoning and a three-stage training strategy including pretraining, supervised finetuning, and reinforcement learning. Experiments show that Robix outperforms open-source and commercial baselines, exceeding Gemini-2.5-Pro by 3.0 and 11.8 percentage points in accuracy on out-of-distribution settings. The unified end-to-end architecture promises enhanced flexibility and adaptability for robots in real-world scenarios. |
| Natural Language Processing | Open Data Synthesis For Deep Research (Read more on [arXiv](https://arxiv.org/abs/2509.00375) or [HuggingFace](https://huggingface.co/papers/2509.00375))| Zheng Liu, Hongjin Qian, Kun Luo, ZiyiXia | The paper introduces InfoSeek, a framework for synthesizing complex Deep Research tasks to train large language models. It formalizes Deep Research tasks as Hierarchical Constraint Satisfaction Problems (HCSPs) and uses a dual-agent system to recursively build Research Trees from webpages. The primary objective is to address the limitations of existing benchmarks and synthetic datasets in capturing the complexity of deep research. Experiments show that models trained on InfoSeek outperform strong baselines, with a 3B LLM surpassing larger 32B models on BrowseComp-Plus. The InfoSeek framework enables the creation of high-quality datasets for advancing deep research capabilities in LLMs. |
| Natural Language Processing | LMEnt: A Suite for Analyzing Knowledge in Language Models from
  Pretraining Data to Representations (Read more on [arXiv](https://arxiv.org/abs/2509.03405) or [HuggingFace](https://huggingface.co/papers/2509.03405))| Yoav Gur-Arieh, Ido Cohen, Alon Gilae-Dotan, Daniela Gottesman, mega | The paper introduces LMEnt, a suite for analyzing knowledge acquisition in language models (LMs) during pretraining. It aims to understand how LMs turn training data into knowledge representations, focusing on the interplay between data, training dynamics, and internal knowledge mechanisms. LMEnt includes a knowledge-rich Wikipedia-based pretraining corpus annotated with entity mentions, an entity-based retrieval method achieving up to 80.4% improvement in relevance compared to string-based methods, and 12 pretrained models with up to 1B parameters. Experiments demonstrate that LMEnt models achieve comparable performance on factual question answering tasks, with fact frequency being a key factor in knowledge acquisition.  LMEnt provides a controlled environment to analyze connections between pretraining data and downstream performance, facilitating studies on knowledge representation, plasticity, and learning dynamics. |
| Computer Vision | Mixture of Global and Local Experts with Diffusion Transformer for
  Controllable Face Generation (Read more on [arXiv](https://arxiv.org/abs/2509.00428) or [HuggingFace](https://huggingface.co/papers/2509.00428))| Kai Li, Yue Li, Xing Fu, Shun Zhang, Xuechao Zou | The paper introduces Face-MoGLE, a framework for high-quality, controllable face generation using a Diffusion Transformer. It addresses the challenge of balancing semantic controllability and photorealism by decoupling semantic masks and employing a mixture of global and local experts. The methodology involves mask-conditioned space factorization, a mixture of global and local expert modules, and a dynamic gating network. Experiments show that Face-MoGLE achieves state-of-the-art performance, demonstrated by an FID score of 22.24 on multimodal face generation. The framework offers AI practitioners a unified and flexible solution for controllable face generation across various tasks. |
| Computer Vision | MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware
  Alignment and Disentanglement (Read more on [arXiv](https://arxiv.org/abs/2509.01977) or [HuggingFace](https://huggingface.co/papers/2509.01977))| Hualiang Wang, Qiaoqiao Jin, Mushui Liu, Siming Fu, Dong She | The paper introduces MOSAIC, a novel framework for multi-subject personalized image generation. It aims to improve identity fidelity and semantic coherence by explicitly modeling semantic correspondence and disentangling features of multiple subjects. The method uses SemAlign-MS, a new dataset with fine-grained semantic correspondences, to train a correspondence-aware attention mechanism and a multi-reference disentanglement loss. Experiments show MOSAIC achieves state-of-the-art performance, maintaining high fidelity with 4+ reference subjects. The method offers AI practitioners a way to generate high-quality images with multiple subjects by improving fine-grained detail retention. |
