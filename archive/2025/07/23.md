

## Papers for 2025-07-23

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.16784) or [HuggingFace](https://huggingface.co/papers/2507.16784))| Tina Li, Nathaniel Morgan, Hongyin Luo, thejackobrien, drkylj | The paper introduces the Thread Inference Model (TIM) and TIMRUN, an inference runtime, to overcome context length limitations in large language models for long-horizon reasoning. It aims to improve reasoning accuracy and efficiency by modeling natural language as reasoning trees with recursive subtasks. TIMRUN enables virtually unlimited working memory and multi-hop tool calls within a single language model inference by pruning irrelevant subtasks to focus on working memory. Experiments show sustained high inference throughput and accurate reasoning on mathematical tasks, handling information retrieval challenges, but specific quantitative metrics beyond “high inference throughput” are not provided. The system allows for efficient single-model reasoning and facilitates the building of agents through a concise toolkit. |
| Multi-Modal | Step-Audio 2 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2507.16632) or [HuggingFace](https://huggingface.co/papers/2507.16632))| Chao Yan, Boyong Wu, Insects, SmailAA, petronny | Step-Audio 2 is an end-to-end multi-modal large language model for industry-strength audio understanding and speech conversation. The paper investigates the integration of a latent audio encoder and reasoning-centric reinforcement learning for improved ASR and audio understanding. The model is trained on 680 billion tokens of text and 8 million hours of audio data, using techniques like RAG and external tools. Step-Audio 2 achieves state-of-the-art performance in ASR, with a 3.18% average word error rate (WER) on English test sets and a 3.11% character error rate (CER) on Chinese test sets.  The enhanced audio understanding and conversational abilities offer AI practitioners a robust platform for developing more natural and intelligent speech-based applications, potentially mitigating hallucination and improving real-time performance. |
| Natural Language Processing | MegaScience: Pushing the Frontiers of Post-Training Datasets for Science
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.16812) or [HuggingFace](https://huggingface.co/papers/2507.16812))| Pengfei Liu, SinclairWang, Vfrz | This paper introduces MegaScience, a large-scale dataset for post-training scientific reasoning in AI models. The research aims to address the lack of high-quality, verifiable scientific reasoning datasets by curating and mixing existing open-source datasets and creating a new dataset, TextbookReasoning. Key methodologies involve data selection, systematic ablation studies, and a comprehensive evaluation system across 15 benchmarks. Experiments show that models trained on MegaScience outperform official instruct models, with Qwen2.5-7B achieving an overall average improvement of 2.21%. The results imply a significant scaling benefit for scientific tuning, offering AI practitioners a valuable resource for advancing scientific reasoning research. |
| Computer Vision | Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated
  Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2507.08422) or [HuggingFace](https://huggingface.co/papers/2507.08422))| Se Young Chun, Agorium, hirussell, ignow | This paper introduces Region-Adaptive Latent Upsampling (RALU), a training-free framework for accelerating diffusion transformers along the spatial dimension. The primary objective is to reduce the heavy computation associated with diffusion transformers during high-fidelity image generation. RALU employs a mixed-resolution sampling strategy across three stages, combined with noise-timestep rescheduling to adapt noise levels across resolutions. Results demonstrate up to 7.0x speed-up on FLUX and 3.0x on Stable Diffusion 3 with minimal image quality degradation. RALU offers AI practitioners a computationally efficient method for deploying diffusion transformers without extensive retraining or compromising generation quality. |
| Reinforcement Learning | Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.16814) or [HuggingFace](https://huggingface.co/papers/2507.16814))| Songyang Gao, Harold-lkk, vanilla1116, haitengzhao, shenjunhao | This paper introduces Semi-Off-Policy Reinforcement Learning for vision-language slow-thinking reasoning (SOPHIA) to enhance large vision-language models (LVLMs). The research aims to improve LVLMs' reasoning ability in multimodal tasks by addressing limitations of on-policy RL and visual hallucinations in off-policy RL. SOPHIA builds a semi-off-policy behavior model combining on-policy visual understanding from a trainable LVLM with off-policy slow-thinking reasoning from a language model, assigning outcome-based rewards and propagating visual rewards backward. Experiments with InternVL3.0-38B show an 8.50% average improvement in performance across multiple benchmarks, even outperforming GPT-4.1 on MathVision and OlympiadBench. SOPHIA offers a better policy initialization for further on-policy training by leveraging propagated rewards and visual understanding. |
| Computer Vision | ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent
  Planning (Read more on [arXiv](https://arxiv.org/abs/2507.16815) or [HuggingFace](https://huggingface.co/papers/2507.16815))| Fu-En Yang, Yu-Chiang Frank Wang, Yueh-Hua Wu, cmhungsteve, jasper0314-huang | The paper introduces ThinkAct, a dual-system framework for vision-language-action reasoning using reinforced visual latent planning. It aims to bridge high-level reasoning with low-level action execution for complex embodied AI tasks. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency, then compresses these plans into a visual plan latent. Experiments demonstrate that ThinkAct achieves an 84.4% overall success rate on the LIBERO benchmark, enabling few-shot adaptation and long-horizon planning. The proposed framework offers a scalable approach for developing more adaptable embodied AI systems by combining structured reasoning with real-world actions. |
| Computer Vision | HOComp: Interaction-Aware Human-Object Composition (Read more on [arXiv](https://arxiv.org/abs/2507.16813) or [HuggingFace](https://huggingface.co/papers/2507.16813))| Rynson W. H. Lau, Jinyuan Jia, Dong Liang, LeoLau | The paper introduces HOComp, a novel approach for interaction-aware human-object composition that seamlessly integrates a foreground object into a human-centric background while ensuring harmonious interactions and visual consistency. It addresses the challenge of generating realistic human-object interactions with consistent appearances by introducing MLLMs-driven region-based pose guidance (MRPG) and Detail-Consistent Appearance Preservation (DCAP). The approach utilizes MLLMs to identify interaction regions and types and incorporates pose landmarks for fine-grained control, along with a multi-view appearance loss and background consistency loss. Experimental results on the newly proposed IHOC dataset demonstrate HOComp's effectiveness, achieving an HOI-Score of 87.39. This research enables AI practitioners to generate more realistic and controllable human-object compositions. |
| Multi-Modal | Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.16746) or [HuggingFace](https://huggingface.co/papers/2507.16746))| Zikui Cai, Kaiyu Yue, deqing, charleslwang, leonli66 | This paper introduces ZEBRA-COT, a large-scale dataset for interleaved vision-language reasoning. It aims to address the challenges of training multimodal models by providing high-quality visual Chain-of-Thought data across diverse tasks. The methodology involves curating a dataset of 182,384 samples across four categories, focusing on tasks where visual reasoning is natural. Fine-tuning the Anole-7B model on ZEBRA-COT results in a 12% improvement in test-set accuracy and up to 13% gain on VLM benchmarks. ZEBRA-COT offers a resource for developing and evaluating visual CoT, enabling models to generate interleaved visual reasoning chains. |
| Multi-Modal | Experience is the Best Teacher: Grounding VLMs for Robotics through
  Self-Generated Memory (Read more on [arXiv](https://arxiv.org/abs/2507.16713) or [HuggingFace](https://huggingface.co/papers/2507.16713))| Christopher E. Mower, Changan Chen, René Zurbrügg, Kaixian Qu, Guowei Lan | This paper introduces EXPTEACH, a framework for grounding Vision Language Models (VLMs) in robotics using a self-generated memory of real-world experiences. The research aims to enhance VLMs' ability to plan and execute robotic tasks by making them aware of robot-specific capabilities and limitations. The methodology involves a closed-loop system where the VLM plans actions, verifies outcomes, reflects on failures, and adapts behaviors, storing these experiences in long-term memory for retrieval-augmented generation (RAG) to guide future tasks. Experiments on 12 real-world scenarios show that grounding with long-term memory boosts single-trial success rates from 22% to 80%. The main implication is a significant improvement in the effectiveness and generalizability of VLMs in robotics applications through autonomous self-grounding. |
| Natural Language Processing | RefCritic: Training Long Chain-of-Thought Critic Models with Refinement
  Feedback (Read more on [arXiv](https://arxiv.org/abs/2507.15024) or [HuggingFace](https://huggingface.co/papers/2507.15024))| Hongyu Lin, Bowen Yu, Le Yu, Hao Xiang, Qiaoyu Tang | The paper introduces RefCritic, a framework to train long chain-of-thought critic models with refinement feedback. It addresses the challenge of creating effective critic modules for large language models capable of providing in-depth critiques and actionable feedback. The method employs a dual-reward reinforcement learning approach, optimizing for both solution-level correctness and policy model refinement. Experiments on Qwen2.5-14B-Instruct and DeepSeek-R1-Distill-Qwen-14B across five benchmarks show RefCritic achieves consistent gains, such as 6.8% and 7.2% on AIME25 for the respective base models. This provides AI practitioners with a methodology to develop more effective critic models that enhance the reasoning and problem-solving abilities of large language models. |
| Natural Language Processing | SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced
  Academic Search (Read more on [arXiv](https://arxiv.org/abs/2507.15245) or [HuggingFace](https://huggingface.co/papers/2507.15245))| Jinxin Xie, Longbin Yu, Qian Kou, Yuduo Li, Xiaofeng Shi | This paper introduces SPAR, a multi-agent framework for academic literature retrieval utilizing large language models (LLMs). The research aims to enhance academic search by incorporating RefChain-based query decomposition and evolution. SPAR employs specialized agents for query understanding, retrieval, and relevance assessment, demonstrating a significant improvement over strong baselines; SPAR achieves an F1 score of 0.3843 on AutoScholar, a 56% improvement over PaSa. The results suggest that structured, agent-based retrieval frameworks effectively address the complexities of modern academic search, providing a scalable and interpretable solution for AI practitioners. |
| Natural Language Processing | Does More Inference-Time Compute Really Help Robustness? (Read more on [arXiv](https://arxiv.org/abs/2507.15974) or [HuggingFace](https://huggingface.co/papers/2507.15974))| Chawin Sitawarin, Weichen Yu, Jiachen T. Wang, Chong Xiang, Tong Wu | The paper investigates whether increased inference-time computation improves the robustness of large language models (LLMs). It challenges the assumption that intermediate reasoning steps are hidden from adversaries and finds an inverse scaling law: exposing intermediate reasoning steps reduces robustness. The methodology involves experimenting with open-source reasoning LLMs against prompt injection, prompt extraction, and harmful requests. Results show robustness consistently deteriorates with increased inference-time computation when reasoning steps are exposed. AI practitioners should carefully weigh the trade-offs before applying inference-time scaling in security-sensitive applications. |
| Machine Learning | Steering Out-of-Distribution Generalization with Concept Ablation
  Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2507.16795) or [HuggingFace](https://huggingface.co/papers/2507.16795))| Senthooran Rajamanoharan, Samuel Marks, Adam Karvonen, Caden Juang, Helena Casademunt | The paper introduces Concept Ablation Fine-Tuning (CAFT), a technique for steering out-of-distribution (OOD) generalization in LLMs using interpretability tools. It addresses the problem of unintended OOD generalization by ablating directions in an LLM's latent space corresponding to undesired concepts during fine-tuning. CAFT is applied to tasks including emergent misalignment, reducing misaligned responses by 10x without degrading performance on the training distribution.  The primary result is a successful demonstration of controlling LLM generalization without modifying training data. The implication for AI practitioners is a novel approach for improving the safety and reliability of LLMs in scenarios where modifying training data is impractical. |
| Computer Vision | ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via
  Gaussian Splatting (Read more on [arXiv](https://arxiv.org/abs/2507.15454) or [HuggingFace](https://huggingface.co/papers/2507.15454))| Yixuan Li, Lihan Jiang, Linning Xu, Mulin Yu, Ruijie Zhu | ObjectGS is a novel object-aware 3D scene reconstruction framework using Gaussian Splatting that incorporates semantic understanding. The research aims to unify 3D scene reconstruction with semantic understanding by modeling individual objects as local anchors that generate neural Gaussians and share object IDs. ObjectGS employs object ID labeling/voting using a SAM-based segmentation pipeline, object-aware neural Gaussian generation using dynamic anchor growing/pruning, and discrete Gaussian semantic modeling with a classification loss. Experiments show ObjectGS outperforms state-of-the-art methods on open-vocabulary segmentation (e.g., achieving 88.2% mIoU on the LERF-Mask dataset) and panoptic segmentation tasks. This unified framework allows for more precise object-level reconstruction and seamless integration with applications like mesh extraction and scene editing. |
