

## Papers for 2025-07-03

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Kwai Keye-VL Technical Report (Read more on [arXiv](https://arxiv.org/abs/2507.01949) or [HuggingFace](https://huggingface.co/papers/2507.01949))| huxiao09, yw95, TinaGao, hjy, caojiangxia | The paper introduces Kwai Keye-VL, an 8-billion parameter multimodal model for short-video understanding. The research aims to improve short-video comprehension while maintaining general vision-language capabilities. Keye-VL is trained using a four-stage pre-training and two-phase post-training process on a massive dataset, including video data and reasoning-focused data mixtures. The model achieves state-of-the-art results on public video benchmarks and KC-MMBench, a new benchmark for short-video scenarios, with Keye-VL obtaining an average accuracy of 68.03% on KC-MMBench. The findings offer insights into building next-generation MLLMs for video, improving performance and user experience in commercial applications. |
| Computer Vision | LongAnimation: Long Animation Generation with Dynamic Global-Local
  Memory (Read more on [arXiv](https://arxiv.org/abs/2507.01945) or [HuggingFace](https://huggingface.co/papers/2507.01945))| Zhendong Mao, Yihao Meng, Mengqi Huang, CNcreator0331 | The paper introduces LongAnimation, a novel framework for long animation colorization focusing on maintaining long-term color consistency. It addresses the limitations of existing local paradigms by proposing a dynamic global-local paradigm that dynamically extracts and fuses color features from historical and current generations using a Dynamic Global-Local Memory (DGLM) module. The methodology includes a SketchDiT for feature extraction and a Color Consistency Reward for refinement. Experiments demonstrate a significant improvement in long-term animation colorization, achieving a 49.1% improvement in FVD compared to existing methods. LongAnimation offers AI practitioners a robust solution for automated animation colorization, reducing labor costs and ensuring temporal color stability. |
| Computer Vision | Depth Anything at Any Condition (Read more on [arXiv](https://arxiv.org/abs/2507.01634) or [HuggingFace](https://huggingface.co/papers/2507.01634))| Qibin Hou, Bowen Yin, Modi Jin, BBBBCHAN | The paper introduces DepthAnything-AC, a foundation monocular depth estimation (MDE) model robust to diverse environmental conditions. It aims to improve depth estimation in complex scenarios by addressing limitations of existing MDE models under challenging conditions like illumination variations and weather. The approach uses unsupervised consistency regularization finetuning with a spatial distance constraint to learn patch-level relationships from unlabeled data. Experiments on the enhanced multi-condition DA-2K benchmark show improved performance under complex conditions (e.g., 0.862 to 0.880 on DA-2K blur), while maintaining general capabilities. The model provides AI practitioners with a robust MDE solution for open-world applications. |
| Multi-Modal | A Survey on Vision-Language-Action Models: An Action Tokenization
  Perspective (Read more on [arXiv](https://arxiv.org/abs/2507.01925) or [HuggingFace](https://huggingface.co/papers/2507.01925))| Zhang Chen, Fengshuo Bai, Yifan Zhong, Feernnn, phython96 | This survey paper provides a unified perspective on Vision-Language-Action (VLA) models by categorizing them based on action tokenization. It aims to understand the design choices in VLA models, focusing on how action tokens are formulated. The paper proposes a taxonomy of action tokens, including language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning, analyzing their strengths and limitations. It synthesizes an outlook on VLA model evolution, highlighting promising directions for future research without concrete quantitative metrics. This work offers guidance for developing general-purpose intelligence by focusing on a core design aspect of VLA models, potentially improving their development and application in various embodied AI tasks. |
| Computer Vision | FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model (Read more on [arXiv](https://arxiv.org/abs/2507.01953) or [HuggingFace](https://huggingface.co/papers/2507.01953))| Ziwei Liu, Jinghao Wang, Chenyang Si, Yukang Cao | This paper introduces FreeMorph, a tuning-free method for generalized image morphing using diffusion models. It aims to address the limitations of existing methods, such as time constraints and semantic/layout discrepancies. FreeMorph employs guidance-aware spherical interpolation and a step-oriented variation trend to achieve controlled transitions. The method outperforms existing approaches by generating high-fidelity image sequences in under 30 seconds, being 10x-50x faster. This technique offers AI practitioners an efficient and effective solution for image morphing without per-instance training. |
| Computer Vision | Locality-aware Parallel Decoding for Efficient Autoregressive Image
  Generation (Read more on [arXiv](https://arxiv.org/abs/2507.01957) or [HuggingFace](https://huggingface.co/papers/2507.01957))| Kelly Peng, Shang Yang, Chengyue Wu, Luke J. Huang, Zhuoyang Zhang | The paper introduces Locality-aware Parallel Decoding (LPD) to accelerate autoregressive image generation by reducing generation latency. It aims to enhance parallelization in next-patch prediction while preserving image quality through a flexible parallelized autoregressive modeling architecture and a locality-aware generation ordering schedule. The key methodology involves using learnable position query tokens and a generation order that minimizes intra-group dependencies. The results demonstrate a reduction in generation steps from 256 to 20 (256x256 res.) on ImageNet class-conditional generation without compromising quality, achieving at least 3.4x lower latency compared to previous parallelized autoregressive models. LPD offers a more efficient approach for image generation suitable for integration with multimodal systems by enabling a more efficient decoding strategy. |
| Multi-Modal | JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching (Read more on [arXiv](https://arxiv.org/abs/2506.23552) or [HuggingFace](https://huggingface.co/papers/2506.23552))| Youngjung Uh, Jaesik Park, Jaeseok Jung, Mingi Kwon, alex4727 | The paper introduces JAM-Flow, a unified flow-matching-based framework for joint audio and facial motion synthesis. It aims to simultaneously model and generate synchronized speech and lip movements conditioned on diverse inputs like text, audio, and video. The methodology employs a novel Multi-Modal Diffusion Transformer (MM-DiT) architecture, incorporating Motion-DiT and Audio-DiT modules coupled via selective joint attention and RoPE. Experimental results demonstrate competitive performance with specialized state-of-the-art systems, achieving, for example, LSE-C of 8.086 on HDTF. JAM-Flow offers AI practitioners a flexible and coherent solution for holistic audio-visual synthesis, enabling tasks like talking head generation, TTS, and cross-modal reconstruction within a single architecture. |
| Computer Vision | STR-Match: Matching SpatioTemporal Relevance Score for Training-Free
  Video Editing (Read more on [arXiv](https://arxiv.org/abs/2506.22868) or [HuggingFace](https://huggingface.co/papers/2506.22868))| Bohyung Han, Junoh Kang, jslee525 | The paper introduces STR-Match, a training-free video editing algorithm for producing spatiotemporally coherent videos. It addresses limitations in previous methods by modeling spatiotemporal pixel relevance using a novel STR score. This score leverages 2D spatial attention and 1D temporal modules from text-to-video diffusion models, enabling latent optimization without 3D attention mechanisms. Experiments demonstrate STR-Match outperforms existing methods quantitatively, achieving improved fidelity to the target prompt (CLIP Similarity of 31.61). The method offers AI practitioners a new state-of-the-art baseline for training-free text-guided video editing with improved visual quality and flexible domain transformation. |
| Multi-Modal | MARVIS: Modality Adaptive Reasoning over VISualizations (Read more on [arXiv](https://arxiv.org/abs/2507.01544) or [HuggingFace](https://huggingface.co/papers/2507.01544))| Chinmay Hegde, Oussama Elachqar, Lennart Purucker, Benjamin Feuer | MARVIS is a training-free method enabling vision-language models to predict any data modality by transforming latent embedding spaces into visual representations for spatial reasoning. It addresses the challenge of foundation models underperforming specialized approaches, especially on non-traditional modalities. MARVIS utilizes a training-free approach leveraging VLMs to interpret transformed latent embeddings, achieving competitive performance across diverse modalities like vision, audio, biological, and tabular data. It achieves an average of 16% improvement over Gemini on the tested datasets. MARVIS decouples predictions from the small embedding-generating models and offers a modality-agnostic system for improving VLM performance with reduced data leakage. |
