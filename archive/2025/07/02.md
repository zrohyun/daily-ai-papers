

## Papers for 2025-07-02

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2507.01006) or [HuggingFace](https://huggingface.co/papers/2507.01006))| tanghme0www, bigganbing, xgeric, iyuge2, wenyi | GLM-4.1V-Thinking is a vision-language model designed for general-purpose multimodal reasoning. The paper aims to enhance reasoning capabilities through scalable reinforcement learning. It employs Reinforcement Learning with Curriculum Sampling (RLCS) to train a 9B model, enhancing capabilities across diverse tasks. The GLM-4.1V-9B-Thinking model achieves state-of-the-art performance among models of comparable size, outperforming Qwen2.5-VL-7B on nearly all tasks and achieving comparable or superior performance on 18 benchmarks relative to the larger Qwen2.5-VL-72B; RLCS substantially boosts the model's performance, with gains of up to +7.3%. This demonstrates the potential of RL-based methods for improving multimodal reasoning, providing a practical, high-performing model for AI practitioners. |
| Multi-Modal | MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional
  Multimodal Embeddings (Read more on [arXiv](https://arxiv.org/abs/2506.23115) or [HuggingFace](https://huggingface.co/papers/2506.23115))| Nan Yang, Liang Wang, roosephu, hongliu9903, Haon-Chen | The paper introduces MoCa, a two-stage framework for transforming pre-trained Vision Language Models (VLMs) into bidirectional multimodal embedding models. It addresses the limitations of causal attention, scalability, and training diversity in existing VLMs by employing modality-aware continual pre-training and heterogeneous contrastive fine-tuning. The method optimizes a joint reconstruction objective for denoising interleaved text and image inputs, scaling effectively with massive unlabeled datasets. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, such as achieving 71.5 overall score on MMEB.  This framework enables AI practitioners to leverage continual pre-training strategies for enhancing cross-modal reasoning in VLMs and improving performance on multimodal tasks. |
| Natural Language Processing | SciArena: An Open Evaluation Platform for Foundation Models in
  Scientific Literature Tasks (Read more on [arXiv](https://arxiv.org/abs/2507.01001) or [HuggingFace](https://huggingface.co/papers/2507.01001))| Ronan Le Bras, Sihong Wu, HughieHu, maxzky, yilunzhao | The paper introduces SciArena, an open and collaborative platform for evaluating foundation models on scientific literature tasks using community voting. SciArena aims to address limitations of traditional benchmarks by engaging the research community directly for model performance evaluation. The platform supports 23 foundation models and has collected over 13,000 votes from researchers across scientific domains, with analysis confirming diverse questions aligned with real-world needs. The study discusses model ranking leaderboards and releases SciArena-Eval, a meta-evaluation benchmark, to promote research in model-based automated evaluation systems showing 65.1% accuracy for 03 comparing with human preferences. |
| Natural Language Processing | Does Math Reasoning Improve General LLM Capabilities? Understanding
  Transferability of LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.00432) or [HuggingFace](https://huggingface.co/papers/2507.00432))| Seungone Kim, Xiaoyu Xu, Yuetai Li, Maggie Huan, aaabiao | This paper investigates the transferability of math reasoning abilities in large language models (LLMs) to other reasoning and non-reasoning tasks. The research questions whether improvements in math reasoning translate into general LLM capabilities or narrow overfitting. The methodology involves evaluating over 20 reasoning-tuned models and conducting controlled experiments using Qwen3-14B models with different tuning methods like SFT and RL. The primary results show that reinforcement learning (RL)-tuned models generalize better across domains, while supervised fine-tuning (SFT)-tuned models often exhibit catastrophic forgetting with measured performance gains in math (e.g., +4.1% on average) not translating to other areas. The main implication is a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models, suggesting RL as a superior method for broad capability enhancement. |
| Computer Vision | Radial Attention: O(nlog n) Sparse Attention with Energy Decay for
  Long Video Generation (Read more on [arXiv](https://arxiv.org/abs/2506.19852) or [HuggingFace](https://huggingface.co/papers/2506.19852))| Shuo Yang, Haocheng Xi, Tianle Cai, Xingyang Li, Lmxyy | The paper introduces Radial Attention, an O(n log n) sparse attention mechanism with energy decay for efficient long video generation. It addresses the challenge of high computational costs in video diffusion models by exploiting spatiotemporal energy decay, where attention scores diminish with distance. Radial Attention employs a static attention mask that translates energy decay into a corresponding compute density decay, achieving a 1.9x speedup on HunyuanVideo while maintaining comparable video quality. This approach reduces tuning costs by up to 4.4x and accelerates inference by up to 3.7x when generating 4x longer videos. The proposed method provides a scalable solution for AI practitioners to train and deploy video diffusion models on longer sequences with limited computational resources. |
| Natural Language Processing | DiffuCoder: Understanding and Improving Masked Diffusion Models for Code
  Generation (Read more on [arXiv](https://arxiv.org/abs/2506.20639) or [HuggingFace](https://huggingface.co/papers/2506.20639))| Navdeep Jaitly, Jiatao Gu, Huangjie Zheng, Ruixiang Zhang, Shansan Gong | The paper introduces DiffuCoder, a 7B-scale masked diffusion model (MDM) for code generation, exploring its decoding behavior and reinforcement learning (RL) methods. It investigates the denoising processes of diffusion large language models (dLLMs) and introduces coupled-GRPO, a novel sampling scheme to reduce variance in token log-likelihood estimates during RL training. Experiments on code generation benchmarks show that coupled-GRPO significantly improves DiffuCoder's performance, achieving a +4.4% boost on EvalPlus. The findings offer deeper insights into dLLM generation and provide a diffusion-native RL training framework, reducing reliance on autoregressive biases. |
| Multi-Modal | HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context (Read more on [arXiv](https://arxiv.org/abs/2506.21277) or [HuggingFace](https://huggingface.co/papers/2506.21277))| Weixuan Chen, Shimin Yao, BBBBCHAN, fushh7, PhilipC | The paper introduces HumanOmniV2, an omni-modal reasoning model addressing insufficient global context understanding and shortcut problems in existing multimodal models. It aims to improve reasoning by emphasizing global context within multimodal inputs, thereby enhancing human intention understanding. The methodology involves implementing a context reward judged by a large language model, alongside format and accuracy rewards, and assessing the logical integration of multimodal information. The model achieves improved performance on omni-modal benchmarks, with a score of 69.33% on IntentBench, demonstrating advancements in complex reasoning compared to open-source alternatives. The main implication is the improved ability of AI practitioners to build models capable of understanding complex human intentions and emotions from diverse multimodal data, but uncertainty exists regarding its performance at larger scales. |
| Artificial General Intelligence | Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive
  Foundations for Artificial General Intelligence and its Societal Impact (Read more on [arXiv](https://arxiv.org/abs/2507.00951) or [HuggingFace](https://huggingface.co/papers/2507.00951))| Abbas Shah, Ranjan Sapkota, Rizwan Qureshi, amanchadha, shainaraza | This paper presents a cross-disciplinary synthesis of AGI development, bridging AI, cognitive neuroscience, psychology, and agent-based systems. It addresses the question of whether machines can truly think, reason, and act like humans. The methodology involves analyzing the architectural and cognitive foundations of general intelligence, emphasizing modular reasoning, persistent memory, and multi-agent coordination. The paper highlights the rise of Agentic RAG frameworks, combining retrieval, planning, and dynamic tool use for more adaptive behavior, noting that true intelligence arises from the integration of memory and reasoning. The study advocates for systems that are not only intelligent but also transparent, value-aligned, and socially grounded, aiming to build the next generation of general-purpose human-level machine intelligence. |
| Natural Language Processing | Data Efficacy for Language Model Training (Read more on [arXiv](https://arxiv.org/abs/2506.21545) or [HuggingFace](https://huggingface.co/papers/2506.21545))| Chong Li, Wenshan Wu, Xin Zhang, Yangyu Huang, Yalun Dai | The paper introduces the concept of Data Efficacy, focusing on optimizing the organization of training data to improve language model performance. It addresses the research question of how to maximize LM performance by strategically organizing training data rather than simply selecting subsets. The proposed DELT paradigm comprises Data Scoring, Data Selection, and Data Ordering, including novel methods like Learnability-Quality Scoring (LQS) and Folding Ordering (FO). Experiments show DELT improves LM performance, with LQS and FO achieving a +1.65% increase in average accuracy across benchmarks. The paradigm provides a new avenue for practitioners to enhance LM training without increasing data scale or model size, suggesting potential benefits in both efficacy and efficiency. |
| Computer Vision | FreeLong++: Training-Free Long Video Generation via Multi-band
  SpectralFusion (Read more on [arXiv](https://arxiv.org/abs/2507.00162) or [HuggingFace](https://huggingface.co/papers/2507.00162))| Yi Yang, Yu Lu | The paper introduces FreeLong++, a training-free framework for extending short video generation models to longer sequences while maintaining temporal consistency and visual fidelity. It addresses the problem of high-frequency distortion in longer videos by employing a multi-band spectral fusion technique. FreeLong++ decomposes the video signal into multiple temporal frequency bands using multiple attention branches and selectively fuses them in the frequency domain. Experiments show that FreeLong++ outperforms existing methods, achieving improved aesthetic quality scores on longer videos. This approach enables AI practitioners to generate longer, more coherent videos from pre-trained short video models without additional training or parameters. |
| Computer Vision | Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image
  Watermarking Technique for AI-Generated Images (Read more on [arXiv](https://arxiv.org/abs/2506.22960) or [HuggingFace](https://huggingface.co/papers/2506.22960))| Vasu Sharma, Shashwat Bajpai, Ashhar Aziz, Shreyas Dixit, amanchadha | The paper introduces PECCAVI, a novel visual paraphrase attack-safe image watermarking technique for AI-generated content. It aims to address the vulnerability of existing watermarking methods to visual paraphrase attacks by embedding watermarks in Non-Melting Points (NMPs) within images. PECCAVI employs multi-channel frequency domain watermarking and noisy burnishing to enhance robustness. The method achieves high watermark detection probability (WDP) even after visual paraphrasing, with an average WDP of 0.90 after paraphrase attacks (s=0.2) using the XRAI saliency method while preserving image quality (PSNR of 29.84 and SSIM of 0.93). This technique offers AI practitioners a more resilient method for authenticating AI-generated images and mitigating misuse. |
