

## Papers for 2025-07-04

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with
  TriMap Video Diffusion (Read more on [arXiv](https://arxiv.org/abs/2507.02813) or [HuggingFace](https://huggingface.co/papers/2507.02813))| Minghui Yang, Jiawei Chi, Hao Li, Fangfu Liu, hanyang-21 | The paper introduces LangScene-X, a novel generative framework for reconstructing generalizable 3D language-embedded scenes from sparse views. It aims to address the limitations of existing methods that rely on dense-view reconstruction and suffer from artifacts with limited views. The approach uses TriMap video diffusion to generate consistent RGB images, normal maps, and semantic segmentation and a language quantized compressor (LQC) for efficient language feature representation. Experiments demonstrate superior performance over state-of-the-art methods, achieving an open-vocabulary localization accuracy of 80.85% on the LERF-OVS dataset. LangScene-X provides AI practitioners with a promising generative paradigm for 3D scene reconstruction and understanding using limited visual input and open-ended language queries. |
| Natural Language Processing | WebSailor: Navigating Super-human Reasoning for Web Agent (Read more on [arXiv](https://arxiv.org/abs/2507.02592) or [HuggingFace](https://huggingface.co/papers/2507.02592))| Liwen Zhang, Huifeng Yin, Zhongwang Zhang, Kuan Li, xxwu | WebSailor introduces a post-training methodology for enhancing reasoning capabilities in web agents. The paper addresses the lack of systematic uncertainty reduction in open-source models for complex information-seeking tasks. Their methodology involves generating high-uncertainty tasks through structured sampling and information obfuscation, along with an agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). WebSailor significantly outperforms open-source agents, matching proprietary agents' performance on BrowseComp-en/zh, achieving scores of 12.0 and 30.1 respectively on the 72B model. The implication is an accessible pipeline to instill superhuman reasoning, facilitating enhanced navigation in complex information landscapes for AI practitioners. |
| Machine Learning | IntFold: A Controllable Foundation Model for General and Specialized
  Biomolecular Structure Prediction (Read more on [arXiv](https://arxiv.org/abs/2507.02025) or [HuggingFace](https://huggingface.co/papers/2507.02025))| He Yan, Wayne Bai, Leon Qiao, The IntFold Team, FuxuLiu | IntFold is a controllable foundation model for biomolecular structure prediction. The paper aims to develop a model with high accuracy and user control for general and specialized tasks, crucial for drug design. It achieves this by utilizing specialized adapters that can be trained for complex allosteric states and binding affinity prediction, while maintaining a fixed base model. IntFold's performance is comparable to AlphaFold 3 on the FoldBench benchmark and demonstrates a 58.5% success rate on the protein-ligand interface prediction task. The model's controllability enables downstream applications such as drug screening and specialized modeling scenarios. |
| Computer Vision | Heeding the Inner Voice: Aligning ControlNet Training via Intermediate
  Features Feedback (Read more on [arXiv](https://arxiv.org/abs/2507.02321) or [HuggingFace](https://huggingface.co/papers/2507.02321))| Aibek Alanov, Andrey Kuznetsov, Maxim Nikolaev, Nina Konovalova | The paper introduces InnerControl, a novel training strategy for ControlNet to improve spatial consistency in text-to-image generation. It addresses the misalignment between input controls and generated images by enforcing spatial consistency across all diffusion steps using lightweight control prediction probes. The method trains small convolutional networks to reconstruct input control signals from intermediate UNet features at every denoising step. Experiments show improved control alignment with a 7.87% RMSE reduction for depth estimation compared to ControlNet++. InnerControl enhances control alignment and image fidelity, offering AI practitioners a refined training approach for spatially controllable image generation. |
| Natural Language Processing | Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy (Read more on [arXiv](https://arxiv.org/abs/2507.01352) or [HuggingFace](https://huggingface.co/papers/2507.01352))| Jiacai Liu, Jujie He, RickyShaw999, zengliangcs, chrisliu298 | The paper introduces Skywork-Reward-V2, a suite of reward models for reinforcement learning from human feedback, designed to scale preference data curation. It addresses the brittleness of current open reward models by curating a large-scale preference dataset, SynPref-40M, using a human-AI synergistic pipeline. The methodology combines human annotation for quality and human-preference-guided LLM judges for scalability. Skywork-Reward-V2 achieves state-of-the-art performance across seven major reward model benchmarks, outperforming existing open reward models by a significant margin. The results suggest that human-AI curation synergy can unlock significantly higher data quality, improving reward model performance and versatility. |
| Multi-Modal | Thinking with Images for Multimodal Reasoning: Foundations, Methods, and
  Future Frontiers (Read more on [arXiv](https://arxiv.org/abs/2506.23918) or [HuggingFace](https://huggingface.co/papers/2506.23918))| Zhenhua Liu, Hangyu Guo, Peng Xia, Zhaochen Su, Xiaoye08 | This survey explores the paradigm shift in multimodal reasoning from "Thinking about Images" to "Thinking with Images," where models actively manipulate visual information. The research objective is to define this paradigm and categorize methods that leverage visual information as intermediate reasoning steps. The survey structures this field across three stages: tool-driven exploration, programmatic manipulation, and intrinsic imagination. It analyzes benchmarks, applications, and challenges, offering a roadmap for human-aligned AI. The survey identifies critical challenges related to efficiency, robustness, and generalization in utilizing visual information. |
| Natural Language Processing | Decoupled Planning and Execution: A Hierarchical Reasoning Framework for
  Deep Search (Read more on [arXiv](https://arxiv.org/abs/2507.02652) or [HuggingFace](https://huggingface.co/papers/2507.02652))| Yutao Zhu, Yuyao Zhang, Guanting Dong, Xiaoxi Li, Jiajie Jin | The paper introduces HiRA, a hierarchical framework for deep search that decouples strategic planning from specialized execution to improve reasoning efficiency and scalability. It aims to address the limitations of single-model approaches in handling complex search tasks by separating planning and execution into distinct modules. HiRA assigns subtasks to domain-specific agents and coordinates results using a structured integration mechanism. Experiments on complex benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems, achieving an accuracy of 42.5% on the GAIA dataset, a considerable improvement over baseline models. The frameworkâ€™s modularity and capability integration offer AI practitioners a more scalable and coherent approach for multi-step information seeking tasks. |
| Machine Learning | Fast and Simplex: 2-Simplicial Attention in Triton (Read more on [arXiv](https://arxiv.org/abs/2507.02754) or [HuggingFace](https://huggingface.co/papers/2507.02754))| Jiecao Yu, Sijia Chen, Sai Surya Duvvuri, Timothy Chou, Aurko Roy | The paper introduces a 2-simplicial Transformer architecture using Triton to improve token efficiency in large language models. The research aims to enhance performance under limited token budgets by generalizing dot-product attention to trilinear functions. The methodology involves implementing the 2-simplicial Transformer with an efficient Triton kernel and evaluating its performance on tasks involving mathematics, coding, reasoning, and logic. Results indicate that 2-simplicial attention changes the scaling law exponent for knowledge and reasoning tasks compared to dot product attention, with a more favorable scaling exponent in certain cases. This suggests that 2-simplicial Transformers can effectively approach the irreducible entropy of natural language under token constraints, offering an alternative to scaling model size and data proportionally. |
| Natural Language Processing | Can LLMs Identify Critical Limitations within Scientific Research? A
  Systematic Evaluation on AI Research Papers (Read more on [arXiv](https://arxiv.org/abs/2507.02694) or [HuggingFace](https://huggingface.co/papers/2507.02694))| Arman Cohan, Lovekesh Vig, Manasi Patwardhan, Yilun Zhao, Zhijian Xu | This paper introduces LIMITGEN, a benchmark to evaluate LLMs in identifying limitations within AI research papers, focusing on assisting peer review. It investigates the research question of how well LLMs can identify critical limitations in scientific research and whether retrieval-augmented generation (RAG) improves this ability. The methodology involves creating synthetic and human-written datasets of limitations and using LLMs with and without RAG to generate limitations. Results show LLMs still struggle with limitations identification, but RAG can enhance their ability to provide more contextual suggestions. The LIMITGEN benchmark and the study's analysis of LLM capabilities in limitations identification provide valuable tools and insights for improving the use of AI to support and enhance peer-review processes. |
| Reinforcement Learning | Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving (Read more on [arXiv](https://arxiv.org/abs/2507.02726) or [HuggingFace](https://huggingface.co/papers/2507.02726))| Jun Wang, Anthony Bordg, Rasul Tutunov, Xiaotong Ji, Matthieu Zimmer | The paper introduces a novel framework, self-generated goal-conditioned Markov Decision Processes (sG-MDPs), for automated theorem proving to address sparse rewards and long proof horizons. It aims to improve theorem proving performance by enabling agents to dynamically generate subgoals based on the evolving proof state. The methodology involves using sG-MDPs with Monte Carlo Tree Search (MCTS) and ensembling multiple large language models (LLMs) for subgoal generation and tactic synthesis. The Bourbaki (7B) system achieves new state-of-the-art results by solving 26 problems on the PutnamBench benchmark. This approach offers AI practitioners a generalizable mechanism for enhancing deductive reasoning in formal mathematical problem-solving. |
| Machine Learning | Energy-Based Transformers are Scalable Learners and Thinkers (Read more on [arXiv](https://arxiv.org/abs/2507.02092) or [HuggingFace](https://huggingface.co/papers/2507.02092))| Peixuan Han, Md Mofijul Islam, Ganesh Nanduru, Alexi Gladstone, amanchadha | The paper introduces Energy-Based Transformers (EBTs), a new class of energy-based models for scalable learning and thinking. The research investigates whether System 2 Thinking can emerge from unsupervised learning. EBTs are trained to assign an energy value to input-prediction pairs, enabling prediction through gradient descent-based energy minimization. Results show EBTs achieve up to 35% higher scaling rates than Transformer++ during training and improve language modeling performance by 29% more than Transformer++. EBTs present a promising paradigm for scaling both the learning and thinking capabilities of AI models. |
| Natural Language Processing | Selecting and Merging: Towards Adaptable and Scalable Named Entity
  Recognition with Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2506.22813) or [HuggingFace](https://huggingface.co/papers/2506.22813))| Wei Wei, Zhuojun Ding, Facico | The paper introduces SaM, a framework for named entity recognition (NER) that dynamically selects and merges expert models at inference time to enhance adaptability and scalability. SaM addresses the limitations of unified models by focusing on domain similarity and sampling evaluation to choose beneficial experts for merging. Experimental results on multiple benchmarks show SaM outperforms unified models by an average of 10%, with certain domains reaching 20% improvement. This model merging improves generalization across diverse domains without extra training, making it easier to add or remove experts based on practical needs. This approach is designed to improve adaptation across domains. |
| Natural Language Processing | Self-Correction Bench: Revealing and Addressing the Self-Correction
  Blind Spot in LLMs (Read more on [arXiv](https://arxiv.org/abs/2507.02778) or [HuggingFace](https://huggingface.co/papers/2507.02778))| Ken Tsui | This paper introduces Self-Correction Bench, a framework to reveal and address a 'Self-Correction Blind Spot' in LLMs, where models fail to correct their own errors but succeed on identical external errors. The objective is to systematically study this self-correction failure through controlled error injection across complexity levels. The methodology involves testing 14 LLMs, revealing an average 64.5% blind spot rate, and exploring the impact of training data composition. Appending 'Wait' reduces blind spots by 89.3%, demonstrating the potential for activation. The main implication is highlighting a critical limitation in current LLMs, offering potential avenues for improving their reliability. |
| Machine Learning | ZeCO: Zero Communication Overhead Sequence Parallelism for Linear
  Attention (Read more on [arXiv](https://arxiv.org/abs/2507.01004) or [HuggingFace](https://huggingface.co/papers/2507.01004))| Tianjian Li, Xinyi Wan, Ruijie Zhu, Zehao Liu, Yuhong Chou | The paper introduces ZeCO, a novel sequence parallelism (SP) approach for linear attention models that minimizes communication overhead. It aims to overcome the communication bottleneck in existing SP methods for long sequence training. ZeCO employs All-Scan, a pipelined collective communication primitive, to reduce communication volume and enable overlap with local computation. Empirical results show ZeCO achieves a 60% speedup compared to state-of-the-art SP methods on 256 devices with an 8M sequence length.  This enables more efficient training of large language models on extremely long sequences previously intractable. |
| Reinforcement Learning | AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM
  Post-Training (Read more on [arXiv](https://arxiv.org/abs/2507.01663) or [HuggingFace](https://huggingface.co/papers/2507.01663))| Guang Yang, Kui Luo, Haibo Wang, Ansheng You, Zhenyu Han | The paper introduces AsyncFlow, an asynchronous streaming RL framework designed for efficient LLM post-training. It addresses the scalability and dataflow challenges in existing RL frameworks by proposing a distributed data storage module and a producer-consumer-based asynchronous workflow to minimize computational idleness. Experiments demonstrate a 1.59x average throughput improvement over state-of-the-art baselines. AsyncFlow's modular architecture and service-oriented APIs provide a customizable solution for integrating diverse training and inference engines, facilitating both research flexibility and industrial deployment scalability. |
