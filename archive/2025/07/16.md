

## Papers for 2025-07-16

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation
  from Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2507.07104) or [HuggingFace](https://huggingface.co/papers/2507.07104))| Jieneng Chen, Yu-cheng Chou, Yitong Li, lambertxiao, PatZhang11 | The paper introduces a Vision-Language-Vision (VLV) auto-encoder for scalable knowledge distillation from diffusion models. It addresses the challenge of training large VLMs by leveraging pretrained vision encoders, text-to-image diffusion models, and LLMs, circumventing the need for massive paired image-text datasets. The VLV pipeline distills knowledge from a text-conditioned diffusion model using continuous embeddings and reconstructs images with high semantic fidelity. By fine-tuning an LLM to decode the intermediate representations, the model achieves state-of-the-art captioning performance comparable to GPT-40 with significantly lower training costs (under $1,000 USD) and an FID score of 6.64. The VLV's cost-efficiency and data requirements make high-quality image captioning accessible to a broader range of AI practitioners. |
| Natural Language Processing | EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and
  Reasoning Modes (Read more on [arXiv](https://arxiv.org/abs/2507.11407) or [HuggingFace](https://huggingface.co/papers/2507.11407))| Stanley Jungkyu Choi, Kibong Choi, Eunbi Choi, Kyunghoon Bae, LG AI Research | The paper introduces EXAONE 4.0, a unified large language model integrating non-reasoning and reasoning modes for improved usability and advanced reasoning capabilities. The primary objective is to create an agentic AI by enabling agentic tool use and extending multilingual support. EXAONE 4.0 employs a hybrid attention mechanism and increases pretraining data to 14T tokens for the 32B model, along with supervised fine-tuning, reinforcement learning, and preference learning. Results demonstrate superior performance compared to open-weight models of similar scale and competitiveness against frontier models in benchmarks such as Math/Coding. EXAONE 4.0 provides AI practitioners with a versatile model capable of handling diverse real-world applications, enhancing both instruction-following and reasoning. |
| Machine Learning | Scaling Laws for Optimal Data Mixtures (Read more on [arXiv](https://arxiv.org/abs/2507.09404) or [HuggingFace](https://huggingface.co/papers/2507.09404))| Enrico Fini, David Grangier, Dan Busbridge, Louis Bethune, Mustafa Shukor | This paper introduces a systematic approach to determine the optimal data mixture for training large foundation models. The research investigates how domain weights influence model performance, aiming to improve upon the trial-and-error method of selecting domain mixtures. The proposed scaling laws accurately predict model loss based on model size, training tokens, and domain weights. Experiments across LLMs, NMMs, and LVMs demonstrate accurate extrapolation to unseen domain weights and larger scales, achieving low Mean Relative Error (MRE%). The scaling laws provide a principled alternative to costly trial-and-error methods for optimizing data mixtures, allowing for efficient training under resource constraints. |
| Multi-Modal | Can Multimodal Foundation Models Understand Schematic Diagrams? An
  Empirical Study on Information-Seeking QA over Scientific Papers (Read more on [arXiv](https://arxiv.org/abs/2507.10787) or [HuggingFace](https://huggingface.co/papers/2507.10787))| Arman Cohan, Chuhan Li, Chengye Wang, yilunzhao | The paper introduces MISS-QA, a benchmark for evaluating multimodal foundation models' ability to interpret schematic diagrams in scientific papers. It aims to assess how well these models understand diagrams illustrating research overviews and answer related information-seeking questions. The methodology involves testing 18 frontier models on 1,500 expert-annotated examples and comparing their performance to human experts. Results show a significant performance gap, with the best open-source model (Qwen2.5-VL-72B) achieving only 61.6% accuracy compared to 89.0% by humans. This highlights the need for improved models capable of comprehending multimodal scientific literature. |
| Machine Learning | LLMalMorph: On The Feasibility of Generating Variant Malware using
  Large-Language-Models (Read more on [arXiv](https://arxiv.org/abs/2507.09411) or [HuggingFace](https://huggingface.co/papers/2507.09411))| Ashish Kundu, Arun Iyengar, Imtiaz Karim, Adrian Shuai Li, Ajwad | This paper explores using Large Language Models (LLMs) to generate new malware variants. The research investigates whether LLMs can modify malware source code to evade antivirus detection while preserving functionality. LLMalMorph, a semi-automated framework, leverages LLMs with custom prompts and code transformations to generate variants, tested on 10 Windows malware samples. Experiments demonstrate a reduction in antivirus engine detection rates, with an average 31% reduction for a simple sample, though several achieved notable success rates against ML-based classifiers. The framework provides AI practitioners insights into the feasibility and limitations of LLMs in adversarial malware generation. |
| Natural Language Processing | OpenCodeReasoning-II: A Simple Test Time Scaling Approach via
  Self-Critique (Read more on [arXiv](https://arxiv.org/abs/2507.09075) or [HuggingFace](https://huggingface.co/papers/2507.09075))| Mehrzad Samadi, Sean Narenthiran, Aleksander Ficek, Wasi Uddin Ahmad, smajumdar94 | This paper introduces OPENCODEREASONING-II, a large-scale dataset for code generation and critique, enabling test-time scaling for improved performance. The research aims to improve code generation performance via test-time scaling using a self-critique approach. The methodology involves a two-stage fine-tuning strategy: code generation and joint code generation/critique. Results demonstrate Qwen2.5-Instruct models achieve state-of-the-art code generation and improved performance on LiveCodeBench (e.g., OCR-2-32B shows performance boost through self-critique). The main implication is improved competitive coding and comprehensive evaluation in C++ facilitating LLM evaluation. |
| Natural Language Processing | AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs (Read more on [arXiv](https://arxiv.org/abs/2507.08616) or [HuggingFace](https://huggingface.co/papers/2507.08616))| Bryan Perozzi, Mikhail Galkin, Jan Tönshoff, Luis Müller, Florian Grötschla | The paper introduces AGENTSNET, a new benchmark for evaluating coordination and collaborative reasoning in multi-agent language models (LLMs). It aims to address the limitations of existing benchmarks by assessing scalable coordination, decentralized communication, and collaborative problem-solving capabilities. AGENTSNET uses fundamental problems from distributed computing and graph theory to measure multi-agent systems' abilities to self-organize and communicate effectively across diverse network topologies. Evaluation of baseline methods, including frontier LLMs, shows that some models demonstrate strong performance on small networks but degrade with increasing network size. The results reveal challenges in strategy coordination and information utilization in multi-agent LLMs, indicating areas for improvement in designing more effective and robust collaborative systems. |
| Natural Language Processing | Planted in Pretraining, Swayed by Finetuning: A Case Study on the
  Origins of Cognitive Biases in LLMs (Read more on [arXiv](https://arxiv.org/abs/2507.07186) or [HuggingFace](https://huggingface.co/papers/2507.07186))| Gabriel Stanovsky, Yonatan Belinkov, itay1itzhak | This paper investigates the origins of cognitive biases in large language models (LLMs). It aims to disentangle the contributions of pretraining, finetuning data, and training randomness on bias emergence. The methodology involves finetuning models with varying random seeds and employing a novel cross-tuning approach to swap instruction datasets. Results show that pretraining primarily shapes biases, with training randomness introducing some variability; clustering analysis reveals that models group more closely by their pretraining model. The paper suggests that bias mitigation strategies should consider the pretraining origins beyond finetuning effects, affecting future evaluation and adjustment of LLMs. |
