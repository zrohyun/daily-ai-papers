

## Papers for 2025-07-24

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | Pixels, Patterns, but No Poetry: To See The World like Humans (Read more on [arXiv](https://arxiv.org/abs/2507.16863) or [HuggingFace](https://huggingface.co/papers/2507.16863))| Xinhao Li, Jingyi Tang, Lin Xu, Zihao Huang, Hongcheng Gao | The paper introduces the Turing Eye Test (TET), a perception-oriented benchmark designed to evaluate the visual understanding capabilities of Multimodal Large Language Models (MLLMs). It investigates whether MLLMs truly perceive the world as humans do, shifting the focus from reasoning to perception. TET consists of four diagnostic tasks using synthetic images that humans process intuitively, including hidden text, 3D captchas, colorblind tests and Chinese ligatures. Results demonstrate that state-of-the-art MLLMs exhibit catastrophic failures on these tasks, achieving near zero success rates in many cases, indicating a vision tower generalization problem, and that finetuning the vision tower improves performance.. The findings suggest that improved visual generalization methods are needed in MLLMs, highlighting a significant gap in visual understanding. |
| Computer Vision | Yume: An Interactive World Generation Model (Read more on [arXiv](https://arxiv.org/abs/2507.17744) or [HuggingFace](https://huggingface.co/papers/2507.17744))| Zhen Li, Shaoheng Lin, Xiaofeng Mao, kpzhang, Jiangmiao | The paper introduces Yume, an interactive world generation model conditioned on an input image. Yume aims to create a dynamic and explorable world using keyboard actions. The model utilizes a Masked Video Diffusion Transformer (MVDT) with a memory module and incorporates techniques like camera motion quantization and training-free anti-artifact mechanisms. Qualitative results on the Sekai dataset demonstrate Yume's ability to generate diverse scenes. The framework offers a foundation for interactive environment creation. |
| Computer Vision | DesignLab: Designing Slides Through Iterative Detection and Correction (Read more on [arXiv](https://arxiv.org/abs/2507.17202) or [HuggingFace](https://huggingface.co/papers/2507.17202))| Shingo Takamatsu, Jaegul Choo, Yotaro Shimose, Heng Wang, YeolJoo | This paper presents DesignLab, an iterative framework for refining presentation slides through separate detection and correction roles. The research aims to improve slide design quality by decomposing the design process into iterative feedback loops involving a design reviewer and a design contributor. The key methodology involves fine-tuning large language models for design error detection and correction and simulating intermediate drafts with controlled perturbations. Experiments show DesignLab outperforms existing methods, including a commercial tool, with GPT-4o showing a preference for DesignLab's output in 62.3% of comparisons against WebRPG. The main implication is an effective approach to refining visual designs, particularly where iterative feedback is essential. |
| Reinforcement Learning | Can One Domain Help Others? A Data-Centric Study on Multi-Domain
  Reasoning via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2507.17512) or [HuggingFace](https://huggingface.co/papers/2507.17512))| Conghui He, Mengyuan Sun, Honglin Lin, Zhuoshi Pan, Yu Li | This paper presents a data-centric study on multi-domain reasoning using reinforcement learning with verifiable rewards (RLVR) to enhance large language models (LLMs). It investigates how training on one reasoning domain affects performance in others, focusing on math, code generation, and puzzle solving. The key methodology involves leveraging the GRPO algorithm and Qwen-2.5-7B models to evaluate in-domain improvements and cross-domain generalization. Results show that RLVR enhances in-domain performance, with Base-DSR improving MATH500 accuracy by 19.60, but cross-domain effects vary, highlighting the importance of careful domain combination and reward design. The main implication is that optimizing RL methodologies requires understanding domain interactions to foster comprehensive, multi-domain reasoning capabilities in LLMs. |
| Reinforcement Learning | Re:Form -- Reducing Human Priors in Scalable Formal Software
  Verification with RL in LLMs: A Preliminary Study on Dafny (Read more on [arXiv](https://arxiv.org/abs/2507.16331) or [HuggingFace](https://huggingface.co/papers/2507.16331))| Xin Li, Xu Xu, Xuhan Huang, Fengdi Che, Chuanhao Yan | This paper explores reducing human priors in formal software verification using reinforcement learning (RL) with Large Language Models (LLMs). The study addresses the challenge of reliable and scalable verification processes by grounding LLMs in the formal language Dafny. They introduce an automatic data curation pipeline and RL designs integrated with the Dafny verifier.  The DafnyComp benchmark is used to evaluate compositional formal programs and their automated specifications.  Experiments show that supervised fine-tuning enables small models (0.5B) to generate syntactically valid and verifiable Dafny code; RL with regularization further improves performance, surpassing proprietary models and achieving better generalization on the DafnyComp benchmark. |
| Computer Vision | Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention (Read more on [arXiv](https://arxiv.org/abs/2507.17745) or [HuggingFace](https://huggingface.co/papers/2507.17745))| Qin Li, Hu Zhang, Yikai Wang, Zhihao Li, Yiwen Chen | The paper introduces ULTRA3D, a novel framework for efficient and high-fidelity 3D generation using part attention. It aims to address computational inefficiencies in sparse voxel-based 3D generation by reducing the quadratic complexity of attention mechanisms. The key methodology involves a two-stage process: coarse object layout generation via VecSet representation followed by per-voxel latent feature refinement using a geometry-aware localized part attention mechanism. Experiments demonstrate that ULTRA3D achieves a 3.3x speed-up without compromising quality and attains state-of-the-art performance in visual fidelity. This approach enables AI practitioners to generate high-resolution 3D content more efficiently, potentially accelerating applications in gaming, AR/VR, and robotics. |
| Computer Vision | Elevating 3D Models: High-Quality Texture and Geometry Refinement from a
  Low-Quality Model (Read more on [arXiv](https://arxiv.org/abs/2507.11465) or [HuggingFace](https://huggingface.co/papers/2507.11465))| Joo-Haeng Lee, Minsu Gong, Jooeun Son, Jiyun Won, Nuri Ryu | This paper introduces Elevate3D, a novel framework for refining low-quality 3D models into high-quality assets by enhancing both texture and geometry. The main objective is to address the scarcity of high-quality 3D models by refining readily accessible, lower-quality ones. Elevate3D uses a specialized texture enhancement method called HFS-SDEdit and refines both texture and geometry in a view-by-view manner leveraging geometric cues from monocular geometry predictors. Quantitative results on the GSO dataset demonstrate state-of-the-art performance with Elevate3D achieving a MUSIQ score of 66.527, improving the quality of 3D model refinement and addressing the scarcity of open-source 3D assets for AI practitioners. |
| Computer Vision | Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less
  Local Than Assumed (Read more on [arXiv](https://arxiv.org/abs/2507.16880) or [HuggingFace](https://huggingface.co/papers/2507.16880))| Adam Dziedzic, Kristian Kersting, Lukas Struppek, Dominik Hintersdorf, Antoni Kowalczuk | This paper investigates memorization in text-to-image diffusion models (DMs), arguing it is less local than previously assumed. It questions the effectiveness of existing pruning-based mitigation strategies by demonstrating that adversarial text embeddings can re-trigger memorization after pruning. The study crafts such embeddings and uses the SSCD metric (values above 0.7 indicate successful replication) to show memorization persists. The paper proposes adversarial fine-tuning to permanently remove memorized content, enhancing the trustworthiness and compliance of generative AI. These approaches are permanent and do not increase inference time. |
| Natural Language Processing | RAVine: Reality-Aligned Evaluation for Agentic Search (Read more on [arXiv](https://arxiv.org/abs/2507.16725) or [HuggingFace](https://huggingface.co/papers/2507.16725))| Jinhua Gao, Zhi Zheng, Xiang Long, Yilong Xu | The paper introduces RAVine, a reality-aligned evaluation framework for agentic LLMs with search capabilities to address the limitations of existing evaluation methods. It aims to improve evaluation by targeting more realistic, multi-point queries, constructing attributable ground truth, and evaluating the iterative process of agentic search. RAVine employs a nugget-centered evaluation approach, assessing report quality through attributability and flexibility, and incorporates process-oriented metrics for intermediate behaviors and efficiency. Experiments benchmark models using RAVine, revealing limitations in task completeness and faithfulness, alongside a tendency to rely on internal knowledge, achieving citation recall of 13.2% and precision of 11.9% for Qwen3-32B. RAVine offers a full-process, reproducible evaluation sandbox enabling more goal-aligned development of agentic search systems for practical applications. |
