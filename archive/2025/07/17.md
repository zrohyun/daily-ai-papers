

## Papers for 2025-07-17

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning
  Systems in LLMs (Read more on [arXiv](https://arxiv.org/abs/2507.09477) or [HuggingFace](https://huggingface.co/papers/2507.09477))| Wei-Chieh Huang, Yuyao Yang, Yangning Li, TreeForest, WZDavid | This paper presents a survey of Retrieval-Augmented Generation (RAG) systems integrated with reasoning in Large Language Models (LLMs). It investigates how reasoning can enhance RAG and how RAG can improve LLM reasoning, further spotlighting synergized frameworks for iterative search and reasoning. The methodology involves categorizing methods, datasets, and challenges to outline research avenues for deeper RAG-Reasoning systems. The survey demonstrates how retrieval-reasoning coupling improves factual grounding, logical coherence, and adaptability beyond one-way enhancement. The findings imply AI practitioners should consider synergistic RAG-Reasoning systems for more effective, trustworthy, and human-centric applications but need to also be cognizant of increased computational complexity. |
| Computer Vision | PhysX: Physical-Grounded 3D Asset Generation (Read more on [arXiv](https://arxiv.org/abs/2507.12465) or [HuggingFace](https://huggingface.co/papers/2507.12465))| Linag Pan, liuziwei7, FrozenBurning, Caoza | The paper introduces PhysX, a new paradigm for physical-grounded 3D asset generation. It addresses the gap in existing 3D datasets by systematically annotating 3D assets across five foundational dimensions: absolute scale, material, affordance, kinematics, and function description. The authors propose PhysXGen, a feedforward framework with a dual-branch architecture to explicitly model latent correlations between 3D structures and physical properties. Experiments validate the framework's superior performance, with an F-score of 77.3, indicating promising generalization and high-quality 3D asset generation. The work offers AI practitioners a new approach and dataset for developing more physically realistic and applicable 3D generative models. |
| Computer Vision | MOSPA: Human Motion Generation Driven by Spatial Audio (Read more on [arXiv](https://arxiv.org/abs/2507.11949) or [HuggingFace](https://huggingface.co/papers/2507.11949))| Leo Ho, Liang Pan, Mingyi Shi, frankzydou, JimSYXu | The paper introduces MOSPA, a novel framework for generating human motion conditioned on spatial audio. It aims to address the lack of existing methods that model the impact of spatial features encoded in spatial audio on human movement. MOSPA employs a diffusion-based generative model that incorporates features like MFCCs and RMS energy extracted from spatial audio to generate realistic and responsive human motion. Evaluated on a newly curated SAM dataset, MOSPA achieves state-of-the-art performance, outperforming existing baselines in generating diverse motion responses. This framework provides a novel approach and dataset for AI practitioners looking to create more realistic and interactive virtual humans. |
| Computer Vision | MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2507.12463) or [HuggingFace](https://huggingface.co/papers/2507.12463))| Mingyang Wu, Renjie Li, vztu, waynefan, jerryye0110 | The paper introduces MMHU, a large-scale multimodal benchmark for human behavior understanding in driving scenarios. It aims to address the lack of comprehensive datasets for evaluating algorithms focused on human behavior crucial for autonomous driving safety. The methodology involves collecting and annotating a dataset of 57k human instances with 1.73M frames, providing 3D motion, trajectory, text descriptions, and critical behavior labels. The paper evaluates baseline methods showing improvements in motion prediction on 3DPW data by 9.49 MPJPE and 1.1 ACCL when finetuned with MMHU data. The dataset provides AI practitioners with a unified resource for developing and evaluating human-centric algorithms, fostering advancements in safe autonomous driving systems. |
| Machine Learning | SWE-Perf: Can Language Models Optimize Code Performance on Real-World
  Repositories? (Read more on [arXiv](https://arxiv.org/abs/2507.12415) or [HuggingFace](https://huggingface.co/papers/2507.12415))| Zhijie Fan, Lin Yan, Xinyi He, Elfsong, SivilTaram | This paper introduces SWE-Perf, a benchmark for evaluating language models on code performance optimization within real-world repositories. The research investigates whether LLMs can effectively enhance code performance at the repository level. The methodology involves curating 140 instances from GitHub repositories, each including codebase, target functions, performance tests, and expert patches, then evaluating LLMs under oracle and realistic settings. Results show a substantial performance gap between LLMs and expert-level optimization (e.g., Claude-3.7-sonnet achieved a performance gain of 1.24% in the oracle setting compared to 10.85% by experts). This highlights the need for advancements in LLMs to meet practical requirements of performance-aware code optimization. |
| Natural Language Processing | DrafterBench: Benchmarking Large Language Models for Tasks Automation in
  Civil Engineering (Read more on [arXiv](https://arxiv.org/abs/2507.11527) or [HuggingFace](https://huggingface.co/papers/2507.11527))| Yi Shao, zhendongucb, Eason666 | DrafterBench is introduced as a benchmark for evaluating Large Language Model (LLM) agents in automating tasks within civil engineering, specifically technical drawing revision. The research aims to comprehensively assess distinct capabilities like structured data comprehension, function execution, instruction following, and critical reasoning within this industrial context. DrafterBench contains 12 task types, 46 customized functions/tools, and 1920 total tasks derived from real-world drawing files. Experiments conducted on mainstream LLMs demonstrate the benchmark's ability to identify models' strengths and deficiencies, achieving a top average task score of 81.92. DrafterBench enables AI practitioners to gain insights into LLM agent capabilities and pinpoint improvement targets for integrating LLMs into specialized engineering applications. |
| Computer Vision | AnyI2V: Animating Any Conditional Image with Motion Control (Read more on [arXiv](https://arxiv.org/abs/2507.02857) or [HuggingFace](https://huggingface.co/papers/2507.02857))| Hao Luo, HenghuiDing, XinchengShuai, TribeRinb | AnyI2V is a training-free framework for animating conditional images with user-defined motion trajectories. The paper addresses the challenge of integrating dynamic motion signals and flexible spatial constraints in video generation. It employs structure-preserved feature injection, across-frames alignment, and semantic mask generation for precise control. Experiments demonstrate AnyI2V achieves superior performance with an FVD of 569.89, offering a new perspective in spatial- and motion-controlled video generation. The framework enables AI practitioners to generate diverse and controllable videos from various conditional inputs without task-specific training. |
| Computer Vision | SpatialTrackerV2: 3D Point Tracking Made Easy (Read more on [arXiv](https://arxiv.org/abs/2507.12462) or [HuggingFace](https://huggingface.co/papers/2507.12462))| Yuxi Xiao, bykang, nikkar, cherubicxn, JianyuanWang | SpatialTrackerV2 is a novel feed-forward method for 3D point tracking from monocular videos. The research aims to unify point tracking, monocular depth estimation, and camera pose estimation into a high-performing framework. The method decomposes 3D motion into scene geometry, camera ego-motion, and pixel-wise object motion within a differentiable pipeline, enabling scalable training across diverse datasets. SpatialTrackerV2 outperforms existing 3D tracking methods by 30% on the TAPVid-3D benchmark. The unified framework and scalable training approach offer AI practitioners a more efficient and accurate solution for 3D motion analysis. |
| Natural Language Processing | Lizard: An Efficient Linearization Framework for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2507.09025) or [HuggingFace](https://huggingface.co/papers/2507.09025))| Franck-Dernoncourt, Nikosapa, TrungBui1111, jasubram, haniehds | The paper presents Lizard, a linearization framework for Transformer-based Large Language Models (LLMs) to enable efficient infinite-context generation. It aims to address memory and computational bottlenecks associated with long contexts by introducing a subquadratic attention mechanism. Lizard combines gated linear attention for global context compression with sliding window attention enhanced by meta memory. Experiments demonstrate near-lossless recovery of the teacher model's performance, with an 18-point improvement over prior models on the 5-shot MMLU benchmark. This framework offers AI practitioners a scalable and efficient alternative to traditional softmax attention for long-context language modeling tasks. |
| Machine Learning | Replacing thinking with tool usage enables reasoning in small language
  models (Read more on [arXiv](https://arxiv.org/abs/2507.05065) or [HuggingFace](https://huggingface.co/papers/2507.05065))| Roland Memisevic, Tim Bakker, crainone | The paper explores enhancing reasoning capabilities in small language models (SLMs) by replacing natural language "thinking" tokens with tool-usage traces. It investigates whether a Chain-of-Edits (CoE) approach, where SLMs interact with a stateful text editor, can improve code repair. The methodology involves supervised fine-tuning and reinforcement learning with verifiable rewards (RLVR) to train SLMs to use the CoE approach. Results show that for SLMs up to 3B parameters, the CoE approach improves performance, exhibiting a 13.8% pass@1 rate on the code repair task, in contrast with prior methods, suggesting a new avenue for equipping models with reasoning ability.  This implies that structuring inference-time computation via interaction with tools can enable smaller models to effectively leverage additional compute compared to using CoTs. |
| Reinforcement Learning | RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.07451) or [HuggingFace](https://huggingface.co/papers/2507.07451))| Jingyuan Zhang, Jia Fu, GuoruiZhou, Edrex, hongzhizhang | The paper introduces RLEP, a reinforcement learning framework for LLMs that incorporates experience replay to improve reasoning. It aims to address instability and policy drift in RL training by replaying verified successful trajectories. RLEP optimizes the policy using mini-batches that combine newly generated rollouts with replayed successes. The method achieves improved accuracy on AIME-2024 (38.2% to 39.9%), AIME-2025 (19.8% to 22.3%), and AMC-2023 (77.0% to 82.2%). RLEP offers a more efficient and stable approach to training LLMs for reasoning tasks by leveraging high-quality past experiences. |
