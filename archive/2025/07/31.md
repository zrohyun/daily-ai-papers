

## Papers for 2025-07-31

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | ScreenCoder: Advancing Visual-to-Code Generation for Front-End
  Automation via Modular Multimodal Agents (Read more on [arXiv](https://arxiv.org/abs/2507.22827) or [HuggingFace](https://huggingface.co/papers/2507.22827))| Qunzhong Wang, Yuxuan Wan, Yaozhi Zheng, Yilei Jiang, csuhan | The paper introduces ScreenCoder, a modular multi-agent framework for UI-to-code generation, addressing limitations of end-to-end vision-language models. It aims to improve UI-to-code generation by decomposing the task into grounding, planning, and code generation stages with specialized agents. The methodology involves vision-language models for component detection, hierarchical layout planning using front-end engineering priors, and adaptive prompt-based code synthesis. Experiments demonstrate state-of-the-art performance with a layout accuracy of 0.755 in block matching. The framework offers a path for scalable data creation to improve vision-language model alignment in structured generation tasks, benefitting AI practitioners in front-end automation. |
| Natural Language Processing | Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency
  and Performance (Read more on [arXiv](https://arxiv.org/abs/2507.22448) or [HuggingFace](https://huggingface.co/papers/2507.22448))| Maksim Velikanov, Iheb-Chaabane, ifarhat1993, ybelkada, JingweiZuo | The paper introduces Falcon-H1, a family of hybrid-head language models designed for high performance and efficiency. The research explores a parallel hybrid architecture combining Transformer attention with State Space Models (SSMs). Falcon-H1 models, including a 34B-parameter variant, rival or outperform larger models like Qwen3-32B despite being smaller and trained on less data. Notably, the models support up to 256K tokens for extended context. The Falcon-H1 series provides AI practitioners with efficient and performant options for diverse NLP applications, including long-document processing and multilingual tasks. |
| Computer Vision | BANG: Dividing 3D Assets via Generative Exploded Dynamics (Read more on [arXiv](https://arxiv.org/abs/2507.21493) or [HuggingFace](https://huggingface.co/papers/2507.21493))| Wei Yang, Yinuo Bai, Haoran Jiang, Qixuan Zhang, ZarkLngeW | The paper introduces BANG, a novel framework for dynamically dividing 3D assets into interpretable parts using generative exploded dynamics. The research aims to bridge 3D generation and reasoning by enabling intuitive part-level decomposition of 3D objects. BANG employs a pre-trained latent diffusion model fine-tuned with an exploded view adapter and temporal attention to generate smooth, geometrically coherent exploded sequences. The framework achieves a weighted IoU of 0.8163 in part trajectory tracking, demonstrating accurate reassembly of components. BANG offers AI practitioners a new approach for component-aware 3D creation and manipulation workflows with enhanced control and structural insights. |
| Multi-Modal | VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced
  Multimodal Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.22607) or [HuggingFace](https://huggingface.co/papers/2507.22607))| Sicong Leng, Chenghao Xiao, Ruifeng Yuan, 26hzhang, kenchan0226 | The paper introduces VL-Cogito, a multimodal reasoning model trained with a novel Progressive Curriculum Reinforcement Learning (PCuRL) framework. It addresses the limitations of existing models by systematically guiding the model through tasks of gradually increasing difficulty. PCuRL employs an online difficulty soft weighting mechanism and a dynamic length reward mechanism. Experimental results demonstrate that VL-Cogito achieves state-of-the-art or highly competitive performance across multimodal benchmarks, spanning mathematics, science, logic, and general understanding, achieving a 68.7% accuracy on Geo3K. The model's enhanced reasoning capabilities offer AI practitioners a more robust and versatile solution for complex multimodal tasks. |
| Computer Vision | Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with
  Weak Supervision (Read more on [arXiv](https://arxiv.org/abs/2507.20976) or [HuggingFace](https://huggingface.co/papers/2507.20976))| Celso de Melo, Stanislav Panev, Zheyang Qin, Min0326, xiaofanghf | The paper introduces a novel approach to adapt vehicle detectors in aerial imagery across different geographic regions using generative AI and weak supervision. The research aims to address the domain shift problem where models trained on data from one region fail to generalize to others. Their method synthesizes high-quality aerial images and labels using fine-tuned latent diffusion models (LDMs) and multi-stage knowledge transfer to augment detector training. The experiments demonstrate consistent performance improvements, with a 4-23% increase in AP50 over supervised learning on source domain data. This allows AI practitioners to improve the robustness of aerial imagery object detectors in new environments with limited labeled data by utilizing generative AI for data augmentation. |
| Computer Vision | Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual
  Segmentation (Read more on [arXiv](https://arxiv.org/abs/2507.22886) or [HuggingFace](https://huggingface.co/papers/2507.22886))| Yu-Gang Jiang, Guanquan Jie, Henghui Ding, Kaining Ying | This paper introduces a new dataset, OmniAVS, for omnimodal referring audio-visual segmentation. The research aims to extend RAVS boundaries by integrating multimodal information and complex reasoning about audiovisual content. The proposed method, OISA, utilizes MLLM with audio-visual interleaving and query propagation for segmentation. Experiments demonstrate that OISA outperforms existing methods on OmniAVS, achieving 41.1% J&F score. This work facilitates the development of future omnimodal AI systems with enhanced audio understanding and reasoning capabilities. |
| Reinforcement Learning | Repair-R1: Better Test Before Repair (Read more on [arXiv](https://arxiv.org/abs/2507.22853) or [HuggingFace](https://huggingface.co/papers/2507.22853))| Quanjun Zhang, Xiaochen Xie, Haichuan Hu | The paper introduces Repair-R1, a reinforcement learning-based approach for automated program repair that integrates test case generation into the model's training phase. The objective is to improve defect localization and repair effectiveness by generating discriminative test cases before repair. Repair-R1 employs reinforcement learning to co-optimize test generation and bug repair with three different backbone models. Experiments show that Repair-R1 improves repair success rate by 2.68% to 48.29% compared to vanilla models. This approach offers a novel perspective by prioritizing testing before repair in the automated defect-fixing process. |
| Natural Language Processing | Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2507.22565) or [HuggingFace](https://huggingface.co/papers/2507.22565))| Gilbert Fridgen, Ramin Bahmani, Igor Tchappi, Amir Sartipi, akhadangi | The paper introduces RLDP, a reinforcement learning framework for efficient differentially private fine-tuning of Large Language Models (LLMs). It addresses the privacy-utility trade-off in DP-SGD by dynamically adjusting per-parameter clipping thresholds and noise levels based on learning dynamics. RLDP trains a soft actor-critic hyper-policy online, achieving perplexity reductions of 1.3-30.5% and a 5.6% downstream utility gain across multiple LLMs while maintaining privacy guarantees. This accelerates DP fine-tuning and reduces computational costs, enabling practitioners to train more effective private LLMs more efficiently. |
