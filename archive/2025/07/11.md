

## Papers for 2025-07-11

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | Scaling RL to Long Videos (Read more on [arXiv](https://arxiv.org/abs/2507.07966) or [HuggingFace](https://huggingface.co/papers/2507.07966))| Hanrong Ye, Qinghao Hu, Baifeng Shi, Wei Huang, Yukang Chen | The paper introduces LongVILA-R1, a framework for scaling reasoning in vision-language models to long videos using reinforcement learning. The research aims to improve long video reasoning by integrating a large-scale dataset, a two-stage training pipeline (CoT-SFT and RL), and a training infrastructure (MR-SP). The methodology involves creating LongVideo-Reason dataset with 52K QA pairs, pre-training with chain-of-thought SFT, and fine-tuning with reinforcement learning utilizing the MR-SP system for efficient rollouts and prefilling. LongVILA-R1-7B achieves 68.4% on VideoMME and strong performance on LongVideo-Reason-eval. The framework enables RL training on long videos, offering improvements in temporal, spatial, and goal-oriented reasoning for video understanding. |
| Computer Vision | T-LoRA: Single Image Diffusion Model Customization Without Overfitting (Read more on [arXiv](https://arxiv.org/abs/2507.05964) or [HuggingFace](https://huggingface.co/papers/2507.05964))| Konstantin Sobolev, Andrey Kuznetsov, Vera Soboleva, ai-alanov | The paper introduces T-LoRA, a timestep-dependent low-rank adaptation framework for single-image diffusion model customization to mitigate overfitting. The research aims to improve generalization and output diversity in personalized diffusion models when trained with limited data (single image). T-LoRA employs a dynamic fine-tuning strategy adjusting rank-constrained updates based on diffusion timesteps and orthogonal initialization to ensure independence between adapter components. Experiments demonstrate that T-LoRA achieves superior balance between concept fidelity and text alignment, outperforming standard LoRA and other techniques with an image similarity of 0.900 and text similarity of 0.256. T-LoRA enables more effective customization in data-limited and resource-constrained scenarios, offering improved control over concept injection across timesteps. |
| Multi-Modal | Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and
  Methodology (Read more on [arXiv](https://arxiv.org/abs/2507.07999) or [HuggingFace](https://huggingface.co/papers/2507.07999))| Zilong Huang, garlicisnotmyfavor, stormthunder, LXT, HaochenWang | This paper introduces TreeBench, a new diagnostic benchmark for evaluating visual grounded reasoning (VGR) in large multimodal models. The main research objective is to provide a more comprehensive evaluation of models that "think with images", focusing on nuanced visual grounding, traceable multi-step reasoning, and dynamic cross-modal interaction. The methodology involves the construction of a dataset with 405 challenging visual question-answering pairs annotated with bounding boxes, along with a novel training paradigm called TreeVGR that combines reinforcement learning with a dual IoU reward for precise localization. Results show that even advanced models struggle on TreeBench, with OpenAI-03 scoring only 54.87%, while TreeVGR improves performance by +13.4% on the benchmark. The main implication is that traceable evidence is crucial for advancing vision-grounded reasoning and building more accurate and explainable AI systems. |
| Multi-Modal | OST-Bench: Evaluating the Capabilities of MLLMs in Online
  Spatio-temporal Scene Understanding (Read more on [arXiv](https://arxiv.org/abs/2507.07984) or [HuggingFace](https://huggingface.co/papers/2507.07984))| Xihui Liu, Xiaohan Mao, Runsen Xu, Chenming Zhu, JingLi Lin | The paper introduces OST-Bench, a novel benchmark to evaluate the online spatio-temporal reasoning capabilities of Multi-Modal Large Language Models (MLLMs). It addresses the limitations of existing offline benchmarks by assessing MLLMs' ability to process incrementally acquired observations in dynamic spatial reasoning tasks. The methodology involves creating 1.4k scenes and 10k question-answer pairs from ScanNet, Matterport3D, and ARKitScenes. Evaluation of leading MLLMs reveals a performance gap compared to human performance, with models achieving approximately 30% lower overall accuracy, particularly on tasks requiring complex clue-based spatial reasoning and long-term memory retrieval. The work implies that advancements are needed in MLLMs’ spatio-temporal reasoning and long-term memory capabilities to improve performance in real-world embodied perception tasks. |
| Computer Vision | Multi-Granular Spatio-Temporal Token Merging for Training-Free
  Acceleration of Video LLMs (Read more on [arXiv](https://arxiv.org/abs/2507.07990) or [HuggingFace](https://huggingface.co/papers/2507.07990))| Inwoong Lee, Taeoh Kim, Su Ho Han, Sukjun Hwang, js-hyun | This paper introduces Spatio-Temporal Token Merging (STTM) to accelerate Video LLMs without training. STTM addresses the quadratic complexity of attention by exploiting local spatio-temporal redundancy in video data, merging tokens in a coarse-to-fine manner via quadtrees and directed pairwise merging. STTM achieves up to 3x speed-up with a 2% accuracy drop on video QA benchmarks under a 30% token budget.  The query-agnostic nature of STTM allows KV-cache reuse across different questions. The method can be used to effectively compress video tokens while maintaining high performance in downstream video understanding tasks. |
| Computer Vision | PyVision: Agentic Vision with Dynamic Tooling (Read more on [arXiv](https://arxiv.org/abs/2507.07998) or [HuggingFace](https://huggingface.co/papers/2507.07998))| Qilong Wu, Ming Li, Shaoheng Lin, haoquan03, stzhao | PyVision is an interactive framework for agentic visual reasoning that enables MLLMs to dynamically generate and execute Python tools. The paper addresses the limitation of predefined workflows in visual reasoning by allowing MLLMs to create task-specific tools. PyVision utilizes Python's ecosystem of libraries and a multi-turn interactive loop between an MLLM and a Python interpreter. The framework achieves up to a +31.1% performance gain on VLMsAreBlind-mini using Claude-4.0-Sonnet. The capacity for dynamic tooling equips models with adaptive visual reasoning beyond superficial pattern matching. |
| Computer Vision | Geometry Forcing: Marrying Video Diffusion and 3D Representation for
  Consistent World Modeling (Read more on [arXiv](https://arxiv.org/abs/2507.07982) or [HuggingFace](https://huggingface.co/papers/2507.07982))| Yang Ye, Junliang Guo, Diankun Wu, deeptimhe, Haoyuwu | This paper introduces Geometry Forcing (GF), a method to improve the geometric consistency of video diffusion models. The research aims to incorporate latent 3D representations into video diffusion models by aligning intermediate representations with features from a pretrained geometric foundation model using Angular and Scale Alignment. The method enhances visual quality and 3D consistency, achieving a reduction in FVD from 364 to 243 on the RealEstate10K dataset. GF offers AI practitioners a way to enhance the spatial and temporal coherence of generated videos without explicit 3D supervision, enabling more realistic world modeling. |
| Computer Vision | LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+
  FPS (Read more on [arXiv](https://arxiv.org/abs/2507.07136) or [HuggingFace](https://huggingface.co/papers/2507.07136))| Yuanhao Cai, Yang Liu, Minghan Qin, Yujie Zhao, Wanhua Li | LangSplatV2 significantly accelerates 3D language querying using Gaussian Splatting. The paper aims to improve the speed and accuracy of open-vocabulary 3D querying. It introduces a 3D sparse coefficient field, eliminating the need for a heavyweight decoder and enabling efficient CUDA-optimized splatting. LangSplatV2 achieves up to 384.6 FPS for 3D open-vocabulary text querying, a 47x speedup over LangSplat while maintaining or improving query accuracy. This enhanced efficiency enables real-time language interaction within complex 3D environments for AI practitioners. |
| Natural Language Processing | Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs (Read more on [arXiv](https://arxiv.org/abs/2507.07996) or [HuggingFace](https://huggingface.co/papers/2507.07996))| Yang Li, Ziyue Li, zhoutianyi | This paper introduces a test-time depth adaptation method for pretrained Large Language Models (LLMs) called Chain-of-Layers (CoLa). The research explores whether LLMs can adapt their architecture to different inputs without fine-tuning by dynamically composing layers. CoLa uses Monte Carlo Tree Search (MCTS) to identify optimal layer configurations for each sample, allowing layers to be skipped, repeated, or rearranged. Experiments show that CoLa improves both accuracy and efficiency, with over 75% of samples showing potential for shorter CoLa and over 60% of samples achieving correct predictions through CoLa when the original LLM was incorrect. The implication is that test-time depth adaptation can unlock greater generalization power and efficiency from pretrained LLMs. |
| Computer Vision | A Survey on Long-Video Storytelling Generation: Architectures,
  Consistency, and Cinematic Quality (Read more on [arXiv](https://arxiv.org/abs/2507.07202) or [HuggingFace](https://huggingface.co/papers/2507.07202))| Seunghyun Yoon, Ryan Rossi, Franck-Dernoncourt, taesiri, elmoghany | This survey paper comprehensively examines long-form video generation, focusing on architectures, consistency, and cinematic quality. The research aims to identify key architectural components and training strategies that consistently yield narrative coherence, high-fidelity detail, and character consistency in generated videos. The methodology involves studying 32 papers, constructing a novel taxonomy, and presenting comparative tables based on architectural designs and performance characteristics. The paper introduces a foundational framework and recommends components to address challenges in video synthesis, offering guidance for future research. This survey provides AI practitioners with a structured overview of existing methods and insights for advancing long-form video generation. |
| Computer Vision | Token Bottleneck: One Token to Remember Dynamics (Read more on [arXiv](https://arxiv.org/abs/2507.06543) or [HuggingFace](https://huggingface.co/papers/2507.06543))| Sangdoo Yun, Jeongeun Park, bhheo, calintz, taekyung-k | The paper introduces Token Bottleneck (ToBo), a self-supervised learning pipeline for deriving temporally aware visual representations from dynamic scenes using a bottleneck token. It aims to learn sequential scene representations by conservatively encoding a reference scene into a compact bottleneck token and predicting the subsequent scene with minimal hint patches. The methodology involves squeezing a scene into a single token and guiding the model to predict the target scene using that token along with few target patches, enforcing reliance on the bottleneck token. Experiments demonstrate ToBo's superiority over baselines in video label propagation and robot manipulation, achieving a success rate of 57.3% on the Knob1 task in the Franka Kitchen environment. ToBo's compact and temporally aware representations are valuable for tasks requiring understanding of dynamic transitions, enhancing sequential scene understanding capabilities in embodied agents. |
| Natural Language Processing | Machine Bullshit: Characterizing the Emergent Disregard for Truth in
  Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2507.07484) or [HuggingFace](https://huggingface.co/papers/2507.07484))| Thomas L. Griffiths, Dawn Song, Xuandong Zhao, Haimin Hu, Kaiqu Liang | This paper introduces the concept of "machine bullshit" in large language models (LLMs), defined as statements made without regard to truth, and proposes methods to characterize this behavior. The primary research objective is to quantify LLMs' indifference to truth and analyze qualitative forms of bullshit, including empty rhetoric, paltering, weasel words, and unverified claims. The methodology involves introducing the Bullshit Index to quantify LLMs' indifference to truth and a taxonomy analyzing qualitative forms of bullshit. Empirical evaluations on datasets including BullshitEval reveal that RLHF exacerbates bullshit, with inference-time CoT prompting amplifying empty rhetoric and paltering; the Bullshit Index significantly increased after RLHF (e.g.,  ΔΒΙ = -0.285). The results imply that current AI alignment strategies pose systematic challenges and highlight the need for more truthful LLM behavior. |
| Computer Vision | Beyond the Linear Separability Ceiling (Read more on [arXiv](https://arxiv.org/abs/2507.07574) or [HuggingFace](https://huggingface.co/papers/2507.07574))| Mohit Vaishnav, Tanel Tammet, envomp | This paper investigates the "linear reasoning bottleneck" in Visual-Language Models (VLMs) by introducing the Linear Separability Ceiling (LSC) as a diagnostic framework. The research aims to determine if VLM failures in abstract tasks stem from flawed perception or reasoning. The methodology involves probing a VLM's visual embeddings using a linear classifier to assess separability. The primary finding is a widespread linear reasoning bottleneck, with most VLMs performing statistically equivalent or lower than the LSC on their visual embeddings. The paper suggests that VLMs possess dormant reasoning pathways and, through targeted alignment and fine-tuning, reasoning can be improved substantially, suggesting that robust VLM reasoning requires more than simply improved representation learning. |
| Machine Learning | SciMaster: Towards General-Purpose Scientific AI Agents, Part I.
  X-Master as Foundation: Can We Lead on Humanity's Last Exam? (Read more on [arXiv](https://arxiv.org/abs/2507.05241) or [HuggingFace](https://huggingface.co/papers/2507.05241))| Xinyu Zhu, Yuwen Du, Rui Ye, Shuo Tang, Jingyi Chai | The paper introduces SciMaster, a series of studies aimed at developing general-purpose scientific AI agents, with Part I focusing on X-Master as a foundational architecture. The primary objective is to construct the foundational architecture for general-purpose agents. X-Master is a tool-augmented reasoning agent designed to emulate human researchers through flexible interaction with external tools, employing a scattered-and-stacked agentic workflow to enhance breadth and depth of reasoning. X-Master achieves a state-of-the-art score of 32.1% on Humanity's Last Exam (HLE), surpassing previous records. This work demonstrates a practical roadmap for enhancing LLM capabilities, emphasizing the potential of open-source models to attain state-of-the-art performance on challenging benchmarks without extensive retraining. |
