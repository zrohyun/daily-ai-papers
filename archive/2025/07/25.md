

## Papers for 2025-07-25

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Reinforcement Learning | Group Sequence Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2507.18071) or [HuggingFace](https://huggingface.co/papers/2507.18071))| Bowen Yu, Xiong-Hui Chen, Mingze Li, Shixuan Liu, Chujie Zheng | The paper introduces Group Sequence Policy Optimization (GSPO), a novel RL algorithm designed for stable and efficient training of large language models. GSPO addresses instability issues in existing algorithms like GRPO by defining importance ratios based on sequence likelihood and performing sequence-level clipping, reward, and optimization. Empirical evaluations demonstrate that GSPO outperforms GRPO in training stability, efficiency, and performance, especially for Mixture-of-Experts (MoE) models, and it eliminates the need for complex stabilization strategies. The authors successfully applied GSPO to the latest Qwen3 models. This advancement can simplify RL infrastructure and enable the continued development of large language models. |
| Reinforcement Learning | LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy
  Optimization (Read more on [arXiv](https://arxiv.org/abs/2507.15758) or [HuggingFace](https://huggingface.co/papers/2507.15758))| Linjuan Wu, Shangke Lyu, Xingyu Wu, tricktreat, yanyc | The paper introduces Length-Adaptive Policy Optimization (LAPO), a two-stage reinforcement learning framework designed to improve the efficiency of reasoning models by internalizing appropriate reasoning lengths. LAPO addresses the research question of enabling models to autonomously adjust their reasoning depth based on problem complexity rather than adhering to rigid constraints. The method involves a discovery stage to learn natural reasoning lengths through length-aware rewards and an internalization stage to embed these patterns as self-proposed budgets within the model's reasoning context. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9% while improving accuracy by 2.3%. LAPO allows AI practitioners to create more computationally efficient reasoning models capable of dynamically allocating resources based on problem-specific demands. |
| Natural Language Processing | MUR: Momentum Uncertainty guided Reasoning for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2507.14958) or [HuggingFace](https://huggingface.co/papers/2507.14958))| Jian Zhang, Yifei Li, Rongman Xu, Fangzhi Xu, Hang Yan | The paper introduces MUR, a novel approach to adaptively guide test-time scaling for large language models. It addresses the issue of overthinking by dynamically allocating thinking budgets based on a momentum-uncertainty metric. MUR tracks and aggregates step-wise uncertainty over time to enhance reasoning efficiency without additional training. Experimental results on MATH-500 and other benchmarks demonstrate that MUR reduces computation by over 50% on average while improving accuracy by 0.62-3.37%. This allows practitioners to achieve a better trade-off between reasoning quality and computational cost for LLMs. |
| Computer Vision | TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive
  Generation (Read more on [arXiv](https://arxiv.org/abs/2507.18537) or [HuggingFace](https://huggingface.co/papers/2507.18537))| Yujie Wei, Shiwei Zhang, Yukang Chen, Ruihang Chu, Zhekai Chen | The paper introduces TTS-VAR, a novel test-time scaling framework for visual auto-regressive (VAR) models. It addresses the challenge of scaling visual generation models efficiently. The methodology incorporates an adaptive descending batch size schedule and integrates clustering-based diversity search at coarse scales with resampling-based potential selection at fine scales, modeling the generation process as a path searching problem. Experiments on the Infinity VAR model show a notable 8.7% improvement in GenEval score (0.69 to 0.75). TTS-VAR offers AI practitioners a resource-efficient way to improve VAR model performance without extensive retraining, highlighting the impact of early-stage structural features on final image quality. |
| Computer Vision | Captain Cinema: Towards Short Movie Generation (Read more on [arXiv](https://arxiv.org/abs/2507.18634) or [HuggingFace](https://huggingface.co/papers/2507.18634))| Yang Zhao, Shengqu Cai, Lvmin Zhang, Ceyuan Yang, Junfei Xiao | The paper introduces Captain Cinema, a framework for generating short movies from textual descriptions. It addresses the challenge of long-range coherence and visual consistency by using a top-down keyframe planning module followed by a bottom-up video synthesis module with interleaved training and a multimodal diffusion transformer (MM-DiT). The primary objective is to automate the creation of narrative-consistent and visually coherent short movies. Key to the approach is an interleaved training strategy and a memory mechanism called GoldenMem to compress long-context visual memory. Experiments demonstrate strong performance in long-form narrative video synthesis, achieving a high aesthetic quality score of 57.2 (VBench-2.0), implying the framework's capability to generate high-quality short movies exceeding existing approaches in duration and multimodal video generation. |
| Computer Vision | EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent
  Diffusion (Read more on [arXiv](https://arxiv.org/abs/2507.16535) or [HuggingFace](https://huggingface.co/papers/2507.16535))| Jing Wang, Wen Qian, Chaohui Yu, Chenjie Cao, ShuYaoLiu | The paper introduces EarthCrafter, a novel framework for scalable 3D Earth generation. It addresses the challenge of modeling vast geographic areas by decoupling structural and textural generation via dual-sparse latent diffusion. The methodology involves the creation of Aerial-Earth3D, a large-scale 3D aerial dataset, and employs dual-sparse VAEs for encoding geometric voxels and textural 2D Gaussian Splats, with Flow Matching diffusion models trained independently. EarthCrafter achieves significantly improved performance in large-scale generation (quantitative metrics unspecified, but improvements are mentioned). This framework facilitates versatile applications such as semantic-guided urban layout and unconditional terrain synthesis. |
| Reinforcement Learning | Hierarchical Budget Policy Optimization for Adaptive Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.15844) or [HuggingFace](https://huggingface.co/papers/2507.15844))| Xingyu Wu, Linjuan Wu, tricktreat, yanyc, paradox122 | The paper introduces Hierarchical Budget Policy Optimization (HBPO), a reinforcement learning framework for improving reasoning efficiency in large language models. HBPO addresses the exploration space collapse by partitioning rollout samples into budget-constrained hierarchies to learn problem-specific reasoning depths. The methodology employs differentiated reward mechanisms aligned with problem complexity to enable adaptive resource allocation. Experimental results demonstrate that HBPO reduces average token usage by up to 60.6% while improving accuracy by 3.14% across four reasoning benchmarks. This suggests reasoning efficiency and capability can be simultaneously optimized through structured hierarchical training, allowing models to adapt computational effort to problem complexity. |
| Machine Learning | DriftMoE: A Mixture of Experts Approach to Handle Concept Drifts (Read more on [arXiv](https://arxiv.org/abs/2507.18464) or [HuggingFace](https://huggingface.co/papers/2507.18464))| Ricardo Simón Carbajo, Miguel Aspis, suarezcetrulo, sebasmos | DriftMoE is an online Mixture-of-Experts architecture designed for concept drift adaptation in non-stationary data streams. The paper addresses the challenge of adapting to concept drifts by proposing a symbiotic co-training framework between a compact neural router and a pool of incremental Hoeffding tree experts, which aims to optimize expert specialization. The key methodology involves a multi-hot correctness mask to refine router parameters and reinforce accurate experts to accelerate specialization. Results demonstrate competitive performance with state-of-the-art stream learning ensembles across nine benchmarks, with MoE-Data achieving strong performance on AIRL and LED streams, although performance on class-imbalanced data such as ELEC and COVT needs improvement. The implication is that DriftMoE provides a principled and efficient approach to concept drift adaptation, potentially offering a scalable alternative to large ensemble models. |
| Natural Language Processing | Technical Report of TeleChat2, TeleChat2.5 and T1 (Read more on [arXiv](https://arxiv.org/abs/2507.18013) or [HuggingFace](https://huggingface.co/papers/2507.18013))| Yu Zhao, Chao Wang, Yitong Yao, Xinzhang Liu, Zihan Wang | The paper introduces TeleChat2, TeleChat2.5, and T1, a series of large language models with enhanced pre-training and post-training strategies. The research aims to improve reasoning and general task performance. Key methodologies include pre-training on 10 trillion tokens, supervised fine-tuning (SFT), direct preference optimization (DPO), and reinforcement learning (RL) with domain-specific continual pretraining. The T1-115B model outperforms models such as OpenAI's ol-mini and GPT-40 in reasoning tasks. The released TeleChat2, TeleChat2.5 and T1 models provide state-of-the-art language models for diverse applications. |
| Natural Language Processing | A New Pair of GloVes (Read more on [arXiv](https://arxiv.org/abs/2507.18103) or [HuggingFace](https://huggingface.co/papers/2507.18103))| Christopher D. Manning, John Bauer, Riley Carlson | This paper presents updated GloVe word embeddings trained on recent corpora to reflect contemporary language usage. The study aims to improve upon existing static word embeddings by incorporating a more up-to-date lexicon. The methodology involves training new GloVe models using Wikipedia, Gigaword, and a subset of Dolma, employing a Minimum Frequency Threshold to refine vocabulary selection. Evaluations on Named Entity Recognition tasks demonstrate improved performance on recent newswire data; for instance, F1 scores on the Worldwide dataset showed an increase, with the 2024 50d Wiki/Giga embeddings achieving a per-entity F1 score of 84.64 compared to 82.1 for the 2014 counterparts. The updated embeddings better capture recent cultural and linguistic trends, benefiting NLP applications requiring contemporary language understanding. |
| Reinforcement Learning | DMOSpeech 2: Reinforcement Learning for Duration Prediction in
  Metric-Optimized Speech Synthesis (Read more on [arXiv](https://arxiv.org/abs/2507.14988) or [HuggingFace](https://huggingface.co/papers/2507.14988))| Kaifeng Xu, Cheng Niu, Fei Tao, Xilin Jiang, Yinghao Aaron Li | DMOSpeech 2 addresses the challenge of optimizing duration prediction in zero-shot text-to-speech (TTS) synthesis through reinforcement learning. The paper investigates how to directly optimize the duration predictor component using group relative policy optimization (GRPO) with speaker similarity and word error rate as reward signals. Teacher-guided sampling is introduced to improve output diversity while reducing sampling steps by half. Evaluations demonstrate superior performance across all metrics compared to previous systems, with improvements in speaker similarity and intelligibility; the system achieves a WER of 1.752 on the Seed-TTS-en dataset. The work provides AI practitioners with a metric-optimized TTS pipeline that improves upon previous methods by incorporating a more optimized duration predictor with maintained sample efficiency. |
| Natural Language Processing | GLiNER2: An Efficient Multi-Task Information Extraction System with
  Schema-Driven Interface (Read more on [arXiv](https://arxiv.org/abs/2507.18546) or [HuggingFace](https://huggingface.co/papers/2507.18546))| Ash Lewis, George Hurn-Maloney, Oliver Boyd, Gil Pasternak, Urchade Zaratiana | The paper presents GLiNER2, an efficient multi-task information extraction system unifying NER, text classification, and hierarchical extraction. It addresses the challenge of deploying computationally expensive LLMs by introducing a compact, CPU-efficient model. The methodology involves extending a transformer encoder architecture with a schema-driven interface for multi-task composition. Experiments demonstrate competitive zero-shot performance, with GLINER2 achieving an average accuracy of 0.72 on text classification benchmarks. This offers AI practitioners a resource-efficient alternative for performing diverse IE tasks on CPU hardware. |
| Computer Vision | TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance (Read more on [arXiv](https://arxiv.org/abs/2507.18192) or [HuggingFace](https://huggingface.co/papers/2507.18192))| Zhao Xu, Qing-Guo Chen, Xiaohao Chen, Minghao Fu, Flourish | The paper introduces TeEFusion, a novel and efficient distillation method for text-to-image synthesis that blends text embeddings to incorporate classifier-free guidance. It aims to reduce the inference cost associated with complex sampling strategies by distilling the teacher model's behavior into a student model through linear fusion of conditional and unconditional text embeddings. The methodology involves training a student model to mimic the teacher's denoised outputs generated using a complex sampling strategy, effectively distilling both guidance signals and sampling procedures. Experiments on SD3 demonstrate that TeEFusion enables a student model to achieve up to 6× faster inference speeds while maintaining comparable image quality. TeEFusion offers AI practitioners a practical approach to significantly accelerate text-to-image generation without substantial compromise in image quality. |
| Computer Vision | Discovering and using Spelke segments (Read more on [arXiv](https://arxiv.org/abs/2507.16038) or [HuggingFace](https://huggingface.co/papers/2507.16038))| Luca Thomas Wheeler, Seungwoo Kim, Lilian Naing Chen, Klemen Kotar, Rahul Venkatesh | This paper introduces Spelke segments, a category-agnostic method for grouping pixels based on causal motion relationships, and explores their utility for downstream tasks. The research aims to extract segments that align with intuitive physics, enabling improved manipulation and planning capabilities in AI systems. SpelkeNet, a visual world model trained to predict future motions, is developed and used in a statistical counterfactual probing framework to define Spelke segments. The proposed approach achieves 0.6811 mean Intersection over Union (mIoU) on the SpelkeBench dataset, outperforming supervised baselines like SegmentAnything (SAM). Spelke segments offer a more functional notion of segmentation for robotics tasks, leading to superior performance in physical object manipulation compared to traditional semantic segmentation. |
| Computer Vision | SegDT: A Diffusion Transformer-Based Segmentation Model for Medical
  Imaging (Read more on [arXiv](https://arxiv.org/abs/2507.15595) or [HuggingFace](https://huggingface.co/papers/2507.15595))| Abdenour Hadid, Fadi Dornaika, Gaby Maroun, Bekhouche | The paper introduces SegDT, a diffusion transformer-based model for efficient skin lesion segmentation in medical imaging. The research aims to improve segmentation accuracy and efficiency on low-cost hardware. SegDT employs a compact Diffusion Transformer (DiT) architecture and incorporates Rectified Flow to accelerate inference, evaluated on ISIC datasets. SegDT achieves a Dice score of 94.76% on ISIC 2016 with significantly fewer GFLOPs compared to existing methods. SegDT's efficient architecture enables faster clinical analysis and broader accessibility of advanced medical imaging segmentation on resource-constrained GPUs. |
| Computer Vision | Deep Learning-Based Age Estimation and Gender Deep Learning-Based Age
  Estimation and Gender Classification for Targeted Advertisement (Read more on [arXiv](https://arxiv.org/abs/2507.18565) or [HuggingFace](https://huggingface.co/papers/2507.18565))| Nisar Ahmed, ImranzamanML | The paper introduces a deep learning approach for simultaneous age and gender classification from facial images to improve targeted advertising. The main objective is to develop a CNN architecture that leverages the correlation between age and gender. The proposed CNN is trained on a large dataset of pre-processed facial images. The experimental results show a 95% gender classification accuracy and a 5.77 years mean absolute error for age estimation. This approach offers valuable insights for future research in targeted advertising by demonstrating the effectiveness of shared representations learned by a custom CNN. |
| Natural Language Processing | Agentar-Fin-R1: Enhancing Financial Intelligence through Domain
  Expertise, Training Efficiency, and Advanced Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.16802) or [HuggingFace](https://huggingface.co/papers/2507.16802))| Zhaowen Zhou, Xiaoke Zhao, Longfei Liao, Xiyang Du, Yanjun Zheng | This paper introduces Agentar-Fin-R1, a financial large language model (LLM) series designed to enhance reasoning, reliability, and domain specialization. It addresses the limitations of existing models in complex reasoning and adaptation to financial contexts. The methodology integrates a high-quality financial task label system with a multi-layered trustworthiness assurance framework and employs label-guided, difficulty-aware training for efficient optimization. Agentar-Fin-R1 achieves state-of-the-art performance on financial benchmarks like Fineva (92.38) while maintaining general reasoning capabilities. The research provides AI practitioners with a trustworthy and efficient solution for high-stakes financial applications, validated by a novel evaluation benchmark called Finova. |
