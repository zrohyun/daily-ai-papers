

## Papers for 2025-07-18

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | A Survey of Context Engineering for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2507.13334) or [HuggingFace](https://huggingface.co/papers/2507.13334))| ShowerMaker, LImax72, YuyaoGe, Theodyy, Chevalier | This paper surveys Context Engineering, a discipline focused on optimizing information payloads for Large Language Models (LLMs). The main objective is to provide a comprehensive taxonomy decomposing Context Engineering into foundational components and sophisticated implementations. The survey analyzes over 1400 research papers, examining components like Context Retrieval, Processing, and Management and System Implementations like RAG and Multi-Agent Systems.  The analysis reveals an asymmetry: LLMs are proficient in understanding but limited in generating complex, long-form outputs, with a research road map provided for future work. The work offers a unified framework for researchers and engineers advancing context-aware AI. |
| Multi-Modal | VisionThink: Smart and Efficient Vision Language Model via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2507.13348) or [HuggingFace](https://huggingface.co/papers/2507.13348))| Hengshuang Zhao, Bei Yu, Xin Lai, Junyi Li, Senqiao Yang | The paper introduces VisionThink, a novel paradigm for efficient vision-language models by dynamically adjusting image resolution based on task needs. It addresses the high computational cost of visual tokens in VLMs by using reinforcement learning to determine when to request higher-resolution images. The key methodology involves an LLM-as-Judge approach to guide RL training and a balanced reward function for stable performance. VisionThink achieves comparable or superior performance on general VQA tasks with improvements, such as MathVerse achieving scores of 48.0 representing improvements of 3.7% over the base model, while reducing visual token usage.  This approach offers AI practitioners a way to improve the efficiency of VLMs in real-world scenarios without significantly sacrificing accuracy. |
| Computer Vision | π^3: Scalable Permutation-Equivariant Visual Geometry Learning (Read more on [arXiv](https://arxiv.org/abs/2507.13347) or [HuggingFace](https://huggingface.co/papers/2507.13347))| Yang Zhou, Wenzheng Chang, Haoyi Zhu, Jianjun Zhou, Yifan Wang | The paper introduces π³, a novel feed-forward neural network for visual geometry reconstruction that eliminates reliance on a fixed reference view. It achieves this by employing a fully permutation-equivariant architecture to predict affine-invariant camera poses and scale-invariant local point maps without any reference frames. The model attains state-of-the-art performance on various tasks, including reducing camera pose estimation ATE from 0.167 to 0.074 on the Sintel benchmark. π³ demonstrates improved robustness to input ordering, enhanced scalability with model size, and faster training convergence. This approach enables more stable and versatile 3D vision models, benefiting AI practitioners by offering a bias-free and scalable solution for visual geometry reconstruction. |
| Natural Language Processing | The Imitation Game: Turing Machine Imitator is Length Generalizable
  Reasoner (Read more on [arXiv](https://arxiv.org/abs/2507.13332) or [HuggingFace](https://huggingface.co/papers/2507.13332))| Songyang Gao, Chengqi Lyu, Wenwei Zhang, vanilla1116, ZhouqiHUA | This paper introduces Turing Machine Imitation Learning (TAIL) to improve length generalization in large language models (LLMs). It addresses the challenge of LLMs struggling with longer input sequences by training them to imitate the step-by-step execution of a Turing Machine through synthesized chain-of-thought (CoT) data. The methodology involves creating CoT data with linear transition, atomic state, and memory fetcher mechanisms. Results show TAIL significantly improves length generalization, with Qwen2.5-7B achieving high label accuracy and surpassing DeepSeek-R1 on various tasks using only synthetic data; e.g. over 90% label accuracy for test sets of 50 samples. The study implies that imbuing LLMs with the key concepts of a Turing Machine enables better algorithmic simulation and improves their ability to handle problems with varying input lengths. |
| Multi-Modal | AnyCap Project: A Unified Framework, Dataset, and Benchmark for
  Controllable Omni-modal Captioning (Read more on [arXiv](https://arxiv.org/abs/2507.12841) or [HuggingFace](https://huggingface.co/papers/2507.12841))| Gao Meng, Yu Li, Zhiqiang Lin, Yiming Ren, Ruihang | The paper introduces the AnyCap Project, a unified framework, dataset, and benchmark for controllable omni-modal captioning. It aims to address the lack of fine-grained control and reliable evaluation in existing captioning models. The project proposes AnyCapModel (ACM), a plug-and-play framework leveraging user instructions and modality features to improve captions, and introduces AnyCap-Dataset (ACD), a large-scale dataset. ACM significantly improves caption quality across diverse base models, with ACM-8B boosting GPT-4o's content scores by 45% and style scores by 12% on AnyCapEval. The project's components enable more precise multimodal alignment and instruction following for AI practitioners. |
| Computer Vision | Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos
  with Spatio-Temporal Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2507.13344) or [HuggingFace](https://huggingface.co/papers/2507.13344))| Zhen Xu, Tao Xie, Xuan Wang, Sida Peng, krahets | The paper introduces Diffuman4D, a novel framework for high-fidelity human view synthesis from sparse-view videos. It addresses the challenge of spatio-temporal inconsistency in generated novel views by proposing a sliding iterative denoising process within a diffusion model. The method involves alternately denoising a latent grid along spatial and temporal dimensions using a sliding window. Experiments on DNA-Rendering and ActorsHQ datasets demonstrate that Diffuman4D achieves high-quality and consistent novel-view videos, significantly outperforming existing approaches, reaching a PSNR of 25.393 on the DNA-Rendering dataset with only 4 input views. This enables practitioners to synthesize realistic human performances from limited viewpoints with improved spatio-temporal coherence. |
| Computer Vision | MindJourney: Test-Time Scaling with World Models for Spatial Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.12508) or [HuggingFace](https://huggingface.co/papers/2507.12508))| Reuben Tan, Siyuan Zhou, Zheyuan Zhang, Jiageng Liu, yyuncong | This paper introduces MindJourney, a test-time scaling framework that enhances spatial reasoning in vision-language models (VLMs) by coupling them with a controllable world model based on video diffusion. The research aims to address the limitations of current VLMs in understanding 3D space from 2D images for embodied tasks. MindJourney iteratively sketches camera trajectories using the VLM, while the world model synthesizes corresponding views, enabling multi-view reasoning. Experimental results on the SAT benchmark show an average 8.1% improvement in top-1 accuracy across various VLM back-ends, with some gains reaching 15%. The implication for AI practitioners is a plug-and-play route to robust 3D reasoning by pairing VLMs with world models without requiring fine-tuning. |
| Natural Language Processing | AbGen: Evaluating Large Language Models in Ablation Study Design and
  Evaluation for Scientific Research (Read more on [arXiv](https://arxiv.org/abs/2507.13300) or [HuggingFace](https://huggingface.co/papers/2507.13300))| Yixin Liu, Manasi Patwardhan, Zhijian Xu, Weiyuan Chen, Yilun Zhao | The paper introduces ABGEN, a novel benchmark for evaluating the capabilities of Large Language Models (LLMs) in designing ablation studies for scientific research. ABGEN aims to address how well frontier LLMs perform in ablation study design, how this research can be applied to assist human researchers, and how to develop more reliable automated evaluation systems. The methodology involves constructing a dataset of 1,500 expert-annotated examples from NLP papers and evaluating LLMs on generating ablation study designs based on a given research context, comparing against human experts using metrics like importance, faithfulness, and soundness. Results show a significant performance gap between LLMs (e.g., DeepSeek-R1-0528) and human experts, with Cohen's Kappa scores for inter-annotator agreement being 0.735, 0.782, and 0.710 for importance, faithfulness, and soundness, respectively, suggesting current LLMs need improvement in complex scientific tasks. ABGEN-EVAL is also introduced to improve reliability assessment in LLM-based evaluation systems. |
| Computer Vision | FantasyPortrait: Enhancing Multi-Character Portrait Animation with
  Expression-Augmented Diffusion Transformers (Read more on [arXiv](https://arxiv.org/abs/2507.12956) or [HuggingFace](https://huggingface.co/papers/2507.12956))| Yonggang Qi, Yaqi Fan, Fan Jiang, Mengchao Wang, wangqiang9 | The paper introduces FantasyPortrait, a diffusion transformer framework for generating expressive multi-character portrait animations. It addresses the challenge of creating high-fidelity facial animations, particularly in cross-reenactment and multi-character scenarios, where explicit geometric priors often fail. The method employs an expression-augmented learning strategy using implicit representations and a masked cross-attention mechanism to ensure both expression independence and coordination. Experiments on the newly proposed ExprBench dataset demonstrate superior performance compared to existing methods, achieving a LMD of 5.08 (lower is better) in self-reenactment tasks. The framework provides AI practitioners with improved controllability and expressiveness in portrait animation, particularly for scenarios involving multiple subjects. |
| Machine Learning | Teach Old SAEs New Domain Tricks with Boosting (Read more on [arXiv](https://arxiv.org/abs/2507.12990) or [HuggingFace](https://huggingface.co/papers/2507.12990))| Yaroslav Aksenov, Nikita Koriagin, kefirski, elephantmipt, dlaptev | This paper introduces SAE Boost, a residual learning approach to enhance sparse autoencoders (SAEs) for domain-specific interpretability of Large Language Models (LLMs). The research aims to address the feature blindness of SAEs by training a supplementary model on the reconstruction error of a pretrained SAE for domain-specific texts. The methodology involves training a residual SAE to model the reconstruction error of a pretrained SAE on domain-specific data, effectively capturing features missed by the primary model. Results demonstrate improvements in both LLM cross-entropy and explained variance, with the method achieving a 25.39% increase in explained variance on chemistry data with the Qwen model. The main implication is that researchers can selectively enhance SAE interpretability for specific domains without complete retraining, facilitating mechanistic interpretability of LLMs. |
| Natural Language Processing | FLEXITOKENS: Flexible Tokenization for Evolving Language Models (Read more on [arXiv](https://arxiv.org/abs/2507.12720) or [HuggingFace](https://huggingface.co/papers/2507.12720))| Sachin Kumar, Orevaoghene Ahia, Abraham Toluase Owodunni | The paper introduces FLEXITOKENS, a novel approach to flexible tokenization for language models, enhancing their adaptability to new data distributions. The research aims to mitigate the inflexibility of static subword tokenizers by developing byte-level LMs with learnable tokenizers that adapt to target distributions. FLEXITOKENS uses a simplified training objective that allows the tokenization to effectively adjust to the target distribution by predicting boundaries between input byte sequences and employing a hinge-like loss with a lower bound on compression rate. Results demonstrate up to 10% improvements on downstream task performance compared to subword and gradient-based tokenizers. This flexible tokenization improves model efficiency and generalizability for AI practitioners by reducing over-fragmentation in diverse linguistic contexts. |
| Computer Vision | TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame
  Interpolation (Read more on [arXiv](https://arxiv.org/abs/2507.04984) or [HuggingFace](https://huggingface.co/papers/2507.04984))| Chen Chen, ucfzl | The paper introduces Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI) to efficiently predict intermediate frames in videos. It aims to enhance temporal consistency and reduce computational costs in video frame interpolation. The method employs a 3D-wavelet gating mechanism and temporal-aware autoencoder to extract rich temporal information from video inputs and integrates Brownian Bridge Diffusion in the latent space. TLB-VFI achieves a 20% improvement in FID on challenging datasets compared to image-based diffusion models and requires 3x fewer parameters. This enables AI practitioners to achieve state-of-the-art video frame interpolation with reduced computational resources. |
| Multi-Modal | Automating Steering for Safe Multimodal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2507.13255) or [HuggingFace](https://huggingface.co/papers/2507.13255))| Nay Oo, Tri Cao, Ziwen Xu, Mengru Wang, Lyucheng Wu | This paper introduces AutoSteer, a modular framework to enhance the safety of multimodal large language models (MLLMs) during inference. The research aims to improve MLLM safety against adversarial inputs without requiring fine-tuning, using a safety awareness score (SAS) to identify safety-relevant layers. AutoSteer employs a safety prober and a refusal head to modulate generation based on toxicity estimation, achieving a significant reduction in attack success rate (ASR) on textual, visual, and cross-modal threats. Specifically, ASR drops from 60.0% to 4.2% on VLSafe using AutoSteer for Llava-OV. The implication is a practical and effective approach for safer deployment of multimodal AI systems. |
| Multi-Modal | Voxtral (Read more on [arXiv](https://arxiv.org/abs/2507.13264) or [HuggingFace](https://huggingface.co/papers/2507.13264))| Corentin Barreau, Clément Denoix, Andy Lo, Andy Ehrenberg, Alexander H. Liu | Voxtral presents two multimodal audio chat models, Mini and Small, that comprehend spoken audio and text. The models are trained to achieve state-of-the-art performance across audio benchmarks while maintaining strong text capabilities. Voxtral leverages a 32K context window for processing audio files up to 40 minutes. Voxtral Small outperforms other closed-source models while remaining small enough for local execution. Both models are released under the Apache 2.0 license and evaluated on transcription, translation, and speech understanding tasks, demonstrating competitive performance with GPT-40 mini and Gemini 2.5 Flash, particularly in English Short-Form and MCV with state-of-the-art transcription results. |
| Machine Learning | Einstein Fields: A Neural Perspective To Computational General
  Relativity (Read more on [arXiv](https://arxiv.org/abs/2507.11589) or [HuggingFace](https://huggingface.co/papers/2507.11589))| Johannes Brandstetter, Arturs Berzins, Sandeep Suresh Cranganore, AndreiB137 | The paper introduces Einstein Fields, a novel neural representation for compressing computationally intensive numerical relativity simulations. It explores the feasibility of using neural fields to accurately model spacetime geometry for general relativity. By modeling the metric tensor, the method leverages automatic differentiation to derive physical quantities. Einstein Fields achieve up to 1E-9 relative precision in metric tensor component reconstruction while significantly compressing the data. This approach provides a mesh-agnostic and storage-efficient method for continuum modeling of 4D spacetime, offering AI practitioners a scalable tool for numerical relativity tasks. |
