

## Papers for 2025-07-14

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Machine Learning | Test-Time Scaling with Reflective Generative Model (Read more on [arXiv](https://arxiv.org/abs/2507.01951) or [HuggingFace](https://huggingface.co/papers/2507.01951))| Jie Gao, Mengting Xing, Xiaorui Wang, Yuxin Wang, Zixiao Wang | The paper introduces MetaStone-S1, a reflective generative model, to improve test-time scaling (TTS) performance by focusing on high-quality reasoning trajectory selection. The research aims to eliminate reliance on process-level annotations and provide a unified interface for policy and process reward models. MetaStone-S1 shares the backbone of policy and reward models and uses a self-supervised process reward model. Experiments show that MetaStone-S1 achieves comparable performance to OpenAI o3-mini's series with only 32B parameters, improving performance on mathematical reasoning tasks by +4.4% on AIME24. The implication is that reflective generative models offer a parameter-efficient and annotation-reduced approach to enhance reasoning in LLMs. |
| Computer Vision | CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive
  Neural Rendering (Read more on [arXiv](https://arxiv.org/abs/2507.08776) or [HuggingFace](https://huggingface.co/papers/2507.08776))| Yasutaka Furukawa, Fuyang Zhang, Jiacheng Chen, Yuefan Wu, Zhengqing Wang | The paper introduces Compressive Light-Field Tokens (CLiFTs) for compute-efficient and adaptive neural rendering. It aims to represent a scene as a compact set of light field rays with compressed embeddings to enable efficient novel view synthesis. The methodology involves multi-view encoding, latent-space K-means clustering for ray selection, and neural condensation to compress information into centroid tokens. Experiments on RealEstate10K demonstrate comparable PSNR with 5-7x data reduction compared to MVSplat and DepthSplat, and overall highest PSNR and data size trade-offs. This approach allows for compute-adaptive rendering, offering AI practitioners control over data size, rendering quality, and speed with a single trained network. |
| Computer Vision | NeuralOS: Towards Simulating Operating Systems via Neural Generative
  Models (Read more on [arXiv](https://arxiv.org/abs/2507.08800) or [HuggingFace](https://huggingface.co/papers/2507.08800))| Yuntian Deng, Wenhu Chen, Hongyu Guo, Sun Sun, Luke Rivard | NeuralOS simulates operating system GUIs by directly predicting screen frames from user inputs. The research aims to model OS interfaces as generative processes, predicting frames based on mouse, click, and keyboard events. They combine a recurrent neural network for state tracking with a diffusion-based neural renderer trained on Ubuntu XFCE recordings. Experiments demonstrate the generation of realistic GUI sequences with accurate mouse interactions and state transitions; cursor position error averages 1.4-1.6 pixels. The work offers a step towards adaptive, generative interfaces for future human-computer interaction systems, despite limitations in fine-grained keyboard input modeling. |
| Natural Language Processing | KV Cache Steering for Inducing Reasoning in Small Language Models (Read more on [arXiv](https://arxiv.org/abs/2507.08799) or [HuggingFace](https://huggingface.co/papers/2507.08799))| Cees G. M. Snoek, M. Jehanzeb Mirza, Michael Dorkenwald, Dawid J. Kopiczko, Max Belitsky | This paper introduces cache steering, a lightweight method for implicitly steering language models by intervening in the key-value cache. The research aims to improve chain-of-thought reasoning in small language models without fine-tuning or prompt modifications. The method constructs steering vectors from GPT-4o-generated reasoning traces and applies them to the key-value cache. Experimental results on reasoning benchmarks demonstrate improved reasoning structure and quantitative task performance; for example, cache steering achieves 87.2% accuracy on ARC-Challenge. Cache steering offers hyperparameter stability, inference-time efficiency, and ease of integration, providing a practical solution for controlled generation. |
| Computer Vision | Neural-Driven Image Editing (Read more on [arXiv](https://arxiv.org/abs/2507.05397) or [HuggingFace](https://huggingface.co/papers/2507.05397))| Zilong Ye, Wangbo Zhao, Xiaopeng Peng, Jie Xia, Pengfei Zhou | The paper introduces LoongX, a hands-free image editing framework driven by multimodal neurophysiological signals. Its objective is to enable accessible image editing by decoding user intentions from EEG, fNIRS, PPG, and head motion signals. LoongX employs a cross-scale state space (CS3) module for feature extraction and a dynamic gated fusion (DGF) module for multimodal integration, fine-tuned via a diffusion transformer. Experiments demonstrate that LoongX achieves CLIP-I scores of 0.6605 comparable to text-driven methods. The research implies that neural signals can reliably guide generative models for more intuitive image editing. |
| Computer Vision | Lumos-1: On Autoregressive Video Generation from a Unified Model
  Perspective (Read more on [arXiv](https://arxiv.org/abs/2507.08801) or [HuggingFace](https://huggingface.co/papers/2507.08801))| Jingyun Liang, Hu Yu, Jun Cen, Weihua Chen, Hangjie Yuan | The paper introduces Lumos-1, an autoregressive video generation model that retains the LLM architecture. It addresses the challenge of incorporating spatiotemporal correlations in LLMs for video by proposing MM-ROPE, a novel RoPE scheme with comprehensive frequency spectra and scaled 3D positions. The model also employs a token dependency strategy and Autoregressive Discrete Diffusion Forcing (AR-DF) to handle frame-wise loss imbalance. Lumos-1 achieves comparable performance to EMU3 on GenEval using only 48 GPUs. The research provides a pathway for unifying visual generation and language understanding within a single LLM framework with minimal architectural modification. |
| Multi-Modal | Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for
  Visual Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.05255) or [HuggingFace](https://huggingface.co/papers/2507.05255))| Jisheng Yin, Kangheng Lin, Jianjian Sun, Liang Zhao, Yana Wei | The paper introduces Open Vision Reasoner (OVR), which transfers linguistic cognitive behaviors to Multimodal LLMs for enhanced visual reasoning. It investigates transferring cognitive behaviors from LLMs to MLLMs using a two-stage approach: linguistic cold-start fine-tuning followed by multimodal reinforcement learning. OVR achieves state-of-the-art performance, including 95.3% on MATH500, 51.8% on MathVision, and 54.6% on MathVerse. The research demonstrates the effectiveness of transferring linguistic cognitive behaviors for improving multimodal reasoning capabilities, suggesting a new direction for developing behavior-aligned multimodal reasoners. |
| Computer Vision | From One to More: Contextual Part Latents for 3D Generation (Read more on [arXiv](https://arxiv.org/abs/2507.08772) or [HuggingFace](https://huggingface.co/papers/2507.08772))| Yuxin Wang, Yaokun Li, Xiao Chen, Lihe Ding, Shaocong Dong | The paper introduces CoPart, a novel part-based 3D generation framework that represents 3D objects with multiple contextual part latents for coherent part generation. It addresses the challenge of generating detailed 3D objects by decomposing them into simpler parts and modeling their interrelationships. CoPart employs a mutual guidance strategy to ensure the coherence of part latents, fine-tuning pre-trained diffusion models for joint part latent denoising. Evaluated on a newly collected large-scale 3D part dataset, Partverse, CoPart shows high controllability and improved generation quality with CLIP score of 0.1607. This enables various applications like part-editing, articulated object generation, and mini-scene generation, providing AI practitioners with enhanced control over 3D object creation. |
| Machine Learning | One Token to Fool LLM-as-a-Judge (Read more on [arXiv](https://arxiv.org/abs/2507.08794) or [HuggingFace](https://huggingface.co/papers/2507.08794))| Haitao Mi, S. Y. Kung, Dian Yu, Haolin Liu, Yulai Zhao | This paper investigates the vulnerability of Large Language Models (LLMs) when used as reward models in Reinforcement Learning with Verifiable Rewards (RLVR). It demonstrates that LLMs used as judges are susceptible to simple adversarial "master key" inputs, such as non-word symbols and reasoning openers, resulting in high false positive rates (up to 80% in some cases). To mitigate this, the authors propose a data augmentation strategy and train a new robust reward model called Master-RM. The Master-RM significantly improves robustness, maintaining near-zero false positive rates across diverse datasets. This highlights the need for more reliable LLM-based evaluation methods and introduces a practical approach for improving the robustness of LLM judges. |
| Computer Vision | Vision Foundation Models as Effective Visual Tokenizers for
  Autoregressive Image Generation (Read more on [arXiv](https://arxiv.org/abs/2507.08441) or [HuggingFace](https://huggingface.co/papers/2507.08441))| Tiancai Wang, Chuofan Ma, Xuanyang Zhang, Xin Wen, Anlin Zheng | This paper introduces VFMTok, a novel image tokenizer leveraging vision foundation models for autoregressive image generation. It explores whether features from vision foundation models can serve as robust representations for image reconstruction and generation. The proposed approach employs region-adaptive quantization and a semantic reconstruction objective to enhance token efficiency and semantic fidelity. VFMTok achieves a gFID of 2.07 on ImageNet benchmarks, accelerating model convergence. The research enables high-fidelity class-conditional synthesis without classifier-free guidance, improving efficiency and quality for AI practitioners. |
| Machine Learning | What Has a Foundation Model Found? Using Inductive Bias to Probe for
  World Models (Read more on [arXiv](https://arxiv.org/abs/2507.06952) or [HuggingFace](https://huggingface.co/papers/2507.06952))| Sendhil Mullainathan, Ashesh Rambachan, Peter G. Chang, Keyon Vafa | This paper introduces an inductive bias probe to evaluate whether foundation models have learned a postulated world model. The research investigates whether foundation models, trained on sequence prediction, develop inductive biases aligned with underlying world models when adapted to new tasks. The methodology involves adapting foundation models to synthetic datasets generated from postulated world models and measuring their extrapolative predictability. The primary result indicates that foundation models often fail to develop inductive biases toward the underlying world model, even when excelling at training tasks, and instead rely on task-specific heuristics. This implies that AI practitioners should be cautious about assuming foundation models possess a deep understanding of the domains they are trained on. |
| Multi-Modal | Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality,
  Long Context, and Next Generation Agentic Capabilities (Read more on [arXiv](https://arxiv.org/abs/2507.06261) or [HuggingFace](https://huggingface.co/papers/2507.06261))| Noveen Sachdeva, Ice Pasupat, Mike Schaekermann, Eric Bieber, Gheorghe Comanici | This paper introduces the Gemini 2.X model family, including Gemini 2.5 Pro and Flash, achieving state-of-the-art performance in reasoning and coding benchmarks. The models aim to advance multimodal understanding, long context processing, and agentic capabilities. Gemini 2.5 Pro, utilizing a sparse MoE architecture, demonstrates SoTA performance on tasks such as LiveCodeBench (74.2%) and achieves significant improvements in coding, reasoning, and image understanding. The Gemini 2.X family spans the Pareto frontier of model capability vs cost, offering improvements in safety and helpfulness alongside advanced reasoning and coding skills. This work enables new agentic workflows and provides a suite of models for complex problem-solving. |
| Machine Learning | BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with
  Chunk-Level Activation Sparsity (Read more on [arXiv](https://arxiv.org/abs/2507.08771) or [HuggingFace](https://huggingface.co/papers/2507.08771))| Yingfa Chen, Chaojun Xiao, Xu Han, Weilin Zhao, Chenyang Song | The paper introduces BlockFFN, a novel Mixture-of-Experts (MoE) architecture designed for efficient deployment of large language models (LLMs) on end-side devices by promoting chunk-level sparsity. It addresses the limitations of vanilla MoE's non-differentiable routing and low chunk-level sparsity by integrating ReLU activation and RMSNorm in the router, alongside CLS-aware training objectives. The methodology involves novel router architecture, CLS-aware training objectives (activation locality loss and chunk sparsification loss) and optimized acceleration kernels combining sparsity and speculative decoding. Experiments demonstrate BlockFFN's superior performance over MoE baselines, achieving over 80% token-level sparsity and 70% 8-token chunk-level sparsity, with acceleration kernels achieving up to 3.67x speedup on end-side devices compared to dense models; this enables practical LLM inference on resource-constrained platforms. |
| Multi-Modal | Robust Multimodal Large Language Models Against Modality Conflict (Read more on [arXiv](https://arxiv.org/abs/2507.07151) or [HuggingFace](https://huggingface.co/papers/2507.07151))| Houqiang Li, Jie Zhao, Wengang Zhou, ustc-zhangzm | The paper addresses hallucinations in multimodal large language models (MLLMs) arising from modality conflict in the inputs. It investigates how inherent inconsistencies in visual and textual inputs lead to MLLM inaccuracies, constructing a dataset, MMMC, to simulate these conflicts. The research evaluates prompt engineering, supervised fine-tuning, and reinforcement learning to alleviate modality conflict; reinforcement learning demonstrates the best mitigation, reducing hallucination. The primary result of using Reinforcement learning, reduced the Hallu-Rate by 10% to 50%. This work provides insights into modality conflict-induced hallucinations, aiming to bolster the robustness of MLLMs by addressing input-level inconsistencies. |
