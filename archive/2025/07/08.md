

## Papers for 2025-07-08

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | MemOS: A Memory OS for AI System (Read more on [arXiv](https://arxiv.org/abs/2507.03724) or [HuggingFace](https://huggingface.co/papers/2507.03724))| Hanyu Wang, Chenyang Xi, Shichao Song, Zhiyu Li, Wentao-PKU | The paper introduces MEMOS, a memory operating system for large language models to address limitations in long-context reasoning and knowledge consistency. MEMOS unifies representation, scheduling, and evolution of memory types using MemCubes, which encapsulate memory content and metadata. The primary objective is to establish a memory-centric framework for LLMs that provides controllability, plasticity, and evolvability. Results show that MEMOS achieves state-of-the-art performance on the LOCOMO benchmark, outperforming baselines with an LLM-Judge score of 73.31. MEMOS provides a foundation for continual learning and personalized modeling, offering a pathway towards more sustainable and adaptable AI systems. |
| Computer Vision | 4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous
  Capture (Read more on [arXiv](https://arxiv.org/abs/2507.05163) or [HuggingFace](https://huggingface.co/papers/2507.05163))| Xiuyuan Yu, Lihe Ding, Tianshuo Yang, Shi Guo, Yutian Chen | The paper introduces 4DSloMo, a novel approach for high-speed 4D scene reconstruction from multi-view video using asynchronous capture. It aims to increase the effective capture frame rate of a multi-camera system without specialized hardware. The method proposes an asynchronous capture scheme and a video-diffusion-based artifact-fix model to refine reconstruction results. Experiments on real and synthetic datasets demonstrate improved performance, with the method achieving a PSNR of 26.76 on the DNA-Rendering dataset, surpassing synchronous capture. This work provides AI practitioners with an improved method for high-speed dynamic scene reconstruction using readily available low FPS cameras. |
| Natural Language Processing | Should We Still Pretrain Encoders with Masked Language Modeling? (Read more on [arXiv](https://arxiv.org/abs/2507.00994) or [HuggingFace](https://huggingface.co/papers/2507.00994))| Emmanuel Malherbe, Duarte M. Alves, Manuel Faysse, Nicolas-BZRD, hgissbkh | This paper investigates the necessity of Masked Language Modeling (MLM) for pretraining encoders compared to Causal Language Modeling (CLM). Through large-scale pretraining ablations, the study trains 38 models and finds MLM generally yields better performance, but CLM demonstrates improved data efficiency and fine-tuning stability. A biphasic training strategy that sequentially applies CLM and then MLM achieves optimal performance under a fixed computational budget. Specifically, a 610M parameter model achieved an accuracy of 87% on MNLI when pretrained with a combined CLM+MLM approach. The main implication is that practitioners can leverage pretrained CLM models and adapt them with MLM for efficient encoder training. |
| Computer Vision | DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive
  World Knowledge (Read more on [arXiv](https://arxiv.org/abs/2507.04447) or [HuggingFace](https://huggingface.co/papers/2507.04447))| Yunnan Wang, Hongsi Liu, Wenyao Zhang, RunpeiDong, qizekun | DreamVLA is introduced as a novel Vision-Language-Action (VLA) model aimed at enhancing robot manipulation through comprehensive world knowledge forecasting. It addresses limitations in existing methods by integrating dynamic-region-guided world knowledge prediction with spatial and semantic cues, and employs a block-wise structured attention mechanism and a diffusion-based transformer for disentangling representations. DreamVLA achieves a 76.7% success rate on real robot tasks and 4.44 average length on the CALVIN ABC-D benchmarks, demonstrating state-of-the-art performance. The perception-prediction-action loop facilitates more effective planning and execution in robotic tasks. The structured attention mechanism prevents information leakage, ensuring coherent multi-step reasoning. |
| Natural Language Processing | Pre-Trained Policy Discriminators are General Reward Models (Read more on [arXiv](https://arxiv.org/abs/2507.05197) or [HuggingFace](https://huggingface.co/papers/2507.05197))| Yunhua Zhou, Yicheng Zou, Shichun Liu, Shihan Dou, Umean | This paper introduces POLAR, a scalable pre-training method for reward modeling formulated as a policy discriminator. The research aims to train a reward model that discerns between identical policies and discriminates different ones, capturing relative differences without relying on absolute preferences. POLAR pre-trains reward models (1.8B to 7B parameters) to discern identical policies and differentiate between distinct policies. Empirical results demonstrate that POLAR-7B improves preference accuracy from 54.8% to 81.0% on STEM tasks and from 57.9% to 85.5% on creative writing tasks. The approach enhances reward signal reliability for policy learning, potentially enabling the development of more general and robust reward models. |
| Multi-Modal | BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning
  Dataset (Read more on [arXiv](https://arxiv.org/abs/2507.03483) or [HuggingFace](https://huggingface.co/papers/2507.03483))| Yufang Liu, Honglin Guo, Yutao Fan, Guanyu Li, Zhiheng Xi | The paper introduces BMMR, a large-scale bilingual, multimodal, multi-disciplinary dataset for evaluating large multimodal models (LMMs). BMMR aims to develop resources for assessing knowledge and reasoning across diverse disciplines. The authors collected and curated 110k college-level questions spanning 300 UNESCO-defined subjects and proposed BMMR-Verifier for evaluating reasoning paths. Experiments on 24 models reveal that SOTA models leave substantial headroom, exhibiting discipline bias and open-source models trail proprietary counterparts, though fine-tuning on BMMR-Train improves performance by 19.07%. BMMR provides a valuable resource for advancing multidisciplinary reasoning capabilities in LMMs, facilitating more robust and generalizable AI systems. |
| Multi-Modal | RoboBrain 2.0 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2507.02029) or [HuggingFace](https://huggingface.co/papers/2507.02029))| Zhoues, Caozhou1995, MinglanLin, yuheng2000, cmyopu | RoboBrain 2.0 is a new generation of embodied vision-language foundation models designed to unify perception, reasoning, and planning for complex embodied tasks. The research aims to address limitations in spatial understanding, temporal modeling, and reasoning chains in current models. RoboBrain 2.0 employs a lightweight vision encoder and a 7B/32B language model, trained through a three-stage curriculum and chain-of-thought reasoning. The 32B variant achieves state-of-the-art results on spatial and temporal reasoning benchmarks, outperforming open-source and proprietary models, with results in part detailed in Table 2. RoboBrain 2.0 advances embodied AI research, serving as a practical step toward building generalist embodied agents. |
| Natural Language Processing | Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM
  Fine-Tuning Data from Unstructured Documents (Read more on [arXiv](https://arxiv.org/abs/2507.04009) or [HuggingFace](https://huggingface.co/papers/2507.04009))| Jingyuan Wang, Qiyu Sun, Ziyang Miao, hiyouga, oGYCo | This paper introduces Easy Dataset, a unified framework for synthesizing LLM fine-tuning data from unstructured documents via an intuitive graphical user interface. The research aims to address the challenge of extracting reliable fine-tuning data from heterogeneous documents effectively. Easy Dataset integrates adaptive document processing and persona-driven data synthesis, allowing users to configure text extraction models and chunking strategies, then leverages persona-driven prompting with public-available LLMs to generate question-answer pairs. Experiments on a financial question-answering task show that fine-tuning LLMs on the synthesized dataset significantly improves domain-specific performance while preserving general knowledge, achieving a score of 59.6 on a domain-specific evaluation dataset using persona-driven data synthesis. The implication for AI practitioners is a reduced manual effort in curating high-quality domain-specific datasets for LLM fine-tuning. |
| Natural Language Processing | RefineX: Learning to Refine Pre-training Data at Scale from
  Expert-Guided Programs (Read more on [arXiv](https://arxiv.org/abs/2507.03253) or [HuggingFace](https://huggingface.co/papers/2507.03253))| Dayiheng Liu, Xingzhang Ren, Shenghua Liu, Baolong Bi, Chevalier | The paper introduces REFINEX, a framework for large-scale, surgical refinement of pre-training data. It addresses the challenge of enhancing data quality at scale by distilling expert-guided end-to-end refinement results into minimal edit-based deletion programs. REFINEX trains an efficient refine model to improve every instance in the corpus, balancing refinement effectiveness and processing efficiency. Experiments on multiple model scales demonstrate that REFINEX yields gains of 2.6%-7.2% on lighteval tasks for a 750M model and achieves comparable performance with fewer tokens. REFINEX offers a scalable and reliable solution for optimizing pre-training data by improving text quality efficiently and precisely. |
| Computer Vision | Reviving Cultural Heritage: A Novel Approach for Comprehensive
  Historical Document Restoration (Read more on [arXiv](https://arxiv.org/abs/2507.05108) or [HuggingFace](https://huggingface.co/papers/2507.05108))| Yongxin Shi, Pengyu Yan, Zhenhua Yang, Peirong Zhang, Yuyi Zhang | The paper introduces AutoHDR, a novel automated solution for comprehensive historical document restoration. It addresses the limitations of existing methods by proposing a full-page HDR pipeline leveraging OCR-assisted damage localization, vision-language context prediction, and patch autoregressive appearance restoration. The key methodology involves a three-stage approach that mimics expert historian workflows. AutoHDR achieves a significant improvement in OCR accuracy, increasing from 46.83% to 84.05% on severely damaged documents, with further enhancements possible through human-machine collaboration. This work provides AI practitioners with an effective method for automated historical document restoration, offering a pathway for enhanced cultural heritage preservation. |
| Computer Vision | StreamDiT: Real-Time Streaming Text-to-Video Generation (Read more on [arXiv](https://arxiv.org/abs/2507.03745) or [HuggingFace](https://huggingface.co/papers/2507.03745))| Yue Zhao, Masayoshi Tomizuka, Ji Hou, Tingbo Hou, AkiCumulo | The paper introduces StreamDiT, a real-time streaming text-to-video (T2V) generation model. It addresses the limitations of existing offline T2V models by proposing a flow matching-based training approach with a moving buffer and mixed partitioning schemes to enhance content consistency and visual quality. StreamDiT models are based on adaLN DiT with varying time embedding and window attention, and a multistep distillation method is proposed. The distilled model achieves real-time performance at 16 FPS on a single GPU, generating 512p resolution video streams. This enables real-time applications such as streaming and interactive video generation. |
| Multi-Modal | ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code
  Generation Evaluation (Read more on [arXiv](https://arxiv.org/abs/2507.04952) or [HuggingFace](https://huggingface.co/papers/2507.04952))| Ao Liu, Jiaheng Liu, Can Xu, Yuhang Li, Chenchen Zhang | The paper introduces ArtifactsBench, a new benchmark for evaluating Large Language Models (LLMs) in generating interactive visual artifacts. It addresses the evaluation gap between algorithmic correctness and visual fidelity by programmatically rendering generated artifacts and using a Multimodal LLM (MLLM)-as-Judge guided by fine-grained checklists.  The primary objective is to automate the assessment of LLMs' ability to create high-quality interactive visual artifacts from multimodal instructions. The methodology involves capturing temporal screenshots of rendered artifacts and evaluating them using a checklist-guided MLLM. ArtifactsBench achieves 94.4% ranking consistency with WebDev Arena, indicating high agreement with human preferences; analysis of over 30 LLMs reveals generalist models outperform domain-specific ones in this task, implying need for holistic capabilities in modern visual applications. |
| Multi-Modal | VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and
  Visual Documents (Read more on [arXiv](https://arxiv.org/abs/2507.04590) or [HuggingFace](https://huggingface.co/papers/2507.04590))| Xinyi Yang, Mingyi Su, Ye Liu, Rui Meng, ziyjiang | This paper introduces VLM2Vec-V2, a unified multimodal embedding framework for videos, images, and visual documents, addressing the limited support for non-natural image visual forms in existing models. The main objective is to learn unified embeddings across diverse visual modalities. VLM2Vec-V2 trains a general-purpose embedding model using a mixture of instruction-following tasks and diverse input formats. Experiments on the newly introduced MMEB-V2 benchmark, show that VLM2Vec-V2 achieves an overall average score of 58.0, improving over prior baselines like GME and VLM2Vec. VLM2Vec-V2 lays the groundwork for more scalable and adaptable representation learning for tasks involving various visual modalities. |
| Natural Language Processing | VLAI: A RoBERTa-Based Model for Automated Vulnerability Severity
  Classification (Read more on [arXiv](https://arxiv.org/abs/2507.03607) or [HuggingFace](https://huggingface.co/papers/2507.03607))| adulau, cedricbonhomme | The paper introduces VLAI, a RoBERTa-based model for automated software vulnerability severity classification. The research aims to predict vulnerability severity levels directly from textual descriptions, enabling faster triage. VLAI is fine-tuned on a dataset of over 600,000 real-world vulnerabilities and achieves over 82% accuracy in predicting severity categories. The model and dataset are open-source and integrated into a vulnerability lookup service. This allows security analysts to obtain immediate severity estimates and facilitates quicker responses to new threats. |
| Multi-Modal | PresentAgent: Multimodal Agent for Presentation Video Generation (Read more on [arXiv](https://arxiv.org/abs/2507.04036) or [HuggingFace](https://huggingface.co/papers/2507.04036))| Meng Fang, Yanjie Liang, Biao Wu, Jingwei Shi, SteveZeyuZhang | PresentAgent is a multimodal agent designed to generate narrated presentation videos from long-form documents. The research aims to bridge the gap in structured communication by automatically converting documents into synchronized slides and speech. The method employs a modular pipeline that segments documents, renders slide-style frames, generates spoken narration using LLMs and TTS, and composes the video with precise audio-visual alignment. Evaluated on a curated dataset, PresentAgent achieves human-level quality across metrics, including 0.64 quiz accuracy using Claude-3.7-sonnet. The approach offers AI practitioners a controllable and interpretable method for transforming static materials into dynamic presentation formats. |
| Computer Vision | Beyond Simple Edits: X-Planner for Complex Instruction-Based Image
  Editing (Read more on [arXiv](https://arxiv.org/abs/2507.05259) or [HuggingFace](https://huggingface.co/papers/2507.05259))| Yuheng Li, Richard Zhang, Nanxuan Zhao, Yilin Wang, danielchyeh | The paper introduces X-Planner, a multimodal large language model (MLLM) designed to improve complex instruction-based image editing by decomposing instructions into simpler sub-instructions. The research aims to bridge the gap between user intent and editing model capabilities, addressing issues like indirect instructions and identity preservation. X-Planner employs chain-of-thought reasoning, auto-generates precise edit types and segmentation masks, and is trained using a novel automated pipeline for generating a large-scale dataset (COMPIE) comprising over 260K paired complex-simple instructions. Results on the MagicBrush benchmark indicate that X-Planner enhances UltraEdit, achieving a DINO similarity of 0.8982, and its mask often matches or even outperforms human labeled masks. This has a major implication for the practitioners, as it automates instruction decomposition and control guidance generation which reduces manual effort and improves localized edits and identity preservation. |
| Machine Learning | Evaluating LLMs on Real-World Forecasting Against Human Superforecasters (Read more on [arXiv](https://arxiv.org/abs/2507.04562) or [HuggingFace](https://huggingface.co/papers/2507.04562))| Janna Lu | This paper evaluates the forecasting capabilities of Large Language Models (LLMs) against human superforecasters. It investigates the ability of state-of-the-art LLMs to predict real-world events by comparing their Brier scores to those of human experts on a Metaculus forecasting tournament dataset. The primary result indicates that while frontier models achieve Brier scores that ostensibly surpass the average human crowd (e.g., model "o3" achieves 0.1352), they still significantly underperform a group of superforecasters. The main implication is that despite advancements, current LLMs still lack key components of expert judgment in forecasting uncertain future events. |
| Other | MOD-X: A Modular Open Decentralized eXchange Framework proposal for
  Heterogeneous Interoperable Artificial Agents (Read more on [arXiv](https://arxiv.org/abs/2507.04376) or [HuggingFace](https://huggingface.co/papers/2507.04376))| Aaron Elkins, Vinija Jain, Christos Constantinou, Georgios Ioannides, amanchadha | This paper proposes MOD-X, a modular framework for interoperable AI agents, addressing the limitations of existing communication protocols. The main objective is to create a decentralized exchange for heterogeneous specialist AI agents capable of complex coordination without central control. MOD-X utilizes a layered architecture with a Universal Message Bus, translation layer, state management, and blockchain-based security to facilitate agent communication and capability discovery through ontological self-representation and multi-modal matching. The study outlines the architecture and compares it with existing protocols but lacks quantitative results from implementation. The proposed framework aims to enable the development of scalable, decentralized agent ecosystems that can effectively integrate diverse AI technologies. |
| Natural Language Processing | Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs
  More Realistic and Less Risky (Read more on [arXiv](https://arxiv.org/abs/2507.03336) or [HuggingFace](https://huggingface.co/papers/2507.03336))| Sebastian Schreiber, Julien Yu, ashutosh1919 | The paper addresses the challenge of enterprise tool-calling LLMs failing in scenarios with near-duplicate tools or underspecified arguments. It introduces DIAFORGE, a disambiguation-centric pipeline for synthetic data generation, fine-tuning, and dynamic evaluation. The methodology includes synthesizing multi-turn dialogues, fine-tuning LLMs (3B-70B parameters) with reasoning traces, and dynamically evaluating real-world readiness. Results on the DIABENCH benchmark show a tool-invocation success increase of 27 pp over GPT-4o and 49 pp over Claude-3.5-Sonnet under optimized prompting, indicating a practical blueprint for reliable enterprise tool-calling agents. |
| Computer Vision | SeqTex: Generate Mesh Textures in Video Sequence (Read more on [arXiv](https://arxiv.org/abs/2507.04285) or [HuggingFace](https://huggingface.co/papers/2507.04285))| Yan-Pei Cao, Yuan-Chen Guo, Yangtian Sun, Xin Yu, Ze Yuan | SeqTex introduces an end-to-end framework for generating high-fidelity 3D mesh textures by leveraging pre-trained video foundation models. The paper addresses the challenge of limited 3D texture datasets by reformulating texture generation as a sequence-to-sequence problem, jointly optimizing multi-view synthesis and UV texture generation. SeqTex employs a decoupled multi-view and UV branch design, geometry-informed attention, and adaptive token resolution. Experiments demonstrate state-of-the-art performance, achieving an FID of 30.27 on image-conditioned texture generation. This approach enables AI practitioners to leverage video priors for enhanced 3D content creation. |
| Machine Learning | OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device
  Speculative Decoding (Read more on [arXiv](https://arxiv.org/abs/2507.02659) or [HuggingFace](https://huggingface.co/papers/2507.02659))| Yicheng Lin, Chen Feng, Shaojie Zhuo, Ramchalam Kinattinkara Ramakrishnan, justinyyy | The paper introduces OmniDraft, a framework for on-device speculative decoding that uses a universal drafter adaptable to various target models. It addresses cross-vocabulary mismatches via an online n-gram cache and improves decoding speed through adaptive drafting. The research aims to improve the flexibility and efficiency of speculative decoding for on-device LLMs. OmniDraft employs hybrid distillation fine-tuning to align draft and target model token distributions and dynamically adjusts the draft length. Experiments show a Llama-68M model paired with various target models achieves up to 1.5-2x speedup on reasoning, coding, and text generation tasks, indicating a more efficient and customizable on-device LLM deployment strategy. |
