

## Papers for 2025-07-10

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data (Read more on [arXiv](https://arxiv.org/abs/2507.07095) or [HuggingFace](https://huggingface.co/papers/2507.07095))| Lixing Xiao, Runyi Yu, Shunlin Lu, Ke Fan, Jixi111 | The paper introduces Go to Zero, a new approach towards zero-shot human motion generation from text. It investigates how scaling data and model size can improve generalization in text-to-motion synthesis, addressing the limitations of existing datasets. They propose MotionMillion, a dataset of over 2,000 hours of motion data, and a 7B parameter model trained on it. Evaluated on MotionMillion-Eval, the model achieves strong generalization to out-of-domain motions, showing an improved text-motion alignment of 261 (Likert scale). This advancement demonstrates the potential of large-scale datasets to enable zero-shot human motion generation. |
| Multi-Modal | Perception-Aware Policy Optimization for Multimodal Reasoning (Read more on [arXiv](https://arxiv.org/abs/2507.06448) or [HuggingFace](https://huggingface.co/papers/2507.06448))| Hongru Wang, Sofia Stoica, Xuehang Guo, Zhenhailong Wang, xhyandwyy | This paper introduces Perception-Aware Policy Optimization (PAPO) to enhance multimodal reasoning in Large Language Models (LLMs). It addresses the issue of suboptimal performance in multimodal reasoning tasks due to limitations in perceiving visual inputs. The key methodology involves incorporating an Implicit Perception Loss term into the Group Relative Policy Optimization (GRPO) objective to encourage visually grounded reasoning. Results show a 4.4% overall improvement across diverse multimodal benchmarks, with an 8.0% gain on tasks with high vision dependency; they also observed a 30.5% reduction in perception errors. PAPO provides a means for AI practitioners to better integrate perception-aware supervision into RLVR learning objectives without additional data curation or reward models. |
| Computer Vision | 4KAgent: Agentic Any Image to 4K Super-Resolution (Read more on [arXiv](https://arxiv.org/abs/2507.07105) or [HuggingFace](https://huggingface.co/papers/2507.07105))| Xinrui Jiang, Mingyang Wu, Qi Zheng, vztu, YSZuo | The paper presents 4KAgent, an agentic image super-resolution generalist capable of upscaling any image to 4K resolution. It addresses the challenge of restoring diverse imagery with varying degradation levels and domains. 4KAgent leverages a Perception Agent with VLMs to analyze images and create restoration plans, which are then executed by a Restoration Agent using a quality-driven mixture-of-experts policy. Experiments demonstrate state-of-the-art performance across 26 diverse benchmarks, with the agent achieving superior perceptual quality (e.g., NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. 4KAgent offers AI practitioners a flexible and adaptable framework for image super-resolution across diverse applications without retraining. |
| Natural Language Processing | Rethinking Verification for LLM Code Generation: From Generation to
  Testing (Read more on [arXiv](https://arxiv.org/abs/2507.06920) or [HuggingFace](https://huggingface.co/papers/2507.06920))| Minnan Luo, Wenwei Zhang, Maosong Cao, Taolin Zhang, MichaelErchi | This paper addresses the limitations of current LLM code generation evaluation benchmarks, which often overlook subtle faults due to homogeneous test cases. The research investigates the task of test-case generation (TCG) by proposing multi-dimensional metrics to quantify test-suite thoroughness and introduces SAGA, a human-LLM collaborative method to enhance coverage and quality. Experiments on TCG-Bench show that SAGA achieves a detection rate of 90.62% and a verifier accuracy of 32.58%. The findings facilitate scalable and reliable LLM code evaluation, advancing reinforcement learning with verifiable rewards. |
| Reinforcement Learning | First Return, Entropy-Eliciting Explore (Read more on [arXiv](https://arxiv.org/abs/2507.07017) or [HuggingFace](https://huggingface.co/papers/2507.07017))| Xingwei Qu, Taoran Liang, Qingshui Gu, xtsssss, aaabiao | This paper introduces First Return, Entropy-Eliciting Explore (FR3E), a structured exploration framework to improve reasoning in Large Language Models (LLMs) trained with Reinforcement Learning from Verifiable Rewards (RLVR). FR3E identifies high-uncertainty decision points in reasoning trajectories and performs targeted rollouts to construct semantically grounded intermediate feedback, aiming to stabilize exploration and improve reasoning. The methodology involves using entropy to locate critical points and synthesizing localized feedback through partial rollouts. Experimental results on the AIME24 benchmark show that FR3E promotes more stable training and increases the proportion of fully correct trajectories, achieving a 40.2% accuracy compared to 34.1% for GRPO++ on the Qwen2.5-32B model. The framework offers a more robust and structured exploration approach for enhancing LLM reasoning without dense supervision. |
| Natural Language Processing | A Systematic Analysis of Hybrid Linear Attention (Read more on [arXiv](https://arxiv.org/abs/2507.06457) or [HuggingFace](https://huggingface.co/papers/2507.06457))| Taylor Kergan, Yong Shan, Steven Abreu, Dustin Wang, ridger | This paper presents a systematic analysis of hybrid linear attention models, which aim to reduce the computational complexity of Transformers. The primary objective is to evaluate various linear attention models within hybrid architectures, focusing on language modeling and recall performance. The methodology involves training and benchmarking 72 models with different linear attention variants and hybridization ratios on standard language modeling and recall tasks. The study finds that superior standalone linear models do not necessarily excel in hybrids, and recall significantly improves with increased full attention layers, particularly below a 3:1 ratio. The findings suggest that selective gating, hierarchical recurrence, and controlled forgetting are critical for effective hybrid models, enabling Transformer-level recall with reduced KV-cache memory. |
| Reinforcement Learning | AutoTriton: Automatic Triton Programming with Reinforcement Learning in
  LLMs (Read more on [arXiv](https://arxiv.org/abs/2507.05687) or [HuggingFace](https://huggingface.co/papers/2507.05687))| Yuxuan Li, Ye He, Zefan Wang, Shangzhan Li, qshi | This paper introduces AutoTriton, the first reinforcement learning (RL) model dedicated to Triton programming. The research aims to automate the generation of high-performance GPU kernels, traditionally a manual and iterative process. AutoTriton employs supervised fine-tuning (SFT) for initial Triton expertise and then uses RL with Group Relative Policy Optimization (GRPO) for performance enhancement, combining rule-based and execution-based rewards. Experiments on TRITONBENCH and KERNELBENCH show that an 8B parameter AutoTriton model achieves performance comparable to large models like Claude-4-Sonnet, with key metrics including execution accuracy and speedup improvements. The work provides a foundation for building more efficient AI systems by automating the creation of high-performance kernels. |
| Machine Learning | Towards Solving More Challenging IMO Problems via Decoupled Reasoning
  and Proving (Read more on [arXiv](https://arxiv.org/abs/2507.06804) or [HuggingFace](https://huggingface.co/papers/2507.06804))| Feng Zhang, Tao Yang, Yang Li, Linfeng Song, Zhenwen Liang | This paper introduces a novel framework for automated theorem proving (ATP) by decoupling high-level reasoning from low-level proof generation to address the limitations of end-to-end RLVR training. The research aims to bridge the gap between informal reasoning and formal proving by using a general-purpose LLM as a Reasoner for subgoal generation and a specialized ATP model as a Prover. The methodology involves generating strategic lemmas, verifying them, and then constructing the final proof using verified lemmas. The decoupled framework successfully solves 5 challenging post-2000 IMO problems, demonstrating a significant improvement over existing open-source provers. This approach suggests that decoupling reasoning and proving can enhance ATP systems' ability to tackle complex mathematical challenges. |
| Multi-Modal | A Survey on Vision-Language-Action Models for Autonomous Driving (Read more on [arXiv](https://arxiv.org/abs/2506.24044) or [HuggingFace](https://huggingface.co/papers/2506.24044))| Tianze Zhu, Ziang Luo, Kangan Qian, Zilin Huang, Max2045 | This survey paper provides a comprehensive overview of Vision-Language-Action (VLA) models for autonomous driving, termed VLA4AD. It formalizes architectural building blocks, traces the evolution of VLA models, and compares representative models in the autonomous driving domain. The study consolidates datasets and benchmarks, highlighting protocols that measure driving safety, accuracy, and explanation quality. Challenges regarding robustness, real-time efficiency, and formal verification are detailed, outlining future research directions. The survey aims to advance interpretable and socially aligned autonomous vehicles by providing a complete reference of VLA4AD. |
| Machine Learning | DiffSpectra: Molecular Structure Elucidation from Spectra using
  Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2507.06853) or [HuggingFace](https://huggingface.co/papers/2507.06853))| Zhiyuan Liu, Zhenyi Zhong, Tingyang Xu, Yu Rong, AzureLeon1 | The paper introduces DiffSpectra, a novel framework for molecular structure elucidation from multi-modal spectra using diffusion models. It aims to address the limitations of existing methods by directly inferring both 2D and 3D molecular structures from spectral data. The framework employs a diffusion model with a denoising network parameterized by a Diffusion Molecule Transformer and conditioned on spectral embeddings from a transformer-based spectral encoder called SpecFormer. Experimental results demonstrate that DiffSpectra achieves high accuracy in structure elucidation, recovering exact structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through sampling. This unified generative modeling approach enables more accurate de novo molecular structure elucidation from diverse spectral modalities, potentially enhancing compound identification and drug discovery workflows for AI practitioners. |
| Natural Language Processing | ModelCitizens: Representing Community Voices in Online Safety (Read more on [arXiv](https://arxiv.org/abs/2507.05455) or [HuggingFace](https://huggingface.co/papers/2507.05455))| Karolina Naranjo, notaphonologist, hamidpalangi, christinachance, Ashima | The paper introduces ModelCitizens, a novel dataset for toxicity detection that captures community-specific perspectives. It addresses the limitations of existing datasets by incorporating diverse annotations across identity groups and conversational contexts. The study uses LLM-generated conversational scenarios and ingroup/outgroup annotations, finding that state-of-the-art models underperform on the dataset, particularly on context-augmented posts. Fine-tuning LLaMA and Gemma models on ModelCitizens improves performance, with LLAMACITIZEN-8B outperforming GPT-04-mini by 5.5% in in-distribution evaluations. The work implies that inclusive content moderation requires community-informed annotation and modeling. |
| Natural Language Processing | Evaluating the Critical Risks of Amazon's Nova Premier under the
  Frontier Model Safety Framework (Read more on [arXiv](https://arxiv.org/abs/2507.06260) or [HuggingFace](https://huggingface.co/papers/2507.06260))| Vincent Ponzo, Matteo Memelli, Abhinav Mohanty, Ninareh Mehrabi, Satyapriya Krishna | This paper presents a comprehensive evaluation of Amazon's Nova Premier, a multimodal foundation model, under the Frontier Model Safety Framework. It investigates the critical risks associated with the model's capabilities in CBRN proliferation, offensive cyber operations, and automated AI R&D. The methodology combines automated benchmarks, expert red-teaming, and uplift studies to assess whether the model exceeds predefined safety thresholds. The evaluation shows improved factual accuracy on CBRN and cybersecurity knowledge tests, with automated results showing the model remains within safety thresholds for end-to-end workflows, capture-the-flag exercises, and autonomous machine-learning research; for example, Nova Premier attains 0.48 on ProtocolQA-MCQ, versus Nova Pro's 0.34. The paper demonstrates a template for safety audits in frontier models. |
| Natural Language Processing | AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large
  Language Models on Harmfulness (Read more on [arXiv](https://arxiv.org/abs/2507.01702) or [HuggingFace](https://huggingface.co/papers/2507.01702))| Zhen Ye, Ziyang Luo, Kaixin Li, Hongzhan Lin, Zixin Chen | The paper introduces AdamMeme, a novel framework to adaptively evaluate the reasoning capacity of multimodal large language models (mLLMs) on harmful memes. It addresses the limitations of static, accuracy-based benchmarks by dynamically generating challenging meme samples to expose specific weaknesses in mLLMs' interpretation of harmfulness. The framework employs a multi-agent system for harmfulness mining, model scoring, and iterative refinement. Experiments demonstrate that AdamMeme systematically reveals varying performances of different target mLLMs and provides fine-grained analysis of model-specific vulnerabilities, achieving an average inter-annotator agreement of 0.767 on harmfulness categories. AdamMeme provides AI practitioners with a more comprehensive and up-to-date evaluation method that accounts for the dynamic nature of online memes. |
