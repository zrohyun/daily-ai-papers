

## Papers for 2025-07-01

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Ovis-U1 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2506.23044) or [HuggingFace](https://huggingface.co/papers/2506.23044))| Pengxin Zhan, Liangfu Cao, Xinjie Zhang, Shanshan Zhao, Flourish | The Ovis-U1 technical report introduces a 3-billion-parameter unified model for multimodal understanding, text-to-image generation, and image editing. The research aims to integrate these capabilities into a single model using a diffusion-based visual decoder and bidirectional token refiner. Ovis-U1 is trained using a unified approach from a language model, demonstrating performance gains over separate training. It achieves a score of 69.6 on the OpenCompass Multi-modal Academic Benchmark. This advancement provides AI practitioners with a versatile model capable of handling diverse multimodal tasks. |
| Computer Vision | VMoBA: Mixture-of-Block Attention for Video Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2506.23858) or [HuggingFace](https://huggingface.co/papers/2506.23858))| Ye Tian, Xin Tao, Haotian Yang, Jianzong Wu, lianghou | This paper introduces VMoBA, a novel sparse attention mechanism tailored for video diffusion models to address the quadratic complexity of full attention. The research aims to enhance video generation quality and efficiency for long sequences by improving spatio-temporal attention capture. VMoBA incorporates layer-wise recurrent block partition, global block selection, and threshold-based block selection, achieving a 2.92x FLOPs reduction and a 1.48x training time speedup while maintaining comparable or improved generation quality. Experiments indicate VMoBA accelerates DiT training, offering competitive performance in training-free settings and providing a more efficient alternative for AI practitioners working with long-duration, high-resolution video generation. |
| Computer Vision | Calligrapher: Freestyle Text Image Customization (Read more on [arXiv](https://arxiv.org/abs/2506.24123) or [HuggingFace](https://huggingface.co/papers/2506.24123))| Ka Leong Cheng, Hao Ouyang, Qingyan Bai, Yue Ma, JingyeChen22 | The paper introduces Calligrapher, a novel diffusion-based framework for freestyle text image customization. The research aims to automate high-quality typography generation with precise style control from reference images. The method employs a self-distillation mechanism to create a style-centric typography benchmark and a localized style injection framework via a trainable style encoder. Results demonstrate superior performance, achieving a FID score of 38.09, indicating high image quality and style similarity. This automated approach empowers creative practitioners in digital art and branding by reducing manual labor while maintaining artistic integrity. |
| Computer Vision | Listener-Rewarded Thinking in VLMs for Image Preferences (Read more on [arXiv](https://arxiv.org/abs/2506.22832) or [HuggingFace](https://huggingface.co/papers/2506.22832))| Anton Gusarov, Andrey Galichin, Li Pengyi, barracuda049, alexgambashidze | This paper introduces a listener-augmented reinforcement learning framework to improve the alignment of vision-language models (VLMs) with human visual preferences. The research aims to address the problem of reasoning accuracy drops when a model's reasoning trace contradicts an independent VLM listener. The proposed method incorporates a listener model to provide a dense, calibrated confidence score, shaping the RL reward signal to encourage persuasive explanations. The approach achieves state-of-the-art accuracy on the ImageReward benchmark (67.4%) and improves out-of-distribution performance. Listener-based rewards offer a scalable path to aligning VLMs with nuanced human preferences by addressing misalignment between explanation plausibility and decision correctness. |
| Reinforcement Learning | SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via
  Multi-Agent Multi-Turn Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2506.24119) or [HuggingFace](https://huggingface.co/papers/2506.24119))| Penghui Qi, Leon Guertler, lkevinzc, simonycl, Benjamin-eecs | The paper introduces SPIRAL, a self-play framework using zero-sum games to incentivize reasoning in language models. The objective is to develop transferable reasoning capabilities without human supervision by having models play against continuously improving versions of themselves. SPIRAL implements a multi-agent reinforcement learning system with role-conditioned advantage estimation (RAE) to stabilize training. Results show an 8.6% improvement in math and 8.4% in general reasoning after training Qwen3-4B-Base on Kuhn Poker. This implies that zero-sum game self-play is a promising approach for autonomous reasoning development. |
| Machine Learning | Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective (Read more on [arXiv](https://arxiv.org/abs/2506.17930) or [HuggingFace](https://huggingface.co/papers/2506.17930))| LidongBing, Zhiqiang007, Jianyu | This paper introduces PROMPTQUINE, a novel prompt design paradigm for LLMs that challenges the conventional emphasis on carefully crafted instructions. It explores whether pruning random demonstrations into incoherent sequences, termed “gibberish”, can improve performance across tasks. The authors propose an evolutionary search framework to automatically discover effective pruning strategies using low-data regimes. Experiments across classification, question answering, and other tasks demonstrate that pruned prompts achieve comparable or superior performance to state-of-the-art automatic prompt optimization techniques. The study reveals that seemingly nonsensical prompts can enhance LLM performance regardless of alignment and that this warrants further exploration of in-context learning mechanisms. |
| Computer Vision | Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric
  Attention (Read more on [arXiv](https://arxiv.org/abs/2506.23542) or [HuggingFace](https://huggingface.co/papers/2506.23542))| Di Qiu, Jin Zeng, Changyong He, weidawang | This paper introduces a novel Time-of-Flight (ToF) depth denoising network leveraging graph-informed geometric attention. The objective is to enhance both temporal consistency and spatial sharpness in denoised ToF data. The key methodology involves motion-invariant graph fusion for cross-frame geometric attention and MAP problem formulation for ToF denoising, unrolled into iterative filters. Experimental results on the synthetic DVToF dataset show at least a 37.9% improvement in MAE compared to existing methods. The GIGA-ToF network offers AI practitioners an interpretable, robust approach to ToF depth denoising with enhanced generalization capabilities. |
| Multi-Modal | Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in
  Inference-time Scaling? (Read more on [arXiv](https://arxiv.org/abs/2506.17417) or [HuggingFace](https://huggingface.co/papers/2506.17417))| Kaizhuo Yan, Jize Jiang, Jingcheng Yang, Meitang Li, Mingyuan1997 | This paper investigates the self-verification capabilities of Vision-Language Models (VLMs) within an inference-time scaling framework. It examines whether inference-time techniques effectively extend to VLMs, particularly those trained with reinforcement learning (RL). The methodology contrasts decoding strategies such as majority voting and self-verified best-of-N sampling, evaluating performance using reasoning accuracy on GeoQA and MathVista datasets, observing that generation-heavy strategies outperform verification-reliant ones. Results indicate that "aha moments" do not reliably improve VLM reasoning, and accuracy improvements are observed when VLMs verify text without image input, suggesting poor visual context integration. The primary implication for AI practitioners is that current RL techniques do not endow VLMs with robust multimodal self-verification, thereby limiting the efficacy of inference-time scaling. |
| Computer Vision | MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame
  Optical Flow Estimation (Read more on [arXiv](https://arxiv.org/abs/2506.23151) or [HuggingFace](https://huggingface.co/papers/2506.23151))| Dmitriy Vatolin, Egor Chistov, Vladislav Bargatin, a-yakovenko | The paper introduces MEMFOF, a memory-efficient multi-frame optical flow estimation method for high-resolution videos. It aims to reduce the GPU memory footprint of multi-frame optical flow estimation, enabling training at native 1080p resolution. The method integrates reduced correlation volumes, high-resolution training protocols, and multi-frame estimation within a RAFT-like architecture. MEMFOF achieves state-of-the-art performance on multiple benchmarks, including a 1-pixel outlier rate of 3.289 on the Spring benchmark. This allows AI practitioners to train and deploy accurate optical flow models on high-resolution videos with limited GPU resources. |
| Machine Learning | SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity (Read more on [arXiv](https://arxiv.org/abs/2506.16500) or [HuggingFace](https://huggingface.co/papers/2506.16500))| Ligeng Zhu, Junxian Guo, Xiuyu Li, zhijianliu, Skhaki | SparseLoRA accelerates LLM fine-tuning by leveraging contextual sparsity. The research aims to reduce the computational cost of fine-tuning large language models. It introduces a lightweight, training-free SVD sparsity estimator to dynamically select sparse weights for computation. Experiments show up to a 1.6x speedup while maintaining accuracy across diverse downstream tasks. This approach provides AI practitioners with a more efficient method for adapting LLMs to specific tasks without significant performance degradation. |
| Multi-Modal | MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning (Read more on [arXiv](https://arxiv.org/abs/2506.22992) or [HuggingFace](https://huggingface.co/papers/2506.22992))| Maria Brbić, Yekun Chai, mdmoor, yljblues | The paper introduces MARBLE, a new benchmark to evaluate multimodal reasoning and planning capabilities of MLLMs in spatially and physically constrained environments. MARBLE consists of two challenging tasks: M-PORTAL, which requires multistep planning under spatial constraints, and M-CUBE, which involves assembling a 3D cube from jigsaw pieces.  The benchmark aims to address limitations in existing benchmarks by emphasizing step-by-step reasoning. Evaluation of 12 advanced models on MARBLE demonstrates poor performance, with models achieving near-random results on M-PORTAL and 0% accuracy on M-CUBE, highlighting the limitations of current MLLMs.  This work suggests the need for enhanced models with the capacity for deeper multimodal reasoning and planning capabilities. |
| Natural Language Processing | Teaching a Language Model to Speak the Language of Tools (Read more on [arXiv](https://arxiv.org/abs/2506.23394) or [HuggingFace](https://huggingface.co/papers/2506.23394))| s-emanuilov | This paper introduces TUCAN, a methodology for adapting language models to robust tool use in non-English languages, with Bulgarian as a case study. The research aims to enhance function-calling capabilities in a low-resource language while preserving core linguistic competence. It involves continued training of the BgGPT model series on a novel bilingual dataset of function-calling examples and introduces the TUCAN model. TUCAN achieves up to 28.75% improvement in function-calling accuracy while maintaining performance on Bulgarian benchmarks. This demonstrates a practical approach for extending tool-augmented capabilities beyond English-centric systems, offering a blueprint for other languages and AI practitioners seeking to improve localized tool use. |
| Multi-Modal | UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence
  with Spatial Reasoning and Understanding (Read more on [arXiv](https://arxiv.org/abs/2506.23219) or [HuggingFace](https://huggingface.co/papers/2506.23219))| Yong Li, Yanxin Xi, Tianhui Liu, Shengyuan Wang, JJ-TMT | The paper introduces UrbanLLaVA, a multi-modal large language model designed for urban intelligence that integrates spatial reasoning and understanding. The research aims to develop a unified framework for processing diverse urban data types simultaneously. UrbanLLaVA employs a multi-stage training framework decoupling spatial reasoning enhancement from domain knowledge learning and a diverse urban instruction dataset. Experimental results across three cities demonstrate that UrbanLLaVA outperforms open-source and proprietary MLLMs in single-modal and cross-modal urban tasks, achieving gains from 3.48% to 132.23% on an enhanced urban benchmark. The framework provides AI practitioners with a powerful tool for comprehensive urban environment analysis and diverse urban task solutions. |
| Natural Language Processing | VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.22694) or [HuggingFace](https://huggingface.co/papers/2506.22694))| Yifan Zao, Junyoung Park, Mukul Gagrani, Sudhanshu Agrawal, Raghavv Goel | This paper introduces VOCABTRIM, a training-free technique to enhance drafter-based speculative decoding (SpD) in large language models (LLMs). The study aims to reduce drafting overhead by pruning the vocabulary of the drafter LM head, focusing on frequently sampled tokens from the target model's vocabulary. VOCABTRIM reconstructs the drafter LM head to contain only a limited set of tokens selected by the most frequently sampled ones from the vocabulary of the target model. Experiments on Llama-3 models demonstrate a 16% improvement in memory-bound speed-up (MBSU) on Spec-Bench tasks, specifically for Llama-3.2-3B-Instruct. VOCABTRIM offers a practical method for improving generation speed in memory-constrained environments by optimizing drafter LM head without requiring retraining or architectural changes. |
| Computer Vision | RoboScape: Physics-informed Embodied World Model (Read more on [arXiv](https://arxiv.org/abs/2506.23135) or [HuggingFace](https://huggingface.co/papers/2506.23135))| Chen Gao, Lei Jin, Yinzhou Tang, Xin Zhang, Yu Shang | The paper introduces RoboScape, a physics-informed embodied world model for generating realistic robotic videos. It addresses the lack of physical awareness in current models by jointly learning RGB video generation with temporal depth prediction and adaptive keypoint dynamics learning. The methodology involves a multi-task learning framework with auxiliary physics-informed supervision to improve 3D geometric consistency and motion modeling. Experiments demonstrate that RoboScape achieves state-of-the-art performance, including an APSNR of 3.3435, indicating superior action controllability compared to baselines. The model's improved visual fidelity and physical plausibility offer potential for enhanced robotic policy training and evaluation, reducing reliance on real-world data. |
| Computer Vision | Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography (Read more on [arXiv](https://arxiv.org/abs/2506.22753) or [HuggingFace](https://huggingface.co/papers/2506.22753))| Xiaokang Yang, Feiyu Ji, Jiayi Zhu, Jianing Zhang, XiaoyunYuan | The paper introduces a degradation-modeled multipath diffusion framework (DMDiff) for enhancing image reconstruction in metalens photography. It addresses the challenges of complex optical degradation and limited training data by leveraging pre-trained diffusion models and a spatially varying degradation-aware attention (SVDA) module. The key methodology involves a multi-prompt training strategy (positive, neutral, negative) and a tunable decoder for balancing fidelity and perceptual quality. Experiments on a custom-built micro metalens-based camera (MetaCamera) demonstrate superior performance, achieving high-fidelity and sharp image reconstruction, outperforming state-of-the-art methods, with a MUSIQ score of 51.85. The proposed DMDiff framework offers AI practitioners a robust and adaptable solution for image restoration in ultra-compact computational imaging systems, reducing reliance on extensive paired datasets and precise optical calibration. |
| Multi-Modal | ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language
  Models for Audio Generation and Editing (Read more on [arXiv](https://arxiv.org/abs/2506.21448) or [HuggingFace](https://huggingface.co/papers/2506.21448))| Qian Chen, Wen Wang, Kaicheng Luo, Jialei Wang, Huadai Liu | ThinkSound is a novel framework for video-to-audio generation and editing using Chain-of-Thought (CoT) reasoning within Multimodal Large Language Models (MLLMs). The paper addresses the challenge of generating high-fidelity audio that captures nuanced visual content. The method decomposes the task into foundational foley generation, interactive object-centric refinement, and targeted editing, guided by MLLM-generated CoT instructions. Results demonstrate state-of-the-art performance, achieving a CLAPCoT score of 0.46 on the VGGSound dataset, indicating improved semantic alignment. This framework enables stepwise, interactive audio creation, offering improved user control for AI practitioners in audio synthesis. |
| Natural Language Processing | Tower+: Bridging Generality and Translation Specialization in
  Multilingual LLMs (Read more on [arXiv](https://arxiv.org/abs/2506.17080) or [HuggingFace](https://huggingface.co/papers/2506.17080))| Pedro Teixeirinha, João Alves, José Pombal, Nuno M. Guerreiro, RicardoRei | The paper introduces TOWER+, a suite of models designed to balance translation and general-purpose capabilities in multilingual LLMs. It aims to rival frontier models in general capabilities while optimizing for specific business domains like translation and localization. The methodology involves continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards, carefully curating data at each stage. The 72B model delivers best-in-class translation performance for high-resource languages and top results in multilingual Arena Hard evaluations and IF-MT, a new benchmark evaluating translation and instruction-following. The findings suggest it is possible to rival frontier models in general capabilities while optimizing for specific business domains. |
