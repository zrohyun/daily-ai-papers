

## Papers for 2025-07-29

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Reinforcement Learning | Agentic Reinforced Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2507.19849) or [HuggingFace](https://huggingface.co/papers/2507.19849))| Yifei Chen, Licheng Bao, Kai Ma, Hangyu Mao, Guanting Dong | The paper introduces Agentic Reinforced Policy Optimization (ARPO), an algorithm for training LLM-based agents in multi-turn environments. It aims to address the imbalance between intrinsic reasoning and multi-turn tool interaction. ARPO incorporates an entropy-based adaptive rollout mechanism, dynamically balancing trajectory and step-level sampling to promote exploration at high-uncertainty steps after tool usage. Experiments on 13 benchmarks show ARPO outperforms trajectory-level algorithms, achieving better performance with half the tool-use budget; for instance, ARPO achieves improved performance on deep search tasks. This offers a scalable solution for aligning LLM-based agents with dynamic environments, potentially reducing computational costs and improving real-time adaptability. |
| Multi-Modal | ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World
  Shorts (Read more on [arXiv](https://arxiv.org/abs/2507.20939) or [HuggingFace](https://huggingface.co/papers/2507.20939))| Junfu Pu, Teng Wang, Chen Li, Yixiao Ge, Yuying Ge | The paper introduces ARC-Hunyuan-Video-7B, a multimodal model for structured video comprehension of real-world short videos. It addresses the challenge of understanding complex short videos with high information density and fast pacing by integrating visual, audio, and textual information. The model leverages an automated annotation pipeline to train a compact 7B-parameter model using pre-training, instruction fine-tuning, cold start, and reinforcement learning. Quantitative evaluations on ShortVid-Bench demonstrate strong performance, achieving 74.3% accuracy, and it supports zero-shot or few-shot learning for downstream applications. The model's efficient real-world deployment has shown improvements in user engagement and satisfaction, with an inference time of 10 seconds for a one-minute video on H20 GPU. |
| Machine Learning | Rep-MTL: Unleashing the Power of Representation-level Task Saliency for
  Multi-Task Learning (Read more on [arXiv](https://arxiv.org/abs/2507.21049) or [HuggingFace](https://huggingface.co/papers/2507.21049))| Dan Xu, Lupin1998, ZedongWangAI | The paper introduces Rep-MTL, a novel representation-centric approach to multi-task learning that leverages task saliency to mitigate negative transfer and enhance inter-task complementarity. It addresses the limitation of existing multi-task optimization techniques that primarily focus on optimizer-centric loss scaling and gradient manipulation, by explicitly promoting task complementarity through saliency regularization in shared representation space. Rep-MTL comprises two modules: task-specific saliency regulation and cross-task saliency alignment. Experiments on NYUv2 demonstrate that Rep-MTL achieves significant performance gains, increasing task-level performance by 1.70 over existing approaches and maintains competitive performance on diverse MTL benchmarks even with the basic equal weighting. Rep-MTL's innovative utilization of representation space provides AI practitioners with an effective method that boosts multi-task dense prediction in complicated scenarios. |
| Natural Language Processing | SmallThinker: A Family of Efficient Large Language Models Natively
  Trained for Local Deployment (Read more on [arXiv](https://arxiv.org/abs/2507.20984) or [HuggingFace](https://huggingface.co/papers/2507.20984))| Dongliang Wei, Zhenliang Xue, qsstcl, Sorrymaker2024, yixinsong | SmallThinker is a family of LLMs designed for resource-constrained devices. The paper addresses the challenge of deploying LLMs on devices with weak computational power, limited memory, and slow storage. It introduces a two-level sparse architecture, a pre-attention router, and NoPE-ROPE hybrid sparse attention. Results show SmallThinker models exceed 20 tokens/s on consumer CPUs with Q4_0 quantization, using only 1GB and 8GB of memory. This allows for the deployment of capable AI on devices without cloud infrastructure. |
| Machine Learning | A Survey of Self-Evolving Agents: On Path to Artificial Super
  Intelligence (Read more on [arXiv](https://arxiv.org/abs/2507.21046) or [HuggingFace](https://huggingface.co/papers/2507.21046))| Jiayi Geng, Huan-ang Gao, XiangJinYu, didiforhugface, Alphamasterliu | This survey reviews self-evolving agents, a critical step towards Artificial Super Intelligence (ASI), focusing on adaptive AI systems. It examines architectures and methods enabling continual learning, organizing the field by 'what,' 'when,' and 'how' to evolve agent components. The paper analyzes algorithmic and architectural designs guiding adaptation, including reward signals and multi-agent systems. The study highlights applications in coding, education, and healthcare while identifying challenges in safety and scalability, providing a structured framework for developing versatile agentic systems. This comprehensive review sheds lights to pave the way for the realization of ASI. |
| Reinforcement Learning | Geometric-Mean Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2507.20673) or [HuggingFace](https://huggingface.co/papers/2507.20673))| Xun Wu, Jingye Chen, Yue Liu, Yuzhong Zhao, jeepliu | This paper introduces Geometric-Mean Policy Optimization (GMPO), a stabilized variant of Group Relative Policy Optimization (GRPO). The research aims to address the instability of GRPO during training, caused by outlier importance-weighted rewards. GMPO maximizes the geometric mean of token-level rewards, inherently less sensitive to outliers, thereby maintaining stable importance sampling ratios. Experiments on mathematical reasoning benchmarks demonstrate that GMPO outperforms GRPO by an average of 4.1% in Pass@1 accuracy using a 7B model. GMPO offers AI practitioners a more stable and potentially better performing reinforcement learning algorithm for large language model training. |
| Computer Vision | Region-based Cluster Discrimination for Visual Representation Learning (Read more on [arXiv](https://arxiv.org/abs/2507.20025) or [HuggingFace](https://huggingface.co/papers/2507.20025))| Yongle Zhao, Yin Xie, Athinklo, xiangan, Kaichengalex | The paper introduces Region-Aware Cluster Discrimination (RICE) for improved visual representation learning, specifically addressing limitations of global representations in tasks requiring dense predictions and OCR. RICE employs a Region Transformer layer to extract regional semantics from a large candidate region dataset and utilizes a unified region cluster discrimination loss for both object and OCR learning. Experiments demonstrate that RICE outperforms previous methods in segmentation, dense detection, and multimodal large language model tasks. For example, on OCRBench, RICE surpasses CLIP-336px by +50 points, suggesting its efficacy in region-based visual understanding. This offers AI practitioners a new approach to pre-training visual encoders capable of handling region-specific and OCR-related downstream tasks. |
| Computer Vision | GPT-IMAGE-EDIT-1.5M: A Million-Scale, GPT-Generated Image Dataset (Read more on [arXiv](https://arxiv.org/abs/2507.21033) or [HuggingFace](https://huggingface.co/papers/2507.21033))| Qing Liu, Letian Zhang, Siwei Yang, Yuhan Wang, tennant | The paper introduces GPT-IMAGE-EDIT-1.5M, a large-scale, publicly available image editing dataset generated using GPT-4o. The primary objective is to bridge the gap between proprietary and open-source instruction-guided image editing models by providing high-quality training data. The methodology involves regenerating and refining existing datasets (OmniEdit, HQ-Edit, and UltraEdit) using GPT-4o to enhance visual quality and instruction alignment. Fine-tuning FluxKontext on GPT-IMAGE-EDIT-1.5M achieves a GEdit-EN score of 7.24, outperforming prior open-source methods. The dataset facilitates the development of more capable open-source instruction-guided image editing models, reducing the reliance on proprietary systems. |
| Reinforcement Learning | UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing
  Large Language Models' Reasoning Abilities (Read more on [arXiv](https://arxiv.org/abs/2507.19766) or [HuggingFace](https://huggingface.co/papers/2507.19766))| Yang Li, Shaohua Chen, Tao Yang, forestliutc, dongdongdongdu | This paper introduces UloRL, an ultra-long output reinforcement learning approach to enhance the reasoning abilities of large language models. The research focuses on addressing inefficiencies in handling ultra-long outputs and preventing entropy collapse during training. UloRL employs segment rollout and dynamic masking of well-Mastered Positive Tokens (DMMPTs) to improve training speed and stability. Experiments on the Qwen3-30B-A3B model demonstrated an increase in AIME2025 performance from 70.9% to 85.1%. The method offers AI practitioners a means to effectively train LLMs for complex reasoning tasks requiring extended output sequences. |
| Computer Vision | ForCenNet: Foreground-Centric Network for Document Image Rectification (Read more on [arXiv](https://arxiv.org/abs/2507.19804) or [HuggingFace](https://huggingface.co/papers/2507.19804))| Jia Li, Dong Guo, Qiang Li, Peng Cai, Kaichengalex | This paper introduces ForCenNet, a novel network for document image rectification that prioritizes foreground elements. The research aims to improve geometric distortion correction by focusing on readable regions and layout information. ForCenNet employs a foreground-centric label generation method, a foreground-centric mask mechanism, and a curvature consistency loss. The method achieves state-of-the-art results on several benchmarks, including an MS-SSIM of 0.713 on DIR300, indicating enhanced structural similarity after rectification. ForCenNet offers AI practitioners a way to improve document image processing by explicitly modeling and leveraging foreground information, leading to better performance in downstream tasks like OCR. |
| Machine Learning | Met^2Net: A Decoupled Two-Stage Spatio-Temporal Forecasting Model for
  Complex Meteorological Systems (Read more on [arXiv](https://arxiv.org/abs/2507.17189) or [HuggingFace](https://huggingface.co/papers/2507.17189))| Xiaolin Qin, Min Chen, Hao Yang, Shaohan Li | The paper introduces Met²Net, a novel spatio-temporal forecasting model for complex meteorological systems. It aims to address representation inconsistency and task inconformity in multivariate weather forecasting. Met²Net employs a decoupled two-stage training approach with separate encoders and decoders for each variable and a self-attention mechanism for inter-variable fusion. Experiments demonstrate state-of-the-art performance, reducing MSE for near-surface air temperature and relative humidity predictions by 28.82% and 23.39%, respectively. Met²Net offers AI practitioners an improved framework for multivariate spatiotemporal forecasting by better handling variable dependencies and training alignment. |
| Computer Vision | ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with
  Concept Relation Alignment (Read more on [arXiv](https://arxiv.org/abs/2507.19058) or [HuggingFace](https://huggingface.co/papers/2507.19058))| Khodchaphun Hirunyaratsameewong, Chang Liu, Fangfu Liu, Shengjun Zhang, xiac24 | The paper introduces ScenePainter, a novel framework for semantically consistent perpetual 3D scene generation. It addresses the semantic drift issue in long-range 3D view sequences by aligning the outpainter's scene-specific prior with scene comprehension. The core methodology involves constructing a hierarchical SceneConceptGraph to capture multi-level scene concept relations and guide the outpainting process. Experiments demonstrate superior consistency and visual diversity compared to state-of-the-art methods, with the method achieving CLIP-I score of 0.931 in single image customization. This framework enables more reliable and controllable 3D scene generation for applications like video synthesis and scene reconstruction. |
| Multi-Modal | Music Arena: Live Evaluation for Text-to-Music (Read more on [arXiv](https://arxiv.org/abs/2507.20900) or [HuggingFace](https://huggingface.co/papers/2507.20900))| Wei-Lin Chiang, Anastasios N. Angelopoulos, Wayne Chi, Yonghyun Kim, chrisdonahue | The paper introduces Music Arena, an open platform for live, scalable human preference evaluation of text-to-music (TTM) models. The research aims to address the lack of standardized evaluation protocols and renewable preference data in the TTM field. Music Arena employs an LLM-based routing system to handle heterogeneous TTM input types and collects detailed user preferences, including listening data and feedback. The platform also implements user privacy protections and proposes rolling data releases for transparency, addressing key challenges in TTM evaluation and demonstrating adaptable live evaluation for specific AI domains, but does not provide concrete quantitative results. |
| Multi-Modal | JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability
  and Aesthetic Alignment (Read more on [arXiv](https://arxiv.org/abs/2507.20880) or [HuggingFace](https://huggingface.co/papers/2507.20880))| Amir Ali Bagherzadeh, Taylor Gautreaux, Navonil Majumder, Renhang Liu, hungchiayu | This paper introduces JAM, a compact flow-based model for lyrics-to-song generation with fine-grained control over timing and aesthetics. The research aims to provide word-level timing control and aesthetic alignment in generated songs. JAM employs rectified flow matching conditioned on lyrics, duration, and style, along with Direct Preference Optimization (DPO) for aesthetic refinement. JAM achieves a Word Error Rate (WER) of 0.151 and a Phoneme Error Rate (PER) of 0.101, significantly outperforming existing methods. The model's fine-grained control and aesthetic alignment offer AI practitioners a novel approach for high-quality, controllable music generation. |
| Machine Learning | Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty (Read more on [arXiv](https://arxiv.org/abs/2507.16806) or [HuggingFace](https://huggingface.co/papers/2507.16806))| Leshem Choshen, Idan Shenfeld, Stewart Slocum, Isha Puri, Mehul Damani | This paper introduces Reinforcement Learning with Calibration Rewards (RLCR) to improve both accuracy and calibration in language models. RLCR augments a binary correctness reward with a Brier score to incentivize calibrated confidence estimation. The research aims to optimize language models for both correctness and well-calibrated uncertainty, with a focus on reasoning models. Experiments across diverse datasets demonstrate that RLCR substantially improves calibration, achieving a reduction in expected calibration error from 0.37 to 0.03 on HotpotQA, without sacrificing accuracy. This suggests practitioners can leverage RLCR to develop more reliable reasoning models by explicitly optimizing for calibration. |
| Machine Learning | GenoMAS: A Multi-Agent Framework for Scientific Discovery via
  Code-Driven Gene Expression Analysis (Read more on [arXiv](https://arxiv.org/abs/2507.21035) or [HuggingFace](https://huggingface.co/papers/2507.21035))| Haohan Wang, Yijiang Li, Liu-Hy | The paper introduces GenoMAS, a multi-agent framework that automates gene expression analysis for scientific discovery. It addresses the challenge of automating complex scientific workflows while maintaining rigor and adaptability. GenoMAS orchestrates six LLM-based agents with specialized roles through typed message-passing protocols, guiding code generation and revision for tasks like data preprocessing and gene identification. In experiments on the GenoTEX benchmark, GenoMAS achieved a Composite Similarity Correlation of 89.13% for data preprocessing and an F1 of 60.48% for gene identification, surpassing prior art. GenoMAS offers AI practitioners a novel approach for integrating structured workflows with autonomous agents in complex scientific domains. |
