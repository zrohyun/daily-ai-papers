

## Papers for 2025-05-21

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Emerging Properties in Unified Multimodal Pretraining (Read more on [arXiv](https://arxiv.org/abs/2505.14683) or [HuggingFace](https://huggingface.co/papers/2505.14683))| Ziang, codecaution, whyu, gouc, Andy1621 | The paper introduces BAGEL, an open-source foundational model for unified multimodal understanding and generation. It aims to develop a model capable of complex multimodal reasoning by scaling with carefully structured multimodal interleaved data. BAGEL is a decoder-only model pretrained on trillions of tokens from text, image, video, and web sources, utilizing a Mixture-of-Transformer-Experts (MoT) architecture.  BAGEL outperforms open-source VLMs on standard multimodal understanding leaderboards, achieving competitive text-to-image quality compared to public generators. It significantly improves performance on complex editing and free-form visual manipulation tasks, exhibiting superior qualitative results. |
| Machine Learning | SageAttention3: Microscaling FP4 Attention for Inference and An
  Exploration of 8-Bit Training (Read more on [arXiv](https://arxiv.org/abs/2505.11594) or [HuggingFace](https://huggingface.co/papers/2505.11594))| surfingtomchen, whx1003, haofeng666, Guyan, jt-zhang | SageAttention3 enhances attention mechanism efficiency through FP4 acceleration and exploration of 8-bit training. It addresses the quadratic time complexity of attention by leveraging FP4 Tensor Cores for inference and designing an accurate 8-bit attention for training. The study implements FP4 attention, achieving 1038 TOPS on RTX5090, a 5x speedup over FlashAttention, and introduces trainable 8-bit attention, maintaining lossless fine-tuning performance. The research suggests that FP4 attention accelerates inference in various models and 8-bit attention provides efficient training, impacting the practical application of attention mechanisms in large-scale models. However, convergence in pretraining tasks is slower using the 8-bit implementation. |
| Computer Vision | VisualQuality-R1: Reasoning-Induced Image Quality Assessment via
  Reinforcement Learning to Rank (Read more on [arXiv](https://arxiv.org/abs/2505.14460) or [HuggingFace](https://huggingface.co/papers/2505.14460))| Kede Ma, Lei Zhang, Jie Liang, Jian Zou, TianheWu | The paper introduces VisualQuality-R1, a reasoning-induced no-reference image quality assessment (NR-IQA) model. It aims to improve IQA by leveraging reinforcement learning to rank (RL2R) images based on their perceived quality. The methodology uses group relative policy optimization (GRPO) to generate multiple quality scores for each image, enabling a Thurstone model-based comparison. Experimental results show VisualQuality-R1 outperforms existing methods, achieving a SRCC of 0.811 on KADID-10K with multi-dataset training. This approach offers AI practitioners a way to better model human perception for image quality tasks, potentially leading to more reliable image processing pipelines. |
| Computer Vision | Visual Agentic Reinforcement Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2505.14246) or [HuggingFace](https://huggingface.co/papers/2505.14246))| sweetFruit, steins1096, zyshan, yuhangzang, ziyuliu | This paper introduces Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT) to enhance the reasoning abilities of Large Vision-Language Models (LVLMs). The study aims to enable LVLMs to use external tools such as web browsers and code execution for image manipulation and analysis. Visual-ARFT leverages reinforcement learning with verifiable rewards to train LVLMs for agentic search and coding, allowing them to browse websites and write code for image processing. Experiments on the MAT-Coding task show that Visual-ARFT outperforms the baseline by +18.6% F1 and +13.0% EM, even surpassing GPT-40 on MAT-Coding using only a 3B base model. The results demonstrate that Visual-ARFT offers a promising approach to building robust and generalizable multimodal agents by enabling flexible and adaptive reasoning capabilities. |
| Natural Language Processing | The Aloe Family Recipe for Open and Specialized Healthcare LLMs (Read more on [arXiv](https://arxiv.org/abs/2505.04388) or [HuggingFace](https://huggingface.co/papers/2505.04388))| annariasdu, pabberpe, danihinjos, adriantormos, JordiBayarri-bsc | The paper introduces Aloe, an open-source recipe for building specialized healthcare LLMs. It investigates the need for competitive open-source models in healthcare AI, aiming to protect public interest. The methodology involves optimizing data preprocessing and training, including synthetic data generation and direct preference optimization for safety. The resulting Aloe models demonstrate competitive performance on healthcare benchmarks, often preferred by healthcare professionals, and improve model safety against jailbreaking attacks. This work sets a new standard for developing and reporting aligned LLMs in healthcare by defining a novel evaluation methodology and releasing open-source, permissively licensed models. |
| Machine Learning | Optimizing Anytime Reasoning via Budget Relative Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2505.13438) or [HuggingFace](https://huggingface.co/papers/2505.13438))| Wee Sun Lee, duchao, P2333, lkevinzc, QPHutu | The paper introduces AnytimeReasoner, a framework to optimize anytime reasoning performance in LLMs by improving token efficiency and flexibility under varying token budget constraints. It addresses the limitation of existing methods that only optimize final performance under a large, fixed token budget by sampling thinking budgets from a prior distribution and introducing verifiable dense rewards. The key methodology involves decoupled optimization of thinking and summary policies, along with a novel variance reduction technique called Budget Relative Policy Optimization (BRPO). Empirical results on mathematical reasoning tasks demonstrate that AnytimeReasoner consistently outperforms GRPO across all thinking budgets, enhancing both training and token efficiency. The method achieves robust performance and can be applied to improve the efficiency of LLMs under varying computational constraints. |
| Natural Language Processing | Latent Flow Transformer (Read more on [arXiv](https://arxiv.org/abs/2505.14513) or [HuggingFace](https://huggingface.co/papers/2505.14513))| Pei-Chen Ho, dsshiu, menghsichen, FengTing, yenchen | The paper introduces the Latent Flow Transformer (LFT), a novel architecture for efficient language modeling. It aims to compress transformer models by replacing blocks of layers with a learned transport operator trained via flow matching. LFT, enhanced with a Flow Walking algorithm, distills 12 layers into one, achieving a KL divergence of 0.736 surpassing skipping 3 layers (0.932) on the Pythia-410M model. This approach bridges the gap between autoregressive and flow-based generation, offering parameter and compute efficiency. LFT is a promising strategy for structural compression while maintaining performance and compatibility with standard transformer architectures. |
| Machine Learning | Neurosymbolic Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2505.13138) or [HuggingFace](https://huggingface.co/papers/2505.13138))| Antonio Vergari, ducdauge, pminervini, HEmile | The paper introduces Neurosymbolic Diffusion Models (NESYDMs) to address limitations in traditional neurosymbolic predictors related to the independence assumption between extracted symbols. The research aims to model dependencies and uncertainty between symbols in neurosymbolic systems using discrete diffusion. NESYDMs reuse the independence assumption at each diffusion step for scalable learning while employing a continuous-time loss function that incorporates symbolic programs. Experiments demonstrate state-of-the-art accuracy on visual path planning, achieving 99.41% on a 12x12 grid, and improved calibration compared to existing neurosymbolic methods. The results imply that NESYDMs can scale to high-dimensional reasoning tasks while maintaining robustness and reliability, thereby benefiting AI practitioners in safety-critical applications. |
| Natural Language Processing | Exploring Federated Pruning for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.13547) or [HuggingFace](https://huggingface.co/papers/2505.13547))| Liangqiong-QU, limingcv, MENGTINGLIU, jcccy, gpx333 | The paper explores federated pruning techniques for compressing large language models (LLMs) while preserving data privacy. It investigates the optimal approach for collaboratively pruning LLMs across distributed clients without sharing local calibration data. The proposed framework, FedPrLLM, allows clients to share pruning mask matrices, which are then aggregated on the server. Extensive experiments show that one-shot pruning with layer comparison achieves the best perplexity results. The findings suggest that a simple layer comparison for mask aggregation, combined with one-shot pruning, can effectively compress LLMs in privacy-sensitive scenarios. |
| Computer Vision | Visionary-R1: Mitigating Shortcuts in Visual Reasoning with
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.14677) or [HuggingFace](https://huggingface.co/papers/2505.14677))| Yixuan Li, Peng Gao, kaiyangzhou, yuhangzang, Jiaer-Xia | The paper introduces Visionary-R1, a method to improve visual reasoning in VLMs by mitigating shortcut learning using reinforcement learning. The research aims to address the issue where VLMs develop shortcuts by not fully interpreting images, leading to poor generalization. Visionary-R1 enforces visual understanding by training the model with a caption-reason-answer format, generating a detailed image caption before reasoning. Trained on 273K CoT-free visual question-answer pairs, Visionary-R1 outperforms models like GPT-4o on visual reasoning benchmarks, such as achieving 69.4% accuracy on MathVista. This suggests that enforcing visual understanding before reasoning can significantly improve the performance of VLMs, guiding practitioners to explore such approaches. |
| Natural Language Processing | General-Reasoner: Advancing LLM Reasoning Across All Domains (Read more on [arXiv](https://arxiv.org/abs/2505.14652) or [HuggingFace](https://huggingface.co/papers/2505.14652))| wenhu, zhangysk, DongfuJiang, SivilTaram, MrLight | The paper introduces GENERAL-REASONER, a new training paradigm to enhance LLM reasoning across diverse domains. It addresses the limitations of current LLM reasoning methods that focus on mathematical and coding domains by constructing a large-scale, high-quality dataset and developing a generative model-based answer verifier. The methodology involves web crawling to create a diverse dataset of questions with verifiable answers and training a generative model for answer verification. Experiments across 12 benchmarks, including MMLU-Pro, GPQA, and TheoremQA, demonstrate that GENERAL-REASONER outperforms existing methods, achieving robust and generalizable performance while maintaining superior effectiveness in mathematical reasoning tasks; for example, it achieves a 10% improvement on MMLU-Pro and SuperGPQA benchmarks. The main implication is that LLMs can achieve stronger and more generalizable reasoning capabilities through diverse training data and generative verifiers, reducing the reliance on rule-based verification methods. |
| Natural Language Processing | Reasoning Models Better Express Their Confidence (Read more on [arXiv](https://arxiv.org/abs/2505.14489) or [HuggingFace](https://huggingface.co/papers/2505.14489))| YongilKim, Sunkyoung, soheeyang, seungone, DKYoon | This paper investigates the confidence calibration of reasoning models compared to non-reasoning models. The study benchmarks six reasoning models across six datasets, demonstrating that reasoning models achieve superior confidence calibration than their non-reasoning counterparts in most settings. Analysis indicates that slow thinking behaviors, such as exploring alternatives and backtracking, enable more accurate confidence adjustments throughout the reasoning process. Specifically, the Expected Calibration Error (ECE) is often significantly lower in reasoning models. The findings suggest that slow thinking enhances the trustworthiness and reliability of LLMs by improving their ability to express confidence accurately. |
| Natural Language Processing | Reasoning Path Compression: Compressing Generation Trajectories for
  Efficient LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.13866) or [HuggingFace](https://huggingface.co/papers/2505.13866))| Jae-Joon Kim, YulhwaKim, dongwonjo, jiwonsong | The paper introduces Reasoning Path Compression (RPC), a training-free method to accelerate inference in reasoning LLMs by leveraging semantic sparsity in reasoning paths. RPC periodically compresses the KV cache by retaining only KV cache entries that receive high importance scores computed using a selector window of recent queries. Experiments on QwQ-32B show that RPC improves generation throughput by up to 1.60× compared to inference with a full KV cache, with an accuracy drop of only 1.2% on the AIME 2024 benchmark. These results demonstrate the practical benefits of exploiting semantic sparsity to improve the efficient deployment of reasoning LLMs, offering a trade-off between accuracy and throughput. RPC can be integrated into existing inference pipelines of reasoning LLMs without additional training. |
| Natural Language Processing | CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the
  Limits of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.13559) or [HuggingFace](https://huggingface.co/papers/2505.13559))| Eng Siong Chng, Lim Zhi Hao, Tanmay Surana, SkAndMl | The paper introduces CS-Sum, a novel benchmark for code-switching dialogue summarization designed to evaluate the capabilities of Large Language Models (LLMs) in comprehending code-switched text. The research aims to assess the comprehensibility of LLMs when dealing with code-switching through dialogue-to-English summarization tasks across Mandarin-English, Tamil-English, and Malay-English. The methodology involves creating a dataset with 900-1300 human-annotated dialogues per language pair and evaluating ten LLMs using few-shot, translate-summarize, and fine-tuning approaches. The primary results reveal that despite high scores on automated metrics, LLMs often make subtle errors altering the dialogue meaning, indicating limitations in true code-switching comprehension. The paper implies that specialized training on code-switched data is necessary for LLMs to accurately interpret multilingual dialogues, requiring further research into robust methodologies. |
| Information Retrieval | NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search (Read more on [arXiv](https://arxiv.org/abs/2505.14680) or [HuggingFace](https://huggingface.co/papers/2505.14680))| Wenjie Wang, chuats, jrwen, pl8787, KID-22 | The paper introduces NExT-Search, a paradigm aimed at rebuilding the user feedback ecosystem for generative AI search. It addresses the challenge of limited fine-grained feedback in generative AI search pipelines. NExT-Search integrates User Debug Mode and Shadow User Mode to gather detailed feedback at various pipeline stages. The collected feedback is leveraged through online adaptation and offline updates for model refinement. This approach seeks to restore human control and feedback-driven improvement in generative AI search systems. |
| Computer Vision | Training-Free Watermarking for Autoregressive Image Generation (Read more on [arXiv](https://arxiv.org/abs/2505.14673) or [HuggingFace](https://huggingface.co/papers/2505.14673))| Shuai Yang, kaiyangzhou, Apostle723, yutchina02 | The paper introduces IndexMark, a training-free watermarking framework for autoregressive image generation models. It addresses the challenge of watermarking autoregressive image generation, which has been largely underexplored, by leveraging codebook redundancy. IndexMark employs a match-then-replace method to embed watermarks by selectively substituting generated indices with similar watermark tokens, preserving image quality. Experiments show IndexMark achieves state-of-the-art performance with high verification accuracy and robustness to perturbations, including cropping, noise, and JPEG compression. This provides AI practitioners with a readily applicable approach to ensuring traceability and responsible use of autoregressive image generation models. |
| Computer Vision | VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation (Read more on [arXiv](https://arxiv.org/abs/2505.14640) or [HuggingFace](https://huggingface.co/papers/2505.14640))| Ping Nie, Yiming Jia, ZhuofengLi, wren93, tonymwt | The paper introduces VIDEOEVAL-PRO, a new benchmark for evaluating long video understanding (LVU) in large multimodal models (LMMs). It addresses limitations in existing benchmarks, which rely heavily on multiple-choice questions and exhibit strong priors. VIDEOEVAL-PRO utilizes open-ended short-answer questions to assess segment-level and full-video understanding through perception and reasoning tasks. Evaluating 21 video LMMs, the study found a performance drop of >25% compared to MCQ formats and highlights that increased frames benefit VIDEOEVAL-PRO more than existing MCQ benchmarks. The main implication is that VIDEOEVAL-PRO provides a more realistic and reliable measure of LVU, offering a clearer progress view. |
| Natural Language Processing | Think Only When You Need with Large Hybrid-Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2505.14631) or [HuggingFace](https://huggingface.co/papers/2505.14631))| Zewen Chi, Qingxiu Dong, Shaohan Huang, YUSHUIWX, lingjie23 | This paper introduces Large Hybrid-Reasoning Models (LHRMs) that adaptively determine whether to perform extended thinking based on contextual information of user queries. The research aims to address the overthinking problem in Large Reasoning Models (LRMs) by enabling selective engagement of reasoning processes. A two-stage training pipeline is proposed, comprising Hybrid Fine-Tuning (HFT) and Hybrid Group Policy Optimization (HGPO), to implicitly learn appropriate thinking mode selection. Experimental results demonstrate that LHRMs outperform existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency, and a metric called Hybrid Accuracy correlates strongly with human expert judgment. The results show LHRMs can effectively balance computational efficiency with task complexity, suggesting the need to reconsider the appropriate use of extended thinking processes in building hybrid thinking systems. |
| Machine Learning | Fine-tuning Quantized Neural Networks with Zeroth-order Optimization (Read more on [arXiv](https://arxiv.org/abs/2505.13430) or [HuggingFace](https://huggingface.co/papers/2505.13430))| Minxian Li, Jiayi Zhou, kaiyangzhou, chenyulin, sifengshang | This paper introduces Quantized Zeroth-order Optimization (QZO) to minimize memory usage in fine-tuning large language models (LLMs). The research aims to enable memory-efficient training by reducing the memory footprint of model weights, gradients, and optimizer states. QZO perturbs the continuous quantization scale for gradient estimation and employs directional derivative clipping to stabilize training. Experiments show QZO can reduce total memory cost by over 18x for 4-bit LLMs, allowing fine-tuning of Llama-2-13B within a single 24GB GPU. The proposed method offers a practical approach for practitioners with limited resources to adapt large models. |
| Multi-Modal | SSR: Enhancing Depth Perception in Vision-Language Models via
  Rationale-Guided Spatial Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.12448) or [HuggingFace](https://huggingface.co/papers/2505.12448))| Han Zhao, Pengxiang Ding, Xiaomin Yu, Ming Ma, yliu-cs | The paper introduces SSR, a novel approach to enhance depth perception and spatial reasoning in vision-language models (VLMs). The research addresses the limited spatial understanding of VLMs due to their reliance on RGB inputs by converting raw depth data into structured, interpretable textual rationales. SSR leverages knowledge distillation to compress these rationales into compact latent embeddings for seamless integration into existing VLMs without retraining. Experiments on a new million-scale dataset (SSR-COT) and a comprehensive multi-task benchmark (SSRBench) demonstrate substantial improvements in depth utilization and spatial reasoning; for example, achieving a 13.6% maximum improvement in average question answering accuracy. SSR offers AI practitioners a method for improving depth utilization and spatial reasoning in VLMs, thus advancing more human-like multi-modal understanding. |
| Natural Language Processing | Not All Correct Answers Are Equal: Why Your Distillation Source Matters (Read more on [arXiv](https://arxiv.org/abs/2505.14464) or [HuggingFace](https://huggingface.co/papers/2505.14464))| Sitong Zhao, Shuaiting Chen, Haotian Wang, Yunjie Ji, Emperorizzis | This paper investigates the impact of distillation source on the performance of reasoning-oriented language models. The primary question is how the teacher model's characteristics affect the student model's reasoning capabilities after distillation. The methodology involves distilling reasoning data from AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1 and training student models on each resulting dataset. The AM-Thinking-v1-distilled model achieves the best performance, reaching 84.3 on AIME2024. The results imply that high-quality, verified reasoning traces are crucial for effective distillation in open-source language models. |
| Machine Learning | Towards eliciting latent knowledge from LLMs with mechanistic
  interpretability (Read more on [arXiv](https://arxiv.org/abs/2505.14352) or [HuggingFace](https://huggingface.co/papers/2505.14352))| Emil Ryd, NeelNanda, srdm, bcywinski | This paper explores methods for eliciting hidden knowledge from language models using mechanistic interpretability, focusing on a Taboo model designed to conceal a secret word. The main research question is whether interpretability techniques can uncover non-verbalized hidden knowledge. The methodology involves training a Taboo model and evaluating black-box and white-box (Logit Lens and Sparse Autoencoders) elicitation techniques. The results show that both black-box (95% Pass@10 with another LLM guessing) and white-box approaches can elicit the secret word. This demonstrates the potential of interpretability techniques for detecting concealed information in language models, informing safer deployment strategies. |
| Computer Vision | Hunyuan-Game: Industrial-grade Intelligent Game Creation Model (Read more on [arXiv](https://arxiv.org/abs/2505.14135) or [HuggingFace](https://huggingface.co/papers/2505.14135))| vcvcvn, tangjs, YellowAddice, zhengsj, lslrh | The paper introduces Hunyuan-Game, an intelligent game creation model leveraging generative AI for dynamic game content generation. It aims to enhance game asset synthesis and designer efficiency through image and video generation. The methodology involves building customized image and video generation models based on large-scale game and anime datasets, incorporating domain-specific knowledge and aesthetic evaluations. Experiments demonstrate state-of-the-art performance in visual fidelity and motion naturalness, surpassing competitors like Midjourney and Kling in game scenarios; a metric isn't explicitly mentioned, indicating potential future work areas for broader applications in the gaming industry. This project provides AI practitioners with new models and strategies to create AI-generated gaming content. |
| Natural Language Processing | Reward Reasoning Model (Read more on [arXiv](https://arxiv.org/abs/2505.14674) or [HuggingFace](https://huggingface.co/papers/2505.14674))| Qingxiu Dong, Zewen Chi, Jiaxin Guo, YUSHUIWX, unilm | The paper introduces Reward Reasoning Models (RRMs) to enhance reward model performance by executing a deliberate reasoning process before generating final rewards. RRMs address the challenge of effectively utilizing test-time compute to align large language model outputs with human expectations, aiming to improve reward modeling benchmarks. The method involves a reinforcement learning framework that fosters self-evolved reward reasoning capabilities without explicit reasoning traces as training data. Experimental results demonstrate that RRMs achieve superior performance across diverse domains, with improvements on GPQA via reinforcement learning on unlabeled data, improving accuracy from 26.8% to 40.9%. The implementation offers AI practitioners a more effective approach to reward model development, leveraging additional test-time compute for improved accuracy in complex queries. |
| Natural Language Processing | Warm Up Before You Train: Unlocking General Reasoning in
  Resource-Constrained Settings (Read more on [arXiv](https://arxiv.org/abs/2505.13718) or [HuggingFace](https://huggingface.co/papers/2505.13718))| Keith Ross, xanubhav81, AadimNepal, guactastesgood, safal312 | The paper introduces a sample-efficient, two-stage training strategy to develop reasoning-capable LLMs under limited supervision. It investigates whether models can acquire general reasoning strategies and rapidly adapt them to new domains with minimal supervision. The methodology involves a warmup phase distilling Long CoTs from a toy domain (Knights & Knaves) followed by RLVR training on a limited target-domain dataset. Experiments show the warmed-up Qwen2.5-3B model gains +10.2% on MATH benchmark. The main implication is that this warmup strategy improves sample efficiency and generalizability for training robust reasoning LLMs in data-scarce environments. |
| Natural Language Processing | Phare: A Safety Probe for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.11365) or [HuggingFace](https://huggingface.co/papers/2505.11365))| Matteo Dora, inoki-giskard, bmalezieux, pierlj | This paper introduces Phare, a multilingual diagnostic framework to evaluate the safety of large language models (LLMs) across three dimensions: hallucination, social biases, and harmful content generation. The research aims to identify failure modes rather than simply ranking models. Phare evaluates 17 state-of-the-art LLMs and reveals systematic vulnerabilities across all safety dimensions. Key findings include the identification of sycophancy, prompt sensitivity, and stereotype reproduction. Phare offers actionable insights for researchers and practitioners to build more robust, aligned, and trustworthy language systems. |
| Machine Learning | Lessons from Defending Gemini Against Indirect Prompt Injections (Read more on [arXiv](https://arxiv.org/abs/2505.14534) or [HuggingFace](https://huggingface.co/papers/2505.14534))| cchoquette, julsh, tux, iliashum, chongyangs | This paper evaluates and enhances the adversarial robustness of Gemini models against indirect prompt injections. The research investigates methods for defending against malicious instructions embedded in untrusted data sources, focusing on scenarios involving function calling and tool use. The study employs a robust adversarial evaluation framework with adaptive attack techniques, measuring attack success rates (ASR) across various defenses. Adversarial fine-tuning significantly increased Gemini 2.5's resilience, achieving an average 47% reduction in ASR. The paper implies the need for multi-layered defense strategies combining model-level enhancements with system-level mitigations to address the complexities of indirect prompt injection attacks. |
| Natural Language Processing | Truth Neurons (Read more on [arXiv](https://arxiv.org/abs/2505.12182) or [HuggingFace](https://huggingface.co/papers/2505.12182))| ZiningZhu, jordansuchow, ShirleyY, YupengCao, Acatsama | This paper introduces a method for identifying truth neurons within language models. The research aims to understand how truthfulness is encoded within these models. Using integrated gradients, the method identifies neurons contributing to truthful responses and causally links a subset to truthfulness representations. Experiments on TruthfulQA show a statistically significant reduction in accuracy (e.g., small-scale models decrease to 54.25%) when these neurons are suppressed. The finding implies that targeted intervention on these neurons can improve trustworthiness and safety of language models. |
| Machine Learning | MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8 (Read more on [arXiv](https://arxiv.org/abs/2505.09569) or [HuggingFace](https://huggingface.co/papers/2505.09569))| Lin Chen, Qiang Zhou, omidvarb, sliuxl, linboliu | MigrationBench is introduced as a new benchmark for evaluating LLMs in repository-level code migration, specifically from Java 8 to later LTS versions. The research aims to provide a standardized assessment framework for LLMs on the challenging task of code migration across an entire repository. The methodology involves curating a dataset of Java 8 repositories, establishing automated evaluation metrics, and developing a feedback mechanism (SD-Feedback) to enhance migration efficacy.  The SD-Feedback approach with Claude-3.5-Sonnet-v2 achieves a 62.33% success rate (pass@1) for minimal migration tasks on a curated subset. The benchmark provides AI practitioners with a resource to assess and improve LLMs' capability in complex, repository-level coding tasks. |
| Natural Language Processing | Two Experts Are All You Need for Steering Thinking: Reinforcing
  Cognitive Effort in MoE Reasoning Models Without Additional Training (Read more on [arXiv](https://arxiv.org/abs/2505.14681) or [HuggingFace](https://huggingface.co/papers/2505.14681))| Jiahao Xu, Zhiwei He, Yue Wang, Xingyu Chen, Mengru Wang | The paper introduces Reinforcing Cognitive Experts (RICE), a novel inference-time steering method for improving the reasoning performance of Mixture-of-Experts (MoE) based Large Reasoning Models (LRMs) without additional training. It aims to address cognitive inefficiencies like overthinking and underthinking by identifying and reinforcing cognitive experts based on normalized Pointwise Mutual Information (nPMI). The method strategically amplifies the activation of a few cognitive experts correlated with meta-level reasoning tokens like “<think>”. Empirical evaluations on DeepSeek-R1 and Qwen3-235B demonstrate improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization, with accuracy improvements up to 83.3% on AIME24. RICE provides a lightweight, practical, and interpretable direction to enhance cognitive efficiency in advanced reasoning models. |
| Machine Learning | CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via
  Competition (Read more on [arXiv](https://arxiv.org/abs/2505.13380) or [HuggingFace](https://huggingface.co/papers/2505.13380))| Van Nguyen, Quang Pham, Huy Nguyen, nhatho, DavidNguyen | This paper introduces CompeteSMoE, a novel sparse mixture of experts (SMoE) training algorithm leveraging competition among experts for improved routing. The research aims to address the suboptimal routing in conventional SMoE training where experts do not directly contribute to the routing process. CompeteSMoE employs a competition mechanism, routing tokens to experts with the highest neural response and uses a router to learn the competition policy. Empirical results on visual instruction tuning and language pre-training tasks demonstrate that CompeteSMoE achieves better zero-shot performance, improving average accuracy by approximately 1% on visual tasks compared to state-of-the-art baselines. The competition mechanism, efficiently improves training performance by selecting relevant experts, offering an improved method for sparse model training for AI practitioners. |
| Machine Learning | Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair (Read more on [arXiv](https://arxiv.org/abs/2505.13103) or [HuggingFace](https://huggingface.co/papers/2505.13103))| Mathias Payer, Aiden Hall, Tianqi Fan, Han Zheng, iliashum | The paper introduces WILLIAMT, a low-cost automated program repair (APR) system that uses crash-site repair and template-guided patch generation. The research objective is to reduce the cost of APR by minimizing reliance on large language models (LLMs) for root cause analysis. WILLIAMT employs regular expression-based fault localization and LLM-guided template filling for patch generation. Evaluation on the ARVO benchmark shows that when combined with CodeRover-S, it reduces token cost by 45.9% and increases the bug-fixing rate to 73.5%. This approach enables scalable and practical APR solutions, even in resource-constrained environments with smaller LLMs. |
| Natural Language Processing | To Bias or Not to Bias: Detecting bias in News with bias-detector (Read more on [arXiv](https://arxiv.org/abs/2505.13010) or [HuggingFace](https://huggingface.co/papers/2505.13010))| grohg, amosharafa, himel7 | This paper presents a system for detecting bias in news at the sentence level. The main objective is to improve sentence-level bias detection using a RoBERTa-based model. The methodology involves fine-tuning a RoBERTa-base model on the Bias Annotations by Experts (BABE) dataset and comparing it against a domain-adaptively pre-trained DA-RoBERTa baseline. Results show a macro F1 score of 0.9257, indicating statistically significant improvements over the baseline. The main implication is that carefully fine-tuning a RoBERTa model on task-specific data can yield robust and interpretable models for media bias detection, even without domain-adaptive pre-training. |
| Natural Language Processing | Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative
  Verifier (Read more on [arXiv](https://arxiv.org/abs/2505.11966) or [HuggingFace](https://huggingface.co/papers/2505.11966))| Kezhi Li, Zhijian Xu, Zeju Li, XiangyuWen, Jianyuan1 | The paper introduces Solve-Detect-Verify, an inference-time scaling framework to improve the accuracy and efficiency of Large Language Model (LLM) reasoning. It addresses the trade-off between solution accuracy and computational cost in complex reasoning tasks. The framework integrates FlexiVe, a generative verifier that dynamically adjusts its computational resources using a flexible allocation strategy. Experiments on mathematical reasoning benchmarks like AIME 2024 demonstrate that Solve-Detect-Verify achieves higher accuracy while requiring approximately 4x fewer solutions compared to baseline approaches. This scalable system enhances LLM reasoning at test time by balancing rapid and meticulous thinking. |
| Natural Language Processing | Masking in Multi-hop QA: An Analysis of How Language Models Perform with
  Context Permutation (Read more on [arXiv](https://arxiv.org/abs/2505.11754) or [HuggingFace](https://huggingface.co/papers/2505.11754))| Jeff Z. Pan, Mirella Lapata, pvougiou, hwy9855 | This paper investigates how Language Models (LMs) perform in multi-hop question answering (MHQA) under various context permutations. The study aims to understand how the causal mask in decoder-only LMs affects reasoning across complex contexts by permuting retrieved documents. The methodology involves evaluating LMs from the Flan-T5, Qwen2.5, and Llama 3 families on the MuSiQue dataset with different document orderings and analyzing attention distributions. Results show that encoder-decoder models (Flan-T5) generally outperform causal decoder-only LMs and fine-tuned models favor forward-placed documents; Qwen 7B's accuracy improved from 28.6% to 33.7% using peak attention score based sampling. The findings suggest that attention weights can be leveraged to heuristically improve LM performance in MHQA, and bi-directional attention mechanisms enhance model robustness. |
| Multi-Modal | Incorporating brain-inspired mechanisms for multimodal learning in
  artificial intelligence (Read more on [arXiv](https://arxiv.org/abs/2505.10176) or [HuggingFace](https://huggingface.co/papers/2505.10176))| Xin Yang, Qingqun Kong, Yang Li, Dongcheng Zhao, Xiang He | The paper introduces an Inverse Effectiveness driven Multimodal Fusion (IEMF) strategy for enhanced multimodal learning in AI systems. It aims to improve multimodal integration by dynamically adjusting fusion strength based on the inverse effectiveness principle observed in the brain. The key methodology involves incorporating this strategy into neural network architectures, adaptively modulating fusion module weights. Results demonstrate up to a 50% reduction in computational cost and improved performance across audio-visual tasks; for instance, IEMF increased classification accuracy on Kinetics-Sounds dataset to 63.53% using SNN. The IEMF strategy offers AI practitioners a computationally efficient method to create robust multimodal systems. |
| Natural Language Processing | Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic
  Reasoning Limits (Read more on [arXiv](https://arxiv.org/abs/2505.14178) or [HuggingFace](https://huggingface.co/papers/2505.14178))| Yiwei Xu, Jiaqi Wei, Juntai Cao, Charlesyooo, Wyattz23 | This paper investigates tokenization's impact on symbolic and arithmetic reasoning in LLMs. It hypothesizes that tokenization schemes, like BPE, hinder symbolic computation by obscuring atomic reasoning units. The study evaluates LLM performance on arithmetic and symbolic tasks with varying tokenization formats, finding that atomically-aligned formats significantly improve reasoning, allowing smaller models to outperform larger ones. For example, atomically-aligned formats can improve accuracy by over 50%. The findings suggest that symbolic reasoning in LLMs is deeply conditioned on token-level representations, not solely architectural factors, which presents a critical consideration for model design and application. |
| Natural Language Processing | Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for
  Real-world Knowledge Injection (Read more on [arXiv](https://arxiv.org/abs/2505.12306) or [HuggingFace](https://huggingface.co/papers/2505.12306))| Shangbin Feng, Wenhao Yu, Yuwei Zhang, shangjingbo, KomeijiForce | This paper introduces WIKIDYK, a novel benchmark for real-world knowledge injection in large language models (LLMs), derived from Wikipedia's "Did You Know..." entries. The main objective is to assess and improve LLMs' ability to memorize and internalize new knowledge after pre-training. The key methodology involves continued pre-training of both Causal Language Models (CLMs) and Bidirectional Language Models (BiLMs) with WIKIDYK facts, followed by evaluation using a multi-dimensional question-answering suite. The primary result shows that BiLMs exhibit significantly stronger knowledge memorization capabilities than CLMs, with a 23% higher reliability accuracy; furthermore, a modular collaborative framework using BiLMs improves the reliability accuracy by up to 29.1%. The main implication is a call to action to revisit bidirectional LMs for neural knowledge modeling in LLMs, as the architecture may offer enhanced memorization. |
| Natural Language Processing | Understanding Gen Alpha Digital Language: Evaluation of LLM Safety
  Systems for Content Moderation (Read more on [arXiv](https://arxiv.org/abs/2505.10588) or [HuggingFace](https://huggingface.co/papers/2505.10588))| Fausto Giunchiglia, Manisha Mehta | This paper evaluates the ability of large language models (LLMs) to understand and moderate the digital language of Generation Alpha (Gen Alpha). The study investigates whether leading AI systems can effectively detect masked harassment and manipulation within Gen Alpha's unique communication patterns, using a new dataset of 100 Gen Alpha expressions. The methodology involves evaluating GPT-4, Claude, Gemini, and Llama 3 using zero-shot inference across dimensions like meaning recognition, context interpretation, and safety implication detection. Results show significant comprehension gaps in AI systems, particularly in context and safety, with an average safety detection accuracy of 32-42%. The findings imply that AI practitioners need to improve LLMs' understanding of rapidly evolving youth language and context-dependent risks to enhance content moderation effectiveness. |
