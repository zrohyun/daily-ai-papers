

## Papers for 2025-05-14

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable
  Speaker Encoder (Read more on [arXiv](https://arxiv.org/abs/2505.07916) or [HuggingFace](https://huggingface.co/papers/2505.07916))| Congchao Guo, Bowen Zhang, ymzhang0519, mqyang1s, JunjieYan | The paper introduces MiniMax-Speech, an autoregressive Transformer-based TTS model for high-quality speech synthesis and zero-shot voice cloning. It aims to improve speaker similarity and audio quality in zero-shot TTS by employing a learnable speaker encoder and a Flow-VAE architecture. The key methodology involves jointly training the speaker encoder with the AR model and using Flow-VAE to enhance the information representation. The model achieves state-of-the-art results on objective voice cloning metrics, demonstrably lowers WER, and secures the top position on the public TTS Arena leaderboard. MiniMax-Speech offers AI practitioners a more flexible and controllable TTS system capable of producing highly expressive speech with greater speaker similarity without requiring transcribed reference audio. |
| Natural Language Processing | A Multi-Dimensional Constraint Framework for Evaluating and Improving
  Instruction Following in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.07591) or [HuggingFace](https://huggingface.co/papers/2505.07591))| xjhuang, sean-xl-y, wuyilong, avonfwj, Junjie-Ye | This paper introduces a multi-dimensional constraint framework to evaluate and improve instruction following in large language models. The research aims to address limitations of existing benchmarks by capturing the diversity of user constraints. The authors developed an automated instruction generation pipeline incorporating constraint expansion, conflict detection, and rewriting, generating 1,200 code-verifiable test samples. Evaluations across 19 LLMs revealed substantial performance variations across constraint forms, with average performance dropping from 77.67% to 32.96% across difficulty levels. Results show that leveraging their generated data for reinforcement learning yields significant instruction following gains without general performance degradation, primarily affecting the attention modules enhancing constraint recognition. |
| Machine Learning | Measuring General Intelligence with Generated Games (Read more on [arXiv](https://arxiv.org/abs/2505.07215) or [HuggingFace](https://huggingface.co/papers/2505.07215))| William Chen, David Huang, nickatomlin, danjklein, vivekverma | This paper introduces gg-bench, a data generating process for creating game environments to evaluate general reasoning capabilities in language models. The research aims to assess language models' ability to generalize to novel tasks through gameplay. GG-bench synthetically generates games using a large language model for description and implementation, followed by training reinforcement learning agents for self-play. Evaluation involves comparing language model winrates against these agents, with state-of-the-art LLMs achieving winrates of 7-9%. The generated games and evaluation code are released to support future research and expansion of the benchmark to better test for general intelligence. |
| Computer Vision | SkillFormer: Unified Multi-View Video Understanding for Proficiency
  Estimation (Read more on [arXiv](https://arxiv.org/abs/2505.08665) or [HuggingFace](https://huggingface.co/papers/2505.08665))| ucaclio, EdBianchi | The paper introduces SkillFormer, a parameter-efficient architecture for multi-view proficiency estimation from egocentric and exocentric videos. It aims to improve accuracy and efficiency in assessing human skill levels in complex activities. SkillFormer employs a TimeSformer backbone with a novel CrossViewFusion module using multi-head cross-attention and LoRA for efficient fine-tuning. Evaluated on the EgoExo4D dataset, SkillFormer achieves state-of-the-art accuracy in multi-view settings, using 4.5x fewer parameters and requiring 3.75x fewer training epochs than prior baselines, reaching 47.5% accuracy in the combined ego+exo setup. This unified architecture offers AI practitioners a resource-efficient approach to multi-view skill assessment in various application scenarios. |
| Reinforcement Learning | NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged
  Information Guidance (Read more on [arXiv](https://arxiv.org/abs/2505.08712) or [HuggingFace](https://huggingface.co/papers/2505.08712))| Yujian Zhang, Jiaqi Peng, Jiangmiao, fulifuli666, WadeCai | The paper introduces NavDP, a novel end-to-end framework for sim-to-real robot navigation using a diffusion policy trained with privileged information. The research aims to address the challenge of generalizing navigation policies trained in simulation to diverse real-world environments and robot embodiments. NavDP combines diffusion-based trajectory generation with a critic function for trajectory selection, conditioned on local observation tokens and leveraging global environment information in simulation for training. The approach achieves state-of-the-art performance, demonstrating generalization capabilities on quadruped, wheeled, and humanoid robots, with a 30% improvement in success rate using real-to-sim fine-tuning. NavDP provides AI practitioners with a scalable and generalizable solution for robot navigation by reducing reliance on real-world data and enhancing sim-to-real transfer. |
| Natural Language Processing | ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness
  Prediction via Human-AI Collaborative Annotation (Read more on [arXiv](https://arxiv.org/abs/2505.07416) or [HuggingFace](https://huggingface.co/papers/2505.07416))| Kiet Van Nguyen, Dat Minh Nguyen, sonlam1102, trucnguyen28 | The paper introduces ViMRHP, a Vietnamese multimodal review helpfulness prediction dataset. It aims to address the lack of linguistic diversity in MRHP datasets by creating a large-scale Vietnamese dataset. The methodology employs a human-AI collaborative annotation framework to reduce annotation time and cost, while maintaining data quality. Experiments using baseline models demonstrate the quality differences between human-verified and AI-generated annotations; for example, Cohen's Kappa score between human vs Al for ground truth Helpfulness Score is only 31.34% indicating Fair Agreement. The implication is that human verification is essential for ensuring data quality in complex annotation tasks, even with AI assistance. |
