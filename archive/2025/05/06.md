

## Papers for 2025-05-06

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Voila: Voice-Language Foundation Models for Real-Time Autonomous
  Interaction and Voice Role-Play (Read more on [arXiv](https://arxiv.org/abs/2505.02707) or [HuggingFace](https://huggingface.co/papers/2505.02707))| Yu Shu, Yemin Shi, zhitinghu, Jaward, guangyil | The paper introduces Voila, a family of open-sourced, large voice-language foundation models designed for real-time, autonomous, and emotionally expressive voice interactions and role-play. The main objective is to develop AI agents that move beyond reactive, turn-based systems by enabling full-duplex, low-latency conversations with rich vocal nuances. Voila utilizes a hierarchical multi-scale Transformer architecture that integrates LLM reasoning with acoustic modeling, employs a novel Voila-Tokenizer for semantic and acoustic token prediction, and supports efficient voice customization from brief audio samples. Key results include a response latency of 195 milliseconds, an accuracy of 30.56 on the custom Voila Benchmark (surpassing SpeechGPT's 13.29), and a Word Error Rate (WER) of 2.7% on LibriSpeech test-clean for ASR. Voila provides AI practitioners with a unified, open-source framework for building advanced, natural, and dynamic voice-based applications. |
| Reinforcement Learning | RM-R1: Reward Modeling as Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.02387) or [HuggingFace](https://huggingface.co/papers/2505.02387))| Ziqi Wang, zhangdenghui123, Merlin-Hongru, gaotang, XtremSup | This paper introduces Reasoning Reward Models (REASRMS), a novel class of generative reward models under the RM-R1 family, which formulate reward modeling as an explicit reasoning task to enhance Large Language Model (LLM) alignment. The primary objective is to improve the interpretability and performance of generalist reward models by integrating deep reasoning capabilities into the reward generation process. The RM-R1 training pipeline involves two key stages: first, distillation of high-quality reasoning chains using a Chain-of-Rubrics prompting framework, and second, reinforcement learning with verifiable rewards (RLVR). RM-R1 models demonstrate state-of-the-art or near state-of-the-art performance, outperforming larger models like Llama3.1-405B and GPT-4o by up to 13.8% in accuracy on comprehensive reward model benchmarks such as RewardBench. This approach provides AI practitioners with a method to build more robust, interpretable, and accurate reward signals for LLM alignment, particularly by enabling models to self-generate reasoning traces or rubrics for evaluation. |
| Natural Language Processing | Grokking in the Wild: Data Augmentation for Real-World Multi-Hop
  Reasoning with Transformers (Read more on [arXiv](https://arxiv.org/abs/2504.20752) or [HuggingFace](https://huggingface.co/papers/2504.20752))| Gjergji Kasneci, Roman Abramov, fsteinbauer | This paper investigates extending the "grokking" phenomenon, where models transition from memorization to generalization, to real-world multi-hop factual reasoning tasks using Transformers by strategically augmenting datasets with synthetic data. The primary objective is to enable Transformers to perform robust multi-hop reasoning on sparse real-world knowledge graphs by increasing the ratio of inferred facts to atomic facts (denoted as φ) above the empirically determined threshold required for grokking. The authors employ a two-stage approach: first, they augment the 2WikiMultiHopQA dataset with LLM-generated synthetic atomic and inferred facts to increase φ; second, they train a GPT-2 style Transformer on this enriched data for an extended period to induce grokking. The proposed method enabled a grokked GPT2-small model to achieve up to 95-100% accuracy on the 2WikiMultiHopQA benchmark, significantly outperforming baselines, with specific out-of-distribution (OOD) accuracy reaching 96% on the structured comparison task. The findings suggest that targeted data augmentation, even with potentially factually imperfect synthetic data, can be a powerful technique to unlock latent multi-hop reasoning capabilities in Transformers by promoting the formation of generalizing circuits, thereby improving performance on knowledge-intensive tasks. |
| Natural Language Processing | FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2505.02735) or [HuggingFace](https://huggingface.co/papers/2505.02735))| ZhengYuan, yifanzhang114, Liam-Liu, prt66, zhouliang | This paper introduces FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems designed to evaluate the formal mathematical reasoning capabilities of Large Language Models (LLMs). The main objective was to address limitations in existing benchmarks' scope and scale by creating a diverse set of problems spanning high-school Olympiad challenges to undergraduate-level theorems. The benchmark was constructed using a novel human-in-the-loop autoformalization pipeline, integrating LLMs for statement formalization, multi-LLM semantic verification, and negation-based disproof filtering, which retained 72.09% of statements before manual verification. Evaluation of state-of-the-art LLM-based theorem provers on FormalMATH revealed significant limitations, with the strongest models achieving only a 16.46% success rate (Pass@32), and highlighted issues such as pronounced domain bias and over-reliance on simplified automation tactics. For AI practitioners, FormalMATH provides a robust benchmark to drive research in improving the generalizability, deductive rigor, and overall formal reasoning capacity of LLMs. |
| Machine Learning | ReplaceMe: Network Simplification via Layer Pruning and Linear
  Transformations (Read more on [arXiv](https://arxiv.org/abs/2505.02819) or [HuggingFace](https://huggingface.co/papers/2505.02819))| szagoruyko121, stamatisl, madrugado, ammarali32, dimitriish | The paper introduces ReplaceMe, a novel training-free depth pruning method that simplifies transformer networks by replacing contiguous blocks of layers with an estimated linear transformation. The primary objective is to develop an efficient, training-free structural depth pruning approach for transformer models, such as LLMs and Vision Transformers, capable of maintaining high performance without requiring retraining. ReplaceMe identifies a contiguous sequence of transformer blocks for pruning using activation similarity metrics (e.g., cosine distance between hidden states before and after the potential cut) and then estimates an optimal linear transformation from a small calibration dataset to approximate the function of these pruned blocks; this transformation is subsequently merged into an adjacent layer, avoiding additional parameters. Experiments demonstrate that ReplaceMe can prune 25% of layers in models like Llama 2 7B while retaining 92.5% of the original performance (using a cosine-distance-based linear transformation estimation) on open benchmarks, outperforming training-free baselines like UIDL. AI practitioners can leverage ReplaceMe to significantly reduce the computational cost and size of large transformer models with minimal performance degradation and without the resource-intensive step of retraining, thereby facilitating easier deployment. |
| Machine Learning | Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization
  in Rejection Sampling and RL (Read more on [arXiv](https://arxiv.org/abs/2505.02391) or [HuggingFace](https://huggingface.co/papers/2505.02391))| nanjiang, WeiXiong, hendrydong, HanningZhang, FlippyDora | This paper introduces GVM-RAFT, a novel dynamic sample allocation strategy that minimizes stochastic gradient variance during the training of Chain-of-Thought (CoT) reasoners, leading to enhanced efficiency and performance. The research aims to overcome the limitations of static sampling in CoT training by developing a principled method to dynamically allocate computational resources across prompts, thereby minimizing gradient variance and improving stochastic gradient estimation under a fixed budget. GVM-RAFT operates within an Expectation-Maximization framework for CoT reasoning, dynamically assigning inference samples per prompt by monitoring acceptance rates and gradient norms to minimize gradient variance, and is applied to iterative reward-ranked fine-tuning (RAFT) and reinforcement learning algorithms like GRPO. Experimental results on mathematical reasoning tasks demonstrate that GVM-RAFT provides a 2-4× speedup in convergence and notable accuracy gains over standard RAFT; for example, GVM-GRPO achieved a 40.42 5-benchmark average accuracy with Qwen2.5-Math-1.5B, compared to 38.11 for GRPO. AI practitioners can leverage this gradient variance minimization approach for more efficient and effective training of large language models for complex reasoning tasks, particularly when using rejection sampling or reinforcement learning, by adaptively allocating computational resources. |
| Machine Learning | Practical Efficiency of Muon for Pretraining (Read more on [arXiv](https://arxiv.org/abs/2505.02222) or [HuggingFace](https://huggingface.co/papers/2505.02222))| cadarsh-essential, monk-essential, karlstratos, ampolloreno, ishaan-essential | This paper demonstrates that the Muon optimizer enhances pretraining efficiency by expanding the Pareto frontier over AdamW on the compute-time tradeoff, notably by maintaining data efficiency at large batch sizes. The research primarily aims to validate Muon's practical superiority and to develop an efficient hyperparameter tuning strategy using maximal update parameterization (muP). By comparing Muon and AdamW on transformer models up to 4 billion parameters, the study shows Muon requires 10-15% fewer tokens to reach identical loss levels. Furthermore, a novel "telescoping" algorithm is introduced for muP, enabling near-optimal hyperparameter tuning with modest overhead, allowing, for example, over 20% of the total compute budget for the final model run. The key implication for AI practitioners is the recommendation to use Muon over AdamW for large-scale pretraining, coupled with the telescoping muP method for scalable hyperparameter search. |
| Reinforcement Learning | R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2505.02835) or [HuggingFace](https://huggingface.co/papers/2505.02835))| KevinTowne, KaiyuValley, bhsc24, XingyuLu, yifanzhang114 | This paper introduces R1-Reward, a multimodal reward model developed by applying a novel stable reinforcement learning algorithm, StableReinforce, to enhance long-term reasoning and performance in reward modeling. The primary objective is to investigate the application of reinforcement learning to improve multimodal reward models (MRMs) and to develop a stable RL training method, StableReinforce, that overcomes the limitations of existing RL algorithms for this task. The methodology involves reformulating reward modeling as a rule-based RL task, proposing the StableReinforce algorithm with pre-clipping of loss, advantage filtering, and a consistency-based reward design, and employing a progressive training strategy starting with SFT on 200K collected preference data. R1-Reward, trained with StableReinforce, demonstrates significant improvements, achieving an 8.4% increase on the VL Reward-Bench and a 14.3% increase on the Multimodal Reward Bench compared to previous SOTA models. AI practitioners can leverage the StableReinforce algorithm and the findings to build more robust and capable multimodal reward models, leading to better alignment and performance of MLLMs through more effective RL-based training. |
| Natural Language Processing | A Survey on Inference Engines for Large Language Models: Perspectives on
  Optimization and Efficiency (Read more on [arXiv](https://arxiv.org/abs/2505.01658) or [HuggingFace](https://huggingface.co/papers/2505.01658))| Sungryeol Jeon, leejaymin, Devcow, oos2, inputsh | This paper presents a comprehensive survey and evaluation of 25 open-source and commercial inference engines for Large Language Models (LLMs), focusing on their optimization and efficiency. The main objective is to address the lack of systematic studies by examining each engine's ease-of-use, deployment, general-purpose support, scalability, optimization techniques, and ecosystem maturity. The methodology involves a detailed review and comparison of these engines, categorizing their supported optimization techniques (e.g., batching, parallelism, compression, caching) and hardware compatibility. The evaluation shows a diverse landscape, with specific engines excelling in different areas; for instance, the paper includes performance comparisons for commercial engines showing throughput values like Friendli Inference achieving approximately 275 tokens/sec for DeepSeek-R1 (Figure 8). This survey offers AI practitioners practical guidance for selecting and designing optimized LLM inference engines and provides a public repository to track ongoing developments. |
| Reinforcement Learning | Think on your Feet: Adaptive Thinking via Reinforcement Learning for
  Social Agents (Read more on [arXiv](https://arxiv.org/abs/2505.02156) or [HuggingFace](https://huggingface.co/papers/2505.02156))| Xinghua Zhang, Haobo Wang, bingliwu, Yongbin-Li, iiiiwis | This paper introduces Adaptive Mode Learning (AML), a framework enabling social agents to dynamically adjust reasoning depth using reinforcement learning. The main objective is to equip language agents with adaptive reasoning capabilities in social scenarios, addressing the inefficiency of fixed-depth or thoughtless approaches. AML features four thinking modes and the Adaptive Mode Policy Optimization (AMPO) algorithm, which facilitates multi-granular thinking, context-aware mode switching, and token-efficient reasoning. Key results show AML achieves 15.6% higher task performance than state-of-the-art methods, and AMPO outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. The primary implication for AI practitioners is a method to create more human-like, efficient social agents that can adapt their cognitive load to specific social contexts. |
| Reinforcement Learning | SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from
  Sparse and Noisy Demonstrations (Read more on [arXiv](https://arxiv.org/abs/2505.02094) or [HuggingFace](https://huggingface.co/papers/2505.02094))| Hok Wai Tsui, Yinhuai Wang, cqf, Crimnos, IngridYU | SkillMimic-V2 introduces a novel framework for learning robust and generalizable interaction skills for physically simulated robots from sparse and noisy demonstrations. The primary objective is to overcome limitations of demonstration noise and insufficient coverage in Reinforcement Learning from Interaction Demonstration (RLID). The methodology involves two key data augmentation techniques: Stitched Trajectory Graph (STG) to discover skill transitions and State Transition Field (STF) to connect states in the demonstration neighborhood, complemented by Adaptive Trajectory Sampling (ATS) and a History Encoder for memory-dependent skills. Experiments on datasets like BallPlay-M show significant improvements, with the full method achieving an average Skill Transition Success Rate (TSR) of 93.8% and an average ε-Neighborhood Success Rate (εNSR) of 49.26%, representing substantial gains over baseline methods. This work implies that AI practitioners can achieve more robust and generalizable robotic skill acquisition even with imperfect and limited demonstration data, enhancing the feasibility of learning complex interactions. |
| Reinforcement Learning | Agentic Reasoning and Tool Integration for LLMs via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2505.01441) or [HuggingFace](https://huggingface.co/papers/2505.01441))| akshaynambi, akshaynambi, Raghav2002, joykirat | The paper introduces ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a unified framework leveraging reinforcement learning (RL) to enable Large Language Models (LLMs) to perform agentic reasoning and dynamically integrate external tools. The primary objective is to overcome the limitations of static knowledge and text-only reasoning in LLMs by teaching them to autonomously decide when, how, and which tools to invoke for complex, multi-step problem-solving. ARTIST employs an agentic structure where rollouts alternate between model-generated reasoning and tool interactions, trained using outcome-based Group Relative Policy Optimization (GRPO) with a composite reward function and loss masking for tool outputs. Extensive experiments show ARTIST significantly outperforms baselines, achieving up to a 22% absolute improvement on complex math benchmarks like AMC (Qwen2.5-14B-ARTIST reached 0.55 Pass@1) and more than doubling accuracy on multi-turn function calling tasks. The main implication for AI practitioners is that agentic RL integrated with dynamic tool use offers a promising frontier for building more robust, interpretable, and generalizable LLM-based problem-solving systems. |
| Multi-Modal | SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based
  Image Editing (Read more on [arXiv](https://arxiv.org/abs/2505.02370) or [HuggingFace](https://huggingface.co/papers/2505.02370))| Xin Gu, Zilence006, lionwen, xiaoying0505, limingcv | SuperEdit enhances instruction-based image editing by rectifying supervision signals from instruction-image pairs and facilitating learning through contrastive examples. The primary objective is to improve the faithfulness and quality of edits by addressing noisy supervision in existing datasets and enabling models to better understand complex editing instructions. It utilizes a Vision-Language Model (VLM) like GPT-4o to refine original editing instructions to create 'rectified' versions, and then generates 'wrong' instructions from these rectified ones for contrastive learning with a triplet loss. SuperEdit achieves a 9.19% improvement in overall human evaluation scores on the Real-Edit benchmark compared to the previous state-of-the-art (SmartEdit), while using significantly less pre-training data. This work suggests that focusing on improving the quality of supervision through rectification and contrastive learning can be a highly effective and efficient strategy for advancing multi-modal generative models, particularly in instruction-based image editing. |
| Machine Learning | Low-Precision Training of Large Language Models: Methods, Challenges,
  and Opportunities (Read more on [arXiv](https://arxiv.org/abs/2505.01043) or [HuggingFace](https://huggingface.co/papers/2505.01043))| Li Shen, Guoxia, csdvT, GGJY, Zhiwei840 | This survey offers a comprehensive review of low-precision training methods for Large Language Models (LLMs), addressing the challenges posed by their substantial hardware requirements. The paper's main objective is to systematically organize and review existing low-precision training techniques, categorizing them based on numerical formats, and to identify current challenges and future research opportunities. It categorizes methods into three primary groups: fixed-point and integer-based, floating-point-based, and customized format-based approaches, while also discussing quantization-aware training (QAT) and system-level support. The survey highlights significant gains, such as reducing memory usage by half when switching from FP32 to FP16 precision, and notes specific advancements like Q-GaLore enabling LLaMA-7B model training on only 16 GB of GPU memory. For AI practitioners, this review provides a unified understanding of diverse low-precision training strategies, aiding in the development of more efficient and scalable LLMs by clarifying trade-offs and hardware considerations. |
| Multi-Modal | Ming-Lite-Uni: Advancements in Unified Architecture for Natural
  Multimodal Interaction (Read more on [arXiv](https://arxiv.org/abs/2505.02471) or [HuggingFace](https://huggingface.co/papers/2505.02471))| bear-xxy, jianxinsun, chenjingdong, zhengdd0422, BiaoGong | Ming-Lite-Uni is an open-source unified multimodal framework advancing natural interaction by integrating vision and language generation and understanding. The primary objective is to demonstrate the potential of a unified auto-regressive multimodal model built upon multi-scale learnable tokens with fine-tuned diffusion models, and to accelerate community engagement by open-sourcing the framework. Key methodologies include a novel multi-scale learnable token and representation alignment strategy, a fixed MLLM with a learnable diffusion model fine-tuned via a new connector, and integration of FlowMatching loss in the auto-regressive backbone. Ming-Lite-Uni achieves strong performance, with 0.62 overall accuracy on GenEval for text-to-image generation and a 69.7 average score on OpenCompass multimodal understanding benchmarks. This work provides AI practitioners with an open-source, high-performing unified model architecture and novel techniques for developing more capable multimodal AI systems. |
| Multi-Modal | TEMPURA: Temporal Event Masked Prediction and Understanding for
  Reasoning in Action (Read more on [arXiv](https://arxiv.org/abs/2505.01583) or [HuggingFace](https://huggingface.co/papers/2505.01583))| vibhav-vineet, yilche, wchai, hsiangwei0903, andaba | This paper introduces TEMPURA, a two-stage training framework designed to enhance temporal event understanding and reasoning in action within videos for Large Multi-modal Models (LMMs). The primary objective is to address challenges in comprehending causal event relationships and achieving fine-grained temporal grounding in long videos. TEMPURA's methodology involves a first stage of masked event prediction for inferring missing events and generating causal explanations, followed by a second stage for video segmentation and dense captioning using a newly curated dataset, VER, which contains 500K untrimmed videos with dense event captions. Key results show TEMPURA achieving a mIoU of 39.2 on the Charades-STA benchmark (outperforming the baseline by 6.3 points) and a HIT@1 score of 51.7 on the QVHighlights dataset (surpassing the baseline by 6.9 points). The main implication for AI practitioners is that integrating causal reasoning with fine-grained temporal segmentation can significantly improve video understanding, enabling LMMs to better model temporal dynamics and event dependencies in videos. |
| Multi-Modal | LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive
  Streaming Speech Synthesis (Read more on [arXiv](https://arxiv.org/abs/2505.02625) or [HuggingFace](https://huggingface.co/papers/2505.02625))| Yang Feng, Yan Zhou, zhangshaolei, guoshoutao, poeroz | This paper introduces LLaMA-Omni 2, a series of LLM-based speech language models (0.5B to 14B parameters) designed for real-time, high-quality spoken chatbot interactions with autoregressive streaming speech synthesis. The primary objective is to overcome limitations of traditional cascaded systems and large-data-dependent native SpeechLMs by creating a modular system for efficient speech understanding and generation. LLaMA-Omni 2 integrates a Qwen2.5 LLM with a Whisper speech encoder, a speech adapter, and a novel autoregressive streaming speech decoder which uses a text-to-speech language model (generating speech tokens from fused LLM outputs) followed by a causal flow matching model for mel spectrogram synthesis, trained on only 200K multi-turn speech dialogue samples. LLaMA-Omni 2 demonstrates strong performance, with the 14B model achieving 73.0% S2T accuracy on Llama Questions, and models achieving around 600ms latency (e.g., LLaMA-Omni2-7B at 582.91 ms), surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice. This research provides AI practitioners with a robust and data-efficient modular framework for developing advanced spoken language interfaces, enabling real-time, natural interactions with significantly reduced training data requirements. |
| Multi-Modal | MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset
  via Attention Routing (Read more on [arXiv](https://arxiv.org/abs/2505.02823) or [HuggingFace](https://huggingface.co/papers/2505.02823))| Chong Mou, Pengze Zhang, heqian, yanze, Zinan123212 | MUSAR is a novel framework enabling multi-subject image customization using only single-subject training data by tackling data scarcity and attribute entanglement. The research aims to develop a robust method for generating images with multiple distinct subjects, conditioned on text and reference images, without relying on difficult-to-obtain multi-subject training datasets. Key methodologies include debiased diptych learning to create multi-subject training data from single-subject images while mitigating biases with static attention routing and dual-branch LoRA, and dynamic attention routing to disentangle subjects by adaptively mapping image regions to conditions. MUSAR outperforms existing methods, including those trained on multi-subject datasets, achieving for instance a DINO score of 0.704 and CLIP-I of 0.720 on DreamBench for multi-subject customization. This work provides an effective approach to generate complex, coherent multi-subject images with high fidelity using readily available single-subject data, significantly reducing the data collection burden for customized image generation. |
| Computer Vision | Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural
  Radiance Fields (Read more on [arXiv](https://arxiv.org/abs/2505.02005) or [HuggingFace](https://huggingface.co/papers/2505.02005))| Dan Xu, Xue Xiao, Ping Yin, Zhenxing Mi | This paper introduces Switch-NeRF++, a highly scalable framework employing a Heterogeneous Mixture of Hash Experts (HMoHE) to efficiently learn decomposition and representation for large-scale Neural Radiance Fields (NeRFs). The research aims to overcome limitations in existing large-scale NeRFs by enabling learnable scene decomposition, modeling scene heterogeneity, and enhancing overall modeling efficiency. Switch-NeRF++ utilizes a hash-based gating network to dynamically dispatch 3D points to specialized, heterogeneous hash experts with varying resolution ranges, all optimized end-to-end within a sparse framework. The method achieves state-of-the-art rendering accuracy on large-scale datasets like Mega-NeRF (e.g., PSNR 21.79 on the Building scene) and UrbanBIS, while demonstrating significant efficiency gains, including an 8x training acceleration and 16x rendering acceleration compared to its predecessor Switch-NeRF. For AI practitioners, Switch-NeRF++ provides a robust and efficient approach for high-fidelity 3D modeling of extensive and complex environments, making large-scale NeRF applications more feasible. |
| Multi-Modal | Unlearning Sensitive Information in Multimodal LLMs: Benchmark and
  Attack-Defense Evaluation (Read more on [arXiv](https://arxiv.org/abs/2505.01456) or [HuggingFace](https://huggingface.co/papers/2505.01456))| Jie Peng, Peter Hase, mohitbansal, a2889184, vaidehi99 | This paper introduces UNLOK-VQA, a new benchmark, and an "attack-and-defense" framework to evaluate the unlearning of sensitive information in multimodal Large Language Models (MLLMs). The primary objective is to address the lack of datasets and evaluation frameworks for targeted information deletion in MLLMs, particularly concerning multimodal knowledge, and to assess the robustness of unlearning methods against various attacks. The methodology involves an automated pipeline with manual filtering to create UNLOK-VQA for evaluating efficacy, generalization, and specificity, and an attack-defense framework comprising four whitebox and three blackbox attacks against six unlearning defense objectives, including a novel whitebox attack based on hidden state interpretability. Experimental results demonstrate that multimodal extraction attacks achieve a 45.5% success rate, outperforming image-only (32%) and text-only (39%) attacks, while the best overall defense mechanism reduces multimodal attack success to 15.7%. The main implication for AI practitioners is the highlighted vulnerability of MLLMs to multimodal extraction attacks and the critical need for robust, comprehensively evaluated unlearning methods. |
