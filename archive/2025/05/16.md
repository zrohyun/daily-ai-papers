

## Papers for 2025-05-16

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large
  Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2505.10554) or [HuggingFace](https://huggingface.co/papers/2505.10554))| cxiong, amritasaha87, yuhuixu, hendrydong, zhiyuanhucs | This paper introduces a method for aligning large language models (LLMs) with meta-abilities to improve systematic reasoning. It aims to overcome the limitations of relying on emergent reasoning behaviors by explicitly aligning models with deduction, induction, and abduction via automatically generated, self-verifiable tasks. The approach involves a three-stage pipeline: individual alignment, parameter-space merging, and domain-specific reinforcement learning (RL). Results show a performance boost of over 10% relative to instruction-tuned baselines and a 2% average gain from domain-specific RL on math, coding, and science benchmarks. The main implication is that explicit meta-ability alignment provides a more scalable and reliable foundation for reasoning in LLMs compared to relying on spontaneous behaviors. |
| Natural Language Processing | System Prompt Optimization with Meta-Learning (Read more on [arXiv](https://arxiv.org/abs/2505.09666) or [HuggingFace](https://huggingface.co/papers/2505.09666))| Sung Ju Hwang, jinheon, YuminChoi | This paper introduces bilevel system prompt optimization for large language models (LLMs). It aims to design system prompts robust to diverse user prompts and transferable to unseen tasks. The proposed Meta-level System Prompt Optimizer (MetaSPO) meta-learns system prompts by optimizing them over various user prompts across multiple datasets while iteratively updating the user prompts. Experiments on 14 unseen datasets show MetaSPO produces system prompts that generalize effectively, achieving up to 44.5% average accuracy, demonstrating up to a 12.3% improvement over baseline methods. The optimized system prompt enables rapid adaptation to unseen tasks, requiring fewer optimization steps for test-time user prompts. |
| Computer Vision | EnerVerse-AC: Envisioning Embodied Environments with Action Condition (Read more on [arXiv](https://arxiv.org/abs/2505.09723) or [HuggingFace](https://huggingface.co/papers/2505.09723))| hsli-cuhk, pathcn, thuhsy, Shengcong, YuxinJiang | The paper introduces ENERVERSE-AC (EVAC), an action-conditional world model for generating future visual observations to facilitate robotic imitation learning. It aims to provide realistic and controllable robotic inference, reducing the need for real robots or complex simulations. The key methodology involves a multi-level action-conditioning mechanism and ray map encoding for dynamic multi-view image generation. Experiments show significant improvements in maintaining high fidelity robotic manipulation evaluation while accurately following input action trajectories. EVAC allows for cost-effective and scalable testing and evaluation of robotic policies using action-conditioned video generation. |
| Natural Language Processing | The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a
  Reasoning Model will Think (Read more on [arXiv](https://arxiv.org/abs/2505.10185) or [HuggingFace](https://huggingface.co/papers/2505.10185))| hbin0701, dreamgonfly, Minju2136, seungone, Seongyun | This paper introduces the COT ENCYCLOPEDIA, a framework for analyzing and steering reasoning in large language models. The research aims to provide a bottom-up understanding of reasoning strategies in chain-of-thought (CoT) prompting. The methodology involves automatically extracting reasoning criteria from model-generated CoTs, embedding them into a semantic space, and clustering them into interpretable categories. The framework achieves 92-97% perceived reasonableness in human evaluations and improves performance by 2.5-8.3% across benchmarks by guiding models towards more effective strategies. The main implication is that format-aware model design is crucial for shaping reasoning behaviors in LLMs. |
| Computer Vision | EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied
  World Models (Read more on [arXiv](https://arxiv.org/abs/2505.09694) or [HuggingFace](https://huggingface.co/papers/2505.09694))| sundrops, AutobotZero, pathcn, Shengcong, thuhsy | The paper introduces EWMBENCH, a benchmark for evaluating embodied world models (EWMs) focusing on scene consistency, motion correctness, and semantic alignment. It addresses the challenge of evaluating EWMs beyond general perceptual metrics to ensure physically grounded behaviors. The benchmark includes a curated dataset and evaluation toolkit, leveraging multi-dimensional metrics and video-based MLLMs. Experimental results demonstrate the limitations of existing video generation models in meeting embodied task requirements. The benchmark provides insights to guide future advancements, with the dataset and evaluation tools publicly available. |
| Computer Vision | End-to-End Vision Tokenizer Tuning (Read more on [arXiv](https://arxiv.org/abs/2505.10562) or [HuggingFace](https://huggingface.co/papers/2505.10562))| RobertLuo1, Paranioar, YufengCui, ryanzhangfan, gilnore | The paper introduces End-to-End Vision Tokenizer Tuning (ETT) to improve the performance of vision tokenizers in downstream autoregressive tasks. It addresses the misalignment between vision tokenization optimized for low-level reconstruction and the varied representations required by downstream tasks by enabling joint optimization of vision tokenization and target autoregressive tasks. ETT leverages the visual embeddings of the tokenizer codebook and optimizes vision tokenizers end-to-end with both reconstruction and caption objectives. Experiments show ETT unlocks performance gains of 2-6% for multimodal understanding and visual generation tasks compared to frozen tokenizer baselines. This approach empowers multimodal foundation models beyond image generation and understanding by enabling improved visual representations during tokenization. |
| Machine Learning | MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine
  Learning Engineering (Read more on [arXiv](https://arxiv.org/abs/2505.07782) or [HuggingFace](https://huggingface.co/papers/2505.07782))| percyliang, Solute, yinghaoli-yh, yczhuang, Jerrycool | The paper introduces MLE-Dojo, a Gym-style framework for reinforcement learning, evaluation, and improvement of autonomous LLM agents in machine learning engineering (MLE) workflows.  The main objective is to provide an interactive environment for iterative experimentation and debugging of LLM agents on real-world MLE tasks. MLE-Dojo leverages over 200 Kaggle challenges to enable agent training via supervised fine-tuning and reinforcement learning.  Evaluations of eight frontier LLMs show that current models, although capable of improvements, still exhibit limitations in long-horizon solution generation and error resolution, with Gemini-2.5-Pro achieving the highest HumanRank and Elo scores. The framework facilitates model-based agent tuning and promotes interoperability, scalability, and reproducibility. |
| Natural Language Processing | WorldPM: Scaling Human Preference Modeling (Read more on [arXiv](https://arxiv.org/abs/2505.10527) or [HuggingFace](https://huggingface.co/papers/2505.10527))| Zhenru Zhang, Le Yu, Keming Lu, Runji Lin, Binghai Wang | The paper introduces World Preference Modeling (WorldPM) to investigate scaling laws in preference modeling.  It seeks to determine if similar scaling laws exist in preference modeling as observed in language modeling. The methodology involves training models ranging from 1.5B to 72B parameters on 15M-scale preference data collected from public forums and evaluating their performance across various metrics.  Results show that adversarial metrics scale up with training data, objective metrics show emergent behavior in larger models, and subjective metrics do not show scaling trends; performance gains exceeded 5% on key subtasks. WorldPM provides a foundation for preference fine-tuning, broadly improving generalization performance and offering scalable advantages as training data increases, with notable gains in RLHF pipelines. |
| Natural Language Processing | Achieving Tokenizer Flexibility in Language Models through Heuristic
  Adaptation and Supertoken Learning (Read more on [arXiv](https://arxiv.org/abs/2505.09738) or [HuggingFace](https://huggingface.co/papers/2505.09738))| Vinayak Pahalwan, Shaurya Sharthak, adarshxs, adi-kmt | This paper introduces TokenAdapt, a framework for achieving tokenizer flexibility in language models via heuristic adaptation and supertoken learning. The research aims to mitigate tokenizer lock-in by developing a model-agnostic tokenizer transplantation method. TokenAdapt employs a hybrid heuristic that combines local subword decomposition and global semantic similarity to initialize new tokens. Experiments show that TokenAdapt achieves lower perplexity ratios compared to ReTok and TransTokenizer baselines. The framework enables more efficient adaptation of LLMs to specialized domains by reducing the need for extensive retraining. |
| Computer Vision | Style Customization of Text-to-Vector Generation with Image Diffusion
  Priors (Read more on [arXiv](https://arxiv.org/abs/2505.10558) or [HuggingFace](https://huggingface.co/papers/2505.10558))| Jing Liao, CHERRY-Z, intchous | The paper introduces a novel two-stage pipeline for style customization in text-to-vector (T2V) generation. It addresses the challenge of creating vector graphics with consistent visual appearance while maintaining structural regularity. The method employs a T2V diffusion model trained with path-level representations, coupled with style distillation from customized text-to-image (T2I) diffusion models. Results demonstrate that the approach generates high-quality vector graphics with valid SVG structures and diverse customized styles, achieving a Path FID of 37.51. This approach allows AI practitioners to efficiently generate diverse SVGs in custom styles, improving design workflows. |
| Natural Language Processing | J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.10320) or [HuggingFace](https://huggingface.co/papers/2505.10320))| Xian Li, Ping Yu, Tianlu Wang, Chenxi Whitehouse, swarna92 | The paper introduces J1, a reinforcement learning approach to train LLM-as-a-Judge models for improved judgment ability via incentivized thinking. The research objective is to develop a method for training LLMs to think more effectively in judgment tasks, mitigating bias and improving reasoning. J1 converts prompts into judgment tasks with verifiable rewards, outperforming existing 8B and 70B models, including those distilled from DeepSeek-R1. The primary result is that J1-Llama-70B achieves an overall accuracy of 69.6 on the PPE benchmark, outperforming previous methods. This has implications for AI practitioners by providing a method to improve the evaluation and judgment capabilities of LLMs through reinforcement learning. |
| Multi-Modal | PointArena: Probing Multimodal Grounding Through Language-Guided
  Pointing (Read more on [arXiv](https://arxiv.org/abs/2505.09990) or [HuggingFace](https://huggingface.co/papers/2505.09990))| Boyang Li, Haoquan Fang, Yi Ru Wang, Jiafei Duan, Long Cheng | The paper introduces PointArena, a platform for evaluating multimodal pointing capabilities across diverse reasoning scenarios. It aims to address the limited focus of existing benchmarks on object localization by providing a comprehensive evaluation suite. PointArena consists of Point-Bench, Point-Battle, and Point-Act, enabling static evaluations, human preference comparisons, and real-world robotic task execution, respectively. Results show that Molmo-72B achieves the highest performance on Point-Bench, and supervised training significantly enhances model performance. The study underscores the critical role of precise pointing in bridging abstract reasoning with concrete actions, implying the need for more targeted training and evaluation methodologies for multimodal models. |
| Computer Vision | Depth Anything with Any Prior (Read more on [arXiv](https://arxiv.org/abs/2505.10565) or [HuggingFace](https://huggingface.co/papers/2505.10565))| Ziang Zhang, Jialei Wang, Lihe Yang, Siyu Chen, sleetwang6 | The paper introduces Prior Depth Anything, a framework for generating accurate, dense, and detailed metric depth maps by combining incomplete metric measurements with relative geometric structures. The research aims to integrate complementary depth information sources to overcome limitations of individual depth measurement techniques. The methodology involves a coarse-to-fine pipeline with pixel-level metric alignment and a conditioned monocular depth estimation model. Experiments demonstrate zero-shot generalization across various depth completion tasks, matching or surpassing task-specific methods, with an Absolute Relative difference as low as 1.96 on NYUv2 datasets. This work offers AI practitioners a flexible and robust tool for depth estimation across diverse real-world scenarios and depth prior types, adaptable for future monocular depth estimation model improvements. |
| Multi-Modal | OpenThinkIMG: Learning to Think with Images via Visual Tool
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.08617) or [HuggingFace](https://huggingface.co/papers/2505.08617))| Zhengyuan Yang, Yunzhuo Hao, Mingyang Song, Linjie Li, Zhaochen Su | The paper introduces OPENTHINKIMG, an open-source framework for tool-augmented LVLMs, addressing the lack of standardized infrastructure for integrating visual tools. It proposes V-TOOLRL, a reinforcement learning framework, to enable LVLMs to learn adaptive tool-usage policies by optimizing for task success using feedback from tool interactions.  V-TOOLRL, built on a QWEN2-VL-2B model, achieves a significant improvement of +28.83 points over its SFT-initialized counterpart on chart reasoning tasks and surpasses GPT-4.1 by +8.68 points. The framework aims to advance dynamic, tool-augmented visual reasoning and help the community develop AI agents that can "think with images." The implication is a foundational framework for building next-generation LVLMs with enhanced visual reasoning capabilities through tool augmentation. |
| Computer Vision | AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection (Read more on [arXiv](https://arxiv.org/abs/2505.09926) or [HuggingFace](https://huggingface.co/papers/2505.09926))| Weixi Zhang, Yuezhi Cai, Jiangtao Yan, Yue Zhu, Bin-Bin Gao | This paper introduces AdaptCLIP, a novel method for universal visual anomaly detection, enabling anomaly identification across diverse unseen domains without fine-tuning. AdaptCLIP alternately learns adaptive visual and textual representations using CLIP as a foundational model and incorporates comparative learning between query and normal image prompts. The method achieves state-of-the-art performance on 12 anomaly detection benchmarks, outperforming existing methods by a large margin (e.g., a 10%+ improvement in pixel-level AUPR on industrial benchmarks). AdaptCLIP provides a flexible, efficient, and generalizable solution for anomaly detection in practical, data-scarce scenarios. |
| Computer Vision | ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible
  Long-term Tracking (Read more on [arXiv](https://arxiv.org/abs/2505.08581) or [HuggingFace](https://huggingface.co/papers/2505.08581))| Guanyi Qin, Ziyue Wang, Xuxiao Luo, Mingqi Gao, HeverLaw | The paper introduces ReSurgSAM2, a two-stage framework for referring surgical segmentation via credible long-term tracking in surgical videos. It addresses the limitations of existing methods by leveraging Segment Anything Model 2 (SAM2) with a cross-modal spatial-temporal Mamba network (CSTMamba) and diversity-driven long-term memory (DLM) mechanism. The approach aims to improve segmentation accuracy and efficiency in robotic-assisted surgery. ReSurgSAM2 achieves a real-time performance of 61.2 FPS, demonstrating substantial improvements in accuracy and efficiency compared to existing methods. The implication is that the framework offers a practical and efficient solution for real-time surgical video analysis, enhancing interactive experiences for surgeons. |
| Machine Learning | QuXAI: Explainers for Hybrid Quantum Machine Learning Models (Read more on [arXiv](https://arxiv.org/abs/2505.10167) or [HuggingFace](https://huggingface.co/papers/2505.10167))| Rafiul Islam, Md Jafor Sadek, Shehenaz Khaled, imostafizur, AlignAI | The paper introduces QuXAI, a framework with the Q-MEDLEY explainer, to address the lack of explainability in hybrid quantum-classical machine learning (HQML) models. It focuses on developing robust global and local explainability approaches tailored for HQML architectures that employ quantized feature encoding followed by classical learning. Q-MEDLEY combines Drop-Column Importance and Permutation Importance to assess feature importance by perturbing classical input features and re-evaluating the quantum feature mapping stage. Results demonstrate Q-MEDLEY's ability to delineate influential classical aspects in HQML models and compete against established XAI techniques, achieving high Recall@3 scores in classical ML validation settings. The framework provides a route to improve the interpretability and reliability of HQML models, facilitating safer and more responsible use of quantum-enhanced AI technology. |
| Multi-Modal | Exploring the Deep Fusion of Large Language Models and Diffusion
  Transformers for Text-to-Image Synthesis (Read more on [arXiv](https://arxiv.org/abs/2505.10046) or [HuggingFace](https://huggingface.co/papers/2505.10046))| Saining Xie, Sayak Paul, Xichen Pan, Boyang Zheng, Bingda Tang | This paper explores the deep fusion of large language models (LLMs) and diffusion transformers (DiTs) for text-to-image synthesis. The research investigates the design space of deeply fusing LLMs and DiTs to improve multi-modal generation, addressing limitations in previous studies. The study conducts controlled comparisons with shallow fusion baselines, analyzes timestep conditioning and positional encoding strategies, and introduces a scalable training recipe. Results show that a model called FuseDiT achieves a GenEval score of 0.60 and FID of 7.54. The findings provide practical guidelines for future research and development in multi-modal generation architectures. |
| Machine Learning | Parallel Scaling Law for Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.10475) or [HuggingFace](https://huggingface.co/papers/2505.10475))| Dayiheng Liu, Jiaxi Yang, Zeyu Cui, Binyuan Hui, Mouxiang Chen | The paper introduces parallel scaling (PARSCALE), a novel approach to scaling language models by increasing parallel computation during training and inference while reusing existing parameters. It explores the question of whether model capacity is determined by parameters or computation. PARSCALE applies diverse, learnable transformations to input, executes parallel forward passes, and dynamically aggregates outputs, showing that P parallel streams are similar to scaling parameters by O(log P). Experiments demonstrate that PARSCALE can achieve comparable performance gains to parameter scaling with up to 22x less memory increase and 6x less latency increase, facilitating deployment in low-resource scenarios. The discovered scaling law may facilitate more powerful models in low-resource scenarios. |
| Computer Vision | MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning (Read more on [arXiv](https://arxiv.org/abs/2505.09265) or [HuggingFace](https://huggingface.co/papers/2505.09265))| csgaobb | The paper introduces MetaUAS, a novel approach for universal anomaly segmentation using one-prompt meta-learning. It unifies anomaly segmentation with change segmentation by leveraging synthetic image pairs to train a pure vision model. The method incorporates a soft feature alignment module to handle geometrical variations between prompt and query images. Experimental results on industrial anomaly benchmarks demonstrate state-of-the-art performance, achieving, for example, 97.6% I-ROC score on MVTec. MetaUAS offers AI practitioners an efficient and effective solution for anomaly segmentation without relying on vision-language models or specialized anomaly datasets. |
| Computer Vision | Learning to Detect Multi-class Anomalies with Just One Normal Image
  Prompt (Read more on [arXiv](https://arxiv.org/abs/2505.09264) or [HuggingFace](https://huggingface.co/papers/2505.09264))| csgaobb | The paper introduces OneNIP, a novel framework for unified multi-class anomaly detection using only one normal image prompt. It addresses the limitations of self-attention reconstruction models by reconstructing normal features and restoring anomaly features with the normal image prompt. The methodology involves an unsupervised reconstruction and restoration network, along with a supervised refiner to enhance anomaly segmentation. OneNIP outperforms previous methods on MVTec, BTAD, and VisA datasets, achieving a pixel-level anomaly segmentation of 63.7% on the MVTec benchmark. OneNIP offers AI practitioners an effective and efficient method for anomaly detection with improved generalization and reduced data requirements. |
| Computer Vision | Few-Shot Anomaly-Driven Generation for Anomaly Classification and
  Segmentation (Read more on [arXiv](https://arxiv.org/abs/2505.09263) or [HuggingFace](https://huggingface.co/papers/2505.09263))| Yunsheng Wu, Chengjie Wang, Jun Liu, Guan Gui, csgaobb | The paper introduces AnoGen, a few-shot anomaly-driven generation method for anomaly detection tasks. It addresses the scarcity of real anomaly samples by guiding a diffusion model to generate realistic and diverse anomalies from a limited set of real examples. The method optimizes an embedding vector to represent the anomaly distribution and uses bounding boxes to control the generated anomaly regions. Experiments on MVTec demonstrate that models trained with generated anomalies achieve improved performance, with DRAEM achieving a 5.8% improvement in AU-PR metric on segmentation. The work enables practitioners to enhance anomaly detection model training with synthetically generated data that is semantically consistent with real-world anomalies. |
