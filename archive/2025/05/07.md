

## Papers for 2025-05-07

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Unified Multimodal Chain-of-Thought Reward Model through Reinforcement
  Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2505.03318) or [HuggingFace](https://huggingface.co/papers/2505.03318))| Qinglin Lu, Chunyu Wang, Zhimin Li, Yibin Wang, yuhangzang | This paper introduces UNIFIEDREWARD-THINK, a novel unified multimodal Chain-of-Thought (CoT) based reward model that leverages reinforcement fine-tuning to enhance the accuracy and interpretability of reward signals for both visual understanding and generation tasks. The primary objective is to overcome the limitations of existing multimodal reward models, which typically provide direct responses or shallow reasoning, by incorporating explicit, long-chain CoT reasoning to improve their reliability and robustness. The methodology involves a three-stage training pipeline: a "Cold Start" phase using distilled GPT-4o data to learn CoT formatting, "Rejection Sampling" on large-scale preference data to generalize CoT reasoning, and "Group Relative Policy Optimization (GRPO)" based reinforcement fine-tuning using verifiable rewards (format and accuracy) to optimize for robust reasoning paths. UNIFIEDREWARD-THINK demonstrates significant improvements, achieving an overall accuracy of 73.8% on the VLRewardBench for image understanding (compared to 67.5% for its base model UnifiedReward) and outperforming UnifiedReward in image generation (72.5% vs 70.9% diff on GenAI-Bench). The main implication for AI practitioners is that explicitly training multimodal reward models for long-chain CoT reasoning via reinforcement learning can lead to more accurate, robust, and interpretable reward signals, fostering more trustworthy human-aligned AI systems. |
| Reinforcement Learning | Absolute Zero: Reinforced Self-play Reasoning with Zero Data (Read more on [arXiv](https://arxiv.org/abs/2505.03335) or [HuggingFace](https://huggingface.co/papers/2505.03335))| Andrew Zhao, zlzheng, shenzhi-wang, Yang130, kevinwyr | This paper introduces "Absolute Zero," a novel reinforcement learning paradigm where a model learns reasoning by autonomously proposing and solving tasks entirely without external human-curated data. The primary objective is to overcome the scalability limitations of supervised data in training advanced reasoning models and to explore self-improving AI systems. The key methodology involves the Absolute Zero Reasoner (AZR), a system that self-evolves by generating coding tasks (abduction, deduction, induction) and using a code executor for task validation and verifiable reward signals. Despite zero external data, AZR achieves state-of-the-art (SOTA) performance, outperforming existing zero-setting models on combined coding and mathematical reasoning tasks by an average of 1.8 absolute points overall; for example, AZR-Coder-7B improved its math average by 15.2 points. This work implies that robust general reasoning capabilities can emerge and be significantly enhanced through pure self-play, offering a scalable path towards more autonomous and powerful AI systems. |
| Computer Vision | FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios (Read more on [arXiv](https://arxiv.org/abs/2505.03730) or [HuggingFace](https://huggingface.co/papers/2505.03730))| Yansong Tang, Ying Shan, Zhaoyang Zhang, Shiyi Zhang, JunhaoZhuang | The paper introduces FlexiAct, a novel framework for flexible action transfer from a reference video to an arbitrary target image, effectively handling heterogeneous scenarios with varying spatial structures and cross-domain subjects while maintaining identity consistency. Its primary objective is to overcome the limitations of existing methods that require strict spatial alignment, by enabling robust motion adaptation and appearance preservation across diverse subjects. FlexiAct achieves this using two key components: RefAdapter, a lightweight adapter for spatial adaptation and consistency, and Frequency-aware Action Extraction (FAE), which dynamically extracts motion by modulating attention to frequency-specific embeddings during the denoising process. Experimental results demonstrate FlexiAct's superiority, achieving a Motion Fidelity score of 0.4103 and a 79.5% preference in human evaluations for motion consistency over baseline models. For AI practitioners, FlexiAct offers a more versatile and effective method for action customization in video generation, especially in scenarios with significant differences between reference and target subjects, without relying on explicit pose guidance. |
| Natural Language Processing | RADLADS: Rapid Attention Distillation to Linear Attention Decoders at
  Scale (Read more on [arXiv](https://arxiv.org/abs/2505.03005) or [HuggingFace](https://huggingface.co/papers/2505.03005))| Eugene Cheah, Janna Lu, Eric Alcaide, SmerkyG | This paper introduces RADLADS, a rapid and cost-effective protocol for converting large softmax attention transformers into high-performing linear attention decoder models, demonstrated with new RWKV-variants and converted Qwen2.5 models. The primary objective is to enable efficient inference for large language models by converting them to linear attention architectures using minimal training data (350-700M tokens) and cost, while retaining close-to-original performance. The RADLADS methodology comprises a three-step process: attention weight transfer, attention hidden state alignment, knowledge distillation, followed by optional fine-tuning, and leverages new custom RAD-RWKV architectures. Converted models, such as the QRWKV6-72B-Instruct, achieve performance close to their teacher models, for instance, retaining 89.9% of the teacher's MMLU score (adjusted for random chance). RADLADS offers AI practitioners a practical pathway to develop and deploy large-scale, computationally efficient linear attention models, facilitating broader research and application of these architectures. |
| Machine Learning | RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM
  Inference (Read more on [arXiv](https://arxiv.org/abs/2505.02922) or [HuggingFace](https://huggingface.co/papers/2505.02922))| Chengruidong Zhang, Jinkai Zhang, Yaoqi Chen, qianxizhang, baotonglu | RetroInfer is a novel system that enhances scalable long-context LLM inference by reconceptualizing the Key-Value (KV) cache as an attention-aware vector storage system. The primary research objective is to overcome GPU memory and bandwidth constraints in long-context LLM inference by efficiently exploiting attention sparsity while maintaining model accuracy. Its key methodology involves the 'wave index' for critical token retrieval using tripartite attention approximation and segmented clustering, and a 'wave buffer' for managing KV cache placement across GPU/CPU and overlapping computation with data transfer. Experiments show RetroInfer achieves up to 4.5x speedup over full attention within GPU memory limits and up to 10.5x over sparse attention baselines with CPU offloading, all while preserving full-attention-level accuracy. This system provides AI practitioners with a robust approach to deploy LLMs with very long contexts more efficiently, significantly improving throughput and scalability. |
| Multi-Modal | Decoding Open-Ended Information Seeking Goals from Eye Movements in
  Reading (Read more on [arXiv](https://arxiv.org/abs/2505.02872) or [HuggingFace](https://huggingface.co/papers/2505.02872))| Yevgeni Berzak, Yoav Meiri, Omer Shubi, Cfir Avraham Hadar | This paper investigates the automatic decoding of open-ended, text-specific information-seeking goals from eye movements recorded during reading. The central research question is whether arbitrary, text-specific questions a reader has in mind can be decoded from their eye movements over a passage. The study introduces goal classification and reconstruction tasks, utilizing the OneStop eye-tracking dataset and developing discriminative (e.g., RoBERTEye-Fixations) and generative (e.g., DalEye-Llama) multimodal LLMs that integrate textual content with eye movement features. RoBERTEye-Fixations achieved the highest classification accuracy, reaching 49.3% on a three-way task (chance 33.0%) and 57.3% for distinguishing between questions over the same text portion (chance 49.9%), while generative models showed meaningful progress in the challenging reconstruction task. This research demonstrates that LLMs can leverage eye movements to infer reader-specific goals, suggesting potential for real-world applications in personalized education, content adaptation, and accessibility. |
| Machine Learning | An Empirical Study of Qwen3 Quantization (Read more on [arXiv](https://arxiv.org/abs/2505.02214) or [HuggingFace](https://huggingface.co/papers/2505.02214))| Xudong Ma, Yue Feng, Yuye Li, HaoranChu, Xingyu-Zheng | This paper empirically evaluates the performance of the Qwen3 large language model series under various post-training quantization (PTQ) techniques and bit-widths. The study aims to systematically assess Qwen3's robustness to low-bit quantization, identifying performance trade-offs, optimal methods for specific bit-widths, and challenges in extreme compression. The authors applied five classic PTQ methods (Round-To-Nearest, GPTQ, AWQ, SmoothQuant, BiLLM) to Qwen3 models (ranging from 0.6B to 72B parameters) across bit-widths from 1 to 8 bits, evaluating them on perplexity, commonsense reasoning, and MMLU benchmarks. Primary results indicate that while Qwen3 maintains near-lossless performance at 8-bit quantization, it exhibits noticeable degradation at 4-bits (e.g., Qwen-8B's MMLU score drops from 74.7 to 69.3 with 4-bit weight-only quantization), and experiences significant performance loss at 3-bits or lower, showing more sensitivity than models like LLaMA3. The main implication for AI practitioners is that Qwen3's advanced pre-training, leading to less redundancy, makes it more susceptible to quantization-induced information loss, underscoring the need for improved quantization methods for state-of-the-art LLMs. |
| Multi-Modal | Multi-Agent System for Comprehensive Soccer Understanding (Read more on [arXiv](https://arxiv.org/abs/2505.03735) or [HuggingFace](https://huggingface.co/papers/2505.03735))| Yanfeng Wang, Ya Zhang, Zifeng Li, haoningwu, Homie0609 | This paper introduces SoccerAgent, a multi-agent system, alongside SoccerWiki (a multimodal knowledge base) and SoccerBench (a benchmark), to enable comprehensive soccer understanding through knowledge-driven reasoning. The primary objective is to overcome the limitations of existing AI research in soccer, which typically focuses on isolated tasks, by proposing a holistic framework that integrates multimodal data (text, image, video) and domain knowledge. The methodology involves constructing SoccerWiki with information on players, teams, and matches; developing the SoccerBench benchmark featuring around 10,000 multimodal multi-choice QA pairs across 13 distinct tasks; and designing SoccerAgent, which leverages 18 specialized tools to decompose questions and reason over the knowledge base. SoccerAgent demonstrates superior performance on SoccerBench, achieving an overall accuracy of 73.3%, significantly outperforming 11 representative Multimodal Large Language Models, for instance, achieving 85.0% on TextQA tasks. This research provides AI practitioners with a robust benchmark and an agentic system architecture for developing more comprehensive and knowledge-aware sports understanding systems, moving beyond narrow, task-specific models. |
| Machine Learning | Geospatial Mechanistic Interpretability of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.03368) or [HuggingFace](https://huggingface.co/papers/2505.03368))| Kevin Roitero, Stefano Mizzaro, sdesabbata | This paper introduces a novel framework for geospatial mechanistic interpretability to understand how Large Language Models (LLMs) internally process and represent geographical information. The primary objective is to investigate the internal representations LLMs generate for geographical data and to develop methods for making these representations more interpretable, specifically testing if these representations exhibit spatial patterns. The methodology involves extracting activations from LLM layers (e.g., Mistral-7B-Instruct-v0.2) in response to placename prompts, applying spatial autocorrelation (Moran's I) to these activations, and further using sparse autoencoders to disentangle polysemantic representations into more interpretable, monosemantic features, which are also analyzed for spatial patterns. Primary results showed that while 14.98% of raw neurons exhibited significant spatial autocorrelation (p < .01, Moran's I ≥ 0.3), only 0.2% (67 out of 32,768) of features derived from a sparse autoencoder (layer 15) did so, with 99.53% of these features outputting zero for all placenames, indicating a sparse and diffuse encoding of geographical information potentially requiring further refinement in decomposition. The main implication for AI practitioners is that geographical concepts in LLMs are often encoded polysemantically, but mechanistic interpretability techniques like sparse autoencoders, combined with spatial analysis, offer a pathway to uncover and interpret these geospatial representations, crucial for developing more robust and reliable GeoAI applications. |
| Multi-Modal | InfoVids: Reimagining the Viewer Experience with Alternative
  Visualization-Presenter Relationships (Read more on [arXiv](https://arxiv.org/abs/2505.03164) or [HuggingFace](https://huggingface.co/papers/2505.03164))| Kevin Hsu, Ivy Chen, Tongyu Zhou, Ji Won Chung, Franck-Dernoncourt | This paper introduces "InfoVids," a novel mobile AR paradigm integrating data visualizations and presenters in a shared 3D space to create more engaging and equitable viewer-presenter relationships. The primary research objective was to investigate how these co-located and interactive InfoVid setups affect viewer experience, including engagement, attention, and perception of the presenter, compared to traditional 2D slide-based formats. The methodology involved designing four InfoVid case studies using a custom "Body Object Model" (BOM) with ARKit, followed by a mixed-methods study where 30 participants compared these InfoVids to their baseline 2D equivalents on 9 metrics via surveys and semi-structured interviews. Key findings indicate InfoVids significantly enhanced perceived presenter immersion (e.g., for AirplaneVis, 28/30 participants favored the InfoVid format, p<0.001) and engagement, and notably shifted viewer attention from the visualization towards the presenter (e.g., 16/30 participants switched focus to the presenter for AirplaneVis). The main implication for AI practitioners is that leveraging such integrated multi-modal AR experiences can foster more human-centric data storytelling, provided that presenter-visualization interactions and social expectations are carefully designed. |
| Multi-Modal | VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient
  Large Speech-Language Model (Read more on [arXiv](https://arxiv.org/abs/2505.03739) or [HuggingFace](https://huggingface.co/papers/2505.03739))| Lijiang Li, Heting Gao, Chaoyou Fu, Yunhang Shen, Zuwei Long | The paper introduces VITA-Audio, an end-to-end large speech-language model designed for efficient, fast interleaved cross-modal token generation to enable real-time speech interaction. Its primary objective is to overcome the high first-token latency in speech generation by enabling audio output within the initial forward pass of the language model. VITA-Audio employs lightweight Multiple Cross-modal Token Prediction (MCTP) modules that generate multiple audio tokens directly from LLM hidden states in a single forward pass, complemented by a four-stage progressive training strategy. Experimental results demonstrate a 3-5x inference speedup on a 7B LLM, with the 'Turbo' mode achieving approximately 4.72x speedup, and significantly reducing the first audio token chunk generation time from 236ms to 53ms under specific conditions. This work provides AI practitioners with an open-source, reproducible model that improves efficiency and reduces latency for speech-to-speech systems, outperforming existing models on ASR, TTS, and SQA tasks. |
| Natural Language Processing | Invoke Interfaces Only When Needed: Adaptive Invocation for Large
  Language Models in Question Answering (Read more on [arXiv](https://arxiv.org/abs/2505.02311) or [HuggingFace](https://huggingface.co/papers/2505.02311))| Biao Qin, Chunlai Zhou, Robot2050 | This paper introduces AttenHScore, a novel metric for adaptive Large Language Model (LLM) invocation in Question Answering (QA) by detecting hallucinations in Small Language Models (SLMs) to balance performance and cost. The main objective is to precisely determine when to invoke an LLM by identifying hallucination accumulation in SLMs during their generation process. The key methodology involves AttenHScore, which quantifies hallucination propagation using attention mechanisms and token probabilities, alongside an uncertainty-aware knowledge re-ranking strategy for SLMs. Experiments demonstrate that AttenHScore significantly outperforms baselines in hallucination detection (e.g., achieving an AUCS of 0.8715 on SQuAD with Llama3-8B) and improves collaborative QA F1 scores. This provides AI practitioners with an unsupervised, real-time, and plug-and-play method to optimize LLM usage and reduce costs without requiring additional model training. |
| Multi-Modal | HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene
  Generation (Read more on [arXiv](https://arxiv.org/abs/2504.21650) or [HuggingFace](https://huggingface.co/papers/2504.21650))| Yonghong Tian, Xinhua Cheng, Jiawen Guan, Haiyang Zhou, Drexubery | HoloTime introduces a framework for generating immersive panoramic 4D scenes from single panoramic images or text prompts. The primary objective is to overcome limitations in existing diffusion models for creating dynamic, scene-level 360-degree 4D experiences. The methodology involves "Panoramic Animator," a diffusion model trained on a new "360World" dataset to generate panoramic videos from images or prompts, and "Panoramic Space-Time Reconstruction," which converts these videos into 4D Gaussian Splatting representations via space-time depth estimation. In image-driven panoramic video generation, HoloTime achieved superior quantitative results, such as a Temporal Flickering score of 0.9864 (VBench) and a GPT4o MTScore of 2.4111 (ChronoMagic-Bench). This work enables AI practitioners to create more engaging and realistic 4D assets for VR/AR by advancing panoramic video generation and 4D reconstruction from potentially diverse inputs. |
| Natural Language Processing | Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in
  Smart Personal Assistant (Read more on [arXiv](https://arxiv.org/abs/2504.18373) or [HuggingFace](https://huggingface.co/papers/2504.18373))| Xiaoyu Shen, lorashen | The paper introduces Auto-SLURP, a novel benchmark dataset designed to evaluate the end-to-end performance of LLM-powered multi-agent frameworks in the context of smart personal assistants. The primary objective is to address the lack of standardized benchmarks for assessing these multi-agent frameworks by providing a comprehensive dataset covering language understanding, task execution, and response generation. Auto-SLURP extends the existing SLURP dataset through meticulous re-labeling of slots and the integration of simulated backend servers and external APIs, enabling evaluation of a defined multi-agent workflow involving specialized agents for intent recognition, parameter formatting, and tool execution. Experiments show AgentLite achieved the highest accuracy of 0.46 among tested frameworks using GPT-4; notably, finetuning a LLaMA-3 8B model for the intent agent within the AutoGen framework improved its overall task success rate from 0.40 to 0.62. The results highlight that current multi-agent frameworks still face significant challenges with complex personal assistant tasks, underscoring the need for advancements in agent orchestration and reasoning, and emphasizing the critical role of accurate intent understanding for overall system reliability. |
