

## Papers for 2025-05-09

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | On Path to Multimodal Generalist: General-Level and General-Bench (Read more on [arXiv](https://arxiv.org/abs/2505.04620) or [HuggingFace](https://huggingface.co/papers/2505.04620))| Gh0stAR, ChocoWu, LXT, JunchengLi, scofield7419 | The paper introduces an evaluation framework called General-Level to assess the capabilities of multimodal large language models (MLLMs) and presents a large multimodal benchmark, General-Bench. It questions the assumption that higher performance across tasks directly implies stronger MLLM capability and aims to delineate the capabilities and behaviors of current multimodal generalists. The methodology involves establishing a 5-scale taxonomy of MLLM performance based on synergy across comprehension, generation, and multimodal interactions, and utilizes General-Bench, encompassing over 700 tasks and 325,800 instances, to evaluate MLLMs. The evaluation of over 100 MLLMs reveals that most models lack cross-task or cross-modal synergy, with no model yet enhancing language intelligence through non-language modalities, and GPT-4V achieves about 65% task support on image comprehension, but the best open-sourced model, InternVL2.5-8B, achieves 71% task support. The project aims to pave the way for future research on next-generation multimodal foundation models to accelerate the realization of AGI. |
| Computer Vision | Flow-GRPO: Training Flow Matching Models via Online RL (Read more on [arXiv](https://arxiv.org/abs/2505.05470) or [HuggingFace](https://huggingface.co/papers/2505.05470))| dizhang, Xintao, CheeryLJH, Lp256, liuhuohuo | This paper introduces Flow-GRPO, a novel method that integrates online reinforcement learning (RL) with flow matching models for improved image generation. The research aims to enhance the reasoning and compositional abilities of flow matching models, especially in complex scenes and text rendering. Flow-GRPO employs ODE-to-SDE conversion for statistical sampling and a denoising reduction strategy for efficient training, using Group Relative Policy Optimization (GRPO) as the RL method. Empirically, Flow-GRPO significantly improves GenEval accuracy on complex compositions from 63% to 95% and visual text rendering from 59% to 92% with SD3.5-Medium. The integration of online RL offers AI practitioners a way to refine flow-matching models, enhancing their ability to generate high-quality images with improved compositional accuracy and text rendering capabilities while maintaining image quality. |
| Natural Language Processing | Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in
  Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.02847) or [HuggingFace](https://huggingface.co/papers/2505.02847))| Peisong Wang, Qingxuan Jiang, Bang Zhang, zptu, vvibt | This paper introduces Sentient Agent as a Judge (SAGE), an automated evaluation framework designed to measure higher-order social cognition in Large Language Models (LLMs). The primary objective is to assess how well LLMs understand human emotional and cognitive states beyond mere textual comprehension. SAGE employs a Sentient Agent that simulates human-like emotional changes and inner thoughts during multi-turn dialogues, reasoning about its emotional trajectory and generating contextually appropriate responses. Key results show a strong correlation between SAGE's final Sentient emotion score and established psychological metrics like the Barrett–Lennard Relationship Inventory (BLRI) (Pearson r = 0.82), and the Sentient Leaderboard uncovers performance gaps (e.g., GPT-40-Latest achieved a Sentient score of 79.9, significantly outperforming earlier baselines). For AI practitioners, SAGE offers a scalable and interpretable tool to benchmark and advance the development of more empathetic and socially adept language agents. |
| Machine Learning | Scalable Chain of Thoughts via Elastic Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.05315) or [HuggingFace](https://huggingface.co/papers/2505.05315))| cxiong, JunnanLi, doyensahoo, hendrydong, yuhuixu | The paper introduces Elastic Reasoning, a framework to enable Large Reasoning Models (LRMs) to produce scalable and efficient Chain-of-Thought (CoT) outputs under strict inference-time budgets. The objective is to improve LRM reliability under tight resource constraints by explicitly separating reasoning into 'thinking' and 'solution' phases with independent budgets and training models to be robust to truncated thinking. Key methodology employs a budget-constrained rollout strategy integrated into GRPO (an RL algorithm) during training, teaching the model to reason adaptively when the thinking process is cut short and generalize to unseen budget constraints. Empirical results show the E1-Math-1.5B model achieved 35.0% accuracy on the AIME2024 benchmark under budget constraints, outperforming L1-Max (27.1%), and also reduced average token usage by 32.1% on AIME2024 in unconstrained settings compared to its base model. This framework offers a practical approach for deploying LRMs in budget-limited applications by improving concise reasoning and robustness to truncation, without substantial performance loss and with lower training costs than some baselines. |
| Multi-Modal | FG-CLIP: Fine-Grained Visual and Textual Alignment (Read more on [arXiv](https://arxiv.org/abs/2505.05071) or [HuggingFace](https://huggingface.co/papers/2505.05071))| DaweiLiang, jinchenglijc, fanjing, binwang, xiechunyu | The paper introduces Fine-Grained CLIP (FG-CLIP), a novel approach designed to significantly enhance fine-grained visual and textual alignment in multimodal models. FG-CLIP aims to overcome the limitations of existing models like CLIP, which struggle with detailed understanding due to their reliance on coarse-grained, global image-text associations and short captions. The methodology involves three key innovations: generating 1.6 billion long caption-image pairs using Large Multimodal Models, creating a dataset of 12 million images with 40 million region-specific bounding box-caption alignments, and incorporating 10 million hard fine-grained negative samples, all integrated within a two-stage contrastive learning framework. Extensive experiments demonstrate FG-CLIP's superiority; for instance, on the FG-OVD "hard" benchmark, FG-CLIP (ViT-L/14) achieves 48.4% accuracy, significantly outperforming the original CLIP's 15.4%. FG-CLIP offers AI practitioners a more powerful tool for tasks requiring nuanced multimodal understanding, providing improved capabilities in fine-grained image detail capture and better overall model performance across various downstream applications. |
| Computer Vision | 3D Scene Generation: A Survey (Read more on [arXiv](https://arxiv.org/abs/2505.05474) or [HuggingFace](https://huggingface.co/papers/2505.05474))| Fangzhou Hong, liuziwei7, FrozenBurning, hzxie, wenbc21 | This paper presents a comprehensive survey of 3D scene generation, systematically organizing state-of-the-art approaches and discussing their foundations, trade-offs, datasets, and applications. The primary objective is to provide a structured overview of the evolving landscape of 3D scene generation, identify current limitations, and highlight promising future research directions at the intersection of generative AI, 3D vision, and embodied intelligence. The survey categorizes methods into four main paradigms—procedural, neural 3D-based, image-based, and video-based generation—analyzing their technical underpinnings, representative results, common datasets, and evaluation protocols. It highlights a significant growth in the field, noting a surge in video-based generation research which, by April 30th, 2025, accounted for 61 publications, surpassing other paradigms like neural 3D-based (20 publications) for that period as shown in Figure 1. For AI practitioners, this review offers a crucial guide to understanding challenges in generation capacity, 3D representations, data, and evaluation, while pointing towards future work in higher fidelity, physics-aware interactive generation, and unified perception-generation models. |
| Natural Language Processing | ICon: In-Context Contribution for Automatic Data Selection (Read more on [arXiv](https://arxiv.org/abs/2505.05327) or [HuggingFace](https://huggingface.co/papers/2505.05327))| Zhifang Sui, soliz1998, yaolily, Rsy24, yyxsghx | This paper introduces ICON, a novel method for automatic data selection in instruction tuning. ICON leverages in-context learning to measure the contribution of individual training samples without gradient computation or human-designed heuristics. It identifies high-contribution data by assessing performance shifts under implicit learning through ICL, achieving computational efficiency and reduced inductive bias. Experiments on LLaMA3.1-8B showed that models trained on 15% of ICON-selected data outperform full datasets by 5.42% points. ICON offers a more efficient and less biased approach to data selection for improving LLM performance. |
| Computer Vision | LiftFeat: 3D Geometry-Aware Local Feature Matching (Read more on [arXiv](https://arxiv.org/abs/2505.03422) or [HuggingFace](https://huggingface.co/papers/2505.03422))| Jinchi Zhu, Yuxuan Xiong, Zhou Zhao, Wenpeng Lai, pengliu123 | The paper introduces LiftFeat, a lightweight network for robust local feature matching by incorporating 3D geometric information. It addresses the challenge of feature matching in extreme conditions by leveraging a pre-trained monocular depth estimation model to extract pseudo surface normal labels, which are then fused with 2D descriptors using a 3D Geometry-aware Feature Lifting module. The methodology enhances the discriminative ability of 2D features by integrating 3D surface normal cues. Experiments on relative pose estimation, homography estimation, and visual localization demonstrate improved performance, with the model achieving AUC@5 of 44.7 on MegaDepth-1500. LiftFeat offers a practical approach for improving feature matching robustness in robotics applications, especially on resource-constrained platforms due to its low inference latency of 7.4 ms on edge devices. |
| Multi-Modal | StreamBridge: Turning Your Offline Video Large Language Model into a
  Proactive Streaming Assistant (Read more on [arXiv](https://arxiv.org/abs/2505.05467) or [HuggingFace](https://huggingface.co/papers/2505.05467))| Bo Feng, afshindn, ShiyuLi, jefflai, WHB139426 | This paper introduces StreamBridge, a framework to transform offline Video Large Language Models (Video-LLMs) into proactive streaming assistants. The main objective is to enable existing Video-LLMs to handle multi-turn real-time understanding and generate proactive responses in online streaming scenarios. StreamBridge achieves this through a memory buffer with round-decayed compression for long-context interactions and a decoupled, lightweight activation model for proactive responses, complemented by a new dataset called Stream-IT. Experiments show that StreamBridge significantly improves streaming capabilities, with models like Qwen2-VL-7B + Stream-IT achieving 71.30 on OVO-Bench and 77.04 on Streaming-Bench, outperforming proprietary models like GPT-40. This provides AI practitioners with a method to adapt powerful offline Video-LLMs for dynamic, interactive streaming applications without requiring full retraining. |
| Multi-Modal | X-Reasoner: Towards Generalizable Reasoning Across Modalities and
  Domains (Read more on [arXiv](https://arxiv.org/abs/2505.03981) or [HuggingFace](https://huggingface.co/papers/2505.03981))| RustyArchimedes, sidkiblawi, hiaoxui, shengz, qianchu | This paper introduces X-REASONER, a vision-language model post-trained exclusively on general-domain text, demonstrating strong generalizable reasoning across different modalities and specialized domains like medicine. The central research question is whether reasoning capabilities can be generalized across modalities and domains through general-domain text-based post-training. X-REASONER employs a two-stage approach: supervised fine-tuning (SFT) with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards (RLVR), using only general-domain text. Experiments show X-REASONER (7B) achieves 56.4% accuracy on MMMU (Val), outperforming prior SOTA models trained with in-domain multimodal data; its medical variant, X-REASONER-MED, achieved 60.0% on MMLU-Pro-H. The main implication is that general-domain text-based post-training can effectively instill robust reasoning capabilities that transfer to multimodal and out-of-domain settings, potentially reducing reliance on extensive multimodal or domain-specific data for reasoning tasks. |
| Multi-Modal | Generating Physically Stable and Buildable LEGO Designs from Text (Read more on [arXiv](https://arxiv.org/abs/2505.05469) or [HuggingFace](https://huggingface.co/papers/2505.05469))| junyanz, devakramanan, RLCMU, kangled, AvaLovelace | LEGOGPT is a novel approach for generating physically stable and buildable LEGO designs directly from text prompts. The primary objective is to develop an end-to-end system that translates freeform text into 3D LEGO models while ensuring their physical stability and real-world constructibility. The methodology involves fine-tuning an autoregressive large language model (LLaMA-3.2-Instruct-1B) on a custom, large-scale dataset called StableText2Lego, and then employing efficient validity checks and a physics-aware rollback mechanism during inference to prune infeasible brick predictions. LEGOGPT achieves 100% design validity and 98.8% physical stability, significantly outperforming baseline methods while maintaining strong text-design alignment (CLIP score of 0.324). This work provides AI practitioners with a framework for integrating physical constraints into generative models, paving the way for creating physically realizable objects from textual descriptions. |
| Natural Language Processing | Crosslingual Reasoning through Test-Time Scaling (Read more on [arXiv](https://arxiv.org/abs/2505.05408) or [HuggingFace](https://huggingface.co/papers/2505.05408))| JuliaKreutzerCohere, gentaiscool, Muennighoff, MJonibek, yongzx | This paper investigates the crosslingual generalization of English-centric reasoning language models (RLMs) when inference compute for long chain-of-thoughts (CoTs) is scaled up at test time. The main objective is to determine how effectively English reasoning finetuning generalizes to multilingual reasoning tasks, particularly mathematical reasoning, and how test-time scaling influences this. The authors experiment with s1 models (Qwen2.5-Instruct finetuned on English STEM CoTs) on multilingual benchmarks like MGSM, varying the number of "thinking tokens" at inference and analyzing language-mixing patterns such as a novel "quote-and-think" behavior. Key findings show that scaling inference compute significantly improves multilingual mathematical reasoning, with a 14B s1 model achieving a +Δ9.4% average accuracy gain (reaching 81.0% average accuracy) on MGSM (excluding English) when scaling from 0.5k to 8k thinking tokens, outperforming models twice its size. The main implication for practitioners is that test-time scaling of English-centric RLMs can provide strong multilingual reasoning baselines, especially in high-resource languages, though limitations exist for low-resource languages and out-of-domain generalization. |
| Multi-Modal | PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes (Read more on [arXiv](https://arxiv.org/abs/2505.05288) or [HuggingFace](https://huggingface.co/papers/2505.05288))| abdo-eldesokey, zuluquebec, Aileron, Filippo8, Samir55 | This paper introduces PlaceIt3D, a novel task, benchmark, and dataset for language-guided 3D object placement in real-world scenes. The primary objective is to develop AI systems capable of finding physically plausible and semantically coherent placements for a 3D asset within a 3D scene based on natural language instructions, addressing challenges like ambiguity and the need for 3D geometric reasoning. The authors propose "PlaceWizard," a method that builds upon 3D Large Language Models by encoding scene point clouds with uniform spatial pooling and asset geometry, then utilizing a multi-head decoder to predict placement location, rotation, and relevant anchor objects. On their newly proposed benchmark, PlaceWizard achieves 52.6% global constraint accuracy and 29.4% complete placement success, outperforming baseline approaches. This research offers a challenging new framework for evaluating and advancing generalist 3D LLMs, with significant implications for applications in robotics, augmented reality, and virtual reality that require sophisticated 3D spatial understanding and interaction. |
| Natural Language Processing | Chain-of-Thought Tokens are Computer Program Variables (Read more on [arXiv](https://arxiv.org/abs/2505.04955) or [HuggingFace](https://huggingface.co/papers/2505.04955))| Zhifang Sui, peiyiwang89, soliz1998 | This paper investigates the role of Chain-of-Thought (CoT) tokens in Large Language Models (LLMs), proposing that they function similarly to computer program variables. The study empirically analyzes CoT tokens in multi-digit multiplication and dynamic programming tasks, examining their necessity, compressibility, and mutability. Results indicate that only tokens storing intermediate results are crucial for performance, achieving comparable accuracy to full CoT prompting. Intervention experiments confirm that CoT tokens causally affect subsequent reasoning steps and final answers, suggesting that manipulating these tokens affects model outputs akin to modifying program variables. |
