

## Papers for 2025-05-26

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Machine Learning | TabSTAR: A Foundation Tabular Model With Semantically Target-Aware
  Representations (Read more on [arXiv](https://arxiv.org/abs/2505.18125) or [HuggingFace](https://huggingface.co/papers/2505.18125))| Roi Reichart, Alan Arazi, EilamSha | TabSTAR is introduced as a foundation tabular model that leverages semantically target-aware representations. The research aims to improve transfer learning on tabular data with textual features by incorporating target variable context into the model's input. TabSTAR unfreezes a pretrained text encoder and uses target tokens to learn task-specific embeddings. Empirically, TabSTAR achieves state-of-the-art performance on classification benchmarks with text features, demonstrating improvements over GBDTs, and its pretraining phase exhibits scaling laws. The model's architecture, free of dataset-specific parameters, facilitates broader applicability and potential for performance scaling. |
| Natural Language Processing | QwenLong-L1: Towards Long-Context Large Reasoning Models with
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.17667) or [HuggingFace](https://huggingface.co/papers/2505.17667))| Chenliang Li, Yingcheng Shi, Shengyi Liao, Weizhou Shen, Wanfq | The paper presents QwenLong-L1, a framework for adapting short-context Large Reasoning Models (LRMs) to long-context scenarios via reinforcement learning. The research aims to address suboptimal training efficiency and unstable optimization in long-context LRMs. QwenLong-L1 utilizes progressive context scaling with supervised fine-tuning and curriculum-guided reinforcement learning. Experiments on long-context document question-answering benchmarks demonstrate that QwenLong-L1-32B achieves an average gain of 5.1 points over R1-Distill-Qwen-32B. The framework offers a practical approach for developing long-context LRMs capable of robust reasoning in information-intensive environments. |
| Natural Language Processing | Reasoning Model is Stubborn: Diagnosing Instruction Overriding in
  Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2505.17225) or [HuggingFace](https://huggingface.co/papers/2505.17225))| Eunho Yang, Hyun Ryu, Chanjae Park, yjyjyj98, jadohu | This paper investigates reasoning rigidity in large language models (LLMs), where models override explicit instructions in favor of familiar reasoning patterns. The study introduces ReasoningTrap, a diagnostic dataset of modified mathematical problems and puzzles, to assess this behavior.  The methodology involves evaluating various LLMs on ReasoningTrap and categorizing contamination patterns into Interpretation Overload, Input Distrust, and Partial Instruction Attention.  Results show a significant drop in performance on modified datasets compared to original datasets, quantified by p-pass@1 scores and contamination ratios, indicating reasoning rigidity. The findings suggest that AI practitioners should be cautious about the reliability of LLMs in tasks requiring strict adherence to novel constraints. |
| Reinforcement Learning | One RL to See Them All: Visual Triple Unified Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.18129) or [HuggingFace](https://huggingface.co/papers/2505.18129))| Pengfei Li, Shaoxiang Chen, Linge Du, Yan Ma, Ryan1122 | This paper introduces V-Triune, a visual triple unified reinforcement learning system for VLMs. It addresses scaling RL to both reasoning and perception tasks by unifying task inputs and reward computation. V-Triune includes sample-level data formatting, verifier-level reward computation, and source-level metric monitoring with a novel dynamic IoU reward. The resulting model, Orsta, demonstrates improvements on MEGA-Bench Core, ranging from +2.1% to +14.1% across 7B and 32B model variants. V-Triune provides AI practitioners with a unified and scalable RL approach for training VLMs on diverse tasks. |
| Natural Language Processing | Distilling LLM Agent into Small Models with Retrieval and Code Tools (Read more on [arXiv](https://arxiv.org/abs/2505.17612) or [HuggingFace](https://huggingface.co/papers/2505.17612))| Sung Ju Hwang, Jaewoong Cho, Seanie Lee, Jongwon Jeong, Minki Kang | This paper introduces Agent Distillation, a method for transferring reasoning and tool-use capabilities from large language model (LLM) agents to smaller language models (sLMs). The research aims to improve sLMs' performance on complex tasks by enabling them to leverage retrieval and code tools adaptively through first-thought prefix prompting and self-consistent action generation. Experimental results on factual and mathematical reasoning tasks demonstrate that distilled sLMs can achieve performance competitive with larger CoT-tuned models; for example, a 0.5B agent matches the performance of a 1.5B CoT-distilled model. Agent Distillation enables the creation of practical, tool-using small language agents, improving their computational efficiency without sacrificing reasoning abilities. |
| Machine Learning | Quartet: Native FP4 Training Can Be Optimal for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.14669) or [HuggingFace](https://huggingface.co/papers/2505.14669))| Jiale Chen, Oliver Sieberling, Soroush Tabesh, Andrei Panferov, Roberto L. Castro | The paper introduces Quartet, an algorithm for native FP4 training of large language models, aiming to improve computational throughput and energy efficiency. It investigates the performance trade-offs of FP4 training and introduces a novel approach enabling accurate end-to-end FP4 training. The method involves optimizing CUDA kernels for NVIDIA Blackwell GPUs and shows state-of-the-art FP4 accuracy, successfully training billion-scale models with nearly 2x speedup relative to FP8 for linear layer computations. A new low-precision scaling law is revealed. This demonstrates that fully FP4-based training is a competitive alternative to standard-precision and FP8 training, potentially reducing AI computational costs. |
| Multi-Modal | PhyX: Does Your Model Have the "Wits" for Physical Reasoning? (Read more on [arXiv](https://arxiv.org/abs/2505.15929) or [HuggingFace](https://huggingface.co/papers/2505.15929))| Yunta Hsieh, Qi Han, Hui Shen, John-ai-bee, taki555 | The paper introduces PHYX, a new benchmark for evaluating physical reasoning in AI models. It addresses the lack of benchmarks capturing integrated domain knowledge, symbolic reasoning, and real-world understanding. PHYX comprises 3K multimodal questions across six physics domains and reasoning types, requiring visual analysis and causal reasoning. Evaluation shows that even state-of-the-art models like GPT-4o struggle, achieving only 45.8% accuracy, revealing over-reliance on memorization and visual pattern matching. PHYX offers a diagnostic tool and roadmap for developing physically-grounded AI systems. |
| Natural Language Processing | QwenLong-CPRS: Towards infty-LLMs with Dynamic Context Optimization (Read more on [arXiv](https://arxiv.org/abs/2505.18092) or [HuggingFace](https://huggingface.co/papers/2505.18092))| Shaopeng Lai, Shengyi Liao, Chenliang Li, Weizhou Shen, Wanfq | The paper introduces QWENLONG-CPRS, a context compression framework for optimizing long-context processing in large language models. It addresses computation overhead and the 'lost in the middle' phenomenon through dynamic context optimization guided by natural language instructions. The framework incorporates bidirectional reasoning layers, a token critic mechanism with language modeling heads, and window-parallel inference. Evaluations across five benchmarks demonstrate QWENLONG-CPRS achieves up to 21.59x context compression and an average 19.15-point performance gain, with the Qwen2.5-32B-Instruct model surpassing leading proprietary LLMs on Ruler-128K and InfiniteBench. This enables more efficient and accurate long-context processing for AI practitioners. |
| Machine Learning | VeriThinker: Learning to Verify Makes Reasoning Model Efficient (Read more on [arXiv](https://arxiv.org/abs/2505.17941) or [HuggingFace](https://huggingface.co/papers/2505.17941))| Xinchao Wang, Ruonan Yu, Gongfan Fang, Xinyin Ma, Zigeng | VeriThinker is presented as a novel approach for Chain-of-Thought (CoT) compression in large reasoning models (LRMs). The paper aims to reduce overthinking in LRMs by training them on an auxiliary CoT verification task. The key methodology involves Supervised Verification Fine-Tuning (SVFT), where the model learns to verify the correctness of CoT solutions. Experiments show that VeriThinker reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%). This implies AI practitioners can achieve more efficient and accurate reasoning with LRMs by incorporating a verification mechanism without relying on synthetic target reasoning chains. |
| Computer Vision | Model Already Knows the Best Noise: Bayesian Active Noise Selection via
  Attention in Video Diffusion Model (Read more on [arXiv](https://arxiv.org/abs/2505.17561) or [HuggingFace](https://huggingface.co/papers/2505.17561))| Sanghyun Kim, Kwanyoung Kim | This paper introduces a method for selecting optimal initial noise seeds in video diffusion models to improve generation quality. The research aims to leverage internal model signals, specifically attention maps, to quantify noise seed uncertainty. A Bayesian Active Noise Selection via Attention (BANSA) score is proposed, measuring entropy disagreement across attention samples. Experiments on CogVideoX-2B demonstrate an improvement in VBench total score from 81.03 to 81.66, indicating enhanced video quality and semantic alignment with only a slight increase to inference time. This approach provides AI practitioners with a model-aware noise selection strategy that avoids external priors and retraining. |
| Natural Language Processing | MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated
  Experimental Feedback (Read more on [arXiv](https://arxiv.org/abs/2505.17873) or [HuggingFace](https://huggingface.co/papers/2505.17873))| Lidong Bing, Jue Wang, di-zhang-fdu, ZonglinY, wanhaoliu | The paper introduces MOOSE-Chem3, focusing on experiment-guided hypothesis ranking by simulating experimental feedback for scientific discovery. It addresses the challenge of costly wet-lab experiments by proposing a simulator grounded in domain-informed assumptions to prioritize candidate hypotheses. The key methodology involves clustering hypotheses based on functional characteristics and using simulated experimental feedback to guide ranking. Experiments demonstrate the method outperforms pre-experiment baselines, achieving a Spearman correlation of 0.960 between simulated and real experimental outcomes. The simulator facilitates the research of experiment-guided ranking strategies without requiring real-world experimental resources. |
| Multi-Modal | AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.16211) or [HuggingFace](https://huggingface.co/papers/2505.16211))| Jirui Han, Yile Liu, Can Shen, Kai Li, jiaxiaojunQAQ | This paper introduces AudioTrust, a benchmark for evaluating the multifaceted trustworthiness of Audio Large Language Models (ALLMs). It aims to address the gap in comprehensive evaluation benchmarks, particularly concerning risks unique to the audio modality. The methodology involves a dataset of 4,420 audio/text samples across six key dimensions (fairness, hallucination, safety, privacy, robustness, authentication) and nine audio-specific evaluation metrics. Experimental results reveal trustworthiness limitations of current ALLMs, with varying performance across dimensions and models, where no model achieves ideal fairness. AudioTrust provides insights for secure and trustworthy deployment, highlighting the need for addressing biases and vulnerabilities in ALLMs. |
| Computer Vision | Scaling Image and Video Generation via Test-Time Evolutionary Search (Read more on [arXiv](https://arxiv.org/abs/2505.17618) or [HuggingFace](https://huggingface.co/papers/2505.17618))| Di Zhang, Pengfei Wan, Xintao Wang, Jiajun Liang, haoranhe | The paper introduces EvoSearch, a test-time scaling framework for image and video generation applicable to both diffusion and flow models. It addresses the limitation of current test-time scaling methods by reformulating the process as an evolutionary search problem leveraging selection and mutation mechanisms along the denoising trajectory. EvoSearch enables Stable Diffusion 2.1 to exceed GPT40 performance and Wan 1.3B to outperform Wan 14B with 10x fewer parameters. Extensive experiments show substantial improvements in sample quality and human-preference alignment as compute increases. EvoSearch provides a unified and efficient framework for practitioners to enhance generation quality without additional training or model expansion. |
| Multi-Modal | FullFront: Benchmarking MLLMs Across the Full Front-End Engineering
  Workflow (Read more on [arXiv](https://arxiv.org/abs/2505.17399) or [HuggingFace](https://huggingface.co/papers/2505.17399))| Yu Cheng, Linjie Li, Huichen Will Wang, Haoyu Sun, Kuvvi | The paper introduces FullFront, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) across the full front-end development workflow. It aims to address the lack of comprehensive benchmarks by assessing Webpage Design, Webpage Perception QA, and Webpage Code Generation. FullFront reconstructs real-world webpages into clean, standardized HTML using a two-stage process and avoids copyright issues. Evaluations of state-of-the-art MLLMs reveal significant limitations in page perception, code generation, and interaction implementation, with Claude 3.7 Sonnet achieving below 55% average accuracy in Webpage Perception QA. The results highlight a substantial gap between current MLLM capabilities and human expert performance in front-end engineering. |
| Natural Language Processing | Teaching with Lies: Curriculum DPO on Synthetic Negatives for
  Hallucination Detection (Read more on [arXiv](https://arxiv.org/abs/2505.17558) or [HuggingFace](https://huggingface.co/papers/2505.17558))| Ying Ding, Liu Leqi, ashwinnv, SP2001 | The paper introduces a curriculum-guided Direct Preference Optimization (DPO) framework using synthetic negatives for hallucination detection in large language models (LLMs). It aims to improve hallucination detection by using high-quality, difficulty-ranked hallucinated samples as negative examples during DPO alignment. The methodology involves a curriculum learning strategy that progressively trains the model using examples ranked by grounded factuality scores from independent fact-checking models. Experiments show that HaluCheck models, trained with this approach, outperform existing baselines, including the Llama-3.2 3B model, achieving up to a 24% relative gain across core detection metrics. This approach provides a more robust alignment strategy for smaller language models in detecting sophisticated hallucinations. |
| Reinforcement Learning | Thought-Augmented Policy Optimization: Bridging External Guidance and
  Internal Capabilities (Read more on [arXiv](https://arxiv.org/abs/2505.15692) or [HuggingFace](https://huggingface.co/papers/2505.15692))| Zhengqi Wen, Shuai Zhang, Mingkuan Feng, ChonghuaLiao, Jinyang23 | The paper introduces Thought-Augmented Policy Optimization (TAPO), a novel RL framework aimed at bridging external guidance and internal reasoning capabilities for enhanced reasoning in LLMs. It addresses limitations in current RL approaches by incorporating external high-level thought patterns to augment policy optimization. TAPO adaptively integrates structured thoughts during training, balancing model-internal exploration and external guidance exploitation. Experiments demonstrate TAPO's significant performance gains, outperforming GRPO by 99% on AIME and 41% on AMC. TAPO offers a promising direction for developing powerful, generalizable, and interpretable reasoning systems by integrating external knowledge into RL training. |
| Computer Vision | Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration (Read more on [arXiv](https://arxiv.org/abs/2505.16479) or [HuggingFace](https://huggingface.co/papers/2505.16479))| Bin Xiao, Xiuli Bi, Yang Wei, Yunqiu, YuetongLiu | This paper addresses multi-weather nighttime image restoration by introducing a unified framework to remove complex degradations. The research aims to restore clear images from degraded nighttime scenes affected by adverse weather and flare effects. The proposed ClearNight framework integrates Retinex-based dual priors and weather-aware dynamic selection to enhance restoration effectiveness. ClearNight achieves state-of-the-art performance, demonstrating a PSNR of 32.5937 on synthetic datasets. The work offers AI practitioners a robust method for handling multi-weather degradations in nighttime images, which has implications for autonomous driving and surveillance applications. |
| Natural Language Processing | Teaching Large Language Models to Maintain Contextual Faithfulness via
  Synthetic Tasks and Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.16483) or [HuggingFace](https://huggingface.co/papers/2505.16483))| Zhitong Wang, Yuzhuo Bai, Cheng Gao, Shuzheng Si, BleachNick | The paper introduces CANOE, a framework to improve faithfulness in large language models (LLMs) without human annotation, focusing on both short-form and long-form generation. The research aims to address the challenge of LLMs generating unfaithful responses by scaling parameters alone. CANOE uses synthetic short-form QA data and a tailored reinforcement learning method, Dual-GRPO, that includes rule-based rewards derived from QA data, optimizing both short-form and long-form responses. Experiments show CANOE significantly improves LLMs' faithfulness across 11 downstream tasks, achieving a 22.6% overall score improvement for LLaMA3-Instruct-8B and even surpassing GPT-40. This implies a practical method for enhancing LLMs' reliability and contextual understanding without relying on manual data labeling. |
| Natural Language Processing | Time-R1: Towards Comprehensive Temporal Reasoning in LLMs (Read more on [arXiv](https://arxiv.org/abs/2505.13508) or [HuggingFace](https://huggingface.co/papers/2505.13508))| Jiaxuan You, Haoru Li, Haofei Yu, Peixuan Han, m-serious | The paper introduces Time-R1, a novel framework to endow LLMs with comprehensive temporal reasoning abilities including understanding, prediction, and creative generation. The primary research objective is to overcome the limitations of current LLMs in robustly reasoning about time, especially for events beyond their knowledge cutoffs or requiring creative foresight. Time-R1 employs a three-stage reinforcement learning curriculum driven by a dynamic rule-based reward system using a 3B-parameter LLM as the base model.  Experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the 671B DeepSeek-R1, achieving superior performance on future event prediction and creative scenario generation benchmarks, with AVGMaxSim reaching 49.22% on plausibility in creative generation. The implications are significant, suggesting that thoughtfully engineered, progressive RL fine-tuning allows for smaller, efficient models to achieve superior temporal performance, offering a practical and scalable path towards more time-aware AI. |
| Natural Language Processing | Speechless: Speech Instruction Training Without Speech for Low Resource
  Languages (Read more on [arXiv](https://arxiv.org/abs/2505.17417) or [HuggingFace](https://huggingface.co/papers/2505.17417))| Shreyas Gopal, Tuan Le Duc Anh, Huy Hoang Ha, Dinh Bach Vu, alandao | The paper introduces Speechless, a novel method for speech instruction training in low-resource languages without relying on traditional text-to-speech systems. The main objective is to generate synthetic training data for early-fusion speech language models by bypassing waveform generation. Speechless aligns text with quantized Whisper encoder representations to create semantic speech tokens, enabling fine-tuning of large language models. The method achieves comparable ASR performance in Vietnamese without speech-based fine-tuning and comparable LLM performance on speech tasks. Speechless offers a resource-efficient approach for developing voice assistants in languages with limited speech synthesis resources, enabling adaptation and instruction tuning. |
| Computer Vision | RBench-V: A Primary Assessment for Visual Reasoning Models with
  Multi-modal Outputs (Read more on [arXiv](https://arxiv.org/abs/2505.16770) or [HuggingFace](https://huggingface.co/papers/2505.16770))| Qianrui Yang, uyzhang, Mo-ZheHan, CXY07, MenghaoGuo | The paper introduces RBench-V, a new benchmark for assessing visual reasoning capabilities of multi-modal models, focusing on the generation of multi-modal outputs. The research aims to address the limitations of existing benchmarks that primarily evaluate models on multi-modal inputs and text-only reasoning. RBench-V comprises 803 hand-picked questions covering math, physics, counting, and games, requiring models to produce image manipulations and auxiliary lines during reasoning. Evaluation of several open- and closed-source models reveals that the best-performing model, o3, achieves only 25.8% accuracy, significantly below human performance (82.3%). The benchmark highlights the current struggle of models to leverage multi-modal reasoning effectively, implying a need for novel approaches such as M-CoT for solving complex visual tasks. |
| Reinforcement Learning | s3: You Don't Need That Much Data to Train a Search Agent via RL (Read more on [arXiv](https://arxiv.org/abs/2505.14146) or [HuggingFace](https://huggingface.co/papers/2505.14146))| Zifeng Wang, Jinfeng Xiao, Jiacheng Lin, Xueqiang Xu, Pengcheng Jiang | The paper presents s3, a lightweight reinforcement learning framework to train search agents for retrieval-augmented generation (RAG) systems. The objective is to improve retrieval quality without fine-tuning the generator LLM. s3 trains the searcher using a Gain Beyond RAG reward, measuring the improvement in generation accuracy over naive RAG. Experiments show that s3, trained on 2.4k samples, outperforms baselines trained on 70x more data, achieving stronger downstream performance across six general QA and five medical QA benchmarks. The modular approach and data efficiency of s3 offer a scalable path for improving RAG systems. |
| Reinforcement Learning | Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement
  Fine-Tuning of Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.17826) or [HuggingFace](https://huggingface.co/papers/2505.17826))| Daoyuan Chen, Yuchang Sun, Yushuo Chen, Yanxi Chen, Xuchen Pan | The paper introduces Trinity-RFT, a general-purpose framework for reinforcement fine-tuning (RFT) of large language models, designed to be flexible and scalable. The framework addresses challenges in RFT by unifying synchronous/asynchronous modes, seamless agent-environment interaction, and systematic data pipelines. Trinity-RFT facilitates diverse applications and advanced reinforcement learning paradigms. Although quantitative metrics are not explicitly stated, the paper emphasizes the design and implementation details to demonstrate its utility and user-friendliness. The main implication is providing a unified platform to explore advanced reinforcement learning paradigms in language models. |
| Computer Vision | Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse
  Attention (Read more on [arXiv](https://arxiv.org/abs/2505.17412) or [HuggingFace](https://huggingface.co/papers/2505.17412))| Yikang Yang, Yifei Zeng, Feihu Zhang, Youtian Lin, Shuang Wu | The paper introduces Direct3D-S2, a scalable 3D generation framework leveraging sparse volumes for high-resolution outputs with reduced training costs. It addresses the challenge of memory and computation in volumetric 3D generation by proposing a Spatial Sparse Attention (SSA) mechanism to enhance Diffusion Transformer (DiT) efficiency on sparse volumetric data. SSA achieves a 3.9x speedup in the forward pass and 9.6x speedup in the backward pass, enabling training at 1024³ resolution using only 8 GPUs. The framework also includes a sparse volumetric VAE that improves training efficiency and stability. This allows for more accessible and practical gigascale 3D generation with finer geometric details. |
| Natural Language Processing | Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning (Read more on [arXiv](https://arxiv.org/abs/2505.16270) or [HuggingFace](https://huggingface.co/papers/2505.16270))| Ruizhong Qiu, Yunzhe Qi, Zihao Li, Yikun Ban, jiaruz2 | This paper introduces Transformer Copilot, a novel learning framework to enhance LLM fine-tuning. It addresses the limitations of standard fine-tuning by retaining and leveraging the model's learning signals. The key methodology involves tracking the model's learning behavior and recurring errors through a Mistake Log and using a Copilot model to rectify the Pilot's logits. Experiments across 12 benchmarks demonstrate consistent performance improvements, with gains of up to 34.5%. The framework offers a way to improve LLM inference by incorporating an internalized reflection mechanism, adaptively correcting for past mistakes with marginal computational overhead. |
| Multi-Modal | Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark
  Study (Read more on [arXiv](https://arxiv.org/abs/2505.15389) or [HuggingFace](https://huggingface.co/papers/2505.15389))| Hwanjo Yu, Jihae Jeong, Joonwon Jang, oneonlee | This paper introduces MEMESAFETYBENCH, a meme-based benchmark to evaluate the safety of vision-language models (VLMs) against real-world harmful prompts. The research investigates how VLMs respond to meme images, mitigate harmful outputs through conversational context, and relate model scale to safety metrics. The methodology pairs real meme images with both harmful and benign instructions, assessing multiple VLMs across single and multi-turn interactions. Results show VLMs are more vulnerable to meme-based harmful prompts compared to synthetic images, with a significant increase in harmful responses and a decrease in refusals; for example, Refusal Rate (RR) decreases while Harmful Response Rate (HR) and Task Completion Rate (CR) increase significantly when memes are introduced. The main implication for AI practitioners is the necessity for ecologically valid evaluations and stronger safety mechanisms in VLMs, especially concerning multimodal inputs. |
| Multi-Modal | Large Language Models Implicitly Learn to See and Hear Just By Reading (Read more on [arXiv](https://arxiv.org/abs/2505.17091) or [HuggingFace](https://huggingface.co/papers/2505.17091))| Mert Pilanci, Prateek Verma | This paper explores the emergent audio and visual understanding capabilities within text-trained Large Language Models (LLMs). It investigates whether text-LLMs can be repurposed to act as audio and image classifiers/embedding extractors without task-specific pre-training. The methodology involves fine-tuning a pre-trained text-LLM using parameter-efficient techniques such as LoRA on image and audio datasets, replacing traditional encoders. The experiments show that fine-tuned text-LLMs outperform architectures trained from scratch, achieving an accuracy of 44.1% on the FSD-50K audio classification dataset with a 1.5B parameter LLM. This suggests text-LLMs learn modality-agnostic internal circuits that can be adapted for diverse classification tasks, potentially reducing the need for training specialized models from scratch. |
| Reinforcement Learning | Synthetic Data RL: Task Definition Is All You Need (Read more on [arXiv](https://arxiv.org/abs/2505.17063) or [HuggingFace](https://huggingface.co/papers/2505.17063))| Zekai Zhang, Zi-Ang Wang, Chuanwei Huang, Yiduo Guo, zguo0525 | This paper introduces Synthetic Data RL, a framework for adapting foundation models to specialized tasks using only synthetic data generated from a task definition. The method involves generating question-answer pairs, adapting question difficulty based on model solvability, and selecting questions for RL training. On Qwen-2.5-7B, the method achieves a 29.2% absolute improvement over the base model on GSM8K. Synthetic Data RL enables scalable and efficient RL-based model adaptation by reducing human data annotation. Code and demos are available at https://github.com/gydpku/Data_Synthesis_RL/. |
| Computer Vision | RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation
  via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.17540) or [HuggingFace](https://huggingface.co/papers/2505.17540))| Jianjin Zhang, Fangkai Yang, Pu Zhao, Lu Wang, Mingrui Wu | The paper introduces RePrompt, a reasoning-augmented reprompting framework for enhancing text-to-image (T2I) generation by explicitly incorporating reasoning into the prompt enhancement process via reinforcement learning. It addresses the challenge of T2I models struggling to capture user intentions from under-specified prompts by training a language model to generate structured, self-reflective prompts optimized for image-level outcomes using tailored reward models. The approach leverages human preference, semantic alignment, and visual composition to refine prompt generation without human-annotated data. Experiments on GenEval show that RePrompt improves spatial layout fidelity and compositional generalization, boosting position accuracy by +77.1% (FLUX) over Qwen2.5 3B-enhanced baselines. This work provides AI practitioners with a new method to improve T2I generation through explicit reasoning during prompt construction, leading to more realistic and semantically aligned results. |
| Reinforcement Learning | On the Design of KL-Regularized Policy Gradient Algorithms for LLM
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.17508) or [HuggingFace](https://huggingface.co/papers/2505.17508))| Quanquan Gu, Yang Yuan, Huizhuo Yuan, Lewis-Lau, yifAI | The paper investigates the design space of KL-regularized policy gradient algorithms for enhancing reasoning in large language models. It introduces Regularized Policy Gradient (RPG), a framework for deriving and analyzing such algorithms, considering forward and reverse KL divergences and normalized/unnormalized distributions.  The research explores the estimation and integration of KL divergence formulations into surrogate loss functions for online RL. Experiments demonstrate improved or competitive training stability and performance compared to GRPO, REINFORCE++, and DAPO. The framework enables practitioners to systematically construct and analyze KL-regularized policy gradient methods, potentially leading to more stable and effective RL-based LLM fine-tuning strategies. |
| Reinforcement Learning | Interactive Post-Training for Vision-Language-Action Models (Read more on [arXiv](https://arxiv.org/abs/2505.17016) or [HuggingFace](https://huggingface.co/papers/2505.17016))| Philipp Krähenbühl, Yue Zhao, Kairan Dou, tanshh97 | The paper introduces RIPT-VLA, a reinforcement learning-based interactive post-training paradigm for vision-language-action (VLA) models. It addresses the limitation of existing VLA training pipelines that rely heavily on offline expert demonstrations by enabling interactive post-training using only sparse binary success rewards. The methodology employs a stable policy optimization algorithm based on dynamic rollout sampling and leave-one-out advantage estimation. RIPT-VLA improves performance on VLA models, enhancing the lightweight QueST model by 21.2% and achieving a 97.5% success rate on the 7B OpenVLA-OFT model. This approach provides a practical and effective way for post-training VLA models with minimal supervision, allowing them to adapt to new tasks and environments more efficiently. |
| Natural Language Processing | Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA (Read more on [arXiv](https://arxiv.org/abs/2505.16293) or [HuggingFace](https://huggingface.co/papers/2505.16293))| Sai Rajeswar, Shiva Krishna Reddy Malay, Khyati Mahajan, Masoud Hashemi, rmahesh | The paper introduces NotesWriting, a novel method for augmenting Large Language Model (LLM) reasoning in complex question answering tasks by generating concise, query-relevant notes from retrieved documents at each step. It addresses challenges related to lengthy contexts and irrelevant information buildup in iterative Retrieval Augmented Generation (RAG). The key methodology involves extracting and aggregating notes using a smaller language model, reducing noise and increasing the effective context length for the main LLM. Experiments across three iterative RAG baselines, four multi-hop QA datasets, and two LLMs show an average improvement of 15.6 percentage points in performance. The main implication for AI practitioners is a scalable approach to enhance LLM reasoning in complex tasks by managing retrieved information more efficiently, increasing the volume of ingested text with minimal increase in output tokens. |
| Reinforcement Learning | NOVER: Incentive Training for Language Models via Verifier-Free
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.16022) or [HuggingFace](https://huggingface.co/papers/2505.16022))| Yali Du, Chen Qian, Xinyu Wang, Siya Qi, Wei Liu | The paper introduces NOVER, a verifier-free reinforcement learning framework for incentive training of language models, enabling optimization without external verifiers. It addresses the limitations of RLVR which requires external verifiers by creating a reward proxy using the language model itself. The key methodology involves calculating perplexity-based reward based on the model's reasoning process using a proxy model synced with the policy model. NOVER achieves a 7.7% improvement over a same-size distilled model from DeepSeek R1 671B on a range of text-to-text tasks.  This suggests NOVER can be an effective approach for incentive training in domains where external verifiers are infeasible, improving language model performance on complex reasoning tasks. |
| Natural Language Processing | Keep Security! Benchmarking Security Policy Preservation in Large
  Language Model Contexts Against Indirect Attacks in Question Answering (Read more on [arXiv](https://arxiv.org/abs/2505.15805) or [HuggingFace](https://huggingface.co/papers/2505.15805))| Hwanhee Lee, Yonghyun Jun, Yumin Kim, HwanChang0106 | This paper introduces CoPriva, a novel large-scale benchmark dataset to evaluate LLM adherence to contextual non-disclosure policies in question answering. The main research objective is to assess LLMs' ability to preserve security policies against direct and indirect attacks seeking prohibited information. The methodology involves evaluating 10 LLMs on CoPriva, which includes explicit policies and realistic contexts. Results show significant vulnerability, with models often violating user-defined policies and leaking sensitive information, especially against indirect attacks (leakage increased by over 40 percentage points on average). The main implication for AI practitioners is the urgent need for more robust methods to guarantee contextual security in sensitive applications. |
| Natural Language Processing | TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in
  Real-World Scenarios (Read more on [arXiv](https://arxiv.org/abs/2505.12891) or [HuggingFace](https://huggingface.co/papers/2505.12891))| Tianyi Zhuang, Wen Luo, Wei Li, Shaohang Wei, songff | This paper introduces TIME, a multi-level benchmark for evaluating temporal reasoning capabilities of large language models (LLMs) in complex real-world scenarios. The research aims to address the limitations of existing benchmarks by incorporating challenges such as intensive temporal information, fast-changing event dynamics, and complex temporal dependencies in social interactions. The methodology involves constructing three sub-datasets (TIME-WIKI, TIME-NEWS, TIME-DIAL) comprising 38,522 QA pairs across 11 fine-grained sub-tasks and evaluating various reasoning and non-reasoning models. Experimental results show suboptimal performance for existing LLMs on tasks requiring complex temporal understanding, with some models achieving a maximum performance of only 63.33% on Duration Compare and Order Compare tasks in TIME-NEWS. The implication for AI practitioners is a need for improved temporal reasoning capabilities in LLMs to better comprehend and process real-world events. |
| Machine Learning | Not All Models Suit Expert Offloading: On Local Routing Consistency of
  Mixture-of-Expert Models (Read more on [arXiv](https://arxiv.org/abs/2505.16056) or [HuggingFace](https://huggingface.co/papers/2505.16056))| Duyu Tang, Yitong Li, Miren Tian, Siyuan Wang, ljcleo | This paper investigates local routing consistency in Mixture-of-Expert (MoE) models to improve expert offloading. It introduces two metrics, Segment Routing Best Performance (SRP) and Segment Cache Best Hit Rate (SCH), to quantify local routing consistency. Analyzing 20 MoE LLMs, the study finds models with MoE on every layer and without shared experts exhibit the highest local routing consistency. Furthermore, they show optimal cache size is approximately 2x the number of active experts for most models, balancing effectiveness and efficiency. These findings provide insights for memory-efficient MoE design and deployment. |
| Computer Vision | Revisiting Residual Connections: Orthogonal Updates for Stable and
  Efficient Deep Networks (Read more on [arXiv](https://arxiv.org/abs/2505.11881) or [HuggingFace](https://huggingface.co/papers/2505.11881))| Younjae Yu, Suhwan Choi, Siyeol Kim, Woohyun Cho, Giyeong Oh | This paper introduces Orthogonal Residual Update, a novel approach to improve deep network training stability and efficiency. The research explores whether standard residual updates underutilize module capacity by reinforcing existing stream directions. Orthogonal Residual Update decomposes the module output, adding only the component orthogonal to the input stream. Experiments across ResNetV2 and Vision Transformers show improved generalization, with a +4.3%p top-1 accuracy gain for ViT-B on ImageNet-1k. The method offers a computationally efficient way to enhance deep neural networks by promoting novel feature learning. |
