

## Papers for 2025-05-22

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Web-Shepherd: Advancing PRMs for Reinforcing Web Agents (Read more on [arXiv](https://arxiv.org/abs/2505.15277) or [HuggingFace](https://huggingface.co/papers/2505.15277))| Seungone Kim, Junhee Cho, donghalim, KimSHine, hyungjoochae | This paper introduces WEB-SHEPHERD, a process reward model (PRM) for web navigation to address limitations of using MLLMs as reward models due to speed and cost. The research aims to create a step-level PRM that can assess web navigation trajectories using a structured checklist. The methodology involves constructing WEBPRM COLLECTION, a dataset of 40K step-level preference pairs and introducing WEBREWARDBENCH for meta-evaluation. WEB-SHEPHERD achieves a 30-point accuracy improvement over GPT-40 on WEBREWARDBENCH, and in WebArena-lite, improves performance by 10.9 points with 10x less cost than using GPT-40-mini as a verifier; this implies more efficient and reliable web agents through interpretable reward modeling. |
| Natural Language Processing | Scaling Law for Quantization-Aware Training (Read more on [arXiv](https://arxiv.org/abs/2505.14302) or [HuggingFace](https://huggingface.co/papers/2505.14302))| Zeyue Xue, Yutao Zeng, Jing Liu, Chaoyi Zhang, ChenMnZ | This paper explores the scaling behavior of Quantization-Aware Training (QAT) for large language models at 4-bit precision (W4A4). It aims to model the quantization error in QAT as a function of model size, training data volume, and quantization group size. The methodology involves conducting 268 QAT experiments and decomposing quantization error into weight and activation components. The primary result shows that quantization error decreases as model size increases but rises with more training tokens and coarser quantization granularity, with activation quantization in the FC2 layer being a key bottleneck. The findings imply that practitioners should jointly optimize weight and activation quantization errors for improved W4A4 QAT performance. |
| Computer Vision | UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2505.14231) or [HuggingFace](https://huggingface.co/papers/2505.14231))| Jing Tang, Yong Liu, Mingxing Li, Sule Bai, xiaochonglinghu | The paper introduces UniVG-R1, a reasoning-guided MLLM for universal visual grounding using reinforcement learning. It addresses the challenge of complex grounding tasks with implicit instructions across multiple images by enhancing reasoning capabilities. The approach uses a CoT grounding dataset and rule-based reinforcement learning to guide the model's reasoning paths.  UniVG-R1 achieves state-of-the-art performance on MIG-Bench with a 9.1% improvement. This demonstrates a versatile and generalizable visual grounding for real-world scenarios. |
| Multi-Modal | MMaDA: Multimodal Large Diffusion Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.15809) or [HuggingFace](https://huggingface.co/papers/2505.15809))| Ke Shen, Bowen Li, Ling Yang, comin, tyfeld | The paper introduces MMaDA, a multimodal diffusion foundation model for textual reasoning, multimodal understanding, and text-to-image generation. It aims to bridge the gap between pretraining and post-training in unified diffusion architectures through a modality-agnostic design, a mixed long chain-of-thought fine-tuning strategy, and a unified policy-gradient-based RL algorithm (UniGRPO). The key methodology involves a unified diffusion architecture, mixed long-CoT finetuning, and UniGRPO with diversified reward modeling. MMaDA-8B achieves superior generalization capabilities, surpassing LLaMA-3-7B and Qwen2-7B in textual reasoning and achieving a 68.5 MMB score, outperforming Show-o and SEED-X in multimodal understanding and exceeding SDXL and Janus in text-to-image generation. This framework provides a comprehensive approach for future research and development in unified diffusion architectures. |
| Machine Learning | Efficient Agent Training for Computer Use (Read more on [arXiv](https://arxiv.org/abs/2505.13909) or [HuggingFace](https://huggingface.co/papers/2505.13909))| Pengfei Liu, zizi-0123, henryhe0123 | The paper introduces PC Agent-E, an efficient agent training framework for computer use, designed to reduce reliance on large-scale human demonstrations. It aims to improve agent training data quality by synthesizing diverse action decisions using Claude 3.7 Sonnet, starting with only 312 human-annotated trajectories. PC Agent-E achieves a 141% relative improvement over the Qwen2.5-VL-72B baseline on WindowsAgentArena-V2, surpassing Claude 3.7 Sonnet with extended thinking. This indicates that high-quality, augmented trajectory data is sufficient for strong computer use capabilities. The framework also demonstrates strong generalizability across different operating systems. |
| Natural Language Processing | Diffusion vs. Autoregressive Language Models: A Text Embedding
  Perspective (Read more on [arXiv](https://arxiv.org/abs/2505.15045) or [HuggingFace](https://huggingface.co/papers/2505.15045))| Anh Tuan Luu, Arman Cohan, LYGeng, yilunzhao, siyue | The paper investigates diffusion language models for text embeddings, addressing limitations of unidirectional attention in autoregressive models. It explores whether diffusion embeddings, trained with bidirectional attention, are better suited for capturing global context in long and complex documents. The study introduces DIFFEMBED, a diffusion language embedding model that outperforms LLM-based models by 20% on long-document retrieval and 8% on reasoning-intensive tasks. The primary implication suggests that bidirectional attention is crucial for encoding global context, offering improved performance for tasks requiring long-range dependencies. The work also presents REASONAUG, a new dataset designed to enhance retrieval performance for logical documents. |
| Reinforcement Learning | Learn to Reason Efficiently with Adaptive Length-based Reward Shaping (Read more on [arXiv](https://arxiv.org/abs/2505.15612) or [HuggingFace](https://huggingface.co/papers/2505.15612))| Yuzhen Huang, Yiyun Deng, Ruochen Zhou, yuntian-deng, PeterV09 | This paper introduces adaptive length-based reward shaping (LASER) to improve the efficiency of large reasoning models (LRMs) in reinforcement learning. The research investigates how to dynamically optimize reasoning length during RL training to balance performance and token efficiency. LASER and its variants dynamically adjust reward signals based on problem difficulty achieving a +6.1 improvement on AIME2024 while reducing token usage by 63%. RL-based compression produces more concise reasoning patterns with less redundant self-reflections. This approach enables more efficient training of LRMs by adaptively managing reasoning depth based on task complexity. |
| Machine Learning | When to Continue Thinking: Adaptive Thinking Mode Switching for
  Efficient Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.15400) or [HuggingFace](https://huggingface.co/papers/2505.15400))| Haodong Zhao, Yaawennn, Machine981, Amanda2023, DadaCloud01 | This paper introduces Adaptive Self-Recovery Reasoning (ASRR), a framework for efficient reasoning in large language models (LRMs). The paper aims to address the computational overhead of redundant reasoning by enabling implicit recovery mechanisms. ASRR adaptively allocates reasoning effort based on problem difficulty through accuracy-aware length reward regulation. Experiments show ASRR reduces reasoning budget by up to 32.5% (1.5B model) and 25.7% (7B model) with minimal accuracy loss (1.2% and 0.6% pass@1, respectively). The framework enhances efficiency, adaptivity, and safety in LRM reasoning. |
| Computer Vision | Vid2World: Crafting Video Diffusion Models to Interactive World Models (Read more on [arXiv](https://arxiv.org/abs/2505.14357) or [HuggingFace](https://huggingface.co/papers/2505.14357))| Mingsheng Long, Shangchen Miao, Qixing Zhou, manchery, knightnemo | The paper introduces Vid2World, a novel approach for transforming pre-trained video diffusion models into interactive world models. It addresses the challenge of enabling causal generation and action conditioning in world models derived from video diffusion models. Vid2World causally adapts the architecture of a pre-trained video diffusion model and introduces a causal action guidance mechanism. Experiments in robot manipulation (RT1 dataset) and game simulation (CS:GO) show Vid2World achieves competitive performance, with RT-1 achieving an FVD of 21.0. The method provides a scalable pathway for repurposing powerful video diffusion models into interactive world models, enhancing data efficiency for sequential decision-making. |
| Computer Vision | IA-T2I: Internet-Augmented Text-to-Image Generation (Read more on [arXiv](https://arxiv.org/abs/2505.15779) or [HuggingFace](https://huggingface.co/papers/2505.15779))| Yifan Chang, Mingliang Zhai, Yukang Feng, Jianwen Sun, Chuanhao Li | The paper introduces IA-T2I, an Internet-Augmented Text-to-Image generation framework designed to address limitations in current T2I models when dealing with uncertain knowledge. It aims to compel T2I models by providing reference images retrieved from the internet. The key methodology involves an active retrieval module, a hierarchical image selection process using diversity and re-ranking, and a self-reflection mechanism for iterative refinement. Experiments on a new Img-Ref-T2I dataset show that IA-T2I outperforms GPT-40 by approximately 30% in human evaluation.  IA-T2I offers AI practitioners a method to improve T2I generation in scenarios with knowledge gaps by leveraging external reference images. |
| Natural Language Processing | Deliberation on Priors: Trustworthy Reasoning of Large Language Models
  on Knowledge Graphs (Read more on [arXiv](https://arxiv.org/abs/2505.15210) or [HuggingFace](https://huggingface.co/papers/2505.15210))| Jun Liu, Rui Xing, Zhitao Gao, Jie Ma, stillqu | This paper introduces a framework, Deliberation on Priors (DP), to enhance the trustworthiness of large language models (LLMs) in knowledge graph-based reasoning. The research aims to mitigate LLM hallucinations by sufficiently utilizing structural and constraint priors contained in knowledge graphs. DP employs a progressive knowledge distillation strategy to integrate structural priors and a reasoning-introspection strategy to verify responses using extracted constraints. Experimental results on benchmark datasets show that DP achieves state-of-the-art performance, with a 13% Hit@1 improvement on the Complex WebQuestions dataset. The framework's ability to generate trustworthy responses has significant implications for AI practitioners. |
| Machine Learning | lmgame-Bench: How Good are LLMs at Playing Games? (Read more on [arXiv](https://arxiv.org/abs/2505.15146) or [HuggingFace](https://huggingface.co/papers/2505.15146))| Eric P. Xing, Haoyang Yu, Mingjia Huo, Yuxuan13, Snyhlxde | The paper introduces LMGAME-BENCH, a benchmark designed to evaluate large language models (LLMs) in video game playing. The study addresses challenges in using video games for LLM evaluation, such as brittle vision perception, prompt sensitivity, and data contamination, and proposes a benchmark to solve for them. LMGAME-BENCH includes platformer, puzzle, and narrative games with lightweight perception and memory scaffolds to stabilize prompt variance and remove contamination. Experiments across 13 LLMs showed the benchmark is challenging while separating models well, and reinforcement learning on a single game transfers to unseen games and planning tasks, with their best model achieving 86.7% efficacy. The benchmark serves AI practitioners by providing a reliable and diverse environment for assessing and training LLMs for complex, sequential decision-making tasks. |
| Computer Vision | Constructing a 3D Town from a Single Image (Read more on [arXiv](https://arxiv.org/abs/2505.15765) or [HuggingFace](https://huggingface.co/papers/2505.15765))| Xin Eric Wang, Jie Yang, Jing Gu, Ruijian Zhang, Kaizhi Zheng | The paper introduces 3DTown, a training-free framework for synthesizing realistic and coherent 3D scenes from a single top-down image. It addresses the challenge of generating consistent 3D geometry, preserving layout, and achieving high-quality meshes in scene synthesis from a single image. The methodology involves region-based generation with a pretrained 3D object generator and spatial-aware 3D inpainting using masked rectified flow. The experiments demonstrate that 3DTown outperforms state-of-the-art baselines, achieving, for example, a geometry quality win rate of 68.5% against Trellis in human preference evaluations. 3DTown offers AI practitioners a lightweight alternative to costly 3D scene acquisition methods by enabling high-quality 3D scene generation from minimal input. |
| Natural Language Processing | dKV-Cache: The Cache for Diffusion Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.15781) or [HuggingFace](https://huggingface.co/papers/2505.15781))| Xinchao Wang, Gongfan Fang, Runpeng Yu, Xinyin Ma | The paper introduces dKV-Cache, a novel KV-cache mechanism for accelerating inference in Diffusion Language Models (DLMs). It aims to address the slow inference speeds of DLMs by enabling caching of key and value states during the denoising process. The methodology involves a delayed and conditioned caching strategy, with two variants: dKV-Cache-Decode and dKV-Cache-Greedy. Experiments demonstrate a 2-10x speedup in inference across various benchmarks with minimal performance degradation (specific metrics depend on the benchmark, but general acceleration is noted). dKV-Cache largely bridges the efficiency gap between autoregressive models and DLMs, making DLMs a more practical option. |
| Natural Language Processing | How Should We Enhance the Safety of Large Reasoning Models: An Empirical
  Study (Read more on [arXiv](https://arxiv.org/abs/2505.15404) or [HuggingFace](https://huggingface.co/papers/2505.15404))| Qi Zhu, Victor Shea-Jay Huang, Xian Qi Loye, Zhexin Zhang, yangjunxiao2021 | This paper investigates how to enhance the safety of Large Reasoning Models (LRMs) through supervised fine-tuning. The study empirically analyzes limitations in directly distilling safe responses from LRMs and identifies failure patterns like lack of safety awareness, overthinking, and inconsistency. Addressing these issues explicitly during data distillation improves safety significantly, reducing the Attack Success Rate (ASR) of PAIR from 77.0% to 7.0%. Short or template-based reasoning data proves effective and easier to learn for LRMs, and mixing benign reasoning data balances safety and over-refusal. The findings offer insights for AI practitioners aiming to improve LRM safety without compromising performance. |
| Natural Language Processing | Learning to Reason via Mixture-of-Thought for Logical Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.15817) or [HuggingFace](https://huggingface.co/papers/2505.15817))| Heng Huang, R. Thomas McCoy, Simeng Han, Lichang Chen, TongZheng1999 | This paper introduces Mixture-of-Thought (MoT), a framework for enhancing logical reasoning in LLMs by integrating natural language, code, and truth-table modalities. The research aims to improve LLMs' reasoning capabilities by jointly training them across multiple modalities and leveraging a voting mechanism for inference. MoT employs a self-evolving training process using filtered, self-generated rationales and achieves up to +11.7pp average accuracy gain on logical reasoning benchmarks. The results suggest complementary strengths across modalities, with truth-table reasoning mitigating limitations in natural language inference. MoT provides a pathway for AI practitioners to build more robust and versatile LLMs for complex reasoning tasks. |
| Natural Language Processing | Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data
  Could Be Secretly Stolen! (Read more on [arXiv](https://arxiv.org/abs/2505.15656) or [HuggingFace](https://huggingface.co/papers/2505.15656))| Hongning Wang, Shiyao Cui, Yuhao Sun, Zhexin Zhang, yangjunxiao2021 | This paper reveals a vulnerability in fine-tuning open-source Large Language Models (LLMs), where creators can extract downstream fine-tuning data through backdoor training. The research investigates the feasibility of data extraction attacks on fine-tuned LLMs using supervised fine-tuning (SFT) and reinforcement learning (RL). The methodology involves injecting a coded instruction during the initial training phase, enabling later extraction with only black-box access to the fine-tuned model. Experiments on models like Qwen and Llama show that in practical settings, up to 76.3% of the fine-tuning data can be perfectly extracted. This finding underscores a significant risk to proprietary data when fine-tuning open-source LLMs, necessitating the development of robust defense mechanisms. |
| Reinforcement Learning | RLVR-World: Training World Models with Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.13934) or [HuggingFace](https://huggingface.co/papers/2505.13934))| Mingsheng Long, Ningya Feng, Shaofeng Yin, manchery | This paper introduces RLVR-World, a framework that uses reinforcement learning with verifiable rewards (RLVR) to directly optimize world models for task-specific metrics. The research aims to improve world model performance beyond standard maximum likelihood estimation (MLE) objectives. The methodology formulates world modeling as autoregressive prediction of tokenized sequences, evaluating decoded predictions as verifiable rewards for RL. Experiments demonstrate gains on both language- and video-based world models, including a +30.7% accuracy improvement on text-based game state prediction. RLVR offers a post-training paradigm for enhancing generative models' utility. |
| Natural Language Processing | Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous
  Concept Space (Read more on [arXiv](https://arxiv.org/abs/2505.15778) or [HuggingFace](https://huggingface.co/papers/2505.15778))| Chenyang Zhao, Ao Shen, Weixiang Yan, Xuehai He, Zhen Zhang | The paper introduces Soft Thinking, a training-free method to enhance LLM reasoning by operating in a continuous concept space rather than discrete tokens. It addresses the limitation of discrete token-based reasoning in LLMs by utilizing probability-weighted mixtures of token embeddings. The methodology involves creating "soft" abstract concept tokens for richer representations and exploring multiple reasoning paths simultaneously. Results show that Soft Thinking improves pass@1 accuracy by up to 2.48% while reducing token usage by up to 22.4% compared to standard Chain-of-Thought. This approach offers AI practitioners a more efficient and effective reasoning framework by transcending discrete language boundaries. |
| Natural Language Processing | ConvSearch-R1: Enhancing Query Reformulation for Conversational Search
  with Reasoning via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.15776) or [HuggingFace](https://huggingface.co/papers/2505.15776))| Xipeng Qiu, Kai Song, Ruijun Feng, Siyin Wang, BeastyZ | This paper introduces ConvSearch-R1, a novel self-driven framework for conversational query reformulation (CQR) that eliminates dependency on external rewrite supervision. The research aims to effectively align query reformulation models with downstream retrievers without explicit rewrite annotations by leveraging retrieval feedback. ConvSearch-R1 employs a two-stage approach: Self-Driven Policy Warm-Up via self-distillation and Retrieval-Guided Reinforcement Learning with rank-incentive reward shaping. Experiments on TopiOCQA show a 10.3% improvement using Llama3.2-3B. The framework provides AI practitioners with a method for query reformulation that does not require costly external annotations, enhancing alignment with retrieval preferences through self-exploration. |
| Natural Language Processing | BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs (Read more on [arXiv](https://arxiv.org/abs/2505.13529) or [HuggingFace](https://huggingface.co/papers/2505.13529))| Chujie Zheng, Xiaoce Wang, Haoran Liu, Jinzhe Tu, yangjunxiao2021 | The paper introduces BARREL, a framework to improve the factual reliability of Large Reasoning Models (LRMs). It addresses overthinking patterns in LRMs that lead to incorrect answers and aims to promote concise and boundary-aware factual reasoning. The BARREL framework includes knowledge labeling, reasoning trace construction for supervised fine-tuning, and Group Relative Policy Optimization. Experiments show that BARREL training increases the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48% while maintaining comparable accuracy. This implies AI practitioners can improve LRM reliability by explicitly modeling knowledge boundaries and promoting disciplined reasoning strategies. |
| Machine Learning | This Time is Different: An Observability Perspective on Time Series
  Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2505.14766) or [HuggingFace](https://huggingface.co/papers/2505.14766))| Chris Lettieri, Salahidine Lemaachi, Youssef Doubli, Emaad Khwaja, Ben Cohen | The paper introduces TOTO, a time series forecasting foundation model specialized for observability metrics. It aims to improve zero-shot forecasting on diverse, high-dimensional observability data. TOTO employs a decoder-only architecture with causal normalization, proportional factorized attention, and a Student-T mixture model head, pre-trained on a large corpus of observability, public, and synthetic data. Experiments on the introduced BOOM benchmark and general-purpose benchmarks show TOTO achieves state-of-the-art performance, with a 12% CRPS improvement on BOOM compared to the next best model. The model's observability-focused design offers improved generalization for monitoring and anomaly detection tasks. |
| Natural Language Processing | Text Generation Beyond Discrete Token Sampling (Read more on [arXiv](https://arxiv.org/abs/2505.14827) or [HuggingFace](https://huggingface.co/papers/2505.14827))| Jianfeng Gao, Jingbo Shang, Chandan Singh, Liyuan Liu, Yufan Zhuang | The paper introduces Mixture of Inputs (MOI), a training-free method for autoregressive text generation that preserves distributional information beyond discrete token sampling. MOI addresses the limitation of discarding token distributions in standard autoregressive models, aiming to maintain richer internal representations. It employs Bayesian estimation to blend the generated discrete token with the discarded token distribution to create a new model input. Evaluated across mathematical reasoning, code generation, and PhD-level QA tasks, MOI consistently improves performance on models like QwQ-32B and Gemma-3-27B, enhancing accuracy by a reported 1.8% on average. This method provides a practical way to enhance multi-step reasoning in LLMs by effectively marrying discrete choices with probabilistic context. |
| Computer Vision | AutoMat: Enabling Automated Crystal Structure Reconstruction from
  Microscopy via Agentic Tool Use (Read more on [arXiv](https://arxiv.org/abs/2505.12650) or [HuggingFace](https://huggingface.co/papers/2505.12650))| Jiangjie Qiu, Xiao Chen, Yizhe Chen, IvanTang, yaotianvector | The paper introduces AutoMat, an agent-assisted pipeline for automated crystal structure reconstruction from scanning transmission electron microscopy (STEM) images. The research aims to bridge the gap between microscopy and atomistic simulation by automating the conversion of STEM images into simulation-ready formats. AutoMat employs pattern-adaptive denoising, physics-guided template retrieval, symmetry-aware atomic reconstruction, fast relaxation, and property prediction via MatterSim, orchestrated by an LLM agent. Evaluated on a new STEM2Mat-Bench dataset, AutoMat achieves a projected lattice RMSD of 0.11 ± 0.03 Å and outperforms existing vision-language models in structure reconstruction and energy prediction. This end-to-end system enables AI practitioners to leverage automated workflows for material modeling directly from microscopy data, enhancing efficiency and reproducibility. |
| Computer Vision | VARD: Efficient and Dense Fine-Tuning for Diffusion Models with
  Value-based RL (Read more on [arXiv](https://arxiv.org/abs/2505.15791) or [HuggingFace](https://huggingface.co/papers/2505.15791))| Bangyan Liao, Siteng Huang, Yufei Huang, Zifeng Zhuang, Fengyuan Dai | The paper introduces VARD, a novel approach for efficient fine-tuning of diffusion models using value-based reinforcement learning. It addresses the challenge of tailoring pre-trained diffusion models to specific properties by learning a value function to provide dense supervision throughout the generation process. VARD uses this value function with KL regularization for stable training via backpropagation, improving trajectory guidance and enabling applicability to non-differentiable rewards. Experiments demonstrate enhanced sample quality and training efficiency in protein structure design and text-to-image synthesis tasks. VARD's dense supervision and stabilization allow practitioners to optimize diffusion models for complex rewards more efficiently. |
| Natural Language Processing | RL Tango: Reinforcing Generator and Verifier Together for Language
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.15034) or [HuggingFace](https://huggingface.co/papers/2505.15034))| Duane S. Boning, Zhang-Wei Hong, Maohao Shen, Zhengqi Gao, sunshinekevin | The paper introduces TANGO, a novel reinforcement learning framework for enhancing language reasoning capabilities by jointly training an LLM generator and a generative, process-level LLM verifier. The research aims to overcome limitations of existing RL post-training methods that use fixed or discriminatively trained verifiers. TANGO employs an interleaved training approach where the generator produces reasoning trajectories, and the verifier provides step-level assessments, trained solely on outcome-level rewards. Experiments demonstrate state-of-the-art results on math benchmarks, with the generator achieving a 25.5% average relative improvement across five competition-level math benchmarks. TANGO offers a more effective design for co-evolving generator-verifier systems, improving reasoning strategies and generalization performance for AI practitioners. |
| Multi-Modal | Streamline Without Sacrifice - Squeeze out Computation Redundancy in LMM (Read more on [arXiv](https://arxiv.org/abs/2505.15816) or [HuggingFace](https://huggingface.co/papers/2505.15816))| Ziwei Liu, Lewei Lu, Penghao Wu | The paper introduces ProxyV, a novel approach to reduce computation in large multimodal models (LMMs) by addressing computation-level redundancy in vision tokens. The primary objective is to alleviate the computational burden on original vision tokens without sacrificing performance by introducing proxy vision tokens. ProxyV employs proxy tokens to handle heavy computations and guide updates to the original tokens through lightweight modules. Results on fine-grained benchmarks show ProxyV achieving 101% performance with 43% reduction in prefilling FLOPs and a 40% time reduction when applied to Vicuna1.5-7B. ProxyV offers AI practitioners a flexible method to enhance the efficiency of LMMs and potentially combine it with token reduction techniques. |
| Natural Language Processing | Evaluate Bias without Manual Test Sets: A Concept Representation
  Perspective for LLMs (Read more on [arXiv](https://arxiv.org/abs/2505.15524) or [HuggingFace](https://huggingface.co/papers/2505.15524))| Zirui Song, Chenxi Wang, Wei Liu, Kaiyang Wan, Lang Gao | This paper introduces BIASLENS, a test-set-free framework for evaluating bias in Large Language Models (LLMs) by analyzing concept representations. The research aims to overcome the limitations of manual test set creation by examining geometric alignment between intrinsic concept vectors. BIASLENS combines Concept Activation Vectors (CAVs) and Sparse Autoencoders (SAEs) to extract and compare intrinsic representations. Experiments demonstrate that BIASLENS achieves strong agreement with traditional bias evaluation metrics (Spearman correlation r > 0.85) while uncovering biases difficult to detect otherwise. The framework offers a scalable, interpretable, and efficient approach to bias discovery, enabling more transparent and fair LLMs. |
| Machine Learning | PiFlow: Principle-aware Scientific Discovery with Multi-Agent
  Collaboration (Read more on [arXiv](https://arxiv.org/abs/2505.15047) or [HuggingFace](https://huggingface.co/papers/2505.15047))| Hongyu Chen, Tao Lin, Yingming Pu | The paper introduces PiFlow, an information-theoretical framework for structured uncertainty reduction in automated scientific discovery using multi-agent systems. PiFlow treats scientific exploration as a structured uncertainty reduction problem guided by principles (e.g., scientific laws). The key methodology involves a Min-Max optimization to balance exploration and exploitation of scientific principles, integrated with LLM-based multi-agent systems. Experiments across nanomaterial structure discovery, bio-molecules, and superconductor candidates showed a 73.55% increase in AUC and a 94.06% enhancement in solution quality. PiFlow provides a plug-and-play method for AI-driven scientific discovery by establishing more efficient automated scientific discovery. |
| Multi-Modal | Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large
  Audio-Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.15406) or [HuggingFace](https://huggingface.co/papers/2505.15406))| Lang Gao, Mingzhe Li, Mingxuan Cui, Qian Jiang, Zirui Song | The paper introduces AJailBench, a benchmark for evaluating jailbreak vulnerabilities in Large Audio-Language Models (LAMs). The research aims to quantitatively assess and mitigate risks associated with harmful audio outputs from LAMs. It employs text-to-speech synthesis and an Audio Perturbation Toolkit (APT) to create adversarial audio prompts, evaluating LAMs using metrics like Attack Success Rate (ASR). The study reveals that even small, semantically preserved perturbations can significantly reduce the safety performance of LAMs, highlighting the need for robust defense mechanisms. AJailBench provides AI practitioners with a framework for assessing LAM safety and developing targeted defenses. |
| Natural Language Processing | WebNovelBench: Placing LLM Novelists on the Web Novel Distribution (Read more on [arXiv](https://arxiv.org/abs/2505.14818) or [HuggingFace](https://huggingface.co/papers/2505.14818))| Haidong Wang, Jun Zheng, Leon Lin | The paper introduces WebNovelBench, a benchmark for evaluating long-form story generation capabilities of Large Language Models (LLMs) by leveraging a dataset of over 4,000 Chinese web novels. The research aims to provide a scalable and objective framework for assessing LLM performance in generating narratives by framing the evaluation as a synopsis-to-story generation task. The methodology involves a multi-faceted assessment encompassing eight narrative quality dimensions, evaluated automatically using an LLM-as-Judge approach, and aggregated using Principal Component Analysis. Experiments demonstrate that WebNovelBench can effectively differentiate between human-written and LLM-generated content, ranking 24 LLMs and achieving up to 5.21 norm score. This provides AI practitioners with a data-driven approach to evaluate and improve LLM-driven narrative generation with replicable evaluation metrics. |
| Reinforcement Learning | Prior Prompt Engineering for Reinforcement Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2505.14157) or [HuggingFace](https://huggingface.co/papers/2505.14157))| Sarana Nutanong, Potsawee Manakul, kunato, pittawat | This paper explores prior prompt engineering (pPE) within reinforcement fine-tuning (RFT) to influence language model behavior.  The research investigates whether different pPE approaches can guide LMs to internalize distinct behaviors after RFT.  The methodology translates five inference-time prompt engineering strategies into corresponding pPE approaches and trains Qwen2.5-7B models using each.  Results show that all pPE-trained models surpass their iPE-prompted counterparts, with the null-example pPE achieving the largest average performance gain (surpassing the reasoning approach). These findings suggest pPE is a powerful axis for RFT, allowing practitioners to instill diverse behavioral styles in LMs. |
| Natural Language Processing | Language Specific Knowledge: Do Models Know Better in X than in English? (Read more on [arXiv](https://arxiv.org/abs/2505.14990) or [HuggingFace](https://huggingface.co/papers/2505.14990))| Dilek Hakkani-Tür, Nimet Beyza Bozdag, Ishika Agarwal | This paper introduces the concept of Language Specific Knowledge (LSK) in language models, where models exhibit better performance in certain languages for specific topics. The study investigates whether models can improve reasoning by performing chain-of-thought reasoning in languages other than English. It introduces LSKEXTRACTOR, a two-stage framework to identify and leverage expert languages, clustering queries and assigning expert languages based on performance. Results demonstrate an average relative improvement of 10% in accuracy across models and datasets using the LSKEXTRACTOR. The main implication is the potential to improve AI inclusivity by leveraging culturally and linguistically aligned language models. |
| Natural Language Processing | MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation
  of LLM Hallucinations (Read more on [arXiv](https://arxiv.org/abs/2505.14101) or [HuggingFace](https://huggingface.co/papers/2505.14101))| Johannes Bjerva, Katja Hose, Russa Biswas, ernlavr | The paper introduces MultiHal, a new multilingual benchmark for evaluating LLM hallucinations grounded in knowledge graphs. It addresses the lack of KG paths and multilinguality in existing factuality benchmarks by curating a dataset of 25.9k high-quality KG-paths from open-domain KGs after pruning noisy data. The primary methodology involves integrating entity linking and KG querying to generate questions, answers, and KG paths in multiple languages. Baseline evaluations show an increase of 0.12 to 0.36 points in semantic similarity score using KG-RAG over vanilla QA. MultiHal is expected to promote research into graph-based hallucination mitigation and fact-checking. |
| Multi-Modal | HumaniBench: A Human-Centric Framework for Large Multimodal Models
  Evaluation (Read more on [arXiv](https://arxiv.org/abs/2505.11454) or [HuggingFace](https://huggingface.co/papers/2505.11454))| Mukund S. Chettiar, Ashmal Vayani, Vahid Reza Khazaie, Aravind Narayanan, shainaraza | The paper introduces HumaniBench, a new benchmark for evaluating large multimodal models (LMMs) based on human-centered AI principles. It aims to address limitations in existing benchmarks by holistically assessing fairness, ethics, understanding, reasoning, language inclusivity, empathy, and robustness. The methodology involves a dataset of 32K real-world image-question pairs, annotated via a GPT-4o-assisted pipeline and verified by domain experts, across seven diverse tasks. Benchmarking 15 state-of-the-art LMMs reveals that proprietary models generally lead, but gaps remain in robustness and visual grounding, with open-source models struggling to balance accuracy with human-aligned principles. HumaniBench provides a rigorous test-bed for diagnosing alignment gaps and steering LMMs toward socially responsible behavior. |
