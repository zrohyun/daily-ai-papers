

## Papers for 2025-05-27

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Shifting AI Efficiency From Model-Centric to Data-Centric Compression (Read more on [arXiv](https://arxiv.org/abs/2505.19147) or [HuggingFace](https://huggingface.co/papers/2505.19147))| Pppeach33, coderchen01, Steven-Shaobo, zichenwen, xuyang-liu16 | This paper advocates for a paradigm shift in AI efficiency from model-centric to data-centric compression, focusing on token compression. It examines the shift in computational bottlenecks towards long token sequences and the quadratic cost of self-attention. The paper analyzes various token compression methods, their benefits, and challenges, proposing a unified mathematical framework. It empirically shows that token compression can achieve comparable results to more complex compression methods. The authors suggest that token compression represents a universally applicable and compatible strategy for enhancing the efficiency of next-generation LLMs and MLLMs. |
| Natural Language Processing | Mutarjim: Advancing Bidirectional Arabic-English Translation with a
  Small Language Model (Read more on [arXiv](https://arxiv.org/abs/2505.17894) or [HuggingFace](https://huggingface.co/papers/2505.17894))| Sara Chrouf, ZeinaD, Moatasem444, hr99, Hennara | This paper introduces Mutarjim, a compact language model for bidirectional Arabic-English translation. The research focuses on developing a task-specific model that balances performance with efficiency, effectively addressing the complexities of Arabic linguistics. Mutarjim leverages a two-phase training approach (pre-training and fine-tuning) and achieves state-of-the-art performance on the newly introduced Tarjama-25 benchmark, surpassing larger models like GPT-4o mini in English-to-Arabic translation (exact metrics are not specified in the provided summary). The findings suggest that smaller, task-specific models can rival larger models in specific domains while reducing computational costs, offering a practical solution for low-resource settings. |
| Natural Language Processing | BizFinBench: A Business-Driven Real-World Financial Benchmark for
  Evaluating LLMs (Read more on [arXiv](https://arxiv.org/abs/2505.19457) or [HuggingFace](https://huggingface.co/papers/2505.19457))| Ji Liu, Qlisp, Tinker250, xuntao, guilong | BizFinBench is introduced as a novel benchmark for evaluating Large Language Models (LLMs) in real-world financial applications. The paper aims to address limitations of existing benchmarks by providing a business-driven, contextually complex, and robust dataset. It introduces a benchmark of 6,781 queries, spanning five dimensions and nine categories, and a novel evaluation method IteraJudge. Evaluation of 25 models reveals distinct capability patterns, with no single model dominating; for example, Claude-3.5-Sonnet achieved 63.18 in Numerical Calculation. The benchmark facilitates a rigorous assessment of LLMs in finance, highlighting the need for cross-concept reasoning capabilities. |
| Natural Language Processing | Embodied Agents Meet Personalization: Exploring Memory Utilization for
  Personalized Assistance (Read more on [arXiv](https://arxiv.org/abs/2505.16348) or [HuggingFace](https://huggingface.co/papers/2505.16348))| jinyeo, ej0cl6, bwookwak, Lune-Blue, Connoriginal | The paper introduces MEMENTO, a novel evaluation framework for assessing memory utilization in embodied agents providing personalized assistance. It investigates how well agents leverage episodic memory containing personalized knowledge to interpret user instructions in object rearrangement tasks. The framework consists of Memory Acquisition and Memory Utilization stages to quantify the impact of memory. Experiments reveal a 30.5% performance drop for GPT-40 in joint-memory tasks, indicating challenges in utilizing multiple memories, particularly involving user patterns. The findings highlight limitations in current LLM-powered embodied agents and offer insights for future research in personalized agent development. |
| Computer Vision | Alchemist: Turning Public Text-to-Image Data into Generative Gold (Read more on [arXiv](https://arxiv.org/abs/2505.19297) or [HuggingFace](https://huggingface.co/papers/2505.19297))| Sergey Kastryulin, Dmitry Baranchuk, Alexey Kirillov, Alexander Ustyuzhanin, sharfikeg | The paper introduces Alchemist, a method for creating high-quality, general-purpose supervised fine-tuning (SFT) datasets for text-to-image models using a pre-trained generative model as a data quality estimator. It addresses the challenge of generating diverse and aesthetically pleasing images by leveraging a generative model to identify high-impact samples from public datasets. The methodology involves a multi-stage filtering process combined with a diffusion-based scoring function to refine a vast pool of images into a compact dataset of 3,350 samples. Experiments demonstrate that fine-tuning five public T2I models with Alchemist substantially improves their generative quality and aesthetics, achieving human preference win rates up to 20% higher compared to baseline models. Alchemist provides a data-efficient approach for practitioners to improve text-to-image models, enabling the production of higher quality outputs without relying on extensive or proprietary data. |
| Natural Language Processing | PATS: Process-Level Adaptive Thinking Mode Switching (Read more on [arXiv](https://arxiv.org/abs/2505.19250) or [HuggingFace](https://huggingface.co/papers/2505.19250))| Shujian Huang, Jiajun Chen, Shimao Zhang, master-lan, Yi53 | The paper introduces PATS, a process-level adaptive thinking mode switching paradigm for LLMs. It addresses the imbalance between accuracy and efficiency by dynamically adjusting the reasoning strategy at each step based on difficulty. PATS integrates Process Reward Models (PRMs) with Beam Search, incorporating progressive mode switching and bad-step penalty mechanisms. Experiments on mathematical benchmarks demonstrate high accuracy with moderate token usage, achieving an average accuracy close to the All-complex setting while using 55.4% of its tokens. This approach emphasizes the significance of process-level, difficulty-aware reasoning strategy adaptation for efficient LLM inference. |
| Natural Language Processing | ARM: Adaptive Reasoning Model (Read more on [arXiv](https://arxiv.org/abs/2505.20258) or [HuggingFace](https://huggingface.co/papers/2505.20258))| Kai Zhang, Aili Chen, Arist12, hsaest, Siye01 | The paper introduces the Adaptive Reasoning Model (ARM) for adaptively selecting reasoning formats based on task difficulty to balance performance and computational efficiency. The research aims to mitigate the "overthinking" problem in large reasoning models by enabling them to use appropriate reasoning formats like Direct Answer, Short CoT, Code, or Long CoT. ARM is trained using Ada-GRPO, a modified Group Relative Policy Optimization to prevent format collapse. Results show that ARM achieves comparable performance to models using only Long CoT, while reducing token usage by ~30% (and up to ~70% in some instances) and offering a 2x training speedup. The implication for AI practitioners is a more efficient and flexible reasoning model that can adapt to diverse tasks without unnecessary computational overhead. |
| Natural Language Processing | Enigmata: Scaling Logical Reasoning in Large Language Models with
  Synthetic Verifiable Puzzles (Read more on [arXiv](https://arxiv.org/abs/2505.19914) or [HuggingFace](https://huggingface.co/papers/2505.19914))| Zhicheng Cai, Aili Chen, siyuyuan, Abbey4799, jiangjiechen | This paper introduces ENIGMATA, a suite for improving logical reasoning in LLMs using synthetic, verifiable puzzles. The research focuses on scaling LLMs' logical reasoning through multi-task reinforcement learning (RL) with automated verification. The methodology involves creating a diverse set of 36 puzzle tasks with generators and rule-based verifiers, enabling multi-task RL training and RLVR integration. The Qwen2.5-32B-ENIGMATA model surpasses baselines like 03-mini-high on puzzle benchmarks, achieving 32.8% on ARC-AGI and 62.6% on ENIGMATA-Eval, while larger models show improved math and STEM reasoning. ENIGMATA offers a controllable framework to advance logical reasoning in LLMs and shows potential for generalization to knowledge-intensive tasks. |
| Machine Learning | B-score: Detecting biases in large language models using response
  history (Read more on [arXiv](https://arxiv.org/abs/2505.18545) or [HuggingFace](https://huggingface.co/papers/2505.18545))| Daeyoung Kim, anhng8, taesiri, anvo25 | This paper introduces B-score, a novel metric for detecting biases in large language models (LLMs) by analyzing response history in multi-turn conversations. The research investigates whether LLMs can self-debias by observing their prior answers to the same question across subjective, random, and objective question types. The proposed B-score measures the difference in answer probabilities between single-turn and multi-turn settings, effectively capturing bias in model responses. Experiments demonstrate that leveraging B-score substantially improves answer verification accuracy by +9.3% compared to verbalized confidence scores alone on a proposed question set. The B-score metric can be employed by AI practitioners to identify and mitigate biases in LLM outputs without requiring ground truth labels or extensive human evaluations. |
| Natural Language Processing | Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective (Read more on [arXiv](https://arxiv.org/abs/2505.19815) or [HuggingFace](https://huggingface.co/papers/2505.19815))| Linchen Xiao, Hongwei Liu, zsytony, Sudanl, jnanliu | This paper introduces RAML, a framework to understand LLM reasoning through a meta-learning perspective, treating reasoning trajectories as pseudo-gradient descent updates. It investigates the connection between LLM reasoning and meta-learning by formalizing reasoning tasks as a meta-learning setup, where each question represents a task. Empirically, the paper shows a strong correspondence between LLM reasoning and meta-learning, and that SFT leads to a more stable inner loop optimization. The framework facilitates the application of meta-learning insights for enhancing and optimizing LLM reasoning capabilities. |
| Natural Language Processing | Lifelong Safety Alignment for Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.20259) or [HuggingFace](https://huggingface.co/papers/2505.20259))| Min Lin, Chao Du, Yifei Zhao, Zeyu Qin, Haoyu Wang | This paper proposes a lifelong safety alignment framework for language models (LLMs) to continuously adapt to evolving jailbreaking strategies. The research addresses the challenge of LLMs' vulnerability to unseen attacks by introducing a competitive setup between a Meta-Attacker (discovering new attacks) and a Defender (resisting them). The methodology involves warming up the Meta-Attacker with insights from jailbreak-related research papers and iterative adversarial training. The first iteration Meta-Attacker achieves a 73% attack success rate (ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks, while the Defender reduces the Meta-Attacker's success rate to 7%. The framework enables safer and more reliable deployment of LLMs in open-ended environments by improving robustness against both seen and unseen attacks. |
| Natural Language Processing | MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis
  Discovery via Hierarchical Search (Read more on [arXiv](https://arxiv.org/abs/2505.19209) or [HuggingFace](https://huggingface.co/papers/2505.19209))| Wei Li, Yujie Liu, Ben Gao, Wanhao Liu, ZonglinY | The paper explores the limits of Large Language Models (LLMs) in generating fine-grained scientific hypotheses. It addresses the problem of coarse hypothesis generation by formulating it as a combinatorial optimization problem tackled via hierarchical search. The methodology involves incrementally refining hypotheses through levels of abstraction, guided by the LLM's internal heuristics to optimize a reward landscape. Empirical results demonstrate that the proposed method consistently outperforms strong baselines in LLM self-evaluation and expert evaluation, achieving higher recall against expert-annotated ground-truth hypotheses, but quantitative metrics are not specified. These findings imply that hierarchical search can effectively leverage LLMs for detailed scientific hypothesis discovery, but convergence is not guaranteed. |
| Multi-Modal | Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual
  Reasoning from Transit Maps (Read more on [arXiv](https://arxiv.org/abs/2505.18675) or [HuggingFace](https://huggingface.co/papers/2505.18675))| Lingdong Kong, Shuyi Ouyang, Song Wang, Huan-WhoRegisteredMyName, FSCCS | The paper introduces REASONMAP, a benchmark for evaluating fine-grained visual understanding and spatial reasoning abilities of Multimodal Large Language Models (MLLMs) using transit maps. It investigates the performance of MLLMs on tasks requiring precise spatial interpretation from high-resolution transit maps across 30 cities. The study utilizes a two-level evaluation framework, assessing both answer correctness and quality through accuracy and a proposed map score. Results show a counterintuitive trend: open-source base models outperform reasoning variants, while closed-source models exhibit the opposite behavior, with top models reaching ~60% weighted accuracy. This highlights the need for improved integration of multimodal information for robust visual reasoning in AI systems. |
| Multi-Modal | Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal
  Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.18536) or [HuggingFace](https://huggingface.co/papers/2505.18536))| Yifei Zhao, Yifu Luo, Bo Xia, Jiaqi Wu, Haoyuan Sun | This position paper investigates the role of reinforcement fine-tuning (RFT) in enhancing the reasoning capabilities of multimodal large language models (MLLMs). It aims to summarize the improvements of RFT in powering reasoning capability of MLLMs and propose future research directions. The paper provides a comprehensive overview of RFT algorithms, categorizing them as Critic-Model-Driven and Critic-Model-Free and summarizes recent works on RFT for MLLMs. The authors highlight advancements in diverse modalities, tasks, training algorithms, and benchmarks, and propose future research directions for the community. It hopes to provide valuable insights towards advancement in AGI. |
| Reinforcement Learning | Surrogate Signals from Format and Length: Reinforcement Learning for
  Solving Mathematical Problems without Ground Truth Answers (Read more on [arXiv](https://arxiv.org/abs/2505.19439) or [HuggingFace](https://huggingface.co/papers/2505.19439))| Dianbo Sui, Yupeng Zhang, Zecheng Wang, Han Liu, Rihui Xin | This paper explores reinforcement learning (RL) for mathematical problem-solving using format and length as surrogate signals to bypass the need for ground truth answers. The research investigates a Group Relative Policy Optimization (GRPO) approach, rewarding format correctness and penalizing deviations in length from an optimal range.  The study achieved 40.0% accuracy on AIME2024 using a 7B base model by matching and surpassing the performance of standard GRPO with ground truth answers in certain scenarios.  It shows that format and length signals can effectively guide LLMs in mathematical reasoning. The findings offer a practical approach to training LLMs for mathematical tasks when ground truth data is scarce. |
| Natural Language Processing | Flex-Judge: Think Once, Judge Anywhere (Read more on [arXiv](https://arxiv.org/abs/2505.18601) or [HuggingFace](https://huggingface.co/papers/2505.18601))| Se-Young Yun, Sungwoo Cho, Jongwoo Ko, sungnyun | This paper introduces FLEX-Judge, a multimodal judge model designed for efficient and generalizable evaluation across diverse modalities. The research question focuses on whether a small amount of textual reasoning data can effectively train a cost-efficient, modality-agnostic judge model. FLEX-Judge leverages minimal textual reasoning data to enable robust generalization across modalities, such as images, videos, and molecular structures, by fine-tuning a multimodal language model. Experimental results demonstrate that FLEX-Judge achieves competitive or superior performance compared to state-of-the-art commercial APIs and extensively trained multimodal evaluators, notably achieving 62.46% agreement on the MJ-Bench alignment task. FLEX-Judge provides a scalable and cost-effective alternative to traditional annotation-intensive approaches for multimodal model evaluation. |
| Machine Learning | Which Data Attributes Stimulate Math and Code Reasoning? An
  Investigation via Influence Functions (Read more on [arXiv](https://arxiv.org/abs/2505.19949) or [HuggingFace](https://huggingface.co/papers/2505.19949))| Zhijie Deng, Zihao Zeng, Hanwen Xu, Qingyuan Tian, Siqi Kou | The paper investigates the attributes of training data that stimulate math and code reasoning in large language models (LLMs). It leverages influence functions to attribute LLMs' reasoning ability to individual training examples, sequences, and tokens, offering insights into effective data characteristics. The methodology involves fine-tuning LLMs on datasets and computing influence functions to identify impactful training data. Key results show that flipping task difficulty improves AIME24 accuracy from 10% to 20% and LiveCodeBench accuracy for Qwen2.5-7B-Instruct, and that exploratory behaviors enhance reasoning performance.  These findings suggest that strategic data curation, including difficulty-based reweighting, and understanding sequence-level behaviors can improve LLM reasoning abilities for AI practitioners. |
| Machine Learning | Discrete Markov Bridge (Read more on [arXiv](https://arxiv.org/abs/2505.19752) or [HuggingFace](https://huggingface.co/papers/2505.19752))| Ying Nian Wu, Song-Chun Zhu, zlzheng, ColorfulAI, henry12348 | The paper introduces Discrete Markov Bridge (DMB), a novel variational framework for discrete representation learning that addresses limitations of fixed-rate transition matrices in discrete diffusion models. DMB leverages matrix-learning and score-learning components with theoretical guarantees on convergence and space complexity. The approach aims to enhance latent representation expressiveness and expand the design space in discrete data modeling. Empirical evaluation on the Text8 dataset yields an Evidence Lower Bound (ELBO) of 1.38, outperforming existing baselines. This suggests DMB offers improved performance and a unified framework for discrete representation learning, potentially enabling more effective modeling of complex discrete data. |
| Reinforcement Learning | Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System
  Collaboration (Read more on [arXiv](https://arxiv.org/abs/2505.20256) or [HuggingFace](https://huggingface.co/papers/2505.20256))| Zheng Huang, Zongze Du, Muzhi Zhu, Hao Zhong, Canyu | The paper introduces Omni-R1, a reinforcement learning framework for omnimodal reasoning by addressing the trade-off between long-horizon reasoning and fine-grained pixel understanding. It aims to improve omnimodal models' performance by decomposing the problem into a Global Reasoning System and a Detail Understanding System, trained via RL. The key methodology involves Group Relative Policy Optimization to train the Global Reasoning System through collaboration with the Detail Understanding System. Experiments on RefAVS and REVOS show Omni-R1 surpasses supervised baselines, improving out-of-domain generalization and multimodal hallucination; for example, it achieves a +16.4% improvement in J&F score on RefAVS-Unseen. Omni-R1 offers a scalable path for applying RL to large-scale omnimodal reasoning, potentially advancing universally foundation models. |
| Machine Learning | Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured
  Multi-Turn Decomposition (Read more on [arXiv](https://arxiv.org/abs/2505.19788) or [HuggingFace](https://huggingface.co/papers/2505.19788))| Zhijie Deng, Hao Zhang, Boxiu Li, Zihao Zeng, ElysiaTrue | The paper introduces Multi-Turn Decomposition (MinD), a method for improving the reasoning efficiency of large language models. MinD restructures the reasoning process into explicit, structured multi-turn interactions where each turn yields a candidate answer, enabling explicit control over the iterative reasoning process. They use a supervised fine-tuning (SFT) then reinforcement learning (RL) paradigm, training on the MATH dataset using R1-Distill models. MinD achieves up to ~70% reduction in output token usage and a 4.2x speedup in time to first token (TTFT) while maintaining performance. This technique offers AI practitioners a way to significantly reduce latency and computational costs in reasoning models by explicitly structuring and controlling the reasoning process. |
| Multi-Modal | Hard Negative Contrastive Learning for Fine-Grained Geometric
  Understanding in Large Multimodal Models (Read more on [arXiv](https://arxiv.org/abs/2505.20152) or [HuggingFace](https://huggingface.co/papers/2505.20152))| Ji Qi, Jiajie Zhang, Zhen Yang, Yushi Bai, Kai Sun | The paper proposes a novel hard negative contrastive learning framework (MMCLIP) to enhance geometric understanding in large multimodal models (LMMs). The objective is to improve the visual encoder's ability to recognize fine-grained geometric elements for geometric problem-solving. MMCLIP combines image-based contrastive learning with generation-based hard negatives and text-based contrastive learning with rule-based and retrieval-based negatives. The resulting model, MMGeoLM, outperforms existing open-source models and rivals closed-source models, achieving state-of-the-art performance on MM-MATH by surpassing GPT-4o by 7.5%. AI practitioners can leverage MMCLIP to significantly improve LMM performance in geometrically-intensive visual reasoning tasks. |
| Natural Language Processing | The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT
  Distillation (Read more on [arXiv](https://arxiv.org/abs/2505.18759) or [HuggingFace](https://huggingface.co/papers/2505.18759))| Song Wang, Zhen Tan, Rana Muhammad Shahroz Khan, Ruichen Zhang, wjldw | This paper introduces DC-CoT, a benchmark for data-centric chain-of-thought (CoT) distillation in large language models (LLMs). It investigates how data manipulation techniques affect the performance of student LLMs distilled from various teacher models. The methodology involves augmenting, selecting, and mixing CoT data and evaluating student model performance across IID and OOD reasoning datasets. Experiments with Llama-3.1-8B showed that reverse augmentation improves average accuracy by 24.64% compared to Vanilla CoT. The benchmark provides insights into optimizing CoT distillation through data-centric approaches, enabling the development of more efficient reasoning models. |
| Computer Vision | Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV
  Cache Compression (Read more on [arXiv](https://arxiv.org/abs/2505.19602) or [HuggingFace](https://huggingface.co/papers/2505.19602))| Jenq-Neng Hwang, Cheng-Yen Yang, Zigeng Chen, stargazerx0 | The paper introduces ScaleKV, a KV cache compression framework for visual autoregressive (VAR) models. It addresses the memory bottleneck of VAR models by categorizing transformer layers into drafters and refiners based on attention patterns. ScaleKV achieves a 10x memory reduction on Infinity-8B while preserving pixel-level fidelity with a GenEval score of 0.79. The method enables efficient deployment and scaling of VAR models in resource-constrained environments. By identifying and differentiating caching needs across transformer layers, ScaleKV reduces memory consumption and maintains high generation quality. |
| Reinforcement Learning | Learning to Reason without External Rewards (Read more on [arXiv](https://arxiv.org/abs/2505.19590) or [HuggingFace](https://huggingface.co/papers/2505.19590))| Dawn Song, Sergey Levine, Aosong Feng, Zhewei Kang, Xuandong | The paper introduces Reinforcement Learning from Internal Feedback (RLIF), enabling LLMs to learn complex reasoning using intrinsic signals. It explores whether LLMs can enhance reasoning abilities relying solely on self-generated signals without external supervision. INTUITOR, an RLIF method, uses a model's self-certainty as its reward signal, replacing external rewards in Group Relative Policy Optimization (GRPO). Experiments on mathematical benchmarks show INTUITOR matches GRPO's performance while achieving superior generalization in code generation tasks (65% relative improvement on LiveCodeBench). This indicates intrinsic model signals drive effective learning, offering a scalable alternative to RLVR where verifiable rewards are unavailable. |
| Natural Language Processing | From Tens of Hours to Tens of Thousands: Scaling Back-Translation for
  Speech Recognition (Read more on [arXiv](https://arxiv.org/abs/2505.16972) or [HuggingFace](https://huggingface.co/papers/2505.16972))| Shanbo Cheng, Wei Lu, Lu Xu, Tianduo Wang | The paper introduces Speech Back-Translation, a scalable pipeline for enhancing multilingual ASR models. It aims to alleviate data scarcity by converting large-scale text corpora into synthetic speech using off-the-shelf TTS models. The methodology involves fine-tuning TTS models with limited real transcribed speech and then generating synthetic data at scale. Results show an average transcription error reduction of over 30% on Whisper-large-v3. The implication is that synthetic data can significantly augment training, making ASR more accessible for low-resource languages. |
| Natural Language Processing | AdaCtrl: Towards Adaptive and Controllable Reasoning via
  Difficulty-Aware Budgeting (Read more on [arXiv](https://arxiv.org/abs/2505.18822) or [HuggingFace](https://huggingface.co/papers/2505.18822))| Jiazhan Feng, Zhaochen Su, Wanjun Zhong, Hongru Wang, JoeYing | The paper introduces AdaCtrl, a framework for adaptive and controllable reasoning in large language models by dynamically adjusting reasoning length based on problem difficulty. It addresses the efficiency/effectiveness trade-off by employing a two-stage training pipeline: initial fine-tuning for difficulty awareness, followed by difficulty-aware reinforcement learning. AdaCtrl achieves performance gains and reduces response length by up to 12.14% on AIME datasets and 91.04% on GSM8K, compared to standard baselines. It also provides explicit user control via length-triggered tags. The framework enables more efficient and user-controlled reasoning in LLMs by explicitly modelling problem difficulty. |
| Reinforcement Learning | G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language
  Model via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.13426) or [HuggingFace](https://huggingface.co/papers/2505.13426))| Flood Sung, Zhiqi Huang, Tianyu Liu, Hongcheng Gao, Liang Chen | The paper introduces VLM-Gym and G1 models for enhancing VLM decision-making in visually rich environments. It addresses the "knowing-doing" gap in VLMs by training agents via reinforcement learning in a diverse set of visual games. The key methodology involves a perception-enhanced cold start and knowledge distillation prior to RL fine-tuning. G1 models consistently surpass their teacher across games and outperform leading proprietary models (e.g., exceeding Claude-3.7-Sonnet-Thinking), demonstrating perception and reasoning bootstrapping. This suggests that VLMs can effectively learn interactive tasks using a curated RL environment and a perception-enhanced training approach, enabling their application in autonomous agent development. |
| Machine Learning | The Coverage Principle: A Framework for Understanding Compositional
  Generalization (Read more on [arXiv](https://arxiv.org/abs/2505.20278) or [HuggingFace](https://huggingface.co/papers/2505.20278))| Miyoung Ko, Sohee Yang, Hanseul Cho, Jinho Park, Hoyeon Chang | The paper introduces the coverage principle, a data-centric framework, to understand compositional generalization in large language models. It investigates the limitations of pattern matching in Transformers for tasks requiring systematic compositional reasoning. The methodology derives and empirically validates that training data requirements for two-hop generalization grow quadratically with token set size, showing no improvement with 20x parameter scaling. Furthermore, the framework introduces structure-based, property-based and shared-operator generalizations. The proposed framework provides a unified lens for understanding compositional reasoning, highlighting the need for fundamental innovations in architecture or training. |
| Machine Learning | Accelerating Nash Learning from Human Feedback via Mirror Prox (Read more on [arXiv](https://arxiv.org/abs/2505.19731) or [HuggingFace](https://huggingface.co/papers/2505.19731))| Denis Belomestny, Daniele Calandriello, misovalko, kashif, dtiapkin | This paper introduces Nash Mirror Prox (NashMP), an online algorithm for Nash Learning from Human Feedback (NLHF), designed for faster and more stable convergence to the Nash equilibrium. The primary objective is to improve the efficiency of finding Nash Equilibria in regularized preference games within NLHF. NashMP leverages a Mirror Prox optimization scheme, involving iterative mirror descent steps against improved opponent policies, and an approximation technique using stochastic policy gradients for implementation in deep learning architectures. Theoretical analysis demonstrates last-iterate linear convergence to the regularized Nash equilibrium at a rate of O((1 + 2β)^(-N/2)/β), improving over existing methods like NashMD, while experimental results on synthetic and LLM tasks show competitive performance. The results indicate that NashMP can improve preference learning from human feedback in large language models by improving last-iterate convergence. |
| Reinforcement Learning | Interleaved Reasoning for Large Language Models via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2505.19640) or [HuggingFace](https://huggingface.co/papers/2505.19640))| Yanchao Sun, Dong Lin, Deepak Gopinath, David Qiu, Roy Xie | This paper introduces interleaved reasoning, a reinforcement learning (RL) paradigm for large language models (LLMs) where thinking and answering alternate to reduce latency. The research aims to improve reasoning efficiency by enabling timely feedback and reducing Time-to-First-Token (TTFT). The proposed method uses a rule-based reward system to incentivize correct intermediate steps during RL training with PPO, GRPO, and REINFORCE++. Experiments demonstrate up to 19.3% Pass@1 accuracy improvement and an average of 80% TTFT reduction, even generalizing to unseen tasks like MATH and GPQA. Interleaved reasoning offers AI practitioners a path to build LLMs that are both more accurate and interactive. |
| Natural Language Processing | WINA: Weight Informed Neuron Activation for Accelerating Large Language
  Model Inference (Read more on [arXiv](https://arxiv.org/abs/2505.19427) or [HuggingFace](https://huggingface.co/papers/2505.19427))| Colby Banbury, Jongwoo Ko, Dan Zhao, Sihan Chen, tianyic | The paper introduces WINA, a training-free sparse activation framework for accelerating large language model inference. It aims to enhance inference efficiency by jointly considering hidden state magnitudes and weight matrix norms to determine neuron activation. WINA constructs sparse sub-networks based on this joint criterion, obtaining optimal approximation error bounds theoretically tighter than existing methods like TEAL. Empirically, WINA achieves up to 2.94% better average performance than TEAL at the same sparsity levels. WINA provides a new approach for efficient inference and a robust baseline for practitioners deploying LLMs. |
| Natural Language Processing | LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language
  Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2505.19223) or [HuggingFace](https://huggingface.co/papers/2505.19223))| zhenxuan00, jrwen, lyk423, surfingtomchen, xiaolu0714 | The paper introduces Variance-Reduced Preference Optimization (VRPO) for aligning masked diffusion models with human preferences. It addresses the high variance in ELBO-based likelihood estimates during preference optimization. VRPO incorporates unbiased variance reduction techniques, including optimal Monte Carlo budget allocation and antithetic sampling, leading to reduced bias and variance in preference optimization gradients. Applied to LLaDA, the resulting LLaDA 1.5 model shows improvements across benchmarks like GSM8K (+4.7) and HumanEval (+3.0) compared to its SFT-only predecessor, suggesting a viable path for RL-based alignment of diffusion models. The variance reduction techniques are potentially applicable to other alignment algorithms. |
| Natural Language Processing | ModernGBERT: German-only 1B Encoder Model Trained from Scratch (Read more on [arXiv](https://arxiv.org/abs/2505.13136) or [HuggingFace](https://huggingface.co/papers/2505.13136))| Andreas Hotho, Fotis Jannidis, Jan Pfister, Julia Wunderle, Anton Ehrmanntraut | The paper introduces ModernGBERT, a family of German-only encoder models trained from scratch. It explores the trade-offs of training encoders versus converting decoders (LLäMmlein2Vec) for the German language, with a focus on resource-constrained applications. ModernGBERT 1B outperforms previous state-of-the-art German encoders on the SuperGLEBer benchmark, achieving a score of 0.808. The study indicates that dedicated encoder training surpasses decoder conversion in terms of performance and parameter efficiency for German NLP tasks. The release of ModernGBERT models and training data provides valuable resources for the German NLP community, enabling efficient and transparent encoder solutions. |
| Machine Learning | Position: Mechanistic Interpretability Should Prioritize Feature
  Consistency in SAEs (Read more on [arXiv](https://arxiv.org/abs/2505.20254) or [HuggingFace](https://huggingface.co/papers/2505.20254))| Zeyu Tang, Lingjing Kong, Yujia Zheng, aashiqmuhamed, xiangchensong | The paper advocates for prioritizing feature consistency in Sparse Autoencoders (SAEs) used for mechanistic interpretability. It addresses the challenge of inconsistent SAE features across training runs, which undermines the reliability of interpretations. The paper proposes using the Pairwise Dictionary Mean Correlation Coefficient (PW-MCC) to measure feature consistency and demonstrates high levels are achievable (PW-MCC ≈ 0.80 for TopK SAEs on LLM activations) with appropriate architectural choices. The results show high feature consistency strongly correlates with the semantic similarity of learned feature explanations.  The paper suggests a community-wide shift towards systematically measuring feature consistency to foster robust progress in mechanistic interpretability. |
| Machine Learning | MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research (Read more on [arXiv](https://arxiv.org/abs/2505.19955) or [HuggingFace](https://huggingface.co/papers/2505.19955))| Ailin Deng, Wei Han, Yujie Lu, happymio, chchenhui | This paper introduces MLR-Bench, a comprehensive benchmark for evaluating AI agents on open-ended machine learning research. The study aims to rigorously assess the ability of AI agents to conduct end-to-end machine learning research and identify key failure modes. The methodology involves three key components: (1) 201 research tasks from major machine learning conferences; (2) MLR-Judge, an automated evaluation framework combining LLM-based reviewers with carefully designed review rubrics; and (3) MLR-Agent, a modular agent scaffold capable of completing research tasks through four stages. Results show that current coding agents frequently produce fabricated or invalidated experimental results in approximately 80% of cases, posing a significant barrier to scientific reliability. MLR-Bench provides a valuable tool for benchmarking, diagnosing, and improving AI research agents towards trustworthy and transparent scientific discovery. |
| Other | Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications
  of Agentic AI (Read more on [arXiv](https://arxiv.org/abs/2505.19443) or [HuggingFace](https://huggingface.co/papers/2505.19443))| Manoj Karkee, Konstantinos I. Roumeliotis, RanjanSapkota | This review analyzes vibe coding and agentic coding paradigms in AI-assisted software development. The objective is to provide a comprehensive comparison of these paradigms, focusing on autonomy, architectural design, and developer roles. Through workflow analysis and 20 use cases, the authors compare conceptual foundations, execution models, feedback loops, and safety mechanisms. The paper suggests that successful AI software engineering relies on harmonizing the strengths of both vibe and agentic coding within a human-centered development lifecycle, though quantitative metrics directly from experiments are not provided. This synthesis informs the development of better tooling, clearer expectations, and more robust human-AI collaboration. |
| Reinforcement Learning | Rethinking the Sampling Criteria in Reinforcement Learning for LLM
  Reasoning: A Competence-Difficulty Alignment Perspective (Read more on [arXiv](https://arxiv.org/abs/2505.17652) or [HuggingFace](https://huggingface.co/papers/2505.17652))| Jingang Wang, Wei Wang, Qi Guo, xixy, DeyangKong | The paper introduces Competence-Difficulty Alignment Sampling (CDAS) to improve sample efficiency in reinforcement learning for large language model reasoning. CDAS tackles unstable difficulty estimations by aggregating historical problem performance discrepancies and aligning problem difficulty with model competence through a fixed-point system. The key methodology involves modeling problem difficulty as a trajectory of performance discrepancies and using the centroid to provide stable assessment. Experiments show CDAS achieves a 46.77% average accuracy on mathematical benchmarks and reduces computation by 2.33 times compared to dynamic sampling. This suggests practitioners can enhance RL training efficiency by dynamically aligning problem difficulty with model competence based on aggregated historical performance data. |
| Natural Language Processing | StructEval: Benchmarking LLMs' Capabilities to Generate Structural
  Outputs (Read more on [arXiv](https://arxiv.org/abs/2505.20139) or [HuggingFace](https://huggingface.co/papers/2505.20139))| Yuxuan Zhang, Sherman Siu, Lipeng He, Dongfu Jiang, Jialin Yang | The paper introduces StructEval, a benchmark for evaluating LLMs' ability to generate structured outputs in various formats. It systematically assesses structural fidelity through generation and conversion tasks across 18 formats, utilizing novel metrics for format adherence and structural correctness. Results indicate performance gaps, with even state-of-the-art models achieving only 75.58 average score.  Generation tasks are more challenging than conversion tasks, and generating correct visual content is more difficult than text-only structures. The findings highlight the need for improved capabilities in structured output generation for real-world applications. |
| Multi-Modal | InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer
  Interaction (Read more on [arXiv](https://arxiv.org/abs/2505.10887) or [HuggingFace](https://huggingface.co/papers/2505.10887))| Xi Xie, Winson Chen, Zijian Zhang, Weitai Kang, Bin12345 | The paper introduces InfantAgent-Next, a generalist agent capable of multimodal computer interaction using text, images, audio, and video. It aims to address the limitations of existing agents by integrating tool-based and pure vision agents within a modular architecture. The methodology involves a detailed modularization of agent workflows, tool selection, and tool execution, unifying tool-based and pure-vision agent paradigms. InfantAgent-Next achieved 7.27% accuracy on OSWorld, outperforming Claude-Computer-Use. This generalist approach can facilitate the development of more versatile and accurate AI agents for computer automation tasks. |
| Reinforcement Learning | GLEAM: Learning Generalizable Exploration Policy for Active Mapping in
  Complex 3D Indoor Scenes (Read more on [arXiv](https://arxiv.org/abs/2505.20294) or [HuggingFace](https://huggingface.co/papers/2505.20294))| Jiangmiao Pang, Tao Huang, Quanyi Li, Tai Wang, Xiao-HF | The paper introduces GLEAM, a generalizable exploration policy for active mapping in complex 3D indoor scenes. It addresses the challenge of creating exploration policies that can effectively map diverse and complex indoor environments. GLEAM utilizes semantic representations, long-term navigable goals, and randomized training strategies, trained on a new large-scale benchmark called GLEAM-Bench. The method achieves a 66.50% coverage ratio on unseen complex scenes, surpassing existing methods by 9.49%. This provides AI practitioners with a more robust and generalizable active mapping solution, potentially reducing the need for environment-specific fine-tuning. |
| Natural Language Processing | Error Typing for Smarter Rewards: Improving Process Reward Models with
  Error-Aware Hierarchical Supervision (Read more on [arXiv](https://arxiv.org/abs/2505.19706) or [HuggingFace](https://huggingface.co/papers/2505.19706))| Soujanya Poria, Chuan Li, Amir Zadeh, Panshul Sharma, Tej Deep Pala | The paper introduces PathFinder-PRM, a hierarchical process reward model for improving mathematical reasoning in LLMs. It aims to enhance process supervision by decoupling error detection and reward estimation, categorizing errors into math and consistency types. The methodology involves classifying error types at each step and using these signals to estimate step correctness, training on a 400K-sample dataset with enriched step-level labels. PathFinder-PRM achieves a state-of-the-art PRMScore of 67.7 on PRMBench, outperforming prior models and demonstrating a +1.5 point gain in prm@8 at 48.3 when applied to reward-guided greedy search. This suggests that error-aware hierarchical supervision can significantly improve reward-guided mathematical reasoning with greater data efficiency for AI practitioners. |
| Reinforcement Learning | DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning
  System for Multi-Turn Clinical Dialogue (Read more on [arXiv](https://arxiv.org/abs/2505.19630) or [HuggingFace](https://huggingface.co/papers/2505.19630))| Yixue Li, Lu Zhou, Yichun Feng, Jarvis1111 | The paper introduces DoctorAgent-RL, a multi-agent reinforcement learning system for multi-turn clinical dialogue to address limitations in existing medical LLMs. The research aims to optimize the doctor agent's questioning strategy through RL, facilitating dynamic information gathering with a simulated patient.  The methodology involves a doctor agent fine-tuned via supervised learning and further refined with RL using a consultation evaluator to provide multi-dimensional rewards.  Experiments using a newly constructed MTMedDialog dataset demonstrate DoctorAgent-RL outperforms existing models, achieving a comprehensive average score of 53.9% compared to baseline models.  The system offers AI practitioners a framework for developing more proactive and context-aware medical dialogue agents, potentially enhancing clinical consultation effectiveness. |
| Computer Vision | Jodi: Unification of Visual Generation and Understanding via Joint
  Modeling (Read more on [arXiv](https://arxiv.org/abs/2505.19084) or [HuggingFace](https://huggingface.co/papers/2505.19084))| Xilin Chen, Shiguang Shan, Meina Kan, Zhenliang He, xyfJASON | The paper introduces Jodi, a unified diffusion framework for visual generation and understanding. It addresses the challenge of unifying visual generation and perception by jointly modeling image and label domains within a single diffusion model. Jodi employs a linear diffusion transformer with a role switch mechanism to perform joint generation, controllable generation, and image perception. Experiments demonstrate that Jodi excels in both generation and understanding tasks, achieving superior performance in controllable generation with an FID score of 13.6 for depth conditioning. The framework's extensibility and unified approach offer AI practitioners a powerful tool for various visual tasks. |
| Natural Language Processing | An Embarrassingly Simple Defense Against LLM Abliteration Attacks (Read more on [arXiv](https://arxiv.org/abs/2505.19056) or [HuggingFace](https://huggingface.co/papers/2505.19056))| George Turkiyyah, Bernard Ghanem, Hasan Abed Al Kader Hammoud, Harethah Abu Shairah | The paper introduces a defense against LLM abliteration attacks, which compromise safety alignment by suppressing refusal behavior. The research investigates whether fine-tuning LLMs with extended refusals (including topic overviews, explicit refusals, and ethical rationales) can improve robustness. The methodology involves fine-tuning LLAMA-2-7B-CHAT and QWEN2.5-INSTRUCT models on an extended-refusal dataset and evaluating their refusal rates after abliteration. The results show that extended-refusal models maintain high refusal rates (>90%) after abliteration, while baseline models' refusal rates drop significantly (70-80%). This work implies that incorporating more semantically rich refusals disperses the safety signal, making models less susceptible to abliteration and offering AI practitioners a simple way to increase safety in deployed models. |
| Machine Learning | Strong Membership Inference Attacks on Massive Datasets and (Moderately)
  Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.18773) or [HuggingFace](https://huggingface.co/papers/2505.18773))| Matthew Jagielski, Christopher A. Choquette-Choo, Ilia Shumailov, Jamie Hayes, pasta41 | This paper investigates the effectiveness of strong membership inference attacks (MIAs) on pre-trained language models (LLMs). It addresses whether the limitations observed in prior MIA research stem from attack design choices or the fundamental ineffectiveness of MIAs on LLMs. The authors scale the LiRA attack to GPT-2 architectures (10M-1B parameters) using reference models trained on up to 20B tokens from the C4 dataset. Results show MIAs can succeed but are limited (e.g., AUC < 0.7 in practical settings), and the relationship with privacy metrics is complex. This indicates that strong MIAs can be effective under specific training conditions and also helps to clarify the extent of actual privacy risk MIAs pose in this setting. |
| Machine Learning | Dynamic Risk Assessments for Offensive Cybersecurity Agents (Read more on [arXiv](https://arxiv.org/abs/2505.18384) or [HuggingFace](https://huggingface.co/papers/2505.18384))| Zhou Li, Joie Zhang, Jiacen Xu, Benedikt Stroebl, boyiwei | This paper examines the dynamic risks associated with autonomous offensive cybersecurity agents. It investigates how adversaries can leverage computational resources to improve an agent's cybersecurity capabilities. The methodology involves analyzing five degrees of freedom adversaries have in modifying offensive agents, including repeated sampling, increasing interaction rounds, iterative prompt refinement, self-training, and iterative workflow refinement.  The study shows adversaries can improve an agent's InterCode CTF performance by more than 40% with a small compute budget (8 H100 GPU Hours) by dynamically improving the agent's performance.  The primary implication is the need for dynamic risk assessments to account for evolving adversarial capabilities, painting a more representative risk picture. |
| Natural Language Processing | EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via
  Action Pruning (Read more on [arXiv](https://arxiv.org/abs/2505.16312) or [HuggingFace](https://huggingface.co/papers/2505.16312))| Defu Lian, Quan Liu, Jianshu Zhang, Qisi Chen, Jiawei1222 | This paper introduces EquivPruner, a method to improve the efficiency and quality of LLM-based search by pruning semantically equivalent actions. The research addresses the problem of redundant exploration in LLM reasoning due to textually different but semantically equivalent reasoning steps. The proposed method includes a lightweight equivalence detector trained on the MathEquiv dataset, used to identify and prune redundant actions during LLM search.  Experiments on GSM8K with Qwen2.5-Math-7B-Instruct show a 48.1% reduction in token consumption and an accuracy improvement to 96.59%.  EquivPruner offers a practical way to reduce computational overhead and potentially improve accuracy in LLM reasoning by intelligently pruning the search space. |
| Natural Language Processing | MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs (Read more on [arXiv](https://arxiv.org/abs/2505.19800) or [HuggingFace](https://huggingface.co/papers/2505.19800))| Bernard Ghanem, Maged S. Al-Shaibani, Zaid | The paper introduces MOLE, a framework for automated metadata extraction from scientific papers using Large Language Models. It addresses the challenge of efficiently cataloging and validating dataset information, enhancing discoverability and reproducibility. The methodology employs a schema-driven approach with robust validation mechanisms, processing documents across multiple formats. The results show that modern LLMs achieve a weighted average of 67.42% when extracting metadata, indicating the need for further improvement to ensure reliability for automating dataset cataloging. |
| Machine Learning | Architectural Backdoors for Within-Batch Data Stealing and Model
  Inference Manipulation (Read more on [arXiv](https://arxiv.org/abs/2505.18323) or [HuggingFace](https://huggingface.co/papers/2505.18323))| Ilia Shumailov, Conrad Grobler, Ivan Petrov, Nicolas Küchler | This paper introduces architectural backdoors that exploit batched inference in neural networks, enabling data theft and model manipulation across users within the same batch. The research aims to demonstrate and mitigate vulnerabilities arising from batched inference implementations. The methodology involves engineering backdoors into model architectures, allowing attackers to steal or manipulate model inputs and outputs directed at other users. The authors perform a large-scale analysis of models from Hugging Face, finding over 200 models that unintentionally leak information between batch entries due to dynamic quantization. This work implies that AI practitioners need to re-evaluate the security assumptions of batched inference and develop mechanisms for shared ML systems. |
| Multi-Modal | Towards Holistic Evaluation of Large Audio-Language Models: A
  Comprehensive Survey (Read more on [arXiv](https://arxiv.org/abs/2505.15957) or [HuggingFace](https://huggingface.co/papers/2505.15957))| Hung-yi Lee, Neo S. Ho, zenyn | This survey introduces a systematic taxonomy for evaluating Large Audio-Language Models (LALMs). The paper aims to provide a comprehensive overview of LALM evaluation frameworks, categorizing them into four dimensions: General Auditory Awareness and Processing, Knowledge and Reasoning, Dialogue-oriented Ability, and Fairness, Safety, and Trustworthiness. The methodology involves a comprehensive review of existing LALM benchmarks and their organization into the proposed taxonomy. While quantitative metrics are not reported directly for the taxonomy, the survey identifies significant gaps between LALMs and human-level perception across tasks. The taxonomy enables AI practitioners to select appropriate benchmarks and identify areas for improvement in LALM development. |
| Natural Language Processing | TAGS: A Test-Time Generalist-Specialist Framework with
  Retrieval-Augmented Reasoning and Verification (Read more on [arXiv](https://arxiv.org/abs/2505.18283) or [HuggingFace](https://huggingface.co/papers/2505.18283))| Haochen Xue, Ming Hu, Yulong Li, Feilong Tang, JianghaoWu | This paper introduces TAGS, a test-time framework for medical question answering that combines generalist and specialist reasoning with retrieval-augmented prompting and verification. The research aims to improve the robustness and adaptability of large language models (LLMs) in medical question answering without requiring model fine-tuning. TAGS incorporates a hierarchical retrieval mechanism and an uncertainty-aware answer aggregation module to enhance reasoning diversity and reliability. Experiments across nine MedQA benchmarks show that TAGS boosts GPT-4o accuracy by 13.8% and DeepSeek-R1 by 16.8%. The framework offers AI practitioners a parameter-efficient approach to improving LLM performance in medical contexts by leveraging structured reasoning and verification strategies at inference time. |
