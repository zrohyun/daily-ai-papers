

## Papers for 2025-05-29

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Reinforcement Learning | The Entropy Mechanism of Reinforcement Learning for Reasoning Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2505.22617) or [HuggingFace](https://huggingface.co/papers/2505.22617))| Haozhan72, yuxinzuo, JC-Chen, YucZhang2003, ganqu | This paper tackles the challenge of policy entropy collapse in reinforcement learning (RL) for reasoning with large language models (LLMs). It investigates the dynamics of policy entropy and proposes Clip-Cov and KL-Cov techniques to control it by restricting updates of high-covariance tokens. Empirically, these methods help policy escape entropy collapse and achieve better downstream performance, outperforming GRPO by 2.0% on average for the 7B model and 6.4% for the 32B model. The findings suggest that managing policy entropy is crucial for scaling RL effectively, enabling increased exploration and better utilization of training compute. The authors establish a transformation equation R = -a exp H + b between entropy H and downstream performance R, where a and b are fitting coefficients. |
| Natural Language Processing | R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large
  Model Token Routing (Read more on [arXiv](https://arxiv.org/abs/2505.21600) or [HuggingFace](https://huggingface.co/papers/2505.21600))| Zhihang Yuan, Enshu Liu, Yi Ge, youyc22, fuvty | The paper introduces Roads to Rome (R2R), a token routing method for efficient reasoning with small and large language models. R2R addresses the challenge of performance degradation in small language models by selectively utilizing large language models only for path-divergent tokens. The key methodology involves automatically generating token-level routing labels and training a lightweight router to predict divergent tokens, leveraging SLM uncertainty and token rarity. Results show that R2R achieves a 2.8x wall-clock speedup over R1-32B with comparable accuracy and surpasses R1-14B by 1.1x in accuracy, with an average activated parameter size of 5.6B. This work enables more efficient test-time scaling of LLMs by advancing the Pareto frontier of efficiency and performance. |
| Reinforcement Learning | Skywork Open Reasoner 1 Technical Report (Read more on [arXiv](https://arxiv.org/abs/2505.22312) or [HuggingFace](https://huggingface.co/papers/2505.22312))| Chaojie Wang, Rui Yan, Jujie He, chrisliu298, skydownacai | Skywork Open Reasoner 1 (OR1) is presented as an effective RL implementation for enhancing reasoning in long Chain-of-Thought models. The paper explores how RL can improve performance on reasoning tasks, focusing on efficient and scalable methods. They use a modified Group Relative Policy Optimization (GRPO) to achieve gains in accuracy. The Skywork-OR1-32B model improves average accuracy across AIME24/25 and LiveCodeBench by 15.0% (reaching 72.8%) compared to the base model. The paper provides insights and code for community research on mitigating entropy collapse in RL training for LLMs. |
| Multi-Modal | Sherlock: Self-Correcting Reasoning in Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.22651) or [HuggingFace](https://huggingface.co/papers/2505.22651))| Ruqi Zhang, Tuwhy | The paper introduces Sherlock, a framework for improving self-correction in reasoning vision-language models (VLMs). It aims to address challenges like sensitivity to reasoning errors and data dependence. Sherlock employs a trajectory-level self-correction objective, visual perturbation-based preference data construction, and dynamic beta tuning. The framework achieves a 65.4% average accuracy across eight benchmarks after self-correction, outperforming LLaVA-CoT and Mulberry with less annotated data. Sherlock enables VLMs to self-improve without external supervision, reducing dependence on extensive annotated data. |
| Multi-Modal | Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO (Read more on [arXiv](https://arxiv.org/abs/2505.22453) or [HuggingFace](https://huggingface.co/papers/2505.22453))| Chen Wang, Yuting Li, weiranhuang, weiranhuang, WaltonFuture | This paper introduces MM-UPT, a framework for unsupervised post-training of multi-modal LLMs using the GRPO algorithm. The research aims to enable continuous self-improvement of MLLMs without external supervision by using majority voting over sampled responses as a reward signal. The proposed method updates the MLLM using majority voting to derive implicit reward signals. Experiments show a significant improvement in reasoning ability on benchmarks like MathVista (66.3% -> 72.9%) using Qwen2.5-VL-7B. MM-UPT offers AI practitioners a new paradigm for continual and autonomous MLLM enhancement without external supervision. |
| Machine Learning | SWE-rebench: An Automated Pipeline for Task Collection and
  Decontaminated Evaluation of Software Engineering Agents (Read more on [arXiv](https://arxiv.org/abs/2505.20411) or [HuggingFace](https://huggingface.co/papers/2505.20411))| Anton Shevtsov, Maksim Nekrashevich, sbkarasik, djalexj, ibragim-bad | This paper introduces SWE-rebench, an automated pipeline for extracting and evaluating software engineering tasks for training and benchmarking LLM-based agents. The research aims to address the scarcity of high-quality, interactive training data and the problem of benchmark contamination in software engineering agent development. The pipeline automates the collection of real-world interactive tasks from GitHub repositories and constructs a public dataset of over 21,000 Python-based tasks. Experimental results compare various LLMs on the new benchmark and SWE-bench Verified, indicating that some language models may have inflated performance due to contamination issues. The work enables the continuous supply of fresh, decontaminated tasks for more reliable evaluation and advancement of software engineering agents. |
| Machine Learning | SageAttention2++: A More Efficient Implementation of SageAttention2 (Read more on [arXiv](https://arxiv.org/abs/2505.21136) or [HuggingFace](https://huggingface.co/papers/2505.21136))| Pengle Zhang, Haofeng Huang, Jia Wei, Xiaoming Xu, jt-zhang | The paper introduces SageAttention2++, an optimized implementation of SageAttention2 for improved efficiency. It aims to accelerate SageAttention2 by utilizing FP8 Matmul with FP16 accumulation, leveraging faster instruction sets. SageAttention2++ achieves a 3.9x speedup over FlashAttention while maintaining comparable attention accuracy to SageAttention2. This improvement is realized by narrowing the quantization range of P and V to satisfy the accumulator range in FP16. The implication is a plug-and-play acceleration for diverse models in language, image, and video generation with negligible end-to-end metric loss. |
| Multi-Modal | Advancing Multimodal Reasoning via Reinforcement Learning with Cold
  Start (Read more on [arXiv](https://arxiv.org/abs/2505.22334) or [HuggingFace](https://huggingface.co/papers/2505.22334))| Kaipeng Zheng, Yuting Li, weiranhuang, weiranhuang, WaltonFuture | The paper explores enhancing multimodal reasoning in large language models (MLLMs) through reinforcement learning with a cold start approach. It investigates the impact of supervised fine-tuning (SFT) as a cold start, followed by reinforcement learning (RL) using GRPO, on multimodal reasoning benchmarks. The methodology involves structured chain-of-thought reasoning patterns during SFT and refinement via GRPO in RL. The resulting 7B model achieves state-of-the-art performance with a 73.4% score on MathVista, demonstrating substantial improvements over base models. The study suggests that SFT provides a robust foundation for RL scaling in MLLMs, enabling more effective multimodal reasoning. |
| Computer Vision | Fostering Video Reasoning via Next-Event Prediction (Read more on [arXiv](https://arxiv.org/abs/2505.22457) or [HuggingFace](https://huggingface.co/papers/2505.22457))| Kenji Kawaguchi, Chao Du, Xiangyan Liu, Hongfu Liu, Haonan Wang | This paper introduces next-event prediction (NEP) as a self-supervised learning task to improve temporal reasoning in MLLMs. The research aims to equip MLLMs with the ability to predict future events in videos based on observed past frames. They propose a dataset called V1-33K and FutureBench to evaluate temporal reasoning capabilities. Experiments show that NEP improves temporal reasoning, achieving a 53.5 T-Avg performance, while maintaining performance on general video tasks. NEP provides a scalable and effective training paradigm for fostering temporal reasoning in MLLMs, which could enable better understanding of dynamic visual events. |
| Natural Language Processing | DeepResearchGym: A Free, Transparent, and Reproducible Evaluation
  Sandbox for Deep Research (Read more on [arXiv](https://arxiv.org/abs/2505.19253) or [HuggingFace](https://huggingface.co/papers/2505.19253))| Abhijay Paladugu, Kangrui Mao, Jingyuan He, Jingjie Ning, jmvcoelho | The paper introduces DeepResearchGym, an open-source framework for reproducible evaluation of deep research systems addressing limitations of commercial APIs. It combines a reproducible search API with large-scale web corpora and a rigorous evaluation protocol based on LLM-as-a-judge assessments. Experimental results demonstrate comparable performance to commercial APIs while maintaining consistent performance rankings across evaluation metrics. Human evaluations confirm the alignment of the automatic protocol with human preferences, validating the framework's ability to support controlled assessment. DeepResearchGym enables researchers to conduct transparent and reproducible experiments in deep information synthesis. |
| Computer Vision | RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with
  Global Illumination (Read more on [arXiv](https://arxiv.org/abs/2505.21925) or [HuggingFace](https://huggingface.co/papers/2505.21925))| Xin Tong, Hongzhi Wu, Pieter Peers, doyleconan, NCJ | The paper presents RenderFormer, a novel neural rendering pipeline that generates images from triangle meshes with global illumination effects without per-scene training. The research objective is to formulate rendering as a sequence-to-sequence transformation, converting triangle tokens into pixel tokens using transformers. The pipeline comprises a view-independent stage modeling triangle-to-triangle light transport and a view-dependent stage that transforms ray bundles to pixel values. Experiments demonstrate RenderFormer's ability to render complex lighting effects, achieving a PSNR of 29.77 on test scenes. This approach offers AI practitioners a data-driven alternative to physics-based rendering pipelines, potentially accelerating rendering and enabling inverse rendering applications. |
| Computer Vision | Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and
  Preference Alignment (Read more on [arXiv](https://arxiv.org/abs/2505.18600) or [HuggingFace](https://huggingface.co/papers/2505.18600))| Jong Chul Ye, Jeongsol Kim, Bryan Sangwoo Kim | The paper introduces Chain-of-Zoom (CoZ), a novel framework for extreme single-image super-resolution (SISR) that addresses the scalability limitations of existing models. It aims to achieve high-resolution images beyond training scale factors by decomposing SISR into an autoregressive chain of intermediate scale-states enhanced with multi-scale-aware prompts. CoZ employs a fine-tuned vision-language model (VLM) guided by Generalized Reward Policy Optimization (GRPO) to generate these prompts, aligning them with human preference. Experiments demonstrate that CoZ, when applied to a standard 4× diffusion SR model, enables beyond 256× enlargement with improved perceptual quality, as indicated by better no-reference perceptual metric scores. This framework offers a cost-effective approach for achieving extreme resolution without retraining models and opens new avenues for image enhancement strategies. |
| Natural Language Processing | Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for
  Frozen LLMs (Read more on [arXiv](https://arxiv.org/abs/2505.19075) or [HuggingFace](https://huggingface.co/papers/2505.19075))| Jong Chul Ye, Choonghan Kim, Hyunmin Hwang, Hangeol Chang, kjm981995 | The paper introduces Universal Reasoner (UniR), a lightweight, composable module for enhancing reasoning in frozen LLMs. It aims to improve reasoning capabilities without requiring extensive retraining or compromising generalization by decoupling reward model training from full policy updates. UniR is trained independently using predefined rewards and combined with frozen LLMs by additively integrating its output logits during inference. Experiments on mathematical reasoning and machine translation tasks show UniR significantly outperforms baseline fine-tuning methods.  UniR facilitates cost-efficient, adaptable, and robust reasoning enhancement in LLMs without architectural modifications. |
| Reinforcement Learning | SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem (Read more on [arXiv](https://arxiv.org/abs/2505.21887) or [HuggingFace](https://huggingface.co/papers/2505.21887))| Zangir Iklassov, Salem Lahlou, Martin Takac, Yahia Salaheldin Shaaban, ahmedheakl | The paper introduces SVRPBench, a realistic benchmark for evaluating stochastic vehicle routing problem (SVRP) algorithms under urban conditions. It aims to address the limitations of existing benchmarks by simulating time-dependent congestion, probabilistic accidents, and empirically grounded customer time windows. The benchmark includes over 500 instances with up to 1000 customers and benchmarks state-of-the-art RL solvers, revealing that solvers such as POMO and AM degrade by over 20% under distributional shift. The classical and metaheuristic methods remain more robust in these realistic simulations, which encourages the design of SVRP solvers adaptable to real-world uncertainties. The dataset and evaluation suite have been released to facilitate reproducible research and the creation of more generalizable solvers. |
| Computer Vision | What Makes for Text to 360-degree Panorama Generation with Stable
  Diffusion? (Read more on [arXiv](https://arxiv.org/abs/2505.22129) or [HuggingFace](https://huggingface.co/papers/2505.22129))| Jing Zhang, Qiang Zhang, allencbzhang, mcleanie | The paper investigates adapting text-to-image diffusion models for 360-degree panorama generation. It explores the roles of attention module matrices in fine-tuning Stable Diffusion for panoramic images, hypothesizing distinct behaviors of trainable components. The analysis reveals that query and key matrices handle shared information, while value and output weight matrices specialize in adapting pre-trained knowledge, resulting in a novel UniPano framework. UniPano achieves state-of-the-art FAED and FID scores on 512x1024 panorama generation while reducing memory usage. The framework's efficiency enables scalable end-to-end panorama generation with higher resolutions, offering AI practitioners an improved method for panorama synthesis. |
| Natural Language Processing | WebDancer: Towards Autonomous Information Seeking Agency (Read more on [arXiv](https://arxiv.org/abs/2505.22648) or [HuggingFace](https://huggingface.co/papers/2505.22648))| Liwen Zhang, Wenbiao Yin, Runnan Fang, Baixuan Li, callanwu | The paper introduces WebDancer, a framework for building autonomous information-seeking web agents. The research aims to create an end-to-end pipeline for agents capable of multi-step reasoning and web navigation. The proposed methodology involves scalable QA data synthesis and a two-stage training approach using supervised fine-tuning (SFT) followed by on-policy reinforcement learning (RL). Empirical evaluations on GAIA and WebWalkerQA benchmarks demonstrate WebDancer's strong performance, achieving competitive results. The framework provides actionable pathways for developing sophisticated agentic models for complex information-seeking tasks. |
| Natural Language Processing | Judging Quality Across Languages: A Multilingual Approach to Pretraining
  Data Filtering with Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.22232) or [HuggingFace](https://huggingface.co/papers/2505.22232))| Abbas Goher Khan, Elias Wendt, Max Lübbering, Mehdi Ali, mbrack | The paper introduces JQL, a multilingual approach to pretraining data filtering using language models. It addresses the limited availability and heuristic-based limitations of existing multilingual datasets by efficiently curating diverse, high-quality data at scale. JQL distills LLM annotation capabilities into lightweight annotators based on multilingual embeddings, achieving robust cross-lingual performance, even on unseen languages. Evaluated across 35 languages, JQL outperforms heuristic methods like Fineweb2 while increasing data retention, notably enhancing downstream model training. The approach provides practical insights and valuable resources for multilingual data curation, raising standards for dataset development. |
| Natural Language Processing | LIMOPro: Reasoning Refinement for Efficient and Effective Test-time
  Scaling (Read more on [arXiv](https://arxiv.org/abs/2505.19187) or [HuggingFace](https://huggingface.co/papers/2505.19187))| Kaishuai Xu, Chunpu Xu, Ruifeng Yuan, Jiashuo Wang, YangXiao-nlp | This paper introduces LIMOPro, a novel reasoning refinement framework for improving the efficiency and effectiveness of large language models (LLMs) during test-time scaling. The research aims to reduce verbose elements in reasoning chains distilled from powerful reasoning models, specifically targeting functional elements while preserving progressive reasoning components. LIMOPro quantitatively evaluates the importance of reasoning steps via a perplexity-based metric, selectively pruning low-importance steps. Experiments on benchmarks like AIME, AMC, and GPQA Diamond show accuracy improvements (+0.9% to +6.6%) with significantly reduced token usage (-3% to -41%). LIMOPro provides a practical solution for deploying reasoning-capable LLMs under computational constraints by creating more concise and efficient reasoning chains. |
| Natural Language Processing | Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal
  Evolution of Human States (Read more on [arXiv](https://arxiv.org/abs/2505.17663) or [HuggingFace](https://huggingface.co/papers/2505.17663))| Chunpu Xu, Changhe Song, Qiancheng Xu, Jiashuo Wang, YangXiao-nlp | This paper introduces DYNTOM, a novel benchmark for evaluating LLMs' ability to understand and track the temporal evolution of human mental states in social interactions. The research aims to assess LLMs' ToM capabilities beyond static snapshots by considering continuous changes across interconnected scenarios. DYNTOM includes 1,100 social contexts, 5,500 scenarios, and 78,100 multiple-choice questions generated through a four-step process and human validation. Evaluation of ten LLMs shows an average accuracy of 33.0%, significantly underperforming humans (77.7%) and highlighting limitations in tracking state changes. The study implies that AI practitioners need to focus on enhancing LLMs' dynamic ToM reasoning to better model real-world social dynamics. |
| Reinforcement Learning | VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich
  Information Understanding via Iterative Reasoning with Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.22019) or [HuggingFace](https://huggingface.co/papers/2505.22019))| Zehui Chen, Ruixue Ding, Lin-Chen, YuZeng260, autumncc | The paper introduces VRAG-RL, a novel reinforcement learning framework designed for complex reasoning across visually rich information. The research aims to improve the performance of Vision-Language Models (VLMs) in Retrieval-Augmented Generation (RAG) tasks by enabling iterative reasoning with visual perception. VRAG-RL utilizes visual perception tokens in the action space and a fine-grained reward structure integrating retrieval performance. Experiments on diverse benchmarks show VRAG-RL outperforms existing methods by 20% (Qwen2.5-VL-7B) and 30% (Qwen2.5-VL-3B), demonstrating improved effectiveness. The framework aligns VLMs with real-world applications by addressing key limitations in current multi-modal RAG approaches. |
| Natural Language Processing | Let's Predict Sentence by Sentence (Read more on [arXiv](https://arxiv.org/abs/2505.22202) or [HuggingFace](https://huggingface.co/papers/2505.22202))| Hoyeon Chang, Jiyeon Kim, Seungone Kim, Byeongguk Jeon, Hyeonbin Hwang | The paper introduces a framework for sentence-level reasoning in language models by predicting continuous embeddings of sentences autoregressively. It investigates whether pre-trained language models can be adapted to reason over structured semantic units, leveraging semantic and contextual embeddings. Key methodology involves training a latent model to predict embeddings of sentences, evaluated under discretized and continuous inference regimes. Experiments across four domains (mathematics, logic, commonsense, and planning) showed that contextual embeddings with continuous inference achieve performance competitive with Chain-of-Thought while reducing FLOPs by approximately half. The results indicate that pre-trained language models can transition to structured reasoning within latent embedding spaces, offering improved efficiency for AI practitioners. |
| Computer Vision | RICO: Improving Accuracy and Completeness in Image Recaptioning via
  Visual Reconstruction (Read more on [arXiv](https://arxiv.org/abs/2505.22613) or [HuggingFace](https://huggingface.co/papers/2505.22613))| Linli Yao, Sihan Yang, Shuhuai Ren, Yishuo Cai, Yuchi Wang | The paper introduces RICO, a framework to improve image recaptioning accuracy and completeness via visual reconstruction. RICO iteratively refines captions by reconstructing them into images using a text-to-image model and prompting a MLLM to identify discrepancies between original and reconstructed images. To reduce computational costs, RICO-Flash is introduced, learning the captioning like RICO using DPO. Experiments show RICO achieves approximately 10% improvement over baselines on CapsBench and CompreCap. The method offers AI practitioners a way to generate more faithful and comprehensive datasets for multimodal tasks. |
| Multi-Modal | Thinking with Generated Images (Read more on [arXiv](https://arxiv.org/abs/2505.22525) or [HuggingFace](https://huggingface.co/papers/2505.22525))| Jiadi Su, Siqi Kou, Steffi Chern, Zhulin Hu, ethanchern | The paper introduces Thinking with Generated Images, a paradigm for visual reasoning in LMMs through spontaneous generation of intermediate visual thinking steps. The research explores enabling LMMs to actively construct and refine visual hypotheses via native long-multimodal thought processes. The methodology involves vision generation with intermediate visual subgoals and self-critique mechanisms within a unified LMM. Experiments demonstrate up to 50% relative improvement in vision generation benchmarks for complex multi-object scenarios. This approach enables AI models to engage in creative, analytical, and strategic thinking across text and vision. |
| Computer Vision | PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image
  Generative Models (Read more on [arXiv](https://arxiv.org/abs/2505.22523) or [HuggingFace](https://huggingface.co/papers/2505.22523))| Ji Li, Keming Wu, Yanbin Wang, Heyang Jiang, Junwen Chen | This paper introduces PRISMLAYERS, a large-scale dataset for high-quality multi-layer transparent image generation. The research addresses the lack of suitable training data by synthesizing images using LayerFLUX and MultiLayerFLUX, a training-free pipeline leveraging diffusion models and semantic layouts. The fine-tuned ART+ model, trained on PRISMLAYERSPRO, outperforms the original ART in user studies, achieving a 57.9% win rate in layer quality and matches the visual quality of FLUX.1-[dev] to some extent. The dataset and pipeline aim to enable research and applications requiring precise, editable, and visually compelling layered imagery. |
| Reinforcement Learning | Text2Grad: Reinforcement Learning from Natural Language Feedback (Read more on [arXiv](https://arxiv.org/abs/2505.22338) or [HuggingFace](https://huggingface.co/papers/2505.22338))| Si Qin, Tianjun Mao, Chaoyun Zhang, Lu Wang, Hanyang Wang | This paper introduces Text2Grad, a reinforcement learning framework that converts natural language feedback into span-level gradients for policy optimization. The research aims to address the limitations of scalar reward RLHF by leveraging fine-grained feedback to guide model improvements. Text2Grad aligns feedback phrases with relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates. Experiments across summarization, code generation, and question answering show Text2Grad surpasses scalar-reward RL and prompt-only baselines, achieving a 12% win-rate improvement over PPO in summarization. This approach provides AI practitioners with a more interpretable and efficient method for aligning language models with human preferences. |
| Reinforcement Learning | Pitfalls of Rule- and Model-based Verifiers -- A Case Study on
  Mathematical Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.22203) or [HuggingFace](https://huggingface.co/papers/2505.22203))| Junxian He, Qi Zhu, Xingshan Zeng, Weihao Zeng, yuzhen17 | The paper investigates the reliability and impact of rule-based and model-based verifiers in reinforcement learning with verifiable rewards (RLVR), focusing on mathematical reasoning. It aims to understand the limitations of these verifiers and their effect on RL training performance. The study conducts static evaluations and RL training experiments, finding that rule-based verifiers have high false negative rates, while model-based verifiers are vulnerable to reward hacking, leading to artificially inflated rewards. A key finding is that rule-based recall rate is about 86% and model-based verifiers improve the Skywork-OR1 recall rate from 84% to 92% in static evaluation, but trained model-based verifiers are more vulnerable to hacking during RL training. The research highlights the need for robust verifier systems that balance accuracy and resistance to exploitation in RL training. |
| Computer Vision | EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video
  Guidance (Read more on [arXiv](https://arxiv.org/abs/2505.21876) or [HuggingFace](https://huggingface.co/papers/2505.21876))| Han Lin, Jialu Li, Jaemin Cho, Zun Wang, jaehong31 | The paper introduces EPiC, an efficient framework for learning controllable 3D camera trajectories in video diffusion models using precise anchor-video guidance. It addresses the challenge of creating high-quality anchor videos without expensive camera trajectory annotations by masking source videos based on first-frame visibility. EPiC integrates Anchor-ControlNet, a lightweight conditioning module, to guide pretrained video diffusion models using anchor videos, achieving state-of-the-art performance on RealEstate10K and MiraData with lower rotation error (e.g., 0.40 on RE10K and 0.66 on MIRA) and less data. This approach enables efficient training and robust generalization for both I2V and V2V camera control tasks. |
| Computer Vision | GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language
  Models and Enhanced Reasoning Chains (Read more on [arXiv](https://arxiv.org/abs/2505.18700) or [HuggingFace](https://huggingface.co/papers/2505.18700))| Yiren Song, Haofan Wang, Zihao Pan, Xiaoran Pan, Chun Wang | The paper introduces the GRE Suite, a novel framework for accurate and interpretable geo-localization inference using Vision-Language Models (VLMs). It addresses the challenge of geo-localization by augmenting VLMs with structured reasoning chains to extract multigranular visual cues and integrate them with external world knowledge. The methodology involves the creation of a high-quality geo-localization reasoning dataset, GRE30K, and a multi-stage reasoning strategy with a fine-tuned VLM. Experiments demonstrate that GRE significantly outperforms existing methods across all granularities of geo-localization tasks, achieving improvements of +0.5% to +4.2% on Im2GPS3k and +0.2% to +9.1% on GWS15k at different thresholds. The suite enables more effective reasoning for complex geographic inference, offering dual benefits for geospatial data mining and location privacy preservation. |
| Natural Language Processing | Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat
  Falsehoods (Read more on [arXiv](https://arxiv.org/abs/2505.17870) or [HuggingFace](https://huggingface.co/papers/2505.17870))| Deval Pandya, Marcelo Lotif, Rizwan Qureshi, amanchadha, Shainarazavi | This paper introduces model immunization, a novel training framework to enhance the factuality of AI models by fine-tuning them on quarantined sets of labeled falsehoods. The research aims to proactively combat misinformation by strengthening a model's ability to reject misleading claims. The proposed methodology involves periodically injecting curated falsehoods during fine-tuning, effectively building resistance against similar misinformation. An illustrative case study on a 1.5-billion-parameter Transformer demonstrates that immunized models exhibit a truthful response rate of 78%, a 30% increase, without significantly impacting general QA accuracy (85% to 84%). The model immunization approach offers a preventative paradigm for aligning AI systems with factuality, indicating an effective countermeasure for AI practitioners in the fight against misinformation. |
| Computer Vision | One-Way Ticket:Time-Independent Unified Encoder for Distilling
  Text-to-Image Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2505.21960) or [HuggingFace](https://huggingface.co/papers/2505.21960))| Jiehang Xie, Tao Liu, Kai Wang, Lei Wang, senmaonk | The paper introduces a novel approach to accelerate text-to-image diffusion models via a time-independent unified encoder. The research addresses the trade-off between inference speed and image quality in T2I diffusion models by sharing encoder features across multiple decoder time steps. The proposed Time-independent Unified Encoder (TiUE) architecture utilizes a one-pass scheme and a KL divergence term to enhance perceptual realism and diversity. Experimental results demonstrate that TiUE outperforms state-of-the-art methods, achieving competitive FID scores (e.g., 23.11 on COCO2017) with reduced computational complexity. This allows for more efficient deployment of T2I diffusion models without significant compromise in image quality and diversity. |
| Natural Language Processing | Unveiling Instruction-Specific Neurons & Experts: An Analytical
  Framework for LLM's Instruction-Following Capabilities (Read more on [arXiv](https://arxiv.org/abs/2505.21191) or [HuggingFace](https://huggingface.co/papers/2505.21191))| Zhaorui Hou, Jungang Li, Yibo Yan, Yubo Gao, Junyan Zhang | This paper introduces an analytical framework, SPARCOM, to examine how finetuning reconfigures large language model (LLM) computations by isolating instruction-specific sparse components (neurons and experts) in dense and Mixture-of-Experts (MoE) architectures. The research aims to understand the computational mechanisms that drive the improvements in LLM instruction-following capabilities after finetuning. The framework involves identifying these sparse components, evaluating their functional generality and uniqueness using a new instructional dataset, HEXAINST, and comparing their alterations before and after finetuning. Experiments demonstrate functional generality, uniqueness, and the critical role of these components in instruction execution, showing that instruction-specific neurons exhibit a darker coloration along the diagonal, particularly for classification, summarization, code, and math instruction types. The work elucidates the relationship between finetuning-induced adaptations and sparse computational substrates, providing deeper insights for the trustworthy LLM community. |
| Machine Learning | Benchmarking Recommendation, Classification, and Tracing Based on
  Hugging Face Knowledge Graph (Read more on [arXiv](https://arxiv.org/abs/2505.17507) or [HuggingFace](https://huggingface.co/papers/2505.17507))| Yuanning Cui, Weiqing Luo, Xiao Zhou, Kaijia Huang, cqsss | The paper introduces HuggingKG, a large-scale knowledge graph for managing open-source machine learning resources on Hugging Face. It addresses the limitations of existing platforms by providing a structured representation to enable advanced queries and analyses. The methodology involves constructing the KG from Hugging Face metadata, creating a benchmark called HuggingBench, and evaluating performance on tasks like resource recommendation, classification, and model tracing. Experiments show that KG-based methods, specifically KGCL with Homo subgraph, achieve Recall@5 of 0.1054 in resource recommendation. This enhanced structural data can facilitate more effective resource discovery and management for AI practitioners, though the benefits of social recommendation methods appear limited in this context. |
| Machine Learning | Meta-Learning an In-Context Transformer Model of Human Higher Visual
  Cortex (Read more on [arXiv](https://arxiv.org/abs/2505.15813) or [HuggingFace](https://huggingface.co/papers/2505.15813))| Jacob S. Prince, Hossein Adeli, Mu Nan, Muquan Yu, aluo-x | The paper introduces BraInCoRL, a meta-learning framework for predicting voxelwise neural responses in the human higher visual cortex. It addresses the challenge of subject-specific neural organization by using in-context learning to predict voxel responses from few-shot examples, without requiring fine-tuning. BraInCoRL utilizes a transformer architecture trained across multiple subjects to generate voxelwise models, conditioned on image features and voxel activations. Experiments demonstrate that BraInCoRL outperforms existing voxelwise encoders in low-data regimes, generalizing to new datasets and exhibiting strong test-time scaling behavior. The model achieves high alignment between predicted voxel selectivity and query semantics, enabling interpretable mappings from natural language to voxel selectivity. |
| Computer Vision | Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking (Read more on [arXiv](https://arxiv.org/abs/2505.12667) or [HuggingFace](https://huggingface.co/papers/2505.12667))| Junhao Zhuang, Tangyu Jiang, Hongbin Xu, Xuerui Qiu, Sugewud | The paper introduces Safe-Sora, a novel framework for embedding graphical watermarks into text-to-video generation to address copyright protection. The research focuses on developing a method for robustly embedding and extracting graphical watermarks in video, leveraging visual similarity between watermark and content. Safe-Sora employs a hierarchical coarse-to-fine adaptive matching mechanism and a 3D wavelet transform-enhanced Mamba architecture for spatiotemporal watermark fusion. Experiments demonstrate state-of-the-art performance, achieving a Fréchet Video Distance (FVD) of 3.77, significantly outperforming existing methods. This framework provides AI practitioners with a new approach to copyright protection for generated video content by applying state space models to watermarking. |
| Natural Language Processing | Characterizing Bias: Benchmarking Large Language Models in Simplified
  versus Traditional Chinese (Read more on [arXiv](https://arxiv.org/abs/2505.22645) or [HuggingFace](https://huggingface.co/papers/2505.22645))| Allison Koenecke, Jian Kang, Jiebo Luo, Hanjia Lyu | This paper investigates biases in large language models (LLMs) when prompted in Simplified versus Traditional Chinese. The study aims to determine if LLMs exhibit differential performance across these language variants, focusing on potential representational harms. The methodology involves two benchmark tasks: regional term choice and regional name choice, auditing 11 LLMs. Results indicate biases dependent on task and prompting language, with a mixed preference between Simplified and Traditional Chinese, depending on the task; e.g., the models often favored Simplified Chinese for regional terms but Traditional Chinese for names. The findings underscore the need for further bias analysis in LLMs to ensure fair application across diverse linguistic and cultural contexts. |
| Computer Vision | MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware
  Multi-Segment Grounding (Read more on [arXiv](https://arxiv.org/abs/2505.20715) or [HuggingFace](https://huggingface.co/papers/2505.20715))| Chenliang Li, Ziyue Wang, Chi Chen, Shengfeng Lou, Fuwen Luo | The paper introduces MUSEG, a reinforcement learning (RL) method to enhance video temporal understanding in multimodal large language models (MLLMs). It addresses the limitation of current MLLMs in fine-grained temporal reasoning by using timestamp-aware multi-segment grounding, which aligns queries with multiple relevant video segments. A customized RL training recipe with phased rewards progressively guides the model towards temporally grounded reasoning. Experiments on temporal grounding and time-sensitive video QA tasks show that MUSEG outperforms existing methods, achieving improvements on Charades-STA, THUMOS14 and THUMOS15, and demonstrates generalization across different tasks. MUSEG's architecture and training methodology provide a framework for developing more robust and temporally aware MLLMs. |
| Multi-Modal | MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal
  Manga Understanding (Read more on [arXiv](https://arxiv.org/abs/2505.20298) or [HuggingFace](https://huggingface.co/papers/2505.20298))| Yuki Imajuku, Atsuyuki Miyai, Shota Onohara, Kazuki Egashira, Jeonghun Baek | The paper introduces MangaVQA, a benchmark for multimodal manga understanding, and MangaLMM, a specialized model. It aims to evaluate and improve large multimodal models' (LMMs) abilities to understand manga narratives through visual question answering and in-page text recognition. The authors fine-tune Qwen2.5-VL to create MangaLMM, which jointly handles OCR and VQA tasks, and evaluate it against proprietary models like GPT-4o and Gemini 2.5. MangaLMM achieves promising performance, outperforming GPT-4o on MangaVQA with a score of 6.57 compared to 5.76. The study provides a comprehensive resource for evaluating LMMs in richly narrative visual domains. |
| Natural Language Processing | Efficient Data Selection at Scale via Influence Distillation (Read more on [arXiv](https://arxiv.org/abs/2505.19051) or [HuggingFace](https://huggingface.co/papers/2505.19051))| Vahab Mirrokni, Dan Alistarh, Vincent Cohen-Addad, Mahdi Nikdan | This paper introduces Influence Distillation for efficient data selection in large language models (LLMs). The primary objective is to optimize training sample weights for LLM fine-tuning towards strong performance on a target domain. The method employs second-order information to assign model-specific weights and landmark-based approximations for scalability. Experiments on Tulu V2 using Llama and Qwen models demonstrate matching or outperforming state-of-the-art performance while achieving up to 3.5x faster selection. Influence Distillation offers a computationally efficient approach to curate training data for improved LLM performance. |
| Computer Vision | Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and
  Styles (Read more on [arXiv](https://arxiv.org/abs/2505.21060) or [HuggingFace](https://huggingface.co/papers/2505.21060))| Peidong Liu, Xiang Liu, Peng Wang | Styl3R is a feed-forward network for instant 3D stylization of arbitrary scenes from sparse, unposed images and a style image. The paper addresses the challenge of fast, multi-view consistent 3D stylization without test-time optimization. It employs a dual-branch architecture separating structure and appearance modeling, coupled with an identity loss for pre-training. The method achieves state-of-the-art zero-shot 3D stylization, producing high-quality content in 0.15 seconds. This approach provides AI practitioners with an efficient method for generating stylized 3D content while maintaining multi-view consistency. |
| Natural Language Processing | AITEE -- Agentic Tutor for Electrical Engineering (Read more on [arXiv](https://arxiv.org/abs/2505.21582) or [HuggingFace](https://huggingface.co/papers/2505.21582))| Christian Bernhardt, Alexander Bernhardt, CKnievel | The paper introduces AITEE, an agent-based tutoring system for electrical engineering that leverages large language models (LLMs) to provide personalized education. The research aims to improve LLMs' ability to understand and solve electrical circuit problems through domain-specific knowledge and Socratic dialogue. AITEE combines circuit image processing, a graph neural network-based similarity measure for context retrieval, and a SPICE simulation for accuracy. Experiments demonstrate AITEE significantly outperforms baseline approaches in domain-specific knowledge application, with even medium-sized LLM models showing acceptable performance. This indicates the potential of agentic tutors to deliver scalable, personalized learning environments in technical domains, though arithmetic inaccuracies in superposition-based circuits remain. |
| Natural Language Processing | First Finish Search: Efficient Test-Time Scaling in Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2505.18149) or [HuggingFace](https://huggingface.co/papers/2505.18149))| Tanmoy Chakraborty, Ayan Sengupta, aradhye | This paper introduces First Finish Search (FFS), a novel training-free parallel decoding strategy for efficient test-time scaling in large language models. FFS aims to improve reasoning accuracy by selecting the shortest generated trace, hypothesizing that shorter traces are more likely to be correct. The methodology involves launching n independent samples in parallel and returning the first completed output, eliminating the need for equality checks or special tokens. Experiments with DeepSeek-R1 on AIME datasets showed FFS achieved 82.23% accuracy, a 15% improvement over standalone performance, nearing OpenAI's o4-mini. FFS provides a scalable and efficient alternative for enhancing LLM reasoning by dynamically allocating compute during inference. |
