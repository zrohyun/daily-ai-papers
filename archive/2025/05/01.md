

## Papers for 2025-05-01

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Sadeed: Advancing Arabic Diacritization Through Small Language Model (Read more on [arXiv](https://arxiv.org/abs/2504.21635) or [HuggingFace](https://huggingface.co/papers/2504.21635))| Sara Chrouf, hr99, Moatasem444, Hennara, ZeinaD | This paper introduces Sadeed, a compact language model specifically fine-tuned for the task of Arabic diacritization, alongside a new benchmark, SadeedDiac-25. The primary objective is to address the challenges of Arabic diacritization, such as data scarcity, contextual dependence, and benchmark limitations, by developing an efficient and accurate model. The methodology involves fine-tuning a pre-trained decoder-only Arabic language model (Kuwain 1.5B) on rigorously cleaned and curated diacritized datasets, reformulating the task as question-answering. Sadeed achieves competitive results, outperforming traditional models and achieving a Word Error Rate (WER) of 1.7115 on the phonologically corrected Fadel test set (excluding no diacritic, without case ending) and a WER of 9.9245 on the new SadeedDiac-25 benchmark. The main implication for AI practitioners is that smaller, task-specific models fine-tuned on high-quality data can achieve strong performance on complex NLP tasks like Arabic diacritization, highlighting the importance of curated datasets and tailored benchmarks over simply scaling model size. |
| Natural Language Processing | WebThinker: Empowering Large Reasoning Models with Deep Research
  Capability (Read more on [arXiv](https://arxiv.org/abs/2504.21776) or [HuggingFace](https://huggingface.co/papers/2504.21776))| Yutao Zhu, Hongjin Qian, Guanting Dong, Jiajie Jin, Xiaoxi Li | This paper introduces WebThinker, a deep research agent designed to enhance Large Reasoning Models (LRMs) by enabling autonomous web exploration and report generation during reasoning. The primary objective is to overcome LRM limitations tied to static internal knowledge for complex, knowledge-intensive tasks requiring synthesis of diverse web information. WebThinker employs a Deep Web Explorer for dynamic web search and navigation, coupled with an Autonomous Think-Search-and-Draft strategy allowing real-time interleaving of reasoning, information gathering, and report writing, optimized via RL-based DPO training. Experiments show WebThinker significantly outperforms existing methods, achieving a 70.7 average score on GPQA compared to 67.2 for Search-o1-32B and an 8.1 overall score on scientific report generation tasks (Glaive). The main implication is providing a pathway towards more capable and versatile LRMs equipped for deep research in complex, dynamic information environments. |
| Machine Learning | Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language
  Models in Math (Read more on [arXiv](https://arxiv.org/abs/2504.21233) or [HuggingFace](https://huggingface.co/papers/2504.21233))| Yen-Chun Chen, Dongdong Chen, Hany Awadalla, Baolin Peng, Haoran Xu | This paper presents a multi-stage training paradigm to enhance the mathematical reasoning capabilities of Small Language Models (SLMs). The primary objective is to investigate whether SLMs can achieve strong reasoning performance, rivaling larger models, through a carefully designed training recipe and high-quality data. The methodology consists of four stages applied to the Phi-4-Mini (3.8B) model: large-scale distillation mid-training, supervised fine-tuning on high-quality Chain-of-Thought (CoT) data, Rollout Direct Preference Optimization (DPO), and Reinforcement Learning (RL) with Verifiable Reward, incorporating techniques like GRPO with stability enhancements. Key results show the resulting Phi-4-Mini-Reasoning model achieves 94.6% accuracy on the Math-500 benchmark, outperforming models nearly twice its size. The main implication for AI practitioners is that deliberate multi-stage training, leveraging quality data and combining distillation with preference and reinforcement learning, can unlock significant reasoning capabilities in parameter-constrained SLMs. |
| Machine Learning | Softpick: No Attention Sink, No Massive Activations with Rectified
  Softmax (Read more on [arXiv](https://arxiv.org/abs/2504.20966) or [HuggingFace](https://huggingface.co/papers/2504.20966))| Alham Fikri Aji, Erland Hilman Fuadi, Zayd M. K. Zuhri | This paper introduces Softpick, a rectified, non-sum-to-one function proposed as a drop-in replacement for softmax in transformer attention mechanisms. The primary objective is to eliminate attention sink phenomena and massive hidden state activations commonly associated with softmax, while maintaining model performance. Softpick achieves this by modifying softmax to allow zero-valued outputs using a ReLU function in the numerator and normalizing by the sum of absolute values, thus removing the strict sum-to-one constraint. Experiments on 340M parameter models demonstrate that Softpick achieves performance parity with softmax on benchmarks, completely eliminates attention sinks (0% sink rate vs 63.41%), significantly reduces activation kurtosis (340 vs 33,510), induces sparse attention maps (46.97% sparsity), and shows improved robustness to quantization. The key implication for practitioners is that Softpick offers potential benefits for model quantization, low-precision training, sparsity optimization, and interpretability by mitigating known issues with standard softmax attention. |
| Natural Language Processing | Phi-4-reasoning Technical Report (Read more on [arXiv](https://arxiv.org/abs/2504.21318) or [HuggingFace](https://huggingface.co/papers/2504.21318))| Harkirat Behl, Vidhisha Balachandran, Ahmed Awadallah, Sahaj Agarwal, Marah Abdin | This paper introduces Phi-4-reasoning and Phi-4-reasoning-plus, 14-billion parameter language models optimized for complex reasoning tasks. The main objective was to develop efficient, smaller models with strong reasoning abilities competitive with much larger models. The methodology involved supervised fine-tuning (SFT) of Phi-4 on carefully curated, high-quality reasoning prompts and demonstrations generated by 03-mini, with the 'plus' variant undergoing an additional phase of outcome-based reinforcement learning (RL). Phi-4-reasoning models demonstrate strong performance, outperforming larger open-weight models like the 70B DeepSeek-R1-Distill-Llama (e.g., achieving 78.0% vs 51.5% on AIME 25 for Phi-4-reasoning-plus vs Distill-70B) and approaching frontier models on various reasoning benchmarks. The key implication is that meticulous data curation combined with SFT and RL enables the creation of highly capable, smaller reasoning models, challenging the necessity for extremely large parameter counts for complex tasks. |
| Natural Language Processing | Beyond the Last Answer: Your Reasoning Trace Uncovers More than You
  Think (Read more on [arXiv](https://arxiv.org/abs/2504.20708) or [HuggingFace](https://huggingface.co/papers/2504.20708))| Bernard Ghanem, Hani Itani, Hasan Abed Al Kader Hammoud | This paper investigates whether analyzing intermediate reasoning steps ('subthoughts') in Large Language Models (LLMs) can provide a more reliable assessment of their reasoning capabilities than relying solely on the final generated answer. The main objective is to determine if the final answer is the optimal conclusion and if alternative reasoning paths yield better results. The key methodology involves segmenting an initial reasoning trace based on linguistic cues, generating multiple reasoning completions from the end of each intermediate subthought, extracting potential answers from each completion, and aggregating them by selecting the most frequent answer (mode). Experiments show that this mode aggregation significantly improves accuracy, yielding gains of up to 13% on the AIME2024 dataset compared to the standard final-answer evaluation. The main implication is that evaluating beyond the single final answer, specifically by analyzing subthought consistency and using mode aggregation, can lead to more robust LLM evaluation and potentially more accurate conclusions. |
| Machine Learning | Taming the Titans: A Survey of Efficient LLM Inference Serving (Read more on [arXiv](https://arxiv.org/abs/2504.19720) or [HuggingFace](https://huggingface.co/papers/2504.19720))| Tong Liu, Zhenlin Yang, Yixin Ji, Juntao Li, zenRRan | This paper provides a comprehensive survey of methods for efficient Large Language Model (LLM) inference serving. The objective is to systematically review techniques addressing the high memory, computational cost, latency, and throughput challenges inherent in deploying large-scale LLMs. The methodology involves a hierarchical review spanning instance-level optimizations (e.g., model parallelism, KV cache management, request scheduling) and cluster-level strategies (e.g., heterogeneous deployment, load balancing, cloud solutions), also covering emerging scenarios like RAG and speculative decoding. While the survey doesn't present new quantitative results, it highlights techniques aimed at meeting Service Level Objectives (SLOs) using metrics like throughput and latency, referencing advancements such as near-zero memory waste via techniques like PagedAttention. For AI practitioners, this survey offers a structured guide to state-of-the-art optimization techniques for deploying LLMs efficiently under various constraints. |
| Multi-Modal | COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning (Read more on [arXiv](https://arxiv.org/abs/2504.21850) or [HuggingFace](https://huggingface.co/papers/2504.21850))| Olga Russakovsky, Polina Kirichenko, Hee Seung Hwang, Xindi Wu | This paper introduces COMPACT, a data recipe designed to enhance the compositional visual reasoning capabilities of Multimodal Large Language Models (MLLMs). The primary objective is to improve MLLM performance on complex visual tasks requiring multiple integrated skills, addressing the limitation that standard Visual Instruction Tuning (VIT) datasets lack sufficient compositional complexity. COMPACT's methodology involves defining 10 atomic visual capabilities and systematically generating training questions that combine these capabilities (k=1, 2, 3) to create a dataset balanced across complexity levels, mixed with a small VIT subset. Results show COMPACT achieves performance comparable to full-scale VIT (LLAVA-665K) using less than 10% of the data (achieving 100.18% relative score overall) and significantly outperforms on high-complexity tasks. For AI practitioners, this implies that focusing on structured compositional complexity in training data offers a more data-efficient approach to improving complex visual reasoning than simply scaling dataset size. |
| Reinforcement Learning | RoboVerse: Towards a Unified Platform, Dataset and Benchmark for
  Scalable and Generalizable Robot Learning (Read more on [arXiv](https://arxiv.org/abs/2504.18904) or [HuggingFace](https://huggingface.co/papers/2504.18904))| Bangjun Wang, Yuyang Li, Songlin Wei, Feishi Wang, Haoran Geng | ROBOVERSE introduces a unified platform, large-scale synthetic dataset (>10M transitions, >1k tasks), and benchmark suite to address data scaling and evaluation challenges in robot learning. The main objective is to create a comprehensive, extensible framework standardizing simulation-based robotics research through a universal infrastructure called METASIM. Key methodologies include simulator abstraction, large-scale data migration and generation with augmentation (domain randomization, trajectory augmentation), and unified benchmarks for imitation learning (IL) and reinforcement learning (RL) across different generalization levels. Experiments demonstrate enhanced performance for IL (e.g., ACT 50.0% average success), RL, and world models, alongside successful direct sim-to-real transfer (e.g., OpenVLA achieving 7.0/10.0 on a real-world Wash Soap task after sim training). ROBOVERSE provides practitioners a standardized, scalable tool to develop, train, and evaluate generalizable robot learning models, facilitating sim-to-real transfer. |
| Computer Vision | ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D
  Physics Modeling for Complex Motion and Interaction (Read more on [arXiv](https://arxiv.org/abs/2504.21855) or [HuggingFace](https://huggingface.co/papers/2504.21855))| Alan Yuille, Liang-Chieh Chen, Qihang Yu, Ju He, Qihao Liu | ReVision presents a framework enhancing pre-trained video diffusion models with explicit 3D physics for generating complex motions and interactions. Its objective is to achieve high-quality, controllable, and physically plausible video generation without massive model sizes. The core method uses a three-stage pipeline: coarse video generation, extraction and optimization of 3D representations via a proposed Parameterized Physical Prior Model (PPPM), and refined video regeneration conditioned on the optimized 3D motion. ReVision significantly improves Stable Video Diffusion, surpassing a 13B parameter model in user preferences for motion quality and achieving lower FVD (e.g., 130.14 on dance generation). This demonstrates that incorporating explicit 3D physical knowledge is a potent, low-cost approach to advance complex video generation realism and control. |
| Natural Language Processing | Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report (Read more on [arXiv](https://arxiv.org/abs/2504.21039) or [HuggingFace](https://huggingface.co/papers/2504.21039))| Anu Vellore, Blaine Nelson, Alexander Chen, Baturay Saglam, paulkass | This technical report introduces Foundation-Sec-8B, a cybersecurity-specialized Large Language Model (LLM) based on the Llama 3.1-8B architecture. The primary objective is to address the limited adoption of LLMs in cybersecurity due to challenges like data scarcity and knowledge complexity. The methodology involves continued pretraining of the base Llama 3.1-8B model on a curated cybersecurity corpus of approximately 5 billion tokens. Key results demonstrate that Foundation-Sec-8B significantly outperforms the base Llama 3.1-8B and achieves performance comparable to or better than Llama 3.1-70B and GPT-4o-mini on specific benchmarks, such as achieving 0.720 accuracy on CTIBench-RCM compared to 0.630 for Llama 3.1-8B. The main implication for AI practitioners is the availability of a publicly released, specialized foundation model designed to accelerate the development and deployment of AI-driven tools in cybersecurity contexts. |
| Multi-Modal | UniBiomed: A Universal Foundation Model for Grounded Biomedical Image
  Interpretation (Read more on [arXiv](https://arxiv.org/abs/2504.21336) or [HuggingFace](https://huggingface.co/papers/2504.21336))| Hao Chen, Jiaxin Zhuang, Sunan He, Yuxiang Nie, Linshan Wu | This paper introduces UniBiomed, the first universal foundation model designed for grounded biomedical image interpretation, unifying text generation and image segmentation. The primary objective is to overcome the limitations of disjointed AI models by enabling simultaneous clinical text description and corresponding target segmentation across diverse biomedical imaging modalities. UniBiomed achieves this through a novel integration of a Multi-modal Large Language Model (MLLM - InternVL2.5) and a Segment Anything Model (SAM - SAM2), trained on a curated dataset of 27 million image-text-annotation triplets spanning 10 modalities. Extensive validation demonstrated state-of-the-art performance, notably surpassing the previous best segmentation model, BiomedParse, by an average of 10.25% in Dice score across 60 datasets. For AI practitioners, UniBiomed represents a significant step towards unified, end-to-end multi-modal analysis in biomedicine, potentially streamlining clinical workflows by automating grounded interpretation without manual pre-diagnosis or prompting. |
| Multi-Modal | Generative AI for Character Animation: A Comprehensive Survey of
  Techniques, Applications, and Future Directions (Read more on [arXiv](https://arxiv.org/abs/2504.19056) or [HuggingFace](https://huggingface.co/papers/2504.19056))| Alireza Mirrokni, Hossein Behzadasl, Pardis Sadat Zahraei, Omid Ghahroodi, Mohammad Mahdi Abootorabi | This paper comprehensively surveys generative AI techniques for character animation, unifying advancements across domains like facial, gesture, and motion synthesis. The objective is to consolidate state-of-the-art methods, datasets, evaluation metrics, and applications, providing a cohesive overview and identifying future research directions. The methodology involves systematically reviewing and categorizing generative models (GANs, VAEs, Diffusion Models, Transformers), multimodal systems (CLIP, ControlNet), and 3D representations applied to animation components like faces, expressions, avatars, and motion. The survey notes benchmark results, such as AvatarVerse achieving an 85% user preference rate for avatar fidelity compared to prior models. For practitioners, this work offers a foundational resource and roadmap, highlighting key techniques, open challenges like real-time performance and controllability, and ethical considerations to guide future development in AI-driven animation. |
