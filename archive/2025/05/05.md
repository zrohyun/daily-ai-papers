

## Papers for 2025-05-05

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | PixelHacker: Image Inpainting with Structural and Semantic Consistency (Read more on [arXiv](https://arxiv.org/abs/2504.20438) or [HuggingFace](https://huggingface.co/papers/2504.20438))| xinggangw, steelozazala, wenyuliu, SmileTAT, Uyoung | This paper introduces PixelHacker, a diffusion-based image inpainting model focused on achieving high structural and semantic consistency. The primary objective is to address the shortcomings of existing methods in handling complex structures and semantics, which often result in artifacts and illogical generations. The core methodology, Latent Categories Guidance (LCG), involves constructing a large dataset (14M pairs) annotated with 'foreground' and 'background' labels, encoding these with separate fixed-size embeddings, and injecting them into the diffusion model's denoising process via linear attention. PixelHacker significantly outperforms state-of-the-art methods, achieving a FID of 8.59 and LPIPS of 0.2026 on the Places2 test set under specific evaluation conditions. For AI practitioners, this work presents an effective approach for high-fidelity image inpainting where maintaining contextually coherent structure and semantics is critical. |
| Computer Vision | Improving Editability in Image Generation with Layer-wise Memory (Read more on [arXiv](https://arxiv.org/abs/2505.01079) or [HuggingFace](https://huggingface.co/papers/2505.01079))| Jaesik Park, Jaeah Lee, carpedkm | This paper introduces a framework to enhance iterative image editing by enabling sequential modifications while maintaining consistency and context. The primary objective is to address the limitations of single-object editing methods in handling multiple sequential edits, particularly preserving background integrity and integrating new objects naturally based on rough masks and specified order. The key methodology involves layer-wise memory for storing past edit latents and prompts, Background Consistency Guidance (BCG) for efficient background preservation, and Multi-Query Disentanglement (MQD) in cross-attention for natural object integration. Experiments on the proposed Multi-Edit Bench demonstrate superior performance, achieving a BLEU-4 score of 36.59 and a METEOR score of 0.1513 at 1024x1024 resolution, outperforming existing editing and layout-to-image methods. The framework offers AI practitioners a more controllable and efficient approach for complex, multi-step image editing tasks with minimal user input. |
| Natural Language Processing | Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG
  Evaluation Prompts (Read more on [arXiv](https://arxiv.org/abs/2504.21117) or [HuggingFace](https://huggingface.co/papers/2504.21117))| Wenge Rong, Yiqi Liu, Chenghao Xiao, Hanhua Hong, yangwang825 | This paper introduces an inversion learning method to automatically generate highly effective, model-specific evaluation prompts for Natural Language Generation (NLG) systems, addressing the sensitivity of LLM-based evaluators to prompt design. The objective is to replace manual prompt engineering by learning a reverse mapping from evaluation outputs back to the prompting instructions using only a single evaluation sample. The core methodology involves training an 'inversion model' (via inversion dataset distillation and fine-tuning in black-box or white-box settings) to generate tailored evaluation prompts based on a target LLM's behavior. Experiments show that inversion-generated prompts significantly outperform human-crafted and standard forward prompts in correlation with human judgments across various tasks (e.g., achieving average Spearman/Pearson gains of 33%/32% for LLaMA-3.1-8B over forward prompts). The key implication is that model-specific, automatically generated prompts via inversion learning offer a more robust and efficient path for reliable LLM-based NLG evaluation. |
| Natural Language Processing | Llama-Nemotron: Efficient Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2505.00949) or [HuggingFace](https://huggingface.co/papers/2505.00949))| Ran El-Yaniv, Mohammad Dabbah, Izik Golan, Itay Levy, Akhiad Bercovich | This paper introduces Llama-Nemotron, a family of open-licensed, heterogeneous reasoning language models (8B, 49B, 253B) optimized for inference efficiency and derived from Llama 3 models. The main objective is to create powerful reasoning models that are computationally efficient, commercially permissive, and feature a dynamic toggle to switch between standard chat and detailed reasoning modes. The methodology combines neural architecture search (NAS) using the Puzzle framework and FFN Fusion, knowledge distillation, continued pretraining, supervised fine-tuning (SFT) on curated reasoning datasets, and large-scale reinforcement learning (RL) using GRPO. The flagship LN-Ultra (253B) model achieves state-of-the-art open-source performance, notably scoring 76.0% on GPQA-Diamond, outperforming models like DeepSeek-R1 while offering higher throughput. This work provides practitioners with efficient, high-performing reasoning models, along with the associated training datasets and codebases, facilitating development and deployment of reasoning-intensive applications. |
| Natural Language Processing | CORG: Generating Answers from Complex, Interrelated Contexts (Read more on [arXiv](https://arxiv.org/abs/2505.00023) or [HuggingFace](https://huggingface.co/papers/2505.00023))| Trung Bui, aifactoryysh, Franck-Dernoncourt, hyunjilee | This paper introduces CONTEXT ORGANIZER (CORG), a framework designed to improve how language models generate answers from corpora containing complexly interrelated and often inconsistent contexts. The main objective is to address the challenge posed by simultaneously occurring distracting, ambiguous, counterfactual, and duplicated information within real-world text collections. CORG's methodology involves a graph constructor to map context relationships, a reranker to organize contexts into optimized groups based on relationship type, and an aggregator to generate comprehensive answers with citations. Experimental results demonstrate CORG's effectiveness, achieving a Disambig-F1 score of 22.0 with Llama2-7B on the AmbigDocs+ dataset, outperforming various grouping baselines and achieving performance comparable to more resource-intensive individual context processing. The key implication for AI practitioners is that explicitly modeling and organizing context interrelationships significantly enhances answer recall, disambiguation, and efficiency when dealing with complex, real-world knowledge sources. |
| Other | Real-World Gaps in AI Governance Research (Read more on [arXiv](https://arxiv.org/abs/2505.00174) or [HuggingFace](https://huggingface.co/papers/2505.00174))| Tim O'Reilly, sruly, isobelmoure, strauss-NYC | This paper analyzes AI governance research, comparing outputs from leading corporate and academic labs to identify gaps between pre-deployment focus and real-world deployment risks. The central objective is to assess how commercial incentives shape research priorities concerning AI safety and reliability, particularly in post-deployment contexts. Using a dataset of 1,178 safety/reliability papers (from 9,439 generative AI papers, Jan 2020-Mar 2025), the study employs GPT-4o mini for classification and regex searches for specific risk topics, adjusting metrics for fractional authorship. Key findings reveal corporate research concentrates on pre-deployment alignment and testing, while post-deployment issues like bias receive less attention; only 4% of corporate papers address high-risk deployment domains (e.g., healthcare, finance, misinformation, behavioral impacts). The main implication for practitioners is the need for increased focus and independent observability on post-deployment AI behaviors and risks, as current corporate research trends may not fully capture real-world harms. |
| Machine Learning | TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching (Read more on [arXiv](https://arxiv.org/abs/2505.00562) or [HuggingFace](https://huggingface.co/papers/2505.00562))| Chuchu Fan, yuemithucsd | TeLoGraF introduces a novel machine learning framework combining Graph Neural Networks (GNNs) and flow matching for robotic planning under general Signal Temporal Logic (STL) specifications. The main objective is to create a model that can generate valid trajectories for diverse, non-parameterized STL tasks by effectively encoding the logic structure. TeLoGraF employs a GNN to encode STL formulas represented as syntax trees and uses this encoding to condition a flow-matching generative model trained on a large dataset of STL-trajectory pairs. Experiments across five environments show TeLoGraF outperforms baselines in STL satisfaction rates and achieves significantly faster inference (10-100X) compared to classical methods like CEM and gradient-based approaches. This work provides practitioners a data-driven method for complex temporal logic planning, offering speed advantages over traditional techniques but without formal satisfaction guarantees. |
| Machine Learning | X-Cross: Dynamic Integration of Language Models for Cross-Domain
  Sequential Recommendation (Read more on [arXiv](https://arxiv.org/abs/2504.20859) or [HuggingFace](https://huggingface.co/papers/2504.20859))| Haggai Roitman, liorrokach, Bshapira, yeshel, guyhadad01 | This paper presents X-Cross, a novel model for cross-domain sequential recommendation that dynamically integrates multiple domain-specific language models adapted with Low-Rank Adapters (LoRA). The primary objective is to enable recommendation systems to quickly adapt to new domains using knowledge transferred from source domains, requiring minimal fine-tuning data and computational overhead. X-Cross achieves this by operating layer-by-layer, dynamically calculating weights to combine representations from frozen source LoRA adapters based on the input recommendation prompt, propagating refined representations through the network. Key results demonstrate that X-Cross achieves performance comparable to full LoRA fine-tuning while using only 25% of the additional parameters and requiring 50-75% less fine-tuning data for effective cross-domain adaptation. The main implication for AI practitioners is that X-Cross provides a scalable and efficient solution for developing adaptable recommendation systems in data-constrained or rapidly evolving domain environments. |
