

## Papers for 2025-05-23

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Machine Learning | NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop
  System from Hypothesis to Verification (Read more on [arXiv](https://arxiv.org/abs/2505.16938) or [HuggingFace](https://huggingface.co/papers/2505.16938))| Jiakang Yuan, Xiangchao Yan, Shiyang Feng, Bo Zhang, NovelSeek Team | NovelSeek is a closed-loop multi-agent framework for autonomous scientific research across various domains. It addresses the challenge of generating effective and novel research proposals and implementing closed-loop experimental validation. The framework uses self-evolving idea generation with human feedback, comprehensive idea-to-methodology construction, and multi-round automated experiment execution. Experiments on 12 tasks show promising performance gains; for instance, reaction yield prediction increased from 27.6% to 35.4% in 12 hours, though results may vary based on individual tasks.  NovelSeek enables faster scientific discovery by automating idea generation and experimental validation, reducing human effort while potentially accelerating progress. |
| Natural Language Processing | Scaling Reasoning, Losing Control: Evaluating Instruction Following in
  Large Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2505.14810) or [HuggingFace](https://huggingface.co/papers/2505.14810))| Yu Cheng, Xiaoye Qu, Jiawei Gu, yaful, TingchenFu | The paper investigates instruction-following capabilities in large reasoning models (LRMs), revealing a trade-off between reasoning capacity and adherence to user directives.  It introduces MathIF, a new benchmark for evaluating instruction-following in mathematical reasoning tasks. Through evaluation of 23 LRMs, the study finds that models often struggle to comply with instructions, with the best-performing model achieving only 50.71% accuracy on strict instruction-following.  The analysis identifies mutual interference between reasoning and instruction-following abilities, with reasoning-oriented training strategies often degrading instruction adherence, particularly with increased CoT length.  This highlights the need for instruction-aware reasoning models to address the growing gap between intelligence and obedience. |
| Reinforcement Learning | Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement
  Learning (Read more on [arXiv](https://arxiv.org/abs/2505.16410) or [HuggingFace](https://huggingface.co/papers/2505.16410))| Hongjin Qian, Jiajie Jin, Xiaoxi Li, Yifei Chen, Guanting Dong | The paper introduces Tool-Star, a reinforcement learning framework for empowering LLMs to autonomously use multiple external tools during reasoning. It addresses the challenge of effective multi-tool collaborative reasoning by developing a framework that leverages reinforcement learning. The methodology includes a data synthesis pipeline for generating tool-use trajectories and a two-stage training process: cold-start fine-tuning and a multi-tool self-critic RL algorithm. Experiments on over 10 reasoning benchmarks demonstrate Tool-Star's effectiveness, achieving a tool-use accuracy of over 40% on knowledge-intensive and computational reasoning datasets. Tool-Star offers AI practitioners a method for enhancing LLMs with multi-tool integration, improving reasoning capabilities and task performance. |
| Computer Vision | KRIS-Bench: Benchmarking Next-Level Intelligent Image Editing Models (Read more on [arXiv](https://arxiv.org/abs/2505.16707) or [HuggingFace](https://huggingface.co/papers/2505.16707))| Xianfang Zeng, Xinyu Ye, Xinting Hu, Zonghui Li, Yongliang Wu | The paper introduces KRIS-Bench, a benchmark for evaluating knowledge-based reasoning capabilities in instruction-based image editing models. It addresses the under-explored capacity of these models in performing editing tasks requiring real-world knowledge. The benchmark consists of 1,267 high-quality annotated editing instances categorized across factual, conceptual, and procedural knowledge types. Experiments on 10 state-of-the-art models reveal significant gaps in reasoning performance, with GPT-4o achieving the highest overall score of 62.41% across all models. KRIS-Bench provides a diagnostic tool to systematically evaluate the knowledge demands of intelligent image editing systems, revealing the need for knowledge-centric benchmarks to advance the field. |
| Multi-Modal | Pixel Reasoner: Incentivizing Pixel-Space Reasoning with
  Curiosity-Driven Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.15966) or [HuggingFace](https://huggingface.co/papers/2505.15966))| Fangzhen Lin, Weimin Ren, Haozhe Wang, Alex Su, wenhu | The paper introduces pixel-space reasoning, enabling vision-language models (VLMs) to directly interact with visual inputs. The research objective is to allow VLMs to perform reasoning steps directly within the visual modality. The methodology employs a two-phase training approach involving instruction tuning and curiosity-driven reinforcement learning to incentivize pixel-space reasoning. The 7B model, Pixel-Reasoner, achieves 84% on V* bench, demonstrating improved performance on visual reasoning benchmarks. This approach enhances VLM capabilities in visually intensive tasks by enabling interactive visual analysis. |
| Computer Vision | QuickVideo: Real-Time Long Video Understanding with System Algorithm
  Co-Design (Read more on [arXiv](https://arxiv.org/abs/2505.16175) or [HuggingFace](https://huggingface.co/papers/2505.16175))| Wenhu Chen, Tianyu Pang, Chao Du, Dongfu Jiang, Benjamin Schneider | This paper introduces QuickVideo, a system-algorithm co-design for real-time long video understanding. It addresses the computational bottlenecks of sequential video decoding and costly LLM prefilling for long videos. The approach employs QuickCodec for parallelized CPU-based decoding, QuickPrefill with KV-cache pruning for memory-efficient prefilling, and an overlapping scheme to synchronize CPU/GPU tasks. Experiments demonstrate a 3x speedup in processing 30-minute video inputs, reducing inference time from 69.7 to 20.0 seconds. QuickVideo enables more efficient long video processing on limited hardware, facilitating practical real-time applications. |
| Computer Vision | GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation
  with Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.17022) or [HuggingFace](https://huggingface.co/papers/2505.17022))| Linjiang Huang, Kun Wang, Yuqing Wang, Rongyao Fang, Chengqi Duan | The paper introduces GoT-R1, a reinforcement learning framework to enhance semantic-spatial reasoning for visual generation. The research objective is to improve the compositional fidelity of generated images by enabling models to autonomously discover effective reasoning strategies. The key methodology involves a dual-stage multi-dimensional reward framework using MLLMs to evaluate the reasoning process and final output, assessing semantic alignment, spatial accuracy, and visual quality. Results demonstrate significant improvements on the T2I-CompBench benchmark, with a 15% boost in evaluation metrics. GoT-R1 enables AI practitioners to create more accurate and contextually aware visual content through improved reasoning capabilities in generation models. |
| Multi-Modal | LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning (Read more on [arXiv](https://arxiv.org/abs/2505.16933) or [HuggingFace](https://huggingface.co/papers/2505.16933))| Jun Zhou, Jun Hu, Xiaolu Zhang, Shen Nie, Zebin You | The paper introduces LLaDA-V, a diffusion-based multimodal large language model (MLLM) for visual instruction tuning. It explores extending large language diffusion models to multimodal understanding by incorporating a vision encoder and MLP connector to align visual features with the language embedding space. The key methodology involves visual instruction tuning with masked diffusion models, multi-turn multimodal dialogues, and multi-stage training. LLaDA-V achieves state-of-the-art performance in multimodal understanding compared to existing hybrid and purely diffusion-based MLLMs, outperforming LLaMA3-V on several benchmarks when trained on the same dataset, particularly excelling in tasks involving multidisciplinary knowledge and mathematical reasoning. The implication is that large language diffusion models show promise in multimodal contexts and warrant further investigation. |
| Reinforcement Learning | Risk-Averse Reinforcement Learning with Itakura-Saito Loss (Read more on [arXiv](https://arxiv.org/abs/2505.16925) or [HuggingFace](https://huggingface.co/papers/2505.16925))| Alexander Korotin, Evgeny Burnaev, Anita Toleutaeva, Olivier Croissant, i-udovichenko | This paper introduces a risk-averse reinforcement learning (RL) approach using the Itakura-Saito (IS) divergence to address numerical instability issues in exponential utility RL. The research aims to develop a numerically stable and theoretically sound loss function for learning value functions in risk-averse Markov Decision Processes (MDPs). The methodology involves deriving a novel IS-based loss function and proving its adherence to the exponential utility's Bellman equation. Empirically, the proposed IS loss outperforms existing alternatives in portfolio optimization, deep hedging, and combinatorial RL tasks. The main implication is a more stable and reliable training process for risk-averse RL agents, especially in high-stakes applications. |
| Computer Vision | Scaling Diffusion Transformers Efficiently via μP (Read more on [arXiv](https://arxiv.org/abs/2505.15270) or [HuggingFace](https://huggingface.co/papers/2505.15270))| Zhi Tian, Wei Huang, Rongzhen Wang, Xinyu Zhang, ChenyuZheng | This paper focuses on efficiently scaling diffusion Transformers via Maximal Update Parametrization (µP). It addresses the challenge of hyperparameter tuning in large-scale diffusion Transformers by generalizing µP to architectures like DiT, U-ViT, PixArt-a, and MMDIT. The methodology involves proving that the µP formulation aligns with vanilla Transformers, enabling robust HP transferability; for instance, DiT-XL-2-µP achieves 2.9x faster convergence. Validating the effectiveness of µP on text-to-image generation scaled PixArt-a from 0.04B to 0.61B, outperforming baselines with minimal tuning, implying that µP offers a principled framework for scaling diffusion Transformers in vision tasks. |
| Natural Language Processing | Let LLMs Break Free from Overthinking via Self-Braking Tuning (Read more on [arXiv](https://arxiv.org/abs/2505.14604) or [HuggingFace](https://huggingface.co/papers/2505.14604))| Wenqi Zhang, Haolei Xu, Yongliang Shen, Yuchen Yan, Haoran Zhao | The paper introduces Self-Braking Tuning (SBT) to mitigate overthinking in large language models by enabling autonomous reasoning termination. It addresses the research question of how to allow LLMs to intrinsically recognize and halt excessive reasoning without external interventions. SBT constructs overthinking identification metrics and utilizes adaptive data construction with braking prompts to train models. Experiments on mathematical benchmarks demonstrate up to 60% reduction in token consumption while maintaining comparable accuracy. The implication is that SBT offers a method for enhancing reasoning efficiency in LLMs through self-regulation, leading to more practical and cost-effective deployment. |
| Natural Language Processing | Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning (Read more on [arXiv](https://arxiv.org/abs/2505.14684) or [HuggingFace](https://huggingface.co/papers/2505.14684))| Guiyang Hou, Wenqi Zhang, Yongliang Shen, Yuchen Yan, Haolei Xu | The paper introduces the CoT Thought Leap Bridge Task to address incompleteness in Chain-of-Thought (CoT) reasoning due to omitted intermediate steps, termed Thought Leaps, in mathematical reasoning datasets. The research aims to automatically detect and bridge these leaps by generating missing intermediate reasoning steps using a fine-tuned Qwen2.5-Math-7B model called CoT-Bridge.  CoT-Bridge is trained on a specialized dataset, ScaleQM+, constructed by introducing Thought Leaps into the structured ScaleQuestMath dataset. Experimental results on mathematical reasoning benchmarks demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath.  The study implies that enhancing reasoning completeness through bridging Thought Leaps significantly improves model performance and generalization capabilities in mathematical reasoning tasks, suggesting a valuable approach for improving CoT tuning. |
| Multi-Modal | Backdoor Cleaning without External Guidance in MLLM Fine-tuning (Read more on [arXiv](https://arxiv.org/abs/2505.16916) or [HuggingFace](https://huggingface.co/papers/2505.16916))| Xun Xiao, Jinhe Bi, Jian Liang, Wenke Huang, Xuankun Rong | This paper addresses the backdoor vulnerability of Multimodal Large Language Models (MLLMs) in fine-tuning-as-a-service settings. It investigates how backdoor triggers disrupt cross-modal attention, termed "attention collapse", where models overly focus on trigger regions instead of semantic content. To mitigate this, the paper proposes Believe Your Eyes (BYE), a data filtering framework that uses attention entropy patterns as a self-supervised signal to identify and filter backdoor samples via a three-stage pipeline. Experiments across various datasets and models demonstrate that BYE achieves near-zero attack success rates while maintaining clean-task performance, suggesting its effectiveness against backdoor threats. The study's findings indicate that attention entropy is a reliable, model-intrinsic signal for detecting data poisoning in MLLMs, enabling the development of more robust fine-tuning procedures. |
| Multi-Modal | Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel
  Decoding (Read more on [arXiv](https://arxiv.org/abs/2505.16990) or [HuggingFace](https://huggingface.co/papers/2505.16990))| Xinchao Wang, Xinyin Ma, Runpeng Yu | The paper introduces Dimple, the first Discrete Diffusion Multimodal Large Language Model (DMLLM). It addresses instability, suboptimal performance, and length bias in purely discrete diffusion approaches for MLLMs. The proposed training paradigm combines an initial autoregressive phase with a subsequent diffusion phase and a confident decoding strategy to improve inference efficiency. The Dimple-7B model surpasses LLaVA-NEXT in performance by 3.9% and demonstrates competitive performance while enhancing inference efficiency and controllability. Dimple validates the feasibility and advantages of DMLLMs for efficient and controllable multimodal generation. |
| Multi-Modal | VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game
  Quality Assurance (Read more on [arXiv](https://arxiv.org/abs/2505.15952) or [HuggingFace](https://huggingface.co/papers/2505.15952))| Nabajeet Barman, Saman Zadtootaghaj, Abhijay Ghildyal, corpaul, taesiri | The paper introduces VideoGameQA-Bench, a new benchmark for evaluating Vision-Language Models (VLMs) in video game quality assurance (QA). It aims to provide standardized tasks for visual unit testing, regression testing, glitch detection, and bug reporting, where current benchmarks fall short. The methodology involves creating a dataset with 4,786 questions across nine distinct tasks using real-world and synthetic game data. Results demonstrate that frontier VLMs achieve up to 82.8% accuracy in glitch detection but struggle with fine details and common-sense reasoning. This implies AI practitioners need to focus on improving VLMs' capabilities in intricate scene understanding and spatial reasoning for effective deployment in video game QA. |
| Computer Vision | Training-Free Efficient Video Generation via Dynamic Token Carving (Read more on [arXiv](https://arxiv.org/abs/2505.16864) or [HuggingFace](https://huggingface.co/papers/2505.16864))| Bohao Peng, Shaoteng Liu, Bin Xia, Jinbo Xing, Yuechen Zhang | The paper introduces Jenga, a training-free inference pipeline for efficient video generation using Diffusion Transformer (DiT) models. The research aims to reduce the computational cost of video DiTs, addressing the quadratic complexity of self-attention and the multi-step denoising process. Jenga combines dynamic attention carving, which sparsely selects relevant token interactions via 3D space-filling curves, with progressive resolution generation, which gradually increases latent resolution. Experiments show Jenga achieves up to 8.83x speedup with only a 0.01% performance drop on VBench. This enables practical, high-quality video generation on modern hardware without model retraining, reducing inference time from minutes to seconds. |
| Computer Vision | Understanding Generative AI Capabilities in Everyday Image Editing Tasks (Read more on [arXiv](https://arxiv.org/abs/2505.16181) or [HuggingFace](https://huggingface.co/papers/2505.16181))| Franck Dernoncourt, Viet Dac Lai, loganbolton, Franck-Dernoncourt, taesiri | The paper explores the capabilities of generative AI in automating everyday image editing tasks using a dataset of real-world Reddit requests. It investigates the types of editing actions users want to perform and assesses how well current AI editors fulfill these requests compared to human editors. The methodology involves analyzing 83k image editing requests and 305k associated edits, evaluating AI-generated edits using both human and vision language model (VLM) judges. Results show that AI can satisfactorily handle approximately 33.35% of requests, with notable difficulties in preserving subject identity; VLMs judging of AI-edited images differs than human judges. This study highlights areas for improvement in AI-based image editors, particularly in understanding user intent and avoiding unintended image modifications. |
| Multi-Modal | SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward (Read more on [arXiv](https://arxiv.org/abs/2505.17018) or [HuggingFace](https://huggingface.co/papers/2505.17018))| Xiangyu Yue, Dongzhan Zhou, Haoming Lyu, Kaituo Feng, Kaixuan Fan | The paper introduces SophiaVL-R1, a multimodal large language model, which aims to enhance reasoning capabilities through rule-based reinforcement learning by incorporating a thinking reward. The research explores adding reward signals for the thinking process to improve generalization ability. The methodology involves training a thinking reward model and employing a Trust-GRPO method to assign trustworthiness weights to the thinking reward during training. Experiments show that SophiaVL-R1-7B achieves 71.3% accuracy on the MathVista benchmark, surpassing existing MLLMs. This suggests that supervising the reasoning process can significantly improve MLLM performance and generalizability. |
| Computer Vision | SpatialScore: Towards Unified Evaluation for Multimodal Spatial
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2505.17012) or [HuggingFace](https://huggingface.co/papers/2505.17012))| Yanfeng Wang, Ya Zhang, Yaohui Chen, Xiao Huang, Haoning Wu | This paper introduces SpatialScore, a unified benchmark for evaluating multimodal spatial understanding in MLLMs. The research investigates the extent to which existing MLLMs possess 3D spatial perception and understanding abilities. SpatialScore integrates VGBench, a new visual geometry perception benchmark, with data from 11 existing datasets, comprising 28K samples, and uses a novel multi-agent system called SpatialAgent, equipped with specialized tools, to enhance MLLM performance. Evaluations show that SpatialAgent improves spatial understanding, with InternVL3-78B achieving the highest overall accuracy of 60.28% on VGBench; however, current MLLMs still lag behind humans, highlighting a need for fundamental architectural innovations for practitioners developing advanced MLLMs for embodied AI and autonomous navigation. |
| Multi-Modal | LaViDa: A Large Diffusion Language Model for Multimodal Understanding (Read more on [arXiv](https://arxiv.org/abs/2505.16839) or [HuggingFace](https://huggingface.co/papers/2505.16839))| Yusuke Kato, Akash Gokul, Hritik Bansal, Konstantinos Kallidromitis, Shufan Li | This paper introduces LaViDa, a family of diffusion-based Vision-Language Models (VLMs) for multimodal understanding. The research focuses on addressing limitations of autoregressive VLMs by exploring discrete diffusion models for tasks requiring visual reasoning, fast inference, and controllable generation. LaViDa incorporates complementary masking, prefix KV cache, and timestep shifting to enhance training efficiency, inference speed, and sampling quality. Experiments demonstrate that LaViDa achieves competitive performance on benchmarks such as MMMU (43.3%) and surpasses Open-LLaVa-Next-Llama3-8B by +4.1 CIDEr with 1.92x speedup on COCO captioning. LaViDa offers AI practitioners a strong alternative to autoregressive VLMs with advantages in speed-quality tradeoff, controllability, and bidirectional reasoning. |
| Reinforcement Learning | TinyV: Reducing False Negatives in Verification Improves RL for LLM
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.14625) or [HuggingFace](https://huggingface.co/papers/2505.14625))| Luyao Niu, Bhaskar Ramasubramanian, Fengqing Jiang, Yuetai Li, Zhangchen Xu | This paper introduces TinyV, a method to mitigate false negatives in reward signals for reinforcement learning (RL) applied to large language model (LLM) reasoning. The research addresses the problem of unreliable rewards due to verifiers failing to recognize correct model outputs. TinyV, a lightweight LLM-based verifier, augments rule-based methods by dynamically identifying and correcting false negatives. Experiments across math-reasoning benchmarks demonstrate that integrating TinyV boosts pass rates by up to 10% and accelerates convergence. This suggests addressing verifier false negatives is crucial for improving RL-based fine-tuning of LLMs, enabling more effective policy optimization. |
| Multi-Modal | Training-Free Reasoning and Reflection in MLLMs (Read more on [arXiv](https://arxiv.org/abs/2505.16151) or [HuggingFace](https://huggingface.co/papers/2505.16151))| Zhenzhong Chen, Hongchen Wei | The paper presents FRANK, a training-free approach for endowing multimodal large language models (MLLMs) with reasoning and reflection capabilities. It addresses the prohibitive costs of retraining MLLMs and the scarcity of high-quality multimodal reasoning datasets. FRANK leverages a hierarchical weight merging strategy that combines a vision-pretrained MLLM with a reasoning-specialized LLM, based on the observation that shallow layers focus on visual tokens and deeper layers on text. The method achieves an accuracy of 69.2 on the MMMU benchmark, surpassing existing baselines. FRANK offers a practical way to enhance reasoning in MLLMs without additional training. |
| Multi-Modal | GRIT: Teaching MLLMs to Think with Images (Read more on [arXiv](https://arxiv.org/abs/2505.15879) or [HuggingFace](https://huggingface.co/papers/2505.15879))| Ching-Chen Kuo, Kaizhi Zheng, Diji Yang, Xuehai He, Yue Fan | The paper introduces GRIT, a method for training MLLMs to perform grounded reasoning with images and text. It aims to enhance visual reasoning models by explicitly integrating visual information into reasoning chains. GRIT employs a reinforcement learning approach (GRPO-GR) to train models to generate reasoning chains interleaving natural language and bounding box coordinates. Experiments demonstrate that GRIT-trained models achieve higher GPT-as-judge answer accuracy scores on datasets like VSR and TallyQA, demonstrating improved unification of grounding and reasoning. This enables AI practitioners to develop more coherent and visually grounded reasoning capabilities in MLLMs with limited data. |
| Natural Language Processing | AGENTIF: Benchmarking Instruction Following of Large Language Models in
  Agentic Scenarios (Read more on [arXiv](https://arxiv.org/abs/2505.16944) or [HuggingFace](https://huggingface.co/papers/2505.16944))| Youfeng Liu, Amy Xin, Xiaozhi Wang, Hao Peng, Yunjia Qi | AGENTIF is a new benchmark designed to evaluate instruction following capabilities of large language models (LLMs) in agentic scenarios. The research aims to address the gap in evaluating LLMs' ability to follow lengthy instructions with complex constraints common in real-world agentic tasks. The methodology involves constructing a dataset of 707 instructions from 50 real-world agentic applications, annotating constraints, and providing evaluation metrics. Experiments show that current LLMs perform poorly, achieving a constraint success rate (CSR) of only 59.8% even with the best model, indicating a need for improved instruction following capabilities in LLMs for agentic applications. The findings imply that current LLMs struggle with complex constraint structures and tool specifications, hindering their effectiveness in agentic scenarios. |
| Multi-Modal | Think or Not? Selective Reasoning via Reinforcement Learning for
  Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.16854) or [HuggingFace](https://huggingface.co/papers/2505.16854))| Mike Zheng Shou, James Cheng, Kevin Qinghong Lin, Jiaqi Wang | This paper introduces TON, a reinforcement learning framework that enables vision-language models to selectively reason, reducing computational cost without sacrificing performance. The research aims to enable VLMs to decide when reasoning is necessary, mirroring human cognitive processes. TON employs a two-stage training strategy: a supervised fine-tuning stage with "thought dropout," followed by a GRPO stage. Experiments show TON reduces completion length by up to 90% compared to vanilla GRPO while maintaining or improving accuracy. The framework offers a way for AI practitioners to develop more efficient and human-like reasoning strategies in multimodal systems. |
| Reinforcement Learning | AceReason-Nemotron: Advancing Math and Code Reasoning through
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.16400) or [HuggingFace](https://huggingface.co/papers/2505.16400))| Chankyu Lee, Zihan Liu, Yang Chen, wping, zhuoliny | The paper introduces AceReason-Nemotron, a reinforcement learning approach to enhance math and code reasoning in language models. It investigates math-only and code-only RL training strategies, demonstrating cross-domain generalization and improved performance compared to distillation-based methods. Math-only RL significantly boosts performance on both math (e.g., +14.6% on AIME 2025 for 7B models) and code benchmarks (e.g., +6.8% on LiveCodeBench for 7B models). The method also integrates robust data curation and curriculum learning with on-policy updates, implying that large-scale RL effectively enhances reasoning capabilities, leading to solutions for previously unsolvable problems. |
| Multi-Modal | VLM-R^3: Region Recognition, Reasoning, and Refinement for Enhanced
  Multimodal Chain-of-Thought (Read more on [arXiv](https://arxiv.org/abs/2505.16192) or [HuggingFace](https://huggingface.co/papers/2505.16192))| Haiyang Xu, Han Yang, Wei Ye, Yongrui Heng, Chaoya Jiang | This paper introduces VLM-R³, a novel framework for dynamic visual reasoning in MLLMs using region recognition, reasoning, and refinement. It aims to address limitations in existing MLLMs that struggle with tasks requiring dynamic and iterative focusing on visual regions. The key methodology involves Region-Conditioned Reinforcement Policy Optimization (R-GRPO) and a curated Visuo-Lingual Interleaved Rationale (VLIR) corpus for training.  Experiments demonstrate superior performance on MathVista, ScienceQA, and other benchmarks; for example, a 2.2% improvement was observed on MathVista. VLM-R³ provides AI practitioners with an effective approach for enhancing MLLMs' ability to perform fine-grained, visually-grounded inference. |
| Multi-Modal | OViP: Online Vision-Language Preference Learning (Read more on [arXiv](https://arxiv.org/abs/2505.15963) or [HuggingFace](https://huggingface.co/papers/2505.15963))| Cheng Zeng, Jianxiang Wang, Zejun Li, Siyuan Wang, Shujun Liu | The paper introduces Online Vision-Language Preference Learning (OViP), a framework to mitigate hallucination in large vision-language models. It addresses the problem of hallucination by dynamically constructing contrastive training data based on the model's own hallucinated outputs. OViP identifies semantic differences in response pairs and generates negative images using a diffusion model for relevant supervision. Experiments demonstrate OViP effectively reduces hallucinations while preserving multi-modal capabilities, achieving improved performance on hallucination benchmarks with a Hallucination Reduction Index (HRI) of up to 10.00 on LLaVA-1.5-7B. The framework offers AI practitioners a method for adaptive alignment of both textual and visual preferences in vision-language models. |
| Reinforcement Learning | Reinforcement Learning Finetunes Small Subnetworks in Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2505.11711) or [HuggingFace](https://huggingface.co/papers/2505.11711))| Hao Peng, Dilek Hakkani-Tur, Lifan Yuan, sagnikM | The paper investigates parameter update sparsity in reinforcement learning (RL) finetuning of large language models (LLMs), observing that only a small subnetwork is updated. The research aims to determine how RL finetuning impacts model parameters and whether sparse updates are sufficient. Key methodology involves analyzing gradients and performance after finetuning with various RL algorithms. Results show that 5%-30% of parameters are updated, achieving similar test accuracy as full finetuning and a high subnetwork overlap (60%). The sparsity is attributed to training on near policy data, enabling more efficient RL fine-tuning strategies for LLMs. |
| Computer Vision | Let Androids Dream of Electric Sheep: A Human-like Image Implication
  Understanding and Reasoning Framework (Read more on [arXiv](https://arxiv.org/abs/2505.17019) or [HuggingFace](https://huggingface.co/papers/2505.17019))| Yazhe Niu, Chenhao Zhang | The paper introduces Let Androids Dream (LAD), a novel framework for image implication understanding and reasoning, aiming to bridge the gap between superficial perception and deep reasoning in multimodal AI systems. It addresses contextual missing by converting visual information into rich textual representations, iteratively searching for cross-domain knowledge, and generating context-aligned image implications via explicit reasoning. The methodology employs a three-stage process: perception, search, and reasoning. Results show that the LAD framework, using a lightweight GPT-40-mini model, achieves state-of-the-art performance on English and Chinese image implication benchmarks, outperforming several other models. It shows a 36.7% improvement over GPT-4o on the Open-Style Question, suggesting a new direction for solving these tasks and a move towards context-aware reasoning. |
| Natural Language Processing | SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.16186) or [HuggingFace](https://huggingface.co/papers/2505.16186))| Aosong Feng, Jayanth Srinivasa, Gaowen Liu, Xuandong Zhao, Kaiwen Zhou | The paper introduces SafeKey, a framework to enhance the safety of Large Reasoning Models (LRMs) against harmful queries and jailbreak attacks. It aims to improve safety generalization by better activating the safety aha moment in key sentences of the LRM's reasoning process. The methodology involves a Dual-Path Safety Head and Query-Mask Modeling to strengthen safety signals. Experiments show SafeKey reduces the average harmfulness rate by 9.6% across multiple safety benchmarks, while maintaining general abilities. This approach provides a more robust safety alignment strategy for LRMs, addressing the limitations of supervised fine-tuning. |
| Multi-Modal | Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot
  Manipulation Datasets (Read more on [arXiv](https://arxiv.org/abs/2505.15517) or [HuggingFace](https://huggingface.co/papers/2505.15517))| Ken Goldberg, Zehan Ma, Shuangyu Xie, keplerccc | This paper introduces Robo2VLM, a visual question answering (VQA) framework using robot trajectory data to enhance and evaluate vision-language models (VLMs). It addresses the challenge of existing VLM datasets lacking fine-grained spatial information for robotics by generating questions grounded in proprioceptive and kinematic information from real robot trajectories. Robo2VLM segments trajectories into manipulation phases, extracts keyframes, and generates multiple-choice questions based on spatial, goal-conditioned, and interaction reasoning templates. Experiments on the Robo2VLM-1 dataset (684,710 questions) show improvements in VLM capabilities with fine-tuning, achieving up to a 50% accuracy gain in state reasoning and task understanding. The work provides AI practitioners with a novel method and dataset for improving spatial and interaction reasoning in VLMs, particularly useful for robotics applications. |
| Multi-Modal | Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal
  Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.17015) or [HuggingFace](https://huggingface.co/papers/2505.17015))| Xiaodong Wang, Xingyu Chen, Hao Tang, Weiyao Wang, Runsen Xu | The paper introduces Multi-SpatialMLLM, a novel framework for enhancing multi-modal large language models (MLLMs) with robust multi-frame spatial understanding. It addresses the limitations of current MLLMs in real-world applications requiring multi-frame reasoning by integrating depth perception, visual correspondence, and dynamic perception. The research focuses on equipping MLLMs with enhanced spatial reasoning abilities. A large-scale dataset, MultiSPA (27 million samples), and benchmark are introduced to train and evaluate the model, which achieves a 36% average performance gain over baselines. The resulting model demonstrates improved generalizability and scalability, showing promise for applications like robotics by acting as a multi-frame reward annotator. |
| Natural Language Processing | Steering Large Language Models for Machine Translation Personalization (Read more on [arXiv](https://arxiv.org/abs/2505.16612) or [HuggingFace](https://huggingface.co/papers/2505.16612))| Malvina Nissim, Elisabetta Fersini, Arianna Bisazza, Daniel Scalena, gsarti | This paper explores personalization techniques for large language models (LLMs) in low-resource machine translation, focusing on literary texts. The study investigates prompting and steering methods, including a contrastive framework with sparse autoencoders (SAEs) to guide model generations toward personalized styles. Results show that steering methods achieve strong personalization while maintaining translation quality, with contrastive SAE steering outperforming prompting baselines. The classifiers achieved an average accuracy of 86% in discerning human vs machine translations. The paper suggests that steering techniques offer a promising approach for customizing LLM behavior in scenarios where stylistic preferences are difficult to articulate through prompts. |
| Natural Language Processing | When Do LLMs Admit Their Mistakes? Understanding the Role of Model
  Belief in Retraction (Read more on [arXiv](https://arxiv.org/abs/2505.16170) or [HuggingFace](https://huggingface.co/papers/2505.16170))| Robin Jia, ayyyq | This paper investigates when and why Large Language Models (LLMs) retract incorrect answers, defining retraction as acknowledging errors in previously generated responses. It explores whether LLMs retract incorrect answers that contradict their own internal knowledge. The methodology involves constructing model-specific datasets and employing probing and steering techniques to analyze the relationship between internal beliefs and retraction behavior. Results show that LLMs' retraction decisions are closely linked to their internal beliefs about the answer's correctness; steering the model to disbelieve its answer promotes retraction. Supervised fine-tuning significantly improves retraction performance, indicating a potential avenue for creating more reliable LLMs. |
| Natural Language Processing | Date Fragments: A Hidden Bottleneck of Tokenization for Temporal
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.16088) or [HuggingFace](https://huggingface.co/papers/2505.16088))| Wei Zhao, Maxime Peyrard, Gagan Bhatia | The paper identifies date tokenization as a bottleneck in temporal reasoning for large language models (LLMs). It investigates the impact of subword tokenizers on splitting calendar dates and obscuring the inherent structure needed for robust temporal reasoning. The study introduces a date fragmentation ratio metric and the DATEAUGBENCH dataset for evaluation. Experiments reveal a correlation between excessive date fragmentation and accuracy drops (up to 10 points) on temporal reasoning tasks. The research uncovers an emergent date-abstraction mechanism in LLMs to address fragmentation, suggesting implications for improving LLMs' temporal reasoning capabilities by considering date-aware vocabularies and adaptive tokenizers. |
| Multi-Modal | How Do Large Vision-Language Models See Text in Image? Unveiling the
  Distinctive Role of OCR Heads (Read more on [arXiv](https://arxiv.org/abs/2505.15865) or [HuggingFace](https://huggingface.co/papers/2505.15865))| Hwanhee Lee, Sunghyun Ryu, Hwan Chang, Ingeol Baek | This paper investigates how Large Vision-Language Models (LVLMs) locate and interpret text within images by identifying specialized Optical Character Recognition (OCR) heads. The research aims to characterize the distinctive properties of these OCR heads compared to retrieval heads. The methodology involves developing a scoring-based method to identify OCR heads and analyze their behavior through Chain-of-Thought prompting, attention masking, and sink-token redistribution. Results show that OCR heads exhibit reduced sparsity, qualitative distinctiveness, and static activation patterns, with performance improvements observed in OCR-VQA tasks via sink-token redistribution. The findings offer insights for enhancing multimodal reasoning and reducing hallucination in OCR-centric applications by manipulating attention distributions. |
| Natural Language Processing | MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation
  Capabilities in Any Language (Read more on [arXiv](https://arxiv.org/abs/2505.14395) or [HuggingFace](https://huggingface.co/papers/2505.14395))| Jiho Jin, Eunsu Kim, Seogyeong Jeong, aliceoh, seyoungsong | The paper introduces MUG-Eval, a framework for evaluating multilingual generation capabilities of LLMs without language-specific resources. It addresses the challenge of evaluating LLMs across diverse languages, especially low-resource ones, by transforming existing benchmarks into conversational tasks. The methodology involves using task success rate as a proxy for successful conversation generation between two instances of the same LLM. Evaluation of 8 LLMs across 30 languages shows MUG-Eval correlates strongly with established benchmarks (r > 0.75). This provides AI practitioners with a resource-efficient solution for standardized comparisons of multilingual generation capabilities. |
| Machine Learning | SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution (Read more on [arXiv](https://arxiv.org/abs/2505.16048) or [HuggingFace](https://huggingface.co/papers/2505.16048))| philippds | The paper introduces SPhyR, a novel benchmark dataset for evaluating spatial-physical reasoning capabilities of Large Language Models (LLMs) in the context of topology optimization. SPhyR challenges LLMs to predict or complete material distributions within a 2D design space under given boundary conditions, loads, and supports. The dataset includes tasks ranging from masked region completion to full material distribution prediction, requiring an understanding of force flow and structural stability. Experiments with SOTA LLMs demonstrate limited ability to reason about global structure and physical constraints, achieving limited exact matches but moderate normalized scores indicating partial corrections. SPhyR exposes a gap between LLMs' linguistic abilities and grounded physical reasoning, highlighting the need for models to internalize physical concepts for engineering and design tasks. |
