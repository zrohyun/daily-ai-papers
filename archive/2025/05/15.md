

## Papers for 2025-05-15

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception (Read more on [arXiv](https://arxiv.org/abs/2505.04410) or [HuggingFace](https://huggingface.co/papers/2505.04410))| Yichi Chen, Bin Kang, Yulin Li, Bin Chen, xiaomoguhzz | The paper introduces DeCLIP, a framework for enhancing open-vocabulary dense perception by decoupling and refining CLIP's feature representations. It addresses CLIP's limitations in aggregating spatially or semantically related information by decoupling the self-attention module into content and context features, aligning content with image crops and context with vision foundation models. DeCLIP outperforms existing methods on open-vocabulary detection and semantic segmentation tasks, achieving, for example, a 3.5 mAP improvement on novel classes in OV-COCO. The framework improves the applicability of VLMs to downstream dense prediction tasks by enhancing local discriminability and spatial consistency. |
| Multi-Modal | BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,
  Training and Dataset (Read more on [arXiv](https://arxiv.org/abs/2505.09568) or [HuggingFace](https://huggingface.co/papers/2505.09568))| Zhiyang Xu, Jiuhai Chen, xurantju, zhoutianyi, xcpan | The paper introduces BLIP3-0, a family of unified multimodal models for image understanding and generation. It explores optimal architectures and training strategies for image generation within a unified framework, focusing on image representations, modeling objectives, and training strategies. The methodology employs a diffusion transformer to generate semantically rich CLIP image features and a sequential pretraining strategy. The 8B model achieves a score of 1682.6 on MME-P, demonstrating superior performance. The open-sourced models, training scripts, and datasets aim to facilitate future research and advancements in unified multimodal AI. |
| Machine Learning | Insights into DeepSeek-V3: Scaling Challenges and Reflections on
  Hardware for AI Architectures (Read more on [arXiv](https://arxiv.org/abs/2505.09343) or [HuggingFace](https://huggingface.co/papers/2505.09343))| Huazuo Gao, Damai Dai, Chong Ruan, Chengqi Deng, Chenggang Zhao | This paper explores scaling challenges and hardware-aware co-design for large language models (LLMs), focusing on the DeepSeek-V3 architecture. The main objective is to demonstrate how hardware constraints influence model design choices for cost-efficient training and inference. The study employs innovations like Multi-head Latent Attention (MLA), Mixture of Experts (MoE) optimizations, and FP8 mixed-precision training, alongside a Multi-Plane Network Topology. DeepSeek-V3 achieves state-of-the-art performance using just 2,048 NVIDIA H800 GPUs and a 70.272 KB KV Cache Per Token, demonstrating the effectiveness of hardware-aware model design. The insights highlight the critical role of hardware and model co-design in addressing the escalating demands of AI workloads, offering a blueprint for innovation in next-generation AI systems. |
| Computer Vision | Marigold: Affordable Adaptation of Diffusion-Based Image Generators for
  Image Analysis (Read more on [arXiv](https://arxiv.org/abs/2505.09358) or [HuggingFace](https://huggingface.co/papers/2505.09358))| Nando Metzger, Tianfu Wang, Kevin Qu, Bingxin Ke, konradschindler | Marigold is a fine-tuning protocol for adapting diffusion-based image generators for various image analysis tasks. The paper addresses the need for resource-efficient transfer learning by leveraging rich visual knowledge in pretrained latent diffusion models for tasks like depth estimation and surface normal prediction. Marigold fine-tunes Stable Diffusion with synthetic data and demonstrates zero-shot generalization capabilities, achieving strong performance on unseen datasets. For example, Marigold can produce pixel-perfect depth maps and surface normals, requiring minimal modification to the diffusion model's architecture. This approach requires less than 3 GPU-days to train and can produce predictions in under 100ms, enabling affordable image analysis. |
| Machine Learning | SweRank: Software Issue Localization with Code Ranking (Read more on [arXiv](https://arxiv.org/abs/2505.07849) or [HuggingFace](https://huggingface.co/papers/2505.07849))| Xuan Phi Nguyen, Ye Liu, JaeHyeok Doo, Tarun Suresh, Revanth Gangi Reddy | The paper introduces SWERANK, a novel retrieve-and-rerank framework for software issue localization. It aims to improve the accuracy and efficiency of identifying relevant code locations for software issues compared to expensive agent-based systems. SWERANK uses a bi-encoder retriever and listwise LLM reranker trained on SWELOC, a newly constructed dataset of real-world issue descriptions and code modifications. Experiments on SWE-Bench-Lite demonstrate that SWERANK achieves state-of-the-art performance with significantly lower cost (e.g., outperforming costly closed-source LLMs). This framework offers AI practitioners a scalable and cost-effective approach to automated software engineering. |
| Computer Vision | VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large
  Video Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.08455) or [HuggingFace](https://huggingface.co/papers/2505.08455))| Ali Etemad, pritamqu | This paper introduces VCRBench, a novel benchmark for evaluating long-form causal reasoning capabilities of Large Video Language Models (LVLMs). It addresses the gap in benchmarks for visually grounded, goal-driven causal reasoning by using procedural videos with shuffled steps. The study finds that LVLMs struggle with video-based causal reasoning, particularly modeling long-range causal dependencies, achieving accuracies often below random chance.  The authors propose Recognition-Reasoning Decomposition (RRD) to improve performance, demonstrating gains of up to 25.2% on VCRBench, indicating that LVLMs primarily rely on language knowledge in complex video reasoning tasks. RRD shows that by separating video recognition and causal reasoning tasks, the performance of LVLMs on complex video tasks can be improved. |
| Multi-Modal | Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM? (Read more on [arXiv](https://arxiv.org/abs/2505.09439) or [HuggingFace](https://huggingface.co/papers/2505.09439))| Hilde Kuehne, Samuel Thomas, Edson Araujo, Saurabhchand Bhati, h9LtLSb | The paper introduces Omni-R1, a GRPO fine-tuned Qwen2.5-Omni model, to improve audio question answering. It investigates whether audio fine-tuning is truly necessary for audio LLMs by focusing on text-based reasoning capabilities. The methodology involves fine-tuning Qwen2.5-Omni with GRPO on automatically generated audio question-answering datasets and evaluating performance with and without audio inputs. Omni-R1 achieves a new state-of-the-art average accuracy of 71.3% on the MMAU Test-mini split. The research implies that enhancing the text-based reasoning of audio LLMs can lead to substantial gains in audio understanding, even without extensive audio-specific fine-tuning. |
| Computer Vision | DetReIDX: A Stress-Test Dataset for Real-World UAV-Based Person
  Recognition (Read more on [arXiv](https://arxiv.org/abs/2505.04793) or [HuggingFace](https://huggingface.co/papers/2505.04793))| Carolina Fernandes, Satish Mekewad, Pavan Kumar MP, Nzakiese Mbongo, Kailash A. Hambarde | The paper introduces DetReIDX, a large-scale aerial-ground dataset designed to stress-test person re-identification (ReID) under realistic UAV surveillance conditions. It aims to address the limitations of existing datasets by incorporating extreme data variability factors like low resolution, viewpoint changes, and clothing variations. The dataset includes over 13 million bounding boxes from 509 identities captured across multiple sessions and altitudes. Experiments using SOTA methods reveal a catastrophic degradation in performance, with up to 80% reduction in detection accuracy and over 70% decrease in Rank-1 ReID. DetReIDX serves as a challenging benchmark for developing more robust and generalizable ReID algorithms suitable for real-world UAV applications. |
