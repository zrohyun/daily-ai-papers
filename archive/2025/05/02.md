

## Papers for 2025-05-02

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | A Survey of Interactive Generative Video (Read more on [arXiv](https://arxiv.org/abs/2504.21853) or [HuggingFace](https://huggingface.co/papers/2504.21853))| Xintao Wang, Quande Liu, Haoxuan Che, Yiran Qin, Jiwen Yu | This paper surveys the emerging field of Interactive Generative Video (IGV), defining it as technology combining generative capabilities for video with interactive user engagement. The primary objective is to provide a comprehensive overview of IGV applications (gaming, embodied AI, autonomous driving), propose a structured framework, and identify key technical challenges and future directions. The methodology involves a literature review, categorization of existing work, the proposal of a five-module framework (Generation, Control, Memory, Dynamics, Intelligence), and analysis of associated technical hurdles. As a survey paper, it synthesizes the current state and challenges (e.g., achieving real-time generation, open-domain control, physical accuracy) rather than presenting novel quantitative experimental results. The main implication for AI practitioners is the provision of a structured roadmap for understanding, researching, and developing advanced interactive generative video systems across various domains. |
| Natural Language Processing | DeepCritic: Deliberate Critique with Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.00662) or [HuggingFace](https://huggingface.co/papers/2505.00662))| Ji-Rong Wen, Yankai Lin, Jingwen Chen, Keven16 | This paper introduces DeepCritic, a framework to enhance the critique capabilities of Large Language Models (LLMs) specifically for mathematical reasoning tasks. The primary objective is to overcome the limitations of shallow critiques generated by current LLMs by enabling more deliberate, step-wise, and multi-perspective evaluation. The methodology employs a two-stage pipeline: first, supervised fine-tuning (SFT) on 4.5K synthetically generated long-form critiques, followed by reinforcement learning (RL) using either human-labeled (PRM800K) or automatically annotated data. The resulting DeepCritic-7B-RL-PRM800K model significantly outperformed baselines, including GPT-4o and same-sized specialized models, achieving a 67.1 average F1 score across evaluation benchmarks. The main implication for AI practitioners is a demonstrated pathway to create more effective LLM critics for complex reasoning, potentially enabling better automated supervision and scalable oversight. |
| Multi-Modal | T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level
  and Token-level CoT (Read more on [arXiv](https://arxiv.org/abs/2505.00703) or [HuggingFace](https://huggingface.co/papers/2505.00703))| Hao Li, Zhuofan Zong, Renrui Zhang, Ziyu Guo, Dongzhi Jiang | This paper introduces T2I-R1, a reasoning-enhanced text-to-image generation model leveraging reinforcement learning (RL) and a novel bi-level Chain-of-Thought (CoT) process. The primary objective is to improve generation quality and prompt alignment by explicitly coordinating high-level semantic planning (semantic-level CoT) and low-level patch-by-patch synthesis (token-level CoT). The core methodology, BiCoT-GRPO, uses Group Relative Policy Optimization within a Unified Large Multimodal Model (ULM), guided by rewards from an ensemble of vision expert models, to jointly optimize both CoT levels. T2I-R1 demonstrates significant quantitative improvements, achieving +13% on T2I-CompBench and +19% on the WISE benchmark over its baseline, surpassing previous state-of-the-art models like FLUX.1. The main implication is that explicitly integrating structured reasoning via RL and multi-level CoT can enhance the capability and robustness of generative models, particularly for complex prompts. |
| Machine Learning | AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning
  Optimization (Read more on [arXiv](https://arxiv.org/abs/2504.21659) or [HuggingFace](https://huggingface.co/papers/2504.21659))| Rui Liu, Jinluan Yang, Yibo Wang, Haiying He, Haotian Luo | This paper introduces AdaR1, a framework to enhance the inference efficiency of Large Language Models (LLMs) by enabling adaptive reasoning depth. The primary objective is to mitigate the high computational cost of long Chain-of-Thought (Long-CoT) reasoning by allowing models to dynamically choose between detailed (Long-CoT) and concise (Short-CoT) reasoning paths based on input problem complexity. The core methodology involves a two-stage process: first, creating a hybrid model via merging parameters of Long-CoT and Short-CoT fine-tuned models, and second, applying Bi-Level Preference Optimization training to teach the model style selection (group-level) and path conciseness (instance-level). Experiments demonstrate significant efficiency gains, reducing the average reasoning length by over 50% across five mathematical datasets while largely maintaining task performance compared to baseline Long-CoT models. For AI practitioners, AdaR1 offers a strategy to deploy complex reasoning models more efficiently by tailoring computational resource allocation to the specific demands of each problem instance. |
| Multi-Modal | KeySync: A Robust Approach for Leakage-free Lip Synchronization in High
  Resolution (Read more on [arXiv](https://arxiv.org/abs/2505.00497) or [HuggingFace](https://huggingface.co/papers/2505.00497))| Konstantinos Vougioukas, Michał Stypułkowski, Stella Bounareli, Rodrigo Mira, Antoni Bigata | KeySync introduces a robust two-stage diffusion-based framework for high-resolution, leakage-free, and occlusion-aware lip synchronization. The primary objective is to overcome limitations of existing lip-sync methods, specifically expression leakage from the source video, temporal inconsistency, low resolution, and inadequate handling of facial occlusions, particularly in cross-synchronization scenarios. KeySync employs a latent diffusion model adapted from KeyFace, generating keyframes and interpolating between them, conditioned on HuBERT audio features and guided by a carefully designed lower-face mask; it further incorporates an inference-time occlusion removal step using video segmentation. KeySync achieves state-of-the-art results, significantly reducing expression leakage (LipLeak score of 0.16) and improving lip-sync accuracy (LipScore of 0.48) in challenging cross-synchronization tasks compared to prior works. This approach offers AI practitioners a more reliable method for high-fidelity lip synchronization suitable for real-world applications like automated dubbing, effectively addressing leakage and occlusion issues. |
| Natural Language Processing | TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2504.20605) or [HuggingFace](https://huggingface.co/papers/2504.20605))| Laura Diosan, andreeatomescu, andreiPiscoran, mihainadas | This paper introduces TF1-EN-3M, a dataset of three million synthetic English moral fables generated exclusively by small, open-weight language models (up to 8B parameters). The primary objectives were to assess a combinatorial prompt expansion method for diverse fable generation and identify the best-suited open-weight LLMs under resource constraints. The methodology involved using a six-slot structured prompt (character, trait, setting, conflict, resolution, moral) and a hybrid evaluation pipeline combining a GPT-based critic with reference-free metrics (Self-BLEU, Distinct-n, Flesch Reading Ease). Results showed that Llama-3.1-8B-Instruct achieved the highest composite score (0.891), balancing quality and efficiency, generating fables at approximately $0.135 per 1000 on consumer GPUs. The main implication is that large-scale, high-quality synthetic datasets for specialized narrative and moral reasoning tasks can be efficiently created using accessible open models, reducing reliance on proprietary systems. |
| Reinforcement Learning | LLMs for Engineering: Teaching Models to Design High Powered Rockets (Read more on [arXiv](https://arxiv.org/abs/2504.19394) or [HuggingFace](https://huggingface.co/papers/2504.19394))| Toby Simonds | This paper evaluates Large Language Models' (LLMs) capabilities in complex engineering design, specifically high-powered rocketry, finding that reinforcement learning significantly enhances performance. The main objective was to determine if LLMs could effectively perform physical engineering tasks and if RL could overcome limitations observed in foundation models' iterative design processes. The methodology involved benchmarking foundation LLMs and an RL-trained model (Qwen-2.5 7B using GRPO) against human experts on target altitude and precision landing tasks using the RocketPy simulator via a custom interface (RocketBench). While foundation models plateaued below human performance, the RL-trained 7B model surpassed both human experts and state-of-the-art LLMs, achieving a peak score of 79.98 on the altitude challenge (vs. human 76.57) and landing within 12 meters on the precision landing task. The key implication for AI practitioners is that combining LLMs' domain knowledge with RL optimization and suitable simulation interfaces enables superhuman performance in complex engineering domains, suggesting a transformative potential beyond software development but also raising safety concerns. |
| Computer Vision | MediAug: Exploring Visual Augmentation in Medical Imaging (Read more on [arXiv](https://arxiv.org/abs/2504.18983) or [HuggingFace](https://huggingface.co/papers/2504.18983))| Lei Zhang, Hao Zhang, Canxuan Gang, Zeyu Zhang, Xuyin Qi | This paper introduces MediAug, a benchmark for evaluating advanced mix-based visual data augmentation techniques specifically for medical imaging. The primary objective is to address the domain gap between natural and medical images for augmentation methods and provide a systematic comparison of strategies like MixUp, CutMix, YOCO, CropMix, AugMix, and SnapMix, which are often studied in isolation. The methodology involves applying these six augmentation techniques to brain tumor MRI and eye disease fundus datasets, using both ResNet-50 and ViT-B backbones for evaluation. Key results demonstrate task and architecture specificity, with MixUp yielding the best improvement for ResNet-50 on brain tumors (79.19% accuracy) while SnapMix was best for ViT-B (99.44% accuracy), and different methods proving optimal for eye diseases (YOCO for ResNet-50 at 91.60%, CutMix for ViT-B at 97.94%). The main implication for AI practitioners is the provision of a unified benchmark and guidance on selecting appropriate advanced augmentation methods based on the specific medical imaging task and chosen network architecture. |
