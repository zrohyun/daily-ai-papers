

## Papers for 2025-05-13

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Seed1.5-VL Technical Report (Read more on [arXiv](https://arxiv.org/abs/2505.07062) or [HuggingFace](https://huggingface.co/papers/2505.07062))| kuma-zhao, yuanlp, 0nejiawei, chb1997, anyuzx | The Seed1.5-VL Technical Report introduces a vision-language foundation model designed for general-purpose multimodal understanding and reasoning. The main objective is to advance VLM capabilities by addressing challenges in data scarcity, efficient training of asymmetrical architectures, and achieving strong performance across diverse tasks. Seed1.5-VL comprises a 532M-parameter vision encoder (Seed-ViT) and a Mixture-of-Experts (MOE) LLM with 20B active parameters, pre-trained on trillions of multimodal tokens and further refined through supervised fine-tuning and reinforcement learning from human and verifiable rewards. The model achieves state-of-the-art performance on 38 out of 60 public benchmarks, including a 77.9 score on the MMMU benchmark. This report offers AI practitioners insights into building advanced VLMs, detailing experiences in model design, data construction, and training methodologies that can inspire further research and broader applications. |
| Natural Language Processing | MiMo: Unlocking the Reasoning Potential of Language Model -- From
  Pretraining to Posttraining (Read more on [arXiv](https://arxiv.org/abs/2505.07608) or [HuggingFace](https://huggingface.co/papers/2505.07608))| whatseeker, Prestonprom, HugoZHL, dwzhu, xiabingquan | The paper introduces MiMo-7B, a 7-billion parameter language model specifically engineered for advanced reasoning tasks through comprehensive optimization across its entire pre-training and post-training pipeline. Its primary objective is to unlock and significantly enhance the reasoning capabilities of language models by employing tailored strategies throughout the model's development, from data Curation and pre-training architecture to reinforcement learning (RL) fine-tuning. Key methodologies include a three-stage pre-training data mixing strategy on 25 trillion tokens with a Multi-Token Prediction (MTP) objective, followed by post-training using RL on a curated dataset of 130K verifiable math and code problems, incorporating a test-difficulty-driven reward scheme and strategic data resampling. The resulting RL-tuned model, MiMo-7B-RL, demonstrates superior reasoning performance, achieving 55.4% on AIME 2025 (Pass@1) and 57.8% on LiveCodeBench v5 (Pass@1), outperforming models like OpenAI's o1-mini. This research implies that meticulous, full-lifecycle optimization allows smaller models to achieve exceptional reasoning, offering efficient, high-performing alternatives for complex tasks and challenging the notion that reasoning prowess is exclusive to much larger models. |
| Multi-Modal | Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured
  3D Assets (Read more on [arXiv](https://arxiv.org/abs/2505.07747) or [HuggingFace](https://huggingface.co/papers/2505.07747))| PanJianxiong, flybirdtian, shian7, wchengad, xuanyangz | This paper introduces Step1X-3D, an open framework for generating high-fidelity and controllable textured 3D assets. The primary objective is to address key challenges in 3D generation, including data scarcity and algorithmic limitations, by providing a comprehensive and reproducible solution. Its methodology features a rigorous data curation pipeline (resulting in a 2M high-quality asset dataset) and a two-stage 3D-native architecture: geometry is generated by a hybrid VAE-DiT producing Truncated Signed Distance Function (TSDF) representations, while texture synthesis employs a diffusion-based module with geometric conditioning and latent-space synchronization; the framework also supports transferring 2D control techniques like LoRA to 3D. Benchmark comparisons demonstrate state-of-the-art performance, with Step1X-3D achieving a leading CLIP-Score of 0.853 for texture quality against several open-source and proprietary models. For AI practitioners, Step1X-3D provides a fully open-sourced suite of models, training code, and curated data, aiming to advance research and establish new standards in controllable 3D content creation. |
| Natural Language Processing | Learning from Peers in Reasoning Models (Read more on [arXiv](https://arxiv.org/abs/2505.07787) or [HuggingFace](https://huggingface.co/papers/2505.07787))| Benyou, tangzhy, Jiaxi0775, wydu, Zeno-Luo | This paper introduces "Learning from Peers (LeaP)," an inference-time strategy, and an associated fine-tuned model series (LeaP-T), to enhance the reasoning and self-correction abilities of Large Reasoning Models (LRMs) through peer interaction. The primary objective is to address the "Prefix Dominance Trap," where LRMs struggle to recover from poor initial reasoning, by enabling models to learn from concurrently generated peer reasoning paths. LeaP facilitates this by having each reasoning path periodically summarize its intermediate progress and share these summaries with other paths through a routing mechanism, allowing for the incorporation of peer insights. Experiments demonstrate substantial improvements; for instance, QwQ-32B with LeaP achieved nearly 5 absolute points higher on average than its baseline, and the fine-tuned LeaP-T-7B model matched the performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024 with a Pass@1 score of 64.38. The main implication for AI practitioners is that fostering collaborative interaction between parallel reasoning paths via LeaP can significantly improve LRM robustness and error correction in complex tasks. |
| Machine Learning | Unified Continuous Generative Models (Read more on [arXiv](https://arxiv.org/abs/2505.07447) or [HuggingFace](https://huggingface.co/papers/2505.07447))| Yi Jiang, tlin-taolin, sp12138sp | This paper introduces UCGM, a unified framework for training, sampling, and understanding continuous generative models, encompassing multi-step and few-step approaches. The primary objective is to overcome the fragmentation in existing generative model paradigms by establishing a common foundation for their training and sampling. UCGM proposes a unified training objective (UCGM-T) parameterized by a consistency ratio (Î») and a unified sampler (UCGM-S) with self-boosting techniques to enhance model quality and sampling efficiency. Key results include UCGM-T achieving 1.30 FID in 20 sampling steps on ImageNet 256x256 with a 675M parameter model, and UCGM-S improving a pre-trained model's FID from 1.26 (250 steps) to 1.06 (40 steps). This framework offers AI practitioners a versatile tool to develop and deploy high-performance continuous generative models with greater efficiency and flexibility across different NFE budgets. |
| Reinforcement Learning | REFINE-AF: A Task-Agnostic Framework to Align Language Models via
  Self-Generated Instructions using Reinforcement Learning from Automated
  Feedback (Read more on [arXiv](https://arxiv.org/abs/2505.06548) or [HuggingFace](https://huggingface.co/papers/2505.06548))| Pawan Goyal, Somak Aditya, Aniruddha Roy, abhi1nandy2, Pretam | The paper introduces REFINE-AF, a task-agnostic framework designed to align smaller, open-source language models using self-generated instructions and Reinforcement Learning from Automated Feedback (RLAF). Its main objective is to evaluate the capability of small LLMs (LLaMA 2-7B/13B, Mistral 7B) for task-agnostic instruction generation and to assess the effectiveness of an RLAF-based algorithm in this pipeline. The REFINE-AF methodology involves three stages: initial instruction generation from a seed set, refinement of instruction-input-output pairs using RLAF with an automated reward model, and subsequent supervised fine-tuning of the base LLM. The framework demonstrated substantial improvements, with models fine-tuned using REFINE-AF outperforming baseline self-instruct methods in 63-66% of tasks on the SUPER-NI benchmark; for example, LLaMA 2-7B showed a 64.39% task improvement with 15,000 generated instructions. For AI practitioners, REFINE-AF offers a resource-efficient method to create diverse, high-quality instruction datasets for smaller open-source LLMs, reducing dependency on costly large models and extensive human annotation. |
| Reinforcement Learning | DanceGRPO: Unleashing GRPO on Visual Generation (Read more on [arXiv](https://arxiv.org/abs/2505.07818) or [HuggingFace](https://huggingface.co/papers/2505.07818))| appleluo, ChenMnZ, ltzhu, wujie10, xzyhku | The paper introduces DanceGRPO, a novel reinforcement learning (RL) framework adapting Group Relative Policy Optimization (GRPO) to align visual generative models, including diffusion and rectified flows, with human preferences across text-to-image, text-to-video, and image-to-video tasks. Its primary objective is to develop a scalable and stable RL-based alignment solution that overcomes limitations of prior methods concerning compatibility with ODE-based samplers, large-scale training stability, and validation for video generation. DanceGRPO formulates visual generation sampling as a Markov Decision Process, utilizes Stochastic Differential Equations for GRPO's multi-trajectory exploration, and optimizes policy models via a GRPO-style objective, incorporating shared initialization noise and selective timestep optimization. The framework demonstrates substantial improvements, outperforming baselines by up to 181% on benchmarks like VideoAlign, and uniquely enables learning from sparse binary feedback and enhanced Best-of-N inference scaling. This research provides AI practitioners with a robust, unified RLHF solution for visual generation, facilitating superior alignment with human preferences across diverse models and complex tasks. |
| Natural Language Processing | AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong
  Pretraining Data Selection (Read more on [arXiv](https://arxiv.org/abs/2505.07293) or [HuggingFace](https://huggingface.co/papers/2505.07293))| Steven Wu, Kai Hua, shenke18, zhangysk | This paper introduces AttentionInfluence, a training-free method for selecting reasoning-intensive pretraining data to improve Large Language Models' (LLMs) complex reasoning abilities without supervision signals. The main objective is to efficiently and scalably identify diverse, high-quality pretraining data by leveraging the intrinsic mechanisms of attention heads in smaller pretrained language models. The key methodology involves identifying crucial 'retrieval heads' in a small pretrained language model and computing the loss difference when these heads are masked, thereby creating a 'weak model' from a 'strong model' to score data samples. Experimental results demonstrate substantial improvements, ranging from 1.4pp to 3.5pp across benchmarks like MMLU, MMLU-Pro, and GSM8K when pretraining a 7B model with data selected by a 1.3B model using AttentionInfluence. The main implication for AI practitioners is that internal model mechanisms, specifically attention head influence, can serve as reliable, scalable, and efficient indicators for high-quality data selection in LLM pretraining, offering a 'weak-to-strong' scaling property. |
| Natural Language Processing | WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional
  Websites from Scratch (Read more on [arXiv](https://arxiv.org/abs/2505.03733) or [HuggingFace](https://huggingface.co/papers/2505.03733))| shiwk20, hht1113, Houxing, Yqy6, luzimu | This paper introduces WebGen-Bench, a novel benchmark designed to evaluate the ability of Large Language Model (LLM)-based agents to generate interactive and functional multi-file websites entirely from scratch. The main objective is to systematically assess an agent's capacity for high-level planning, organizing complex codebases, and implementing nuanced user requirements specified in natural language instructions. The methodology involves a curated set of diverse website generation instructions (created by human annotators and GPT-4o), 647 manually verified test cases targeting specific functionalities, and an automated evaluation pipeline using a web-navigation agent (WebVoyager) to execute tests. Key results show that the best-performing existing agent framework, Bolt.diy powered by DeepSeek-R1, achieved only 27.8% accuracy, while a fine-tuned Qwen2.5-Coder-32B-Instruct model (WebGen-LM-32B) on their custom WebGen-Instruct dataset reached 38.2% accuracy. This work highlights the current challenges in complex website generation from scratch and provides a robust benchmark and training data for advancing LLM-based code generation agents. |
| Machine Learning | Learning Dynamics in Continual Pre-Training for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.07796) or [HuggingFace](https://huggingface.co/papers/2505.07796))| Daniel Dajun Zeng, Lu Wang, Xingjin Wang, linjinglian, Howe77 | This paper investigates the learning dynamics of Large Language Models (LLMs) during Continual Pre-Training (CPT), proposing a novel scaling law to characterize and predict performance evolution. The primary research objective is to quantitatively describe and trace how general and downstream domain performance, measured by validation loss, changes throughout the CPT process and to identify the key influencing factors. The authors decouple the effects of distribution shift and learning rate annealing on the CPT loss curve, deriving a CPT scaling law that integrates these two components to model the loss transition. Extensive experiments demonstrate the scaling law accurately predicts validation losses (e.g., achieving an RÂ² of 0.9993 for D_cpt loss curves with the original formulation) across various CPT datasets, LLM sizes (106M-1.7B), and hyper-parameters. The proposed CPT scaling law offers AI practitioners a tool to predict CPT performance, optimize training hyper-parameters for specific goals like balancing general and domain-specific capabilities, and provides deeper insights into LLM learning dynamics. |
| Multi-Modal | Skywork-VL Reward: An Effective Reward Model for Multimodal
  Understanding and Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.07263) or [HuggingFace](https://huggingface.co/papers/2505.07263))| Yi Peng, Wei Shen, Jiangbo Pei, OrlandoHugBot, shawn0wang | The paper introduces Skywork-VL Reward, a novel multimodal reward model designed to provide effective reward signals for both multimodal understanding and reasoning tasks. The primary objective is to develop a comprehensive and robust reward model capable of evaluating outputs from standard Vision-Language Models (VLMs) and advanced VLM reasoners across diverse domains and tasks, addressing limitations in existing multimodal RMs. Key methodology includes constructing a large-scale multimodal preference dataset covering a wide range of tasks and fine-tuning a Qwen2.5-VL-7B-Instruct based architecture with an integrated reward head using a multi-stage process on pairwise preference data. Skywork-VL Reward achieves state-of-the-art results on the VL-RewardBench with an overall accuracy of 73.1%, and preference data generated by it significantly improves VLM reasoning capabilities when used in Mixed Preference Optimization (MPO), boosting MathVista scores from 69.2% to 73.5%. This work provides AI practitioners with a general-purpose, reliable reward model for multimodal alignment and demonstrates its practical value in enhancing VLM reasoning. |
| Reinforcement Learning | Reinforced Internal-External Knowledge Synergistic Reasoning for
  Efficient Adaptive Search Agent (Read more on [arXiv](https://arxiv.org/abs/2505.07596) or [HuggingFace](https://huggingface.co/papers/2505.07596))| Kang Liu, Jun Zhao, Yiming Ju, Xiaowei Yuan, hzy | This paper introduces IKEA, a reinforcement learning (RL) agent designed for efficient adaptive search by synergistically integrating its internal parametric knowledge with externally retrieved knowledge. The primary objective is to develop an agent that can discern its own knowledge boundaries to decide when to use internal knowledge versus performing external searches, thereby reducing redundant retrievals and inference latency. IKEA's methodology employs an agent framework prompting for knowledge boundary assessment, a novel knowledge-boundary aware reward function, and a specially constructed knowledge-boundary aware training dataset, all optimized using RL. Evaluations demonstrate that IKEA significantly outperforms baselines; for instance, IKEA-7B achieved an average Exact Match (EM) of 50.05 with an average of 0.91 retrievals, compared to Search-R1-7B's 45.00 EM and 1.85 retrievals. The main implication for AI practitioners is that training LLM agents with RL to understand their knowledge limits and adaptively utilize internal versus external information can lead to more efficient and accurate knowledge-intensive reasoning systems. |
| Reinforcement Learning | H^{3}DP: Triply-Hierarchical Diffusion Policy for Visuomotor
  Learning (Read more on [arXiv](https://arxiv.org/abs/2505.07819) or [HuggingFace](https://huggingface.co/papers/2505.07819))| Pu Hua, Zhecheng Yuan, Yufeng Tian, binaryXwizard, Lyy0725 | This paper introduces HÂ³DP, a Triply-Hierarchical Diffusion Policy, designed to enhance visuomotor learning for robotic manipulation by systematically integrating hierarchical structures across the input, representation, and action generation stages. The primary objective is to improve the coupling between visual perception and action prediction, which is often overlooked in existing methods, by emulating hierarchical processing mechanisms observed in human decision-making. HÂ³DP's methodology incorporates: (1) depth-aware input layering of RGB-D observations, (2) multi-scale visual representations encoding features at varying granularities, and (3) a hierarchically conditioned diffusion process that aligns coarse-to-fine action generation with corresponding visual features. Extensive experiments show HÂ³DP achieved a +27.5% average relative improvement over baselines across 44 simulation tasks and a +32.3% performance improvement over Diffusion Policy in challenging real-world bimanual manipulation tasks. The main implication for AI practitioners is that explicitly embedding hierarchical structures throughout the visuomotor learning pipeline can significantly improve policy performance, generalization, and the ability to handle complex, long-horizon manipulation tasks. |
| Computer Vision | Continuous Visual Autoregressive Generation via Score Maximization (Read more on [arXiv](https://arxiv.org/abs/2505.07812) or [HuggingFace](https://huggingface.co/papers/2505.07812))| Jie Zhou, Fandong Meng, cccczshao | This paper introduces a novel Continuous Visual Autoregressive (VAR) framework that enables direct visual generation without vector quantization by leveraging strictly proper scoring rules. The primary objective is to overcome information loss from quantization in traditional VAR models and enable probabilistic predictions in continuous spaces for visual data. The key methodology involves optimizing a training objective based on the energy score, leading to an Energy-based AutoRegression (EAR) approach using an energy Transformer with an MLP generator. On the ImageNet 256Ã256 benchmark, the EAR-H model (937M parameters) achieves a competitive FrÃ©chet Inception Distance (FID) of 1.97 with guidance, and EAR models demonstrate significantly higher inference efficiency compared to prior continuous autoregressive methods. This work provides AI practitioners with a theoretically grounded approach for continuous autoregressive generation, offering potential for improved quality and efficiency for visual data by avoiding explicit likelihood estimation or restrictive distributional assumptions. |
| Natural Language Processing | Overflow Prevention Enhances Long-Context Recurrent LLMs (Read more on [arXiv](https://arxiv.org/abs/2505.07793) or [HuggingFace](https://huggingface.co/papers/2505.07793))| rgiryes, leokarlin, OmegaLittleBob, ItamarZ, assafbk | This paper introduces OPRM (Overflow Prevention for Recurrent Models), a training-free, chunk-based inference strategy designed to mitigate memory overflows in long-context recurrent LLMs, significantly enhancing their performance. The research investigates how the fixed-size recurrent memory of these models limits their effectiveness on long-context tasks due to information overflow and aims to provide a solution. OPRM operates by segmenting the input context into chunks, processing each chunk speculatively in parallel during a prefill stage, and then selectively decoding from the most relevant chunk, often chosen based on minimizing the output distribution's entropy and employing an IDK (I Don't Know) filter. Key results demonstrate substantial improvements on the LongBench benchmark, with OPRM increasing the performance of Falcon3-Mamba-Inst-7B by 14% and RecurrentGemma-IT-9B by 50%, and achieving state-of-the-art results like a 30.8 score on LongBench v2 with Falcon3-Mamba-Inst-7B. For AI practitioners, OPRM provides a practical, inference-time method to improve the long-context processing capabilities of existing recurrent LLMs without retraining, while also prompting further inquiry into whether these models genuinely leverage long-range dependencies effectively when a single-chunk strategy proves superior. |
| Machine Learning | UMoE: Unifying Attention and FFN with Shared Experts (Read more on [arXiv](https://arxiv.org/abs/2505.07260) or [HuggingFace](https://huggingface.co/papers/2505.07260))| Jing Li, Chaozheng Wang, ysngkil | The paper introduces UMoE, a novel architecture that unifies Mixture of Experts (MoE) designs across both attention and feed-forward network (FFN) layers in Transformer models. Its primary objective is to reformulate the attention mechanism to expose an FFN-like structure, thereby enabling a common expert design and parameter sharing between attention and FFN components. UMoE's methodology involves a 'pre-mixing' attention formulation which separates token mixing from token-wise expert processing, allowing FFN-like experts to be utilized in both layer types, often with shared parameters and top-k routing. Experimental results demonstrate UMoE's effectiveness, with a 540M parameter UMoE model achieving a perplexity of 20.44 on the FineWeb dataset, outperforming comparable FFN-MoE (21.19 PPL) and previous attention-MoE approaches. This work provides AI practitioners with a more performant and parameter-efficient strategy for scaling large models by consolidating MoE application with shared experts across different Transformer layers. |
| Natural Language Processing | Document Attribution: Examining Citation Relationships using Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.06324) or [HuggingFace](https://huggingface.co/papers/2505.06324))| Nedim Lipka, Vipula Rawte, Franck-Dernoncourt, ryanrossi | This paper investigates document attribution in Large Language Models (LLMs), focusing on tracing generated outputs to their source documents to ensure trustworthiness. The primary objective is to assess the reliability of LLM-generated citations and propose techniques to improve attribution accuracy. The authors introduce two methods: a zero-shot approach framing attribution as a textual entailment task, and an attention-based binary classification technique. Their zero-shot method using `flan-ul2` demonstrated an improvement of 0.27% on ID and 2.4% on OOD sets of AttributionBench over the best baseline, achieving an ID-average F1 score of 73.8 and an OOD-average F1 score of 83.43. The main implication for AI practitioners is that simple zero-shot textual entailment can effectively improve LLM attribution without fine-tuning, and attention mechanisms show potential for further enhancements. |
| Machine Learning | Physics-Assisted and Topology-Informed Deep Learning for Weather
  Prediction (Read more on [arXiv](https://arxiv.org/abs/2505.04918) or [HuggingFace](https://huggingface.co/papers/2505.04918))| Yerong Feng, Qing Ling, Yumenomae | This paper introduces PASSAT, a novel Physics-ASSisted And Topology-informed deep learning model designed for improved weather prediction. The main objective is to overcome limitations of existing deep learning models that ignore underlying physics or Earth's topology, and numerical weather prediction (NWP) models that struggle with Earth-atmosphere interactions. PASSAT achieves this by attributing weather evolution to the advection process (numerically solving advection and Navier-Stokes equations on a spherical manifold) and Earth-atmosphere interaction (estimated by a spherical graph neural network, which also generates initial velocity fields). On the 5.625Â°-resolution ERA5 dataset, PASSAT outperforms state-of-the-art deep learning models and the operational NWP model IFS T42; for example, for the t2m variable at a 120-hour lead time, PASSAT achieves an RMSE of 2.44, comparable or better than other deep learning baselines and IFS T42 (RMSE 3.69). The key implication for AI practitioners is that incorporating domain-specific physical laws and relevant topological information can significantly enhance the accuracy and robustness of deep learning models for complex spatio-temporal forecasting tasks. |
| Machine Learning | Multi-Objective-Guided Discrete Flow Matching for Controllable
  Biological Sequence Design (Read more on [arXiv](https://arxiv.org/abs/2505.07086) or [HuggingFace](https://huggingface.co/papers/2505.07086))| Tong Chen, pranamanam, sophtang, yinuozhang | This paper introduces Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a framework for designing biological sequences that satisfy multiple, often conflicting, criteria by steering pretrained discrete flow matching models. The main research objective is to enable controllable generation of Pareto-efficient biological sequences by integrating multi-objective optimization directly into the discrete flow matching sampling process. MOG-DFM's key methodology involves computing a hybrid rank-directional score for candidate transitions and applying an adaptive hypercone filter at each sampling step, using base unconditional models like PepDFM for peptides and EnhancerDFM for DNA. The framework demonstrated strong performance; for instance, when designing peptide binders for target 1B8Q, MOG-DFM achieved a hemolysis score of 0.0785 and a non-fouling score of 0.8445, outperforming classical methods, while EnhancerDFM achieved a FrÃ©chet Biological Distance (FBD) of 5.9. This work provides AI practitioners with a powerful and generalizable tool for multi-property-guided biomolecule sequence design, enabling the balancing of diverse and conflicting objectives. |
