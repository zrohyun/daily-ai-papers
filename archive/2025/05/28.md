

## Papers for 2025-05-28

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | OmniConsistency: Learning Style-Agnostic Consistency from Paired
  Stylization Data (Read more on [arXiv](https://arxiv.org/abs/2505.18445) or [HuggingFace](https://huggingface.co/papers/2505.18445))| Cheng Liu, mikeshou, yiren98 | This paper introduces OmniConsistency, a universal consistency plugin for diffusion models to improve style-agnostic consistency in image stylization. The research aims to address challenges in maintaining consistent stylization and preventing style degradation in complex scenes and I2I pipelines. OmniConsistency employs a two-stage progressive learning strategy decoupling style and consistency learning, trained on paired stylized images using Diffusion Transformers (DiTs). The method achieves comparable performance to GPT-40, enhancing visual coherence and aesthetic quality, with FID scores reported around 39.2. OmniConsistency offers AI practitioners a plug-and-play solution for enhancing I2I stylization tasks, improving both fidelity and coherence. |
| Multi-Modal | MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs (Read more on [arXiv](https://arxiv.org/abs/2505.21327) or [HuggingFace](https://huggingface.co/papers/2505.21327))| Renrui Zhang, Yiting Lu, Yilei Jiang, Tianshuo Peng, Jiakang Yuan | The paper introduces MME-Reasoning, a new benchmark for evaluating logical reasoning in multimodal large language models (MLLMs). The research aims to comprehensively assess inductive, deductive, and abductive reasoning abilities, addressing limitations in existing benchmarks. MME-Reasoning comprises 1,188 curated questions and employs diverse evaluation methods to reveal performance imbalances across reasoning types in MLLMs. Evaluations show that even advanced MLLMs achieve limited performance, with Gemini-Pro-2.5-Thinking scoring 60.19%. The study implies that current MLLMs need significant improvement in comprehensive logical reasoning, particularly in abductive tasks, and highlights the need for approaches such as 'thinking mode' and Rule-based RL. |
| Computer Vision | Paper2Poster: Towards Multimodal Poster Automation from Scientific
  Papers (Read more on [arXiv](https://arxiv.org/abs/2505.21497) or [HuggingFace](https://huggingface.co/papers/2505.21497))| Philip Torr, Xi He, HideOnBush, KevinQHLin, weipang142857 | This paper introduces Paper2Poster, a benchmark and multi-agent pipeline for automating the creation of academic posters from scientific papers. The research aims to address the challenge of condensing long-context, interleaved documents into a single, visually coherent poster. PosterAgent utilizes a parser, planner, and painter-commenter loop to distill paper content, create a binary-tree layout, and refine panels with VLM feedback. Experiments show PosterAgent (Qwen-2.5) outperforms GPT-40 driven multi-agent systems across metrics while using 87% fewer tokens, generating posters for $0.005. The work provides AI practitioners with an efficient approach for poster generation, emphasizing visual semantics and reader engagement. |
| Multi-Modal | VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied
  Iterative Policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2505.19000) or [HuggingFace](https://huggingface.co/papers/2505.19000))| Longyue Wang, Zhenyu Liu, Zitao Li, Xinyu Chen, Yunxin Li | The paper introduces VerIPO, a Verifier-guided Iterative Policy Optimization method for improving long reasoning in Video-LLMs. It addresses the data preparation bottlenecks and unstable improvements observed in Reinforcement Fine-Tuning (RFT) methods by using a Rollout-Aware Verifier to construct high-quality contrastive data. VerIPO achieves significant improvements in reasoning chain quality, with experiments showing a 7x faster optimization speed compared to Group Relative Policy Optimization (GRPO). The trained models generate longer and more contextually consistent CoTs, exceeding direct inference of large-scale instruction-tuned Video-LLMs. This approach offers AI practitioners a more efficient and stable method for cultivating deep reasoning capabilities in Video-LLMs. |
| Natural Language Processing | Exploring the Latent Capacity of LLMs for One-Step Text Generation (Read more on [arXiv](https://arxiv.org/abs/2505.21189) or [HuggingFace](https://huggingface.co/papers/2505.21189))| Ivan Oseledets, glebzok | This paper explores non-autoregressive text generation with Large Language Models (LLMs). It investigates the possibility of LLMs generating accurate multi-token sequences in a single forward pass using only a few learned embeddings, without iterative decoding. The methodology involves training two "proto-tokens" to optimize the LLM's output for a target token sequence in a single forward pass. Results demonstrate perfect reconstruction of up to several hundred tokens with this method, achieving significantly higher decoding throughput, outperforming autoregressive generation by a factor of 279 on average. The study implies potential for fast context compression and decompression in LLMs, enabling efficient on-device inference. |
| Reinforcement Learning | SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning
  Logical Reasoning and Beyond (Read more on [arXiv](https://arxiv.org/abs/2505.19641) or [HuggingFace](https://huggingface.co/papers/2505.19641))| Yongyi Hu, Han Ding, Zhuo Jiang, Yuanxiang Fan, Junteng Liu | The paper introduces SYNLOGIC, a data synthesis framework for generating verifiable logical reasoning data at scale to enhance LLMs' reasoning abilities via reinforcement learning.  The research aims to address the lack of diverse and verifiable reasoning data suitable for RL training by focusing on logical reasoning as a foundational component. SYNLOGIC synthesizes 35 diverse logical reasoning tasks with adjustable difficulty and automated verification, facilitating RL with verifiable rewards. Experiments show SYNLOGIC-trained models achieve state-of-the-art logical reasoning performance, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH, and demonstrates generalization to mathematical and coding domains, improving training efficiency. SYNLOGIC offers AI practitioners a valuable resource for advancing LLMs' broader reasoning capabilities through scalable and verifiable training data. |
| Natural Language Processing | Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.17813) or [HuggingFace](https://huggingface.co/papers/2505.17813))| Roy Schwartz, Yossi Adi, Gabriel Synnaeve, Michael Hassid | This paper challenges the assumption that longer thinking chains improve reasoning in Large Language Models (LLMs). It investigates whether shorter reasoning chains within individual questions are more likely to yield correct answers. The study proposes a novel inference method called short-m@k, which halts computation once the first m thinking processes are completed, selecting the final answer via majority voting. Results demonstrate that shorter chains can be significantly more accurate, up to 34.5% compared to the longest chains, and can reduce computational costs by up to 40% in some settings. The findings suggest that prioritizing shorter reasoning trajectories can enhance performance and efficiency in LLMs, potentially reducing reliance on scaling test-time compute. |
| Multi-Modal | UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based
  Mobile GUI Agents (Read more on [arXiv](https://arxiv.org/abs/2505.21496) or [HuggingFace](https://huggingface.co/papers/2505.21496))| Zimu Lu, Yuxiang Chai, Guozhi Wang, AJZhou, HanXiao1999 | The paper introduces UI-Genie, a self-improving framework to enhance MLLM-based GUI agents. It addresses challenges like trajectory outcome verification and limited high-quality training data through a reward model and a self-improving pipeline. UI-Genie-RM, features an image-text interleaved architecture unifying action-level and task-level rewards. The method progressively expands solvable complex GUI tasks by enhancing agent and reward models via reward-guided exploration and outcome verification. Experimental results demonstrate state-of-the-art performance across multiple GUI agent benchmarks, achieving 77% success rate on high-level tasks in AndroidControl benchmark; the framework facilitates synthetic data generation and model improvement without manual annotation, thereby boosting GUI agent capabilities. |
| Computer Vision | Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via
  Semantic-Aware Permutation (Read more on [arXiv](https://arxiv.org/abs/2505.18875) or [HuggingFace](https://huggingface.co/papers/2505.18875))| han-cai, jt-zhang, ylzhao, xihc-ucb, andy-yang | The paper introduces Sparse VideoGen2 (SVG2), a training-free framework to accelerate video generation by reducing the computational cost of attention in Diffusion Transformers (DiTs). It aims to address the limitations of existing sparse attention methods by improving critical token identification and minimizing computation waste. The core of SVG2 is semantic-aware permutation, which clusters and reorders tokens based on semantic similarity using k-means clustering. SVG2 achieves up to 2.30x and 1.89x end-to-end speedup on Hunyuan-Video and Wan 2.1, respectively, with a PSNR up to 30 and 26, demonstrating a superior trade-off between generation quality and efficiency. This provides AI practitioners with a more efficient approach for video generation with DiTs. |
| Multi-Modal | MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks (Read more on [arXiv](https://arxiv.org/abs/2505.16459) or [HuggingFace](https://huggingface.co/papers/2505.16459))| Chaoran Hu, Ruihang Zhang, Tianhe Gu, Guiyao Tie, zhouxueyang | The paper introduces MMMR, a new benchmark for evaluating multi-modal reasoning in large language models (MLLMs). It aims to address the lack of standardized benchmarks by providing a high-difficulty dataset and a Reasoning Trace Evaluation Pipeline (RTEP). The methodology involves assessing reasoning quality using metrics like relevance and consistency in addition to accuracy. Empirical results demonstrate that even top MLLMs suffer from reasoning pathologies, with Gemini-2.5 Pro achieving 42.45% accuracy while human experts reach 52.85%. The MMMR benchmark provides actionable insights for improving the next generation of multi-modal reasoning systems. |
| Computer Vision | MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in
  Video Scenarios (Read more on [arXiv](https://arxiv.org/abs/2505.21333) or [HuggingFace](https://huggingface.co/papers/2505.21333))| Lijie Zhao, Huanyao Zhang, Wulin Xie, Huanqian Wang, Yang Shi | The paper introduces MME-VideoOCR, a new benchmark for evaluating the capabilities of Multimodal Large Language Models (MLLMs) in video Optical Character Recognition (OCR). The research aims to address the diminished efficacy of MLLMs in video OCR due to factors like motion blur and temporal variations. The methodology involves curating a dataset of 1,464 videos and 2,000 manually annotated question-answer pairs across 10 task categories and 25 individual tasks. Evaluation of 18 state-of-the-art MLLMs showed that the best model (Gemini-2.5 Pro) achieved only 73.7% accuracy, highlighting limitations in tasks requiring spatio-temporal reasoning. The main implication is the need for improved handling of temporal context and high-resolution inputs for reliable OCR in dynamic video scenarios, which can guide further MLLM optimization. |
| Computer Vision | OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for
  Subject-to-Video Generation (Read more on [arXiv](https://arxiv.org/abs/2505.20292) or [HuggingFace](https://huggingface.co/papers/2505.20292))| chongyangma, Jinfa, dyf, pkuhexianyi, BestWishYsh | This paper introduces OpenS2V-Nexus, a new benchmark and dataset for subject-to-video (S2V) generation, aimed at improving the consistency and naturalness of generated videos. The research focuses on developing a fine-grained benchmark (OpenS2V-Eval) with 180 diverse prompts and a large-scale dataset (OpenS2V-5M) of 5 million subject-text-video triples. The methodology involves creating new automatic metrics (NexusScore, NaturalScore, GmeScore) to quantify subject consistency, naturalness, and text relevance and generating multi-view representations. Evaluation shows that the proposed benchmark helps in identifying strengths and weaknesses of different S2V models, with models like Pika achieving high GmeScore. The creation of OpenS2V-Nexus provides AI practitioners with a robust infrastructure for developing and evaluating S2V generation models. |
| Natural Language Processing | GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient
  Fine-Tuning (Read more on [arXiv](https://arxiv.org/abs/2505.20355) or [HuggingFace](https://huggingface.co/papers/2505.20355))| Eunhyeok Park, Taesu Kim, Hyungjun Kim, Daehyun Ahn, yeonjoon-jung | The paper introduces Granular Low-Rank Adaptation (GraLoRA), a novel parameter-efficient fine-tuning (PEFT) method addressing the limitations of Low-Rank Adaptation (LoRA). It investigates the structural bottleneck in LoRA that causes gradient entanglement and limits performance. GraLoRA partitions weight matrices into sub-blocks with independent low-rank adapters to mitigate this issue. Experiments on code generation (HumanEval+) and commonsense reasoning show that GraLoRA outperforms LoRA, achieving up to +8.5% absolute gain in Pass@1. This offers AI practitioners a scalable and robust solution for PEFT by more closely approximating full fine-tuning behavior. |
| Multi-Modal | Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning? (Read more on [arXiv](https://arxiv.org/abs/2505.21374) or [HuggingFace](https://huggingface.co/papers/2505.21374))| Jing Liao, Yixiao Ge, Teng Wang, Yuying Ge, Howe666 | The paper introduces Video-Holmes, a new benchmark for evaluating complex video reasoning in MLLMs. It aims to assess if MLLMs can perform video reasoning akin to human experts by actively locating and connecting relevant visual clues. The benchmark comprises 1,837 questions derived from manually annotated suspense short films, and SOTA MLLMs were evaluated. The best-performing model, Gemini-2.5-Pro, achieves an accuracy of only 45%, indicating that models encounter difficulties with information integration and critical clue detection. Video-Holmes serves as a challenging "Holmes-test," motivating future research on more human-like multimodal reasoning. |
| Natural Language Processing | rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale
  Verified Dataset (Read more on [arXiv](https://arxiv.org/abs/2505.21297) or [HuggingFace](https://huggingface.co/papers/2505.21297))| Xudong Zhou, Bingcheng Dong, Yi Zhu, Li Lyna Zhang, YF-L | The paper introduces rStar-Coder, a large-scale, verified dataset for improving code reasoning in LLMs. The research aims to enhance LLM code reasoning by constructing a dataset of 418K competition-level code problems with 580K long-reasoning solutions and test cases.  The methodology involves curating competitive programming problems, developing a reliable input-output test case synthesis pipeline, and augmenting problems with high-quality, verified solutions.  Experiments show that rStar-Coder improves Qwen2.5-7B on LiveCodeBench from 17.4% to 57.3% pass@1 accuracy and achieves 16.15% pass@1 on USACO 2025, outperforming QWQ-32B.  This dataset enables smaller models to achieve competitive performance, reducing the resource requirements for advanced code reasoning. |
| Natural Language Processing | MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent
  Systems (Read more on [arXiv](https://arxiv.org/abs/2505.18943) or [HuggingFace](https://huggingface.co/papers/2505.18943))| Yixuan Li, Yuxuan Chen, samuelyeh, XUANMINGZHANG | The paper introduces MetaMind, a metacognitively inspired multi-agent framework for enhancing human-like social reasoning in language models. MetaMind addresses the ambiguity and contextual nuance in human communication by decomposing social understanding into stages involving mental state hypothesis generation, cultural/ethical constraint refinement, and response generation. The framework achieves state-of-the-art performance across three benchmarks, including a 35.7% improvement in real-world social scenarios.  Notably, MetaMind enables LLMs to match human-level performance on key Theory of Mind tasks for the first time. This work advances AI systems toward more empathetic and culturally sensitive interactions. |
| Computer Vision | HoliTom: Holistic Token Merging for Fast Video Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.21334) or [HuggingFace](https://huggingface.co/papers/2505.21334))| Haoxuan You, Can Qin, Keda Tao, Huan-WhoRegisteredMyName, keleshao | The paper introduces HoliTom, a training-free holistic token merging method for fast video large language models (LLMs) addressing computational inefficiency. The research aims to reduce video token redundancy in LLMs while preserving critical semantic information. HoliTom employs outer-LLM pruning through global redundancy-aware temporal segmentation and spatial-temporal merging, complemented by a robust inner-LLM token similarity-based merging approach. Evaluations show that HoliTom reduces computational costs to 6.9% of FLOPs while maintaining 99.1% of the original performance on LLaVA-OneVision-7B. This efficient video LLM inference significantly reduces computational overhead and accelerates inference, enabling practical deployment for complex, long-form video understanding. |
| Computer Vision | ImgEdit: A Unified Image Editing Dataset and Benchmark (Read more on [arXiv](https://arxiv.org/abs/2505.20275) or [HuggingFace](https://huggingface.co/papers/2505.20275))| Zongjian Li, Xianyi He, Yang Ye, zhiyuanyan1, BestWishYsh | The paper introduces ImgEdit, a large-scale image editing dataset and benchmark to address the limitations of existing resources. The primary objective is to create a high-quality dataset for training and evaluating image editing models. They employ a multi-stage pipeline integrating vision-language, detection, and segmentation models to curate 1.2 million edit pairs and a new benchmark, ImgEdit-Bench. The trained ImgEdit-E1 model outperforms existing open-source models on various tasks, achieving a GPT-40 score of 4.71 on a subset of the dataset. This work provides AI practitioners with a robust resource for developing and assessing image editing models, potentially bridging the performance gap with proprietary solutions. |
| Natural Language Processing | How does Alignment Enhance LLMs' Multilingual Capabilities? A Language
  Neurons Perspective (Read more on [arXiv](https://arxiv.org/abs/2505.21505) or [HuggingFace](https://huggingface.co/papers/2505.21505))| Xiao Liu, Shuaijie She, Xiang Liu, DreamW1ngs, Shimao-Zhang | This paper analyzes how multilingual alignment enhances Large Language Models (LLMs) using a language neurons perspective. It investigates the function of language-specific, language-related, and language-agnostic neurons during multilingual inference, dividing the process into multilingual understanding, shared semantic space reasoning, multilingual output space transformation, and vocabulary space outputting. The methodology involves a new neuron identification algorithm based on entropy and probability, identifying distinct neuron activation patterns before and after alignment via MAPO. Results show that multilingual alignment enhances the activation of appropriate neurons across functional parts of LLMs, indicated by improvements in tasks like mathematical reasoning (e.g., improved accuracy on MGSM from 57.8% to 63.6% with zh/de alignment). The study implies that targeted alignment strategies, focusing on the activation of specific neuron types, can effectively improve the multilingual capabilities of LLMs. |
| Natural Language Processing | Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with
  Minimalist Rule-Based RL (Read more on [arXiv](https://arxiv.org/abs/2505.17952) or [HuggingFace](https://huggingface.co/papers/2505.17952))| Yong Dai, Zhongwei Wan, Jiazhen Pan, Haozhe Wang, Che Liu | The paper introduces AlphaMed, a medical LLM that achieves strong reasoning without supervised fine-tuning on chain-of-thought data. The main objective is to demonstrate reasoning capability through reinforcement learning with minimalist rule-based rewards on multiple-choice QA datasets. AlphaMed is trained via reinforcement learning with rewards based on answer accuracy, using GRPO and rule-based reward modeling. The model achieves state-of-the-art results on six medical QA benchmarks, surpassing models with SFT+RL pipelines and larger models such as DeepSeek-V3-671B, achieving up to 87.52% accuracy on MedQA. This demonstrates the potential of minimalist RL with high-quality datasets for developing reasoning capabilities in medical LLMs, reducing reliance on costly labeled data and external verifiers. |
| Computer Vision | Active-O3: Empowering Multimodal Large Language Models with Active
  Perception via GRPO (Read more on [arXiv](https://arxiv.org/abs/2505.21457) or [HuggingFace](https://huggingface.co/papers/2505.21457))| Zongze Du, Hao Zhong, MingyuLiu, Canyu, Z-MU-Z | The paper introduces ACTIVE-03, a reinforcement learning framework to equip multimodal large language models (MLLMs) with active perception capabilities. It investigates how to enable MLLMs to intelligently select and acquire task-relevant sensory information. ACTIVE-03 utilizes GRPO to train a two-stage policy for region proposal and task execution, incorporating both task-aware and heuristic feedback. Experiments demonstrate significant improvements in active perception compared to Qwen-VL2.5-CoT, such as increased accuracy in zero-shot V* reasoning. ACTIVE-03 provides a framework and benchmark for developing active perception skills in MLLMs for tasks requiring spatial understanding and goal-directed attention, such as object detection and interactive segmentation. |
| Computer Vision | Frame In-N-Out: Unbounded Controllable Image-to-Video Generation (Read more on [arXiv](https://arxiv.org/abs/2505.21491) or [HuggingFace](https://huggingface.co/papers/2505.21491))| Zezhou Cheng, Matheus Gadelha, Xuweiyi Chen, HikariDawn | The paper introduces Frame In-N-Out, a new task in image-to-video generation focused on unbounded controllable content creation, specifically frame entrance and exit scenarios. It aims to enable users to control objects entering or leaving the scene with specified trajectories while preserving identity. The approach includes a semi-automatically curated dataset and an identity-preserving motion-controllable video Diffusion Transformer architecture. Evaluation shows significant outperformance of existing baselines, with a Trajectory Error reduction of over 50% compared to DragAnything. This provides AI practitioners with a framework for generating videos with precise control over object movement beyond initial frame boundaries. |
| Natural Language Processing | Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering
  Target Atoms (Read more on [arXiv](https://arxiv.org/abs/2505.20322) or [HuggingFace](https://huggingface.co/papers/2505.20322))| Shumin Deng, Shengyu Mao, Ziwen Xu, Mengru Wang, Ningyu | The paper introduces Steering Target Atoms (STA), a novel method for robust behavior control in Large Language Models (LLMs). STA aims to improve safety and reliability by isolating and manipulating disentangled knowledge components in LLMs. The method utilizes sparse autoencoders to identify target atoms, enabling fine-grained interventions and achieving an average detoxification performance increase to 97.56% in Gemma-2-9B-it. STA's superior robustness and flexibility in adversarial settings compared to prompt engineering, offers AI practitioners a way to achieve finer-grained and more reliable control over LLM behaviors while minimizing unintended side effects. |
| Computer Vision | NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in
  Brain MRI (Read more on [arXiv](https://arxiv.org/abs/2505.14064) or [HuggingFace](https://huggingface.co/papers/2505.14064))| Lena Schmitzer, Evamaria O. Riedel, Philipp Raffler, Jun Li, ci-ber | The paper introduces NOVA, a novel benchmark for evaluating anomaly localization and clinical reasoning in brain MRI. The research aims to address the challenge of out-of-distribution generalization in medical imaging by creating a real-world benchmark with diverse pathologies. NOVA consists of 906 brain MRI scans with 281 rare pathologies, expert annotations, and clinical narratives, enabling tasks like anomaly localization, image captioning, and diagnostic reasoning. Baseline results with GPT-40, Gemini 2.0 Flash, and Qwen2.5-VL-72B revealed substantial performance drops (e.g., anomaly localization mAP30 ranging from 20.16 to 37.66), highlighting the limitations of current models. NOVA provides a rigorous testbed for advancing models capable of detecting, localizing, and reasoning about truly unknown anomalies in clinical imaging. |
| Computer Vision | ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in
  Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.21500) or [HuggingFace](https://huggingface.co/papers/2505.21500))| Hang Zhang, Yuchen Yan, Zixuan Wang, Hongxing Li, Dingming Li | The paper introduces ViewSpatial-Bench, a benchmark for evaluating multi-perspective spatial localization in vision-language models (VLMs). It investigates the limitation of VLMs in generalizing from egocentric to allocentric viewpoints when performing spatial reasoning tasks. The methodology involves creating a comprehensive dataset with precise directional labels using an automated 3D annotation pipeline and then fine-tuning VLMs using multi-perspective spatial data. Evaluation shows a 46.24% performance improvement across tasks after fine-tuning, indicating enhanced spatial comprehension capabilities. This suggests that explicitly modeling 3D spatial relationships can significantly improve VLMs' spatial intelligence for embodied AI systems. |
| Natural Language Processing | Code Graph Model (CGM): A Graph-Integrated Large Language Model for
  Repository-Level Software Engineering Tasks (Read more on [arXiv](https://arxiv.org/abs/2505.16901) or [HuggingFace](https://huggingface.co/papers/2505.16901))| Hongen Peng, Zhenhao Tang, Ying Zhang, Hongyuan Tao, Geralt-Targaryen | This paper introduces Code Graph Models (CGMs), a graph-integrated large language model for repository-level software engineering tasks. It addresses the challenge of enabling open-source LLMs to effectively handle complex software engineering tasks by incorporating both semantic information and structural dependencies of code repositories. The methodology involves constructing code graphs, integrating them into the LLM's attention mechanism via a specialized adapter, and using an agentless graph RAG framework. On the SWE-bench Lite benchmark, the approach achieves a 43.00% resolution rate using the open-source Qwen2.5-72B model, which surpasses previous open-source methods by 12.33%. The work suggests a new direction for developing powerful, transparent, and accessible tools for automated SE without relying on proprietary models or complex agent-based approaches. |
| Computer Vision | DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via
  Next-Detail Prediction (Read more on [arXiv](https://arxiv.org/abs/2505.21473) or [HuggingFace](https://huggingface.co/papers/2505.21473))| Xu Wang, Huichao Zhang, Yiheng Liu, JiangYi, leo1117 | The paper introduces DetailFlow, a novel coarse-to-fine 1D autoregressive image generation method. It aims to model images through a next-detail prediction strategy, generating images from global structure to fine details. DetailFlow employs a resolution-aware token sequence learned with progressively degraded images and a parallel inference mechanism with self-correction. On ImageNet 256x256, DetailFlow achieves 2.96 gFID with 128 tokens. DetailFlow offers AI practitioners a more efficient and scalable solution for high-resolution autoregressive image synthesis with reduced token count and accelerated inference. |
| Multi-Modal | SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.19099) or [HuggingFace](https://huggingface.co/papers/2505.19099))| Zirong Liu, Terry Jingchen Zhang, Kun Xiang, yinyahuang, HengLi29 | The paper introduces SEEPHYS, a large-scale multimodal benchmark for evaluating physics reasoning capabilities of LLMs. It investigates whether visual information enhances reasoning in physics questions ranging from middle school to PhD level. The benchmark includes 2,000 questions with 2,245 images across 7 physics domains and 21 diagram types, featuring a high proportion of vision-essential problems. Evaluation of leading MLLMs on SEEPHYS reveals sub-60% accuracy, indicating challenges in visual understanding and integration with physics reasoning. This suggests that current models struggle to emulate human-like scientific observation and deduction. |
| Computer Vision | Adversarial Attacks against Closed-Source MLLMs via Feature Optimal
  Alignment (Read more on [arXiv](https://arxiv.org/abs/2505.21494) or [HuggingFace](https://huggingface.co/papers/2505.21494))| Chao Du, Tianyu Pang, Simeng Qin, Sensen Gao, jiaxiaojunQAQ | This paper addresses the vulnerability of Multimodal Large Language Models (MLLMs) to transferable adversarial examples. It investigates how to improve the transferability of targeted attacks, particularly against closed-source models, by optimizing feature alignment. The proposed FOA-Attack aligns global and local features between adversarial and target samples, employing a cosine similarity-based global feature loss and a clustering optimal transport loss for fine-grained alignment. Experiments demonstrate that FOA-Attack outperforms existing methods, achieving significantly higher attack success rates (e.g., 79.6% on LLaVa-1.5-7B). The improved adversarial transferability allows for more effective attacks on commercial MLLMs, highlighting the need for more robust defenses. |
| Reinforcement Learning | Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2505.20561) or [HuggingFace](https://huggingface.co/papers/2505.20561))| Peter Grabowski, Tianqi Liu, Yinxiao Liu, Yaqing Wang, Shenao Zhang | The paper introduces Bayes-Adaptive RL for Large Language Model (LLM) reasoning to address limitations of Markovian RL. It investigates whether reflective reasoning emerges during Markovian RL training and presents a Bayes-Adaptive RL framework to optimize the expected return under a posterior distribution over Markov decision processes. The proposed algorithm, BARL, stitches and switches strategies based on observed outcomes, incentivizing both reward-maximizing exploitation and information-gathering exploration. Empirical results on mathematical reasoning tasks demonstrated that BARL outperforms standard Markovian RL approaches, achieving superior token efficiency with improved exploration effectiveness. BARL provides principled guidance on when and how LLMs should reflectively explore. |
| Computer Vision | Sci-Fi: Symmetric Constraint for Frame Inbetweening (Read more on [arXiv](https://arxiv.org/abs/2505.21205) or [HuggingFace](https://huggingface.co/papers/2505.21205))| Xianyi He, Xiaoyu Li, Xiaodong Cun, Liuhan Chen, BestWishYsh | The paper introduces Sci-Fi, a novel framework for frame inbetweening that enforces symmetric start-end-frame constraints. It addresses the limitation of current image-to-video diffusion models (I2V-DMs) where end-frame constraints are weaker due to insufficient training. Sci-Fi employs a lightweight module, EF-Net, to encode the end frame and inject temporally adaptive features into the I2V-DM. Experiments on DAVIS and Pexels datasets show improved performance, with Sci-Fi achieving a VBench score of 0.8240 on DAVIS. The framework enables more harmonious inbetweening, offering AI practitioners an efficient approach to generate intermediate frames with more consistent dynamics. |
| Reinforcement Learning | Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.21178) or [HuggingFace](https://huggingface.co/papers/2505.21178))| Mao Zheng, Nickyang | This paper introduces ConciseR, a two-stage reinforcement learning framework to achieve concise reasoning in large language models. It addresses the overthinking phenomenon by incentivizing reasoning capabilities in the first stage using Group Relative Policy Optimization (GRPO++) and enforces conciseness in the second stage via Length-aware GRPO (L-GRPO). ConciseR optimizes response length only when all rollouts are correct, adhering to the "walk before you run" principle. Experiments demonstrate that ConciseR outperforms state-of-the-art reasoning models without RL, improving AIME 2024 accuracy while generating more concise CoT responses, with a reduction of 21-23% in response length observed across benchmarks. The approach's efficiency makes it practically appealing for tasks requiring faster response times and reduced resource consumption. |
| Computer Vision | Minute-Long Videos with Dual Parallelisms (Read more on [arXiv](https://arxiv.org/abs/2505.21070) or [HuggingFace](https://huggingface.co/papers/2505.21070))| Xinchao Wang, Yuecong Xu, Xingyi Yang, Bowen Zheng, Zeqing Wang | The paper introduces DualParal, a distributed inference strategy to reduce the latency and memory costs associated with generating long videos using Diffusion Transformer (DiT) models. It parallelizes both temporal frames and model layers across GPUs using a block-wise denoising scheme. A feature cache reuses key-value features to minimize inter-GPU communication, and coordinated noise initialization maintains global consistency. Experimental results show DualParal achieves up to a 6.54x reduction in latency and a 1.48x reduction in memory cost on 8×RTX 4090 GPUs when generating 1,025-frame videos. This enables AI practitioners to generate high-quality, infinitely long videos more efficiently. |
| Computer Vision | VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual
  Tool Selection (Read more on [arXiv](https://arxiv.org/abs/2505.20289) or [HuggingFace](https://huggingface.co/papers/2505.20289))| Wen Xiao, Zefan Cai, Yuyang Ji, AniSundar18, ZeyiHuang1010 | The paper introduces VisualToolAgent (VisTA), a reinforcement learning framework for visual tool selection. VisTA addresses the challenge of dynamically selecting and combining diverse visual tools for complex visual reasoning tasks. The framework uses Group Relative Policy Optimization (GRPO) to train an agent that autonomously discovers effective tool-selection strategies using task outcomes as feedback, thereby avoiding explicit reasoning supervision. Experiments on ChartQA demonstrate that VisTA achieves up to 88.9% accuracy, significantly outperforming training-free baselines, especially on out-of-distribution examples. This framework enables a more flexible and experience-driven approach to visual reasoning, allowing AI practitioners to create systems that adaptively utilize diverse tools without extensive human supervision. |
| Machine Learning | Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic
  Capabilities in LLM Compression (Read more on [arXiv](https://arxiv.org/abs/2505.19433) or [HuggingFace](https://huggingface.co/papers/2505.19433))| Xiaowen Chu, Lujun Li, Xiang Liu, Zhenheng Tang, Peijie Dong | This paper presents ACBench, a new benchmark to evaluate the impact of post-training compression on LLMs' agentic capabilities beyond language modeling and NLU. The study investigates how quantization and pruning affect workflow generation, tool use, and long-context understanding across 15 LLMs. Experiments reveal a trade-off, where 4-bit quantization preserves workflow generation and tool use (1%-3% drop) but degrades real-world application accuracy by 10%-15%. The paper introduces metrics like ERank to analyze compression impacts and provides actionable insights for optimizing LLM compression in agentic scenarios, aiding AI practitioners in deploying efficient LLMs for complex tasks. |
| Reinforcement Learning | R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs
  via Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2505.17005) or [HuggingFace](https://huggingface.co/papers/2505.17005))| Zhipeng Chen, Wenqing Tian, Jinhao Jiang, Huatong Song, EliverQ | The paper introduces R1-Searcher++, a framework that incentivizes LLMs to dynamically acquire knowledge by leveraging both internal and external resources. It addresses the issue of LLMs over-relying on external knowledge by employing a two-stage training strategy involving SFT Cold-start and RL for dynamic knowledge acquisition. R1-Searcher++ uses outcome-supervision, reward mechanisms for internal knowledge utilization, and memorization techniques to balance internal reasoning and external retrieval. Experiments on the Qwen-2.5-7B-Instruct model show that R1-Searcher++ outperforms previous RAG methods, achieving up to a 4.3% improvement and reducing retrieval count by 42.9% compared to vanilla RL-based methods. This enables more efficient retrieval-augmented reasoning for LLMs and better generalization. |
| Natural Language Processing | DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in
  Digital Forensics and Incident Response (Read more on [arXiv](https://arxiv.org/abs/2505.19973) or [HuggingFace](https://huggingface.co/papers/2505.19973))| Saeed Alshehhi, Aaesha Aldahmani, Richard A. Dubniczky, Tamas Bisztray, Bilel Cherif | This paper introduces DFIR-Metric, a benchmark dataset for evaluating Large Language Models (LLMs) in Digital Forensics and Incident Response (DFIR). The research aims to comprehensively assess LLMs across both theoretical and practical DFIR domains, which is lacking in current benchmarks. The study employs a three-part benchmark including knowledge assessment (MCQ), realistic forensic challenges (CTF), and practical analysis of disk and memory forensics (NIST CFTT), evaluating 14 LLMs based on accuracy and consistency. The results show that GPT-4.1 achieves a Mean Accuracy of 92.75% on the MCQ dataset, while performance lags in the NIST String Search module. The benchmark offers a rigorous, reproducible foundation for advancing AI in digital forensics. |
| Machine Learning | SoloSpeech: Enhancing Intelligibility and Quality in Target Speech
  Extraction through a Cascaded Generative Pipeline (Read more on [arXiv](https://arxiv.org/abs/2505.19314) or [HuggingFace](https://huggingface.co/papers/2505.19314))| Kai Li, Chen Chen, Dongchao Yang, Jiarui Hai, Helin Wang | The paper introduces SoloSpeech, a novel cascaded generative pipeline for target speech extraction (TSE) that enhances intelligibility and quality. The research aims to improve generative models for TSE, which often lag in perceptual quality and intelligibility compared to discriminative models. SoloSpeech integrates compression, extraction, reconstruction, and correction processes using T-F domain VAEs and diffusion models without speaker embeddings. Evaluated on Libri2Mix, SoloSpeech achieves state-of-the-art intelligibility and quality in TSE, achieving a 0.95 dB gain in SI-SNR over existing methods. The proposed pipeline offers AI practitioners a robust and generalizable solution for TSE, demonstrating improved performance across different acoustic conditions and datasets. |
| Multi-Modal | MLLMs are Deeply Affected by Modality Bias (Read more on [arXiv](https://arxiv.org/abs/2505.18657) or [HuggingFace](https://huggingface.co/papers/2505.18657))| Yuanhuiyi Lyu, Kaiyu Lei, Yuqian Fu, Xu Zheng, Chenfei-Liao | The paper investigates the impact of modality bias on Multimodal Large Language Models (MLLMs). It examines how MLLMs often over-rely on language while underutilizing other modalities like visual inputs, arguing that this bias significantly affects their performance. The study analyzes key factors contributing to modality bias, including data characteristics, imbalanced backbone capabilities, and training objectives, and conducts experiments to demonstrate each factor's influence, though quantitative metrics from these experiments are not specified. The research proposes a systematic roadmap to measure, avoid, and reduce modality bias through specific methods and improved datasets. The work implies that balanced training strategies and model architectures are needed for better integration of modalities in MLLMs, to build more robust and generalizable AI systems. |
| Computer Vision | ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and
  Reactive Feedback (Read more on [arXiv](https://arxiv.org/abs/2505.17908) or [HuggingFace](https://huggingface.co/papers/2505.17908))| Jinsong Zhou, Jiantao Lin, Luozhou Wang, Xinli Xu, Litao Guo | The paper introduces ComfyMind, an AI system for robust and scalable general-purpose generation, unifying tasks across modalities using a collaborative approach built on the ComfyUI platform. It addresses the fragility of existing open-source frameworks by employing tree-based planning and reactive feedback. ComfyMind features a Semantic Workflow Interface (SWI) and Search Tree Planning with Local Feedback Execution. Evaluations on ComfyBench show it achieves a 100% workflow pass rate and 83.0% task resolution rate, outperforming baselines and matching GPT-Image-1 on GenEval and Reason-Edit, with a GPT-score of 0.906 on the latter. ComfyMind provides a promising open-source approach for general-purpose generative AI systems, enabling more stable and flexible complex generative workflows for practitioners. |
| Multi-Modal | R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large
  Language Models via Share-GRPO (Read more on [arXiv](https://arxiv.org/abs/2505.16673) or [HuggingFace](https://huggingface.co/papers/2505.16673))| Yibo Wang, Min Yang, Jingyi Zhang, Qixiang Yin, Huanjin Yao | This paper introduces R1-ShareVL, a novel reinforcement learning framework designed to improve the reasoning capabilities of multimodal large language models (MLLMs). The research addresses sparse reward and advantage vanishing issues in MLLM reinforcement learning by expanding the question space through semantically consistent transformations and sharing diverse reasoning trajectories. Share-GRPO also estimates solution advantages hierarchically across question variants, resulting in more robust policy training. Evaluations on six reasoning benchmarks demonstrate superior performance, with R1-ShareVL-7B improving MathVista performance by +7.2% over the base model. The method facilitates more effective and stable reinforcement learning for MLLMs, thereby advancing their complex reasoning skills. |
| Natural Language Processing | Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning
  of LLMs (Read more on [arXiv](https://arxiv.org/abs/2505.11277) or [HuggingFace](https://huggingface.co/papers/2505.11277))| Junfeng Fang, Zhiyuan Liu, Chang Wu, Shihan Li, yrshi | This paper introduces AutoRefine, a reinforcement learning framework that adopts a "search-and-refine-during-think" paradigm to enhance LLMs' autonomous retrieval-augmented reasoning. The research aims to improve the accuracy and efficiency of LLMs in complex reasoning scenarios by incorporating explicit knowledge refinement steps between successive search calls. AutoRefine uses a tailored retrieval-specific reward alongside answer correctness rewards. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, achieving 6.9% higher average accuracy. The framework provides AI practitioners with a method to improve knowledge distillation and utilization in retrieval-augmented reasoning. |
| Machine Learning | AdInject: Real-World Black-Box Attacks on Web Agents via Advertising
  Delivery (Read more on [arXiv](https://arxiv.org/abs/2505.21499) or [HuggingFace](https://huggingface.co/papers/2505.21499))| Mingyang Li, Rupeng Zhang, Xiaojun Jia, Junjie Wang, NicerWang | This paper introduces AdInject, a black-box attack leveraging advertising delivery to compromise VLM-based Web Agents. The research investigates the vulnerability of Web Agents to environment injection attacks via malicious advertising. AdInject designs deceptive ad content and employs a VLM-based optimization technique to target agents, achieving over 60% attack success rates on VisualWebArena and OSWorld. This demonstrates the real-world threat of advertising as an attack vector. The findings highlight the urgent need for robust defense mechanisms against environmental manipulation in Web Agents. |
| Multi-Modal | Modality Curation: Building Universal Embeddings for Advanced Multimodal
  Information Retrieval (Read more on [arXiv](https://arxiv.org/abs/2505.19650) or [HuggingFace](https://huggingface.co/papers/2505.19650))| Shi Feng, Hongzhi Zhang, Yahui Liu, Jingyuan Zhang, friedrichor | The paper introduces UNITE, a universal framework for multimodal information retrieval that addresses challenges in data heterogeneity and cross-modal alignment through data curation and modality-aware training. It investigates the impact of modality-specific data properties on downstream task performance. UNITE proposes Modal-Aware Masked Contrastive Learning (MAMCL) to mitigate competitive relationships between instances of different modalities. The framework achieves state-of-the-art results on multiple multimodal retrieval benchmarks, outperforming existing methods by notable margins (e.g., achieving leading performance on MMEB and WebVid-CoVR). The work provides a foundational blueprint for future research in multimodal systems by demonstrating the importance of strategic modality curation and tailored training protocols. |
| Computer Vision | Absolute Coordinates Make Motion Generation Easy (Read more on [arXiv](https://arxiv.org/abs/2505.19377) or [HuggingFace](https://huggingface.co/papers/2505.19377))| Huaizu Jiang, Yiming Xie, Xiaogang Peng, Zeyu Han, cr8br0ze | The paper introduces ACMDM, a novel text-to-motion generation framework built on absolute joint coordinates. It addresses limitations of local-relative representations by modeling motion using absolute joint coordinates in global space. The method employs a Transformer backbone with AdaLN conditioning and velocity prediction, achieving state-of-the-art performance without auxiliary kinematic losses. Results show ACMDM achieves FID of 0.058 on HumanML3D; the proposed approach also exhibits strong scalability and facilitates downstream tasks such as motion control and SMPL-H mesh generation. This simplifies controllable motion generation and enhances applicability to various motion-related tasks. |
| Machine Learning | Improving Chemical Understanding of LLMs via SMILES Parsing (Read more on [arXiv](https://arxiv.org/abs/2505.16340) or [HuggingFace](https://huggingface.co/papers/2505.16340))| Sungsoo Ahn, Jaehyung Kim, yunhuijang | The paper addresses the challenge of Large Language Models' (LLMs) limited understanding of SMILES, a molecular structure representation, by introducing CLEANMOL. It aims to enhance structural comprehension in LLMs through SMILES parsing, formulated as a suite of deterministic tasks. The methodology involves pre-training LLMs on a molecular dataset generated with adaptive difficulty scoring and structured supervision aligned with molecular structural properties. Experiments showed the CLEANMOL framework improves structural comprehension and achieves performance comparable to or better than baseline models on the Mol-Instructions benchmark. This approach offers a scalable solution for improving molecular understanding in LLMs without relying on costly experimental data. |
| Machine Learning | Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion
  Enhances Protein Representations (Read more on [arXiv](https://arxiv.org/abs/2505.20052) or [HuggingFace](https://huggingface.co/papers/2505.20052))| Ahmed Elnaggar, Mohamed Elkerdawy, Mohamed Elshaffei, Hazem Alsamkary | Ankh3 is a protein language model employing multi-task pretraining to enhance protein representations. The research aims to improve protein sequence understanding by jointly optimizing masked language modeling (MLM) with varying masking probabilities and protein sequence completion tasks. Ankh3 models demonstrate improved performance in downstream tasks like secondary structure prediction, achieving 84.4% accuracy on CASP-12 (SSP-3), and contact prediction. The integration of multiple pretraining tasks leads to more robust and accurate predictions. AI practitioners can use Ankh3 to achieve better protein property prediction and facilitate a more comprehensive understanding of protein sequences. |
| Machine Learning | Beyond Simple Concatenation: Fairly Assessing PLM Architectures for
  Multi-Chain Protein-Protein Interactions Prediction (Read more on [arXiv](https://arxiv.org/abs/2505.20036) or [HuggingFace](https://huggingface.co/papers/2505.20036))| Abdallah Amr, Sara Ossman, Mohamed Soudy, Mohamed Elshaffei, Hazem Alsamkary | This paper addresses protein-protein interaction (PPI) binding affinity prediction using protein language models (PLMs). It investigates the impact of different PLM architectures on prediction accuracy, focusing on multi-chain interactions. The research introduces a curated PPB-Affinity dataset and evaluates four architectures—embeddings concatenation, sequences concatenation, hierarchical pooling, and pooled attention addition—using fine-tuning and ConvBERT heads. Results indicate that hierarchical pooling and pooled attention addition consistently outperform concatenation methods, achieving up to a 12% increase in Spearman correlation. The study implies that sophisticated architectural designs are essential for effectively leveraging PLMs in nuanced PPI binding affinity predictions. |
| Machine Learning | Tropical Attention: Neural Algorithmic Reasoning for Combinatorial
  Algorithms (Read more on [arXiv](https://arxiv.org/abs/2505.17190) or [HuggingFace](https://huggingface.co/papers/2505.17190))| Ruriko Yoshida, Chris Teska, Kurt Pasque, Baran47 | This paper introduces Tropical Attention, a novel attention mechanism for neural algorithmic reasoning on combinatorial optimization problems. The research aims to address the limitations of softmax-based attention in capturing the piecewise-linear structure of dynamic programming algorithms. The authors propose Tropical Attention, operating in the max-plus semiring of tropical geometry, and prove its ability to approximate tropical circuits. Empirical results on eleven combinatorial tasks demonstrate state-of-the-art out-of-distribution generalization in length and value scale, along with improved adversarial robustness, with Tropical Attention achieving 95% Micro-F1 on the ConvexHull task under length OOD, surpassing softmax baselines. The use of Tropical Attention provides practitioners with a method to inject structural inductive biases, enhancing generalization and robustness in neural algorithmic reasoning tasks. |
| Natural Language Processing | Do RAG Systems Suffer From Positional Bias? (Read more on [arXiv](https://arxiv.org/abs/2505.15561) or [HuggingFace](https://huggingface.co/papers/2505.15561))| Fabrizio Silvestri, Yoelle Maarek, Guy Horowitz, Simone Filice, florin-hf | The paper investigates positional bias in Retrieval Augmented Generation (RAG) systems and its effect on LLM answer accuracy. The research examines how positional bias influences LLMs' susceptibility to both relevant and distracting passages retrieved from an external corpus. The methodology involves extensive experiments on three benchmarks, analyzing the distracting effect of retrieved passages and the impact of reordering strategies. The results show that state-of-the-art retrieval pipelines systematically introduce distracting passages, and strategies attempting to rearrange passages based on LLM positional preferences do not outperform random shuffling; specifically, over 60% of queries contain at least one highly distracting passage in the top 10. The finding suggests that future research should focus on improving retrieval quality and LLM robustness to distractions rather than focusing on passage positioning. |
