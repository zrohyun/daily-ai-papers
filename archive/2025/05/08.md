

## Papers for 2025-05-08

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Unified Multimodal Understanding and Generation Models: Advances,
  Challenges, and Opportunities (Read more on [arXiv](https://arxiv.org/abs/2505.02567) or [HuggingFace](https://huggingface.co/papers/2505.02567))| Minghao Fu, Jintao Guo, Xinjie Zhang, Flourish, Suikong | This paper presents a comprehensive survey on unified multimodal models that integrate both understanding and generation capabilities, particularly in vision and language. It aims to guide future research by introducing foundational concepts, reviewing existing models categorized into diffusion-based, autoregressive-based, and hybrid architectures, and analyzing their designs. The survey compiles relevant datasets (e.g., LAION with 5.9B samples) and benchmarks, and discusses key challenges such as tokenization strategies, cross-modal attention, and data. A key observation is the rapid growth in this field, with the number of publicly available/unavailable models increasing from approximately 5 in 2023 to over 35 by early 2025 (as illustrated in Figure 1), highlighted by models like GPT-4O. For AI practitioners, this work offers a structured overview of the current landscape, resources, and open challenges, serving as a valuable reference for advancing unified multimodal AI. |
| Reinforcement Learning | ZeroSearch: Incentivize the Search Capability of LLMs without Searching (Read more on [arXiv](https://arxiv.org/abs/2505.04588) or [HuggingFace](https://huggingface.co/papers/2505.04588))| Yingyan Hou, Xuanbo Fan, Zile Qiao, Hao Sun, SpaceProduct | This paper introduces ZEROSEARCH, a reinforcement learning framework that enhances the search capabilities of Large Language Models (LLMs) by using a simulated search engine, thereby avoiding the high costs and instability of live search engine interactions. The primary objective is to enable LLMs to learn effective search strategies and improve reasoning by interacting with a controllable, simulated environment, addressing challenges of uncontrolled document quality and prohibitive API expenses. ZEROSEARCH fine-tunes an LLM to act as a 'simulation LLM' capable of generating varied quality documents, then employs a curriculum-based rollout during RL training where the policy LLM interacts with this simulation LLM under progressively challenging retrieval scenarios. Experiments demonstrate that a 7B policy model trained with ZEROSEARCH (using a 14B simulation LLM) achieved an average Exact Match (EM) score of 40.54, outperforming a comparable model trained with a real search engine (Search-R1) which scored 39.24; furthermore, a 14B simulation LLM achieved an average EM of 33.97, surpassing Google Search's 32.47 in their setup. This framework offers AI practitioners a scalable and cost-efficient alternative for developing LLMs with robust search and retrieval-augmented generation capabilities without reliance on expensive real-world APIs. |
| Computer Vision | PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with
  Auto-Regressive Transformer (Read more on [arXiv](https://arxiv.org/abs/2505.04622) or [HuggingFace](https://huggingface.co/papers/2505.04622))| Yiqin Zhu, Yanning Zhou, Jingwen Ye, loktarxiao, hyz317 | The paper introduces PrimitiveAnything, a novel framework that reformulates 3D shape primitive abstraction as an auto-regressive sequence generation task, learning from human-crafted assemblies. The objective is to develop a method for decomposing complex 3D shapes into interpretable geometric primitives that generalize across diverse categories and align with human perception, overcoming limitations of existing optimization-based and category-specific learning approaches. PrimitiveAnything employs a shape-conditioned auto-regressive transformer to generate sequences of primitives, utilizing an ambiguity-free parameterization scheme for unified representation of multiple primitive types (cuboids, elliptical cylinders, ellipsoids) and training on a large-scale dataset of human-crafted abstractions (HumanPrim). The method demonstrates superior performance, achieving a Chamfer Distance of 0.0404 and a Voxel-IoU of 0.484 on its HumanPrim test set, outperforming prior methods, and also shows strong generalization to unseen datasets. This work provides a robust, generalizable approach for 3D shape abstraction that generates semantically meaningful and geometrically accurate primitive-based representations, offering potential for improved 3D content creation, analysis, and efficient user-generated content systems due to its interpretability and lightweight nature. |
| Multi-Modal | HunyuanCustom: A Multimodal-Driven Architecture for Customized Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2505.04512) or [HuggingFace](https://huggingface.co/papers/2505.04512))| Yuan Zhou, Sen Liang, Zhengguang Zhou, Zhentao Yu, Teng Hu | This paper introduces HunyuanCustom, a multi-modal customized video generation framework designed to produce videos featuring specific subjects with strong identity consistency, driven by text, image, audio, and video inputs. The primary objective is to address limitations in existing methods regarding subject consistency and restricted input modalities by developing a robust architecture for multi-modal, subject-centric video generation. Key methodological contributions include a LLaVA-based text-image fusion module, an image ID enhancement module using temporal concatenation, an AudioNet for hierarchical audio-video alignment, and a video-driven injection module with patchify-based feature alignment. Extensive experiments show HunyuanCustom significantly outperforms existing methods; for example, it achieves a Face-Sim score of 0.627 in single-subject customization, demonstrating superior ID consistency. This research offers AI practitioners an effective approach for multi-modal conditioning and identity preservation in controllable video generation, paving the way for advanced applications in creative industries. |
| Multi-Modal | Beyond Recognition: Evaluating Visual Perspective Taking in Vision
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2505.03821) or [HuggingFace](https://huggingface.co/papers/2505.03821))| Maciej Wołczyk, Michał Nauman, Piotr Miłoś, Alicja Ziarko, Gracjan | This paper evaluates the visual perspective-taking (VPT) capabilities of advanced Vision Language Models (VLMs) using a novel benchmark of controlled visual tasks. The primary objective is to assess VLM performance across three hierarchical levels of visual cognition: scene understanding, spatial reasoning, and visual perspective taking. The methodology utilizes 144 unique visual tasks, each featuring a humanoid minifigure and an object with systematically varied spatial configurations and camera views, paired with seven diagnostic questions. Results indicate that while models like GPT-4o achieve perfect scene understanding (100.0% prediction correctness), their performance significantly degrades on spatial reasoning (e.g., 72.9% for determining the minifigure's facing direction) and further deteriorates on visual perspective taking (e.g., 59.0% for identifying object location from the minifigure's perspective). The main implication for AI practitioners is that current VLMs lack robust mechanisms for deep spatial and perspective reasoning, necessitating the development of architectures with explicit geometric representations and tailored training protocols. |
| Machine Learning | Benchmarking LLMs' Swarm intelligence (Read more on [arXiv](https://arxiv.org/abs/2505.04364) or [HuggingFace](https://huggingface.co/papers/2505.04364))| Hao Sun, Ji-Rong Wen, Mowen Huang, 6cf | This paper introduces SwarmBench, a novel benchmark designed to evaluate the emergent swarm intelligence capabilities of Large Language Models (LLMs) acting as decentralized agents under strict local perception and communication constraints. The primary objective is to systematically assess whether LLMs can achieve effective coordination and collective intelligence akin to natural swarms when operating with incomplete spatio-temporal information. The methodology involves five foundational multi-agent coordination tasks (Pursuit, Synchronization, Foraging, Flocking, Transport) in a configurable 2D grid environment, where LLM-driven agents operate with a limited local view (k × k) and optional local communication in a zero-shot setting, with performance measured by task-specific scores and group dynamics metrics. Evaluations of contemporary LLMs (e.g., deepseek-v3, 04-mini) revealed significant performance variations and limitations in robust planning under uncertainty; for example, emergent physical group dynamics like behavioral variability (`std_action_entropy`, r = 0.300 with score) were found to be significant drivers of performance, explaining approximately 24.5% of score variance. The main implication for AI practitioners is that SwarmBench provides a dedicated platform to measure progress and guide research towards developing LLMs capable of genuine collective intelligence in decentralized, constrained environments, highlighting current limitations. |
| Machine Learning | Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal
  Problem-Solving (Read more on [arXiv](https://arxiv.org/abs/2505.04528) or [HuggingFace](https://huggingface.co/papers/2505.04528))| Qinxiang Cao, Xingzhi Qi, Renqiu Xia, Xinhao Zheng, purewhite42 | This paper introduces a novel formulation, framework (FPS and D-FPS), and benchmarks for formal problem-solving, aiming to enhance process-level verifiability in AI agents beyond traditional theorem proving. The primary objective is to provide a principled formulation of problem-solving as a deterministic Markov decision process and develop frameworks that enable process-verified problem-solving and human-aligned answer verification. The methodology includes proposing Formal Problem-Solving (FPS) and Deductive FPS (D-FPS) which utilize formal theorem proving environments, introducing Restricted Propositional Equivalence (RPE) for evaluating answer correctness, and constructing three new benchmarks: FormalMath500, MiniF2F-Solving, and PutnamBench-Solving. Baseline evaluations show that prevalent FTP models using the FPS framework solve at most 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving. This research offers AI practitioners new tools and a rigorous methodology for building and assessing AI systems capable of verifiable formal problem-solving, while also highlighting significant challenges for current models in these complex reasoning tasks. |
| Multi-Modal | OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue
  Resolution (Read more on [arXiv](https://arxiv.org/abs/2505.04606) or [HuggingFace](https://huggingface.co/papers/2505.04606))| Jiachi Chen, Yanlin Wang, Runhan Jiang, Lianghong Guo, itaowe | This paper introduces OmniGIRL, a novel multilingual, multimodal, and multi-domain benchmark designed to evaluate the GitHub issue resolution capabilities of Large Language Models (LLMs). The primary objective is to address limitations in existing benchmarks by incorporating diverse programming languages, repository domains, and multimodal inputs like images and website links. OmniGIRL was constructed by collecting 959 task instances from 15 repositories across four languages (Python, JavaScript, TypeScript, Java), featuring textual descriptions, 19 instances with crucial image information, and links to external code platforms. Evaluation of state-of-the-art LLMs on OmniGIRL demonstrated limited performance; for example, the best-performing model on general tasks, GPT-4o, resolved only 8.6% of issues. For tasks requiring image understanding, Claude-3.5-Sonnet achieved the highest resolution rate of 10.5%. OmniGIRL offers AI practitioners a more comprehensive and challenging benchmark for assessing and advancing LLMs in complex, real-world software engineering tasks, highlighting current deficiencies in multimodal reasoning and cross-file issue resolution. |
| Multi-Modal | OpenHelix: A Short Survey, Empirical Analysis, and Open-Source
  Dual-System VLA Model for Robotic Manipulation (Read more on [arXiv](https://arxiv.org/abs/2505.03912) or [HuggingFace](https://huggingface.co/papers/2505.03912))| Xinyang Tong, Shuanghao Bai, Wenxuan Song, Pengxiang Ding, Can Cui | This paper presents "OpenHelix," a survey, empirical analysis, and open-source dual-system Vision-Language-Action (VLA) model aimed at advancing robotic manipulation. The primary objective is to address the scarcity of open-source dual-system VLAs by systematically evaluating core design elements (such as MLLM training, policy training, and integration strategies) and providing a robust, low-cost baseline model. OpenHelix employs a pre-trained LLaVA-7B as the high-level MLLM and a 3D Diffuser Actor as the low-level policy, integrated via a learnable <ACT> token using prompt-tuning for the MLLM, an auxiliary multimodal reasoning task, and a two-stage training process. The proposed model, MLLM (Prompt Tuning) + AUX + Policy(P) + Asy(60), achieved a 46.9% success rate for completing 5 consecutive tasks on the CALVIN ABC-D benchmark, and demonstrated that auxiliary tasks significantly boost performance (e.g., average task completion length increased from 3.27 to 3.45, and up to 4.01 with different MLLM settings on CALVIN). The work offers valuable empirical guidelines for designing dual-system VLAs, highlighting the benefits of prompt-tuning and auxiliary visual reasoning tasks, while also suggesting that current latent representations from MLLMs may not fully capture dynamic environmental changes for continuous low-level control. |
| Multi-Modal | OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents (Read more on [arXiv](https://arxiv.org/abs/2505.03570) or [HuggingFace](https://huggingface.co/papers/2505.03570))| Sinéad Ryan, Arturo Márquez Flores, Patrick Barker, Daniel Jeffries, mariya-davydova | This paper introduces OSUniverse, a benchmark for multimodal GUI-navigation AI agents, featuring complex desktop-oriented tasks with increasing levels of difficulty and an automated validation mechanism. The primary objective is to provide a robust framework to evaluate advanced GUI-navigation agents, addressing limitations of existing benchmarks by focusing on ease of use, extensibility, and comprehensive test coverage. The methodology involves Dockerized environments (AgentDesk), YAML-defined test cases across five complexity levels (Paper to Gold), support for various agent architectures (defaulting to SurfKit runtime), and automated validation using Gemini models, which has an average error rate of less than 2% compared to manual scoring (specifically 1.64% disagreement for Gemini 2.0 Flash). Key results show that State-of-the-Art (SOTA) agents at the time of publication achieve less than 50% weighted score (the top agent, 'computer-use-preview-2025-03-11', scored 47.80%), indicating substantial room for improvement. The main implication for AI practitioners is that OSUniverse provides a challenging and extensible benchmark for measuring progress in GUI navigation, highlighting that the best-performing agents are those specifically trained for this use case with custom action spaces. |
| Machine Learning | Knowledge Augmented Complex Problem Solving with Large Language Models:
  A Survey (Read more on [arXiv](https://arxiv.org/abs/2505.03418) or [HuggingFace](https://huggingface.co/papers/2505.03418))| Yuqi Zhu, Yuchen Tian, Junwei Su, Lun Du, Da Zheng | This survey explores the use of Large Language Models (LLMs) for complex problem-solving, emphasizing knowledge augmentation and discussing current techniques, challenges, and future directions. The paper aims to review the capabilities and limitations of LLMs in complex problem-solving by examining methodologies like Chain-of-Thought (CoT) reasoning, knowledge augmentation strategies, and various result verification techniques across different domains. Key methodologies discussed include multi-step reasoning (e.g., CoT, inference scaling laws), knowledge integration (e.g., RAG, domain-specific pre-training, human-in-the-loop), and result verification (e.g., LLM-as-a-judge, symbolic tools, experimental validation). The survey highlights that while techniques like CoT improve performance, evidenced by an inference scaling law where solution correctness increases with more reasoning paths [10, 20], LLMs struggle with retaining long-tail domain knowledge [121] and require robust verification for practical applications. For AI practitioners, the main implication is the need to focus on enhancing multi-step reasoning, integrating domain-specific knowledge effectively, and developing comprehensive verification mechanisms to advance LLMs' complex problem-solving abilities. |
| Machine Learning | R&B: Domain Regrouping and Data Mixture Balancing for Efficient
  Foundation Model Training (Read more on [arXiv](https://arxiv.org/abs/2505.00358) or [HuggingFace](https://huggingface.co/papers/2505.00358))| Ziyi Chu, Avi Trost, John Cooper, Tzu-Heng Huang, Albert Ge | R&B is a novel framework for efficient foundation model training that first semantically regroups data into finer-grained domains and then dynamically balances their mixture proportions using training gradients. The primary objective is to overcome the suboptimal performance and computational scaling issues of existing data mixing methods that rely on predefined coarse domains and expensive per-skill evaluations. R&B employs a two-stage process: "Regroup" re-partitions data based on embedding similarity (e.g., k-means), and "Balance" dynamically adjusts domain weights using a Gram matrix of domain gradients and a regularized optimization objective. Empirically, R&B matches or surpasses state-of-the-art methods on diverse tasks, achieving, for instance, a 2.381 loss on SUP-NATINST with only 0.009% compute overhead, while cutting computational FLOPs by over 99% compared to some existing approaches. AI practitioners can leverage R&B to train foundation models more efficiently, achieving improved performance or similar performance with substantially lower computational resources by optimizing both data partitioning and dynamic mixture proportions. |
| Multi-Modal | Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly
  Detection (Read more on [arXiv](https://arxiv.org/abs/2505.02393) or [HuggingFace](https://huggingface.co/papers/2505.02393))| Mohsen Imani, Paper9795, Eavn | This paper presents IEF-VAD, a novel uncertainty-weighted multimodal fusion framework for video anomaly detection that synthesizes event data from RGB videos. The primary objective is to improve the capture of transient motion cues, often missed by RGB-only systems, by robustly fusing image features with these synthetic event representations under varying uncertainty. IEF-VAD achieves this by modeling sensor noise with a Student's-t likelihood, deriving inverse-variance weights via Laplace approximation for fusion, employing Kalman-style temporal updates, and iteratively refining the fused state. The proposed method sets new state-of-the-art results on several benchmarks, achieving an Area Under the Curve (AUC) of 88.67% on UCF-Crime and 97.98% on ShanghaiTech. This research demonstrates that practitioners can leverage synthetic event data combined with uncertainty-aware fusion to significantly enhance video anomaly detection performance without needing specialized event hardware. |
| Other | Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI
  Knowledge Co-Creation (Read more on [arXiv](https://arxiv.org/abs/2505.03105) or [HuggingFace](https://huggingface.co/papers/2505.03105))| linxule | This paper introduces Cognitio Emergens (CE), a comprehensive theoretical framework designed to understand and guide the dynamic, co-evolutionary nature of human-AI partnerships in scientific knowledge co-creation. The primary objective is to develop a novel framework that addresses the limitations of existing models by capturing how scientific understanding emerges through recursive, evolving human-AI interactions over time, moving beyond static roles or narrow performance metrics. The CE framework is conceptualized through a synthesis of autopoiesis theory, social systems theory, and organizational modularity, and it integrates three core components: Agency Configurations (describing authority distribution), Epistemic Dimensions (six emergent collaborative capabilities), and Partnership Dynamics (evolutionary forces and risks). As a conceptual paper, it proposes the CE framework itself as its main result, offering a new lens for analysis and guidance rather than presenting empirical findings with quantitative metrics; it critiques existing metric-centric approaches for limitations like temporal myopia. The main implication for AI practitioners is that they should adopt a co-evolutionary perspective on human-AI collaboration, using the CE framework to design systems and interactions that foster emergent capabilities, manage dynamic partnerships effectively, and mitigate risks such as epistemic alienation, thereby enabling more transformative scientific breakthroughs. |
