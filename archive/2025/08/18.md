

## Papers for 2025-08-18

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Reinforcement Learning | SSRL: Self-Search Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2508.10874) or [HuggingFace](https://huggingface.co/papers/2508.10874))| Yanxu Chen, Yuxin Zuo, Heng Zhou, Kaiyan Zhang, Yuchen Fan | The paper investigates the potential of LLMs as efficient simulators for agentic search tasks in RL. It explores whether LLMs can answer search-based QA tasks using only internal knowledge and whether full-sim search RL enables effective sim-to-real transfer with real web search. The proposed Self-Search RL (SSRL) enhances LLMs' self-search capability through format-based and rule-based rewards, allowing internal knowledge refinement without external tools. Empirical evaluations demonstrate SSRL-trained policy models provide a cost-effective environment for search-driven RL training, reducing reliance on external search engines; for example, Llama-3.1-8B-Instruct achieves 87.2% accuracy for pass@1024 on Bamboogle, a 150% improvement over pass@1. The main implication is that LLMs can support more scalable RL agent training by effectively leveraging internal knowledge. |
| Computer Vision | Thyme: Think Beyond Images (Read more on [arXiv](https://arxiv.org/abs/2508.11630) or [HuggingFace](https://huggingface.co/papers/2508.11630))| Wei Chen, Chaoyou Fu, Shukang Yin, Xingyu Lu, Yi-Fan Zhang | The paper introduces Thyme, a novel paradigm for multimodal large language models to autonomously generate and execute image processing operations via executable code. It addresses the limitations of existing methods by enabling diverse image manipulations and logical reasoning capabilities. The approach involves a two-stage training strategy: supervised fine-tuning on a dataset for code generation followed by reinforcement learning for decision-making. Comprehensive evaluations on nearly 20 benchmarks demonstrate that Thyme yields significant and consistent performance gains, particularly in high-resolution perception and complex reasoning tasks. For example, Thyme shows an improvement of over 25% in perception tasks. Thyme's ability to autonomously adapt image manipulations can potentially improve a variety of computer vision tasks. |
| Computer Vision | DINOv3 (Read more on [arXiv](https://arxiv.org/abs/2508.10104) or [HuggingFace](https://huggingface.co/papers/2508.10104))| Maxime Oquab, Federico Baldassarre, Maximilian Seitzer, Huy V. Vo, Oriane Sim√©oni | DINOv3 introduces a self-supervised vision foundation model that excels in dense feature extraction. It addresses the degradation of dense feature maps during long training schedules using a novel Gram anchoring method. The methodology involves scaling dataset and model size with optimized data preparation and post-hoc strategies for resolution and model size flexibility. DINOv3 achieves state-of-the-art performance on various vision tasks without fine-tuning, including a mAP of 66.1 on COCO object detection. This work provides AI practitioners with a versatile vision model applicable to diverse tasks and resource constraints. |
| Natural Language Processing | PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical
  Register Indexing (Read more on [arXiv](https://arxiv.org/abs/2508.11116) or [HuggingFace](https://huggingface.co/papers/2508.11116))| Xianpei Han, Yaojie Lu, Hongyu Lin, Xuanang Chen, lzq2021 | The paper introduces PaperRegister, a novel system for flexible-grained paper search. It addresses the limitation of existing abstract-based paper search systems by using a hierarchical register indexing approach. PaperRegister employs offline hierarchical indexing and online adaptive retrieval to transform the traditional abstract-based index into a hierarchical index tree, supporting queries at varying granularity levels. Experiments on paper search tasks demonstrate state-of-the-art performance, particularly excelling in fine-grained scenarios, achieving a recall@5 score of 80.8 in F.g.Search-3 under DPR-based matching, a 22.6 improvement over using abstract-based indices. This work provides AI practitioners with an effective solution for handling flexible-grained paper searches in real-world applications. |
| Natural Language Processing | XQuant: Breaking the Memory Wall for LLM Inference with KV Cache
  Rematerialization (Read more on [arXiv](https://arxiv.org/abs/2508.10395) or [HuggingFace](https://huggingface.co/papers/2508.10395))| Rishabh Tiwari, Haocheng Xi, Minjae Lee, Coleman Hooper, Aditya Tomar | The paper introduces XQUANT, a novel approach to reduce memory consumption during LLM inference by quantizing layer input activations instead of the KV cache. The research aims to break the memory wall for LLM inference by trading computation for reduced memory bandwidth requirements. XQUANT quantizes and rematerializes the Keys and Values on-the-fly, and introduces XQUANT-CL to exploit cross-layer similarity for extreme compression. Results show up to 7.7x memory savings with minimal perplexity degradation (<0.1) compared to FP16. XQUANT allows AI practitioners to drastically reduce the memory footprint of LLM inference, enabling deployment on resource-constrained hardware or larger models with existing resources. |
| Computer Vision | StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image
  Translation (Read more on [arXiv](https://arxiv.org/abs/2508.11203) or [HuggingFace](https://huggingface.co/papers/2508.11203))| Junyong Noh, Kwan Yun, Seungmi Lee | The paper introduces StyleMM, a novel framework for constructing stylized 3D Morphable Models (3DMMs) from text descriptions. It addresses the challenge of creating stylized 3D faces with maintained correspondence, disentangled control, and expressive stylization beyond realistic models. The method fine-tunes pre-trained mesh deformation and texture generation networks using stylized images generated via text-guided image-to-image translation, explicitly preserving facial attributes through a proposed Explicit Attribute-preserving Stylization (EAS) module. StyleMM achieves higher face diversity compared to state-of-the-art methods, reaching a face diversity score of 12.005. StyleMM enables AI practitioners to generate controllable and animatable stylized 3D faces without requiring specialized 3D datasets, facilitating broader applications in character creation and animation. |
| Computer Vision | FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for
  Audio-Driven Portrait Animation (Read more on [arXiv](https://arxiv.org/abs/2508.11255) or [HuggingFace](https://huggingface.co/papers/2508.11255))| Mu Xu, Fan Jiang, MengChao Wang, wangqiang9 | The paper introduces FantasyTalking2, a novel framework for audio-driven portrait animation using timestep-layer adaptive preference optimization. It addresses the challenge of aligning generated animations with fine-grained human preferences for motion naturalness, lip-sync accuracy, and visual quality. The method decouples preferences into specialized expert modules fused across timesteps and network layers using a learned gating mechanism. The approach achieves state-of-the-art results, showing improvements in lip synchronization (12.7%), motion naturalness (15.0%), and visual quality (13.7%) over baselines in user studies. TLPO enables practitioners to generate more realistic and expressive portrait animations that are closely aligned with subjective human expectations. |
| Computer Vision | TexVerse: A Universe of 3D Objects with High-Resolution Textures (Read more on [arXiv](https://arxiv.org/abs/2508.10868) or [HuggingFace](https://huggingface.co/papers/2508.10868))| Nan Cao, Rui Ma, Li Zhang, YiboZhang2001 | The paper introduces TexVerse, a large-scale 3D dataset featuring high-resolution textures for advancing research in computer vision and graphics. The primary objective is to address the scarcity of suitable datasets for high-resolution texture generation in 3D objects. TexVerse comprises 858K unique 3D models from Sketchfab, including 158K with PBR materials and 1.6M total instances across varying resolutions. The dataset includes specialized subsets for rigged and animated models and model annotations generated by GPT-5. The dataset aims to facilitate texture synthesis, PBR material development, animation, and various 3D vision tasks. |
| Multi-Modal | Controlling Multimodal LLMs via Reward-guided Decoding (Read more on [arXiv](https://arxiv.org/abs/2508.11616) or [HuggingFace](https://huggingface.co/papers/2508.11616))| Michal Drozdzal, Adriana Romero-Soriano, Koustuv Sinha, Pierluca D'Oro, oscmansan | The paper introduces multimodal reward-guided decoding (MRGD) for controlling the behavior of multimodal large language models (MLLMs). It addresses the need to adapt MLLMs for diverse user needs by controlling object precision and recall during visual grounding. MRGD employs two reward models, one for hallucination reduction and another for improving object recall, guiding the decoding process. Evaluation on object hallucination benchmarks shows that MRGD reduces CHAIR by approximately 70% compared to greedy decoding, enabling fine-grained control over MLLM inference. This approach offers AI practitioners a method to dynamically balance object precision and recall during MLLM inference, surpassing existing hallucination mitigation techniques. |
| Machine Learning | X-Node: Self-Explanation is All We Need (Read more on [arXiv](https://arxiv.org/abs/2508.10461) or [HuggingFace](https://huggingface.co/papers/2508.10461))| Islem Rekik, prajit123 | The paper introduces X-Node, a self-explaining GNN framework enabling node-level reasoning. It addresses the challenge of GNN interpretability by having each node generate its explanation based on local graph topology and features. The methodology involves constructing a context vector encoding interpretable cues and using a Reasoner module with a pre-trained LLM for explanation generation and GNN guidance via text injection. Experiments on MedMNIST and MorphoMNIST show competitive classification accuracy with faithful per-node explanations; for instance, on OrganAMNIST, X-Node raises F1 from 91.19% to 93.16%. X-Node offers AI practitioners a modular interpretability layer for GNNs, enhancing trust in high-stakes clinical applications. |
| Computer Vision | SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via
  Class-Conditioned Image Translation (Read more on [arXiv](https://arxiv.org/abs/2508.06429) or [HuggingFace](https://huggingface.co/papers/2508.06429))| Paolo Soda, Loredana Zollo, Clemente Lauretti, Guido Manni | The paper introduces a novel GAN-based semi-supervised learning framework for medical image classification in low labeled-data regimes. It aims to improve classification accuracy when only a few labeled samples are available by leveraging unlabeled data through class-conditioned image translation. The methodology involves a three-player GAN architecture and a dynamic training schedule alternating between supervised and unsupervised phases. The approach achieves statistically significant improvements over six state-of-the-art semi-supervised methods, particularly in the 5-shot setting, improving the accuracy from ~64% to ~66%. The framework offers a practical solution for medical imaging applications where annotation costs are prohibitive, enabling robust classification performance with minimal labeled data. |
| Computer Vision | MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and
  Multispectral Earth Observation Data (Read more on [arXiv](https://arxiv.org/abs/2508.10894) or [HuggingFace](https://huggingface.co/papers/2508.10894))| Nicolas Gonthier, Anatol Garioud, Nina Lardiere, Michael Vaccaro, Antoine Labatie | MAESTRO introduces a novel masked autoencoder for self-supervised learning on multimodal, multitemporal, and multispectral Earth Observation data. The paper aims to adapt masked autoencoding to the unique characteristics of EO data by optimizing fusion strategies and introducing a spectral prior. MAESTRO employs token-based early/late fusion and patch-group-wise normalization to inject spectral information during pretraining. Evaluated on four EO datasets, MAESTRO achieves state-of-the-art results on multitemporal tasks, improving weighted F1 by 2.7% on TreeSatAI-TS. The framework provides a versatile approach to leverage unlabeled EO data for representation learning, enhancing performance on diverse downstream tasks that rely on complex spatiotemporal dynamics. |
