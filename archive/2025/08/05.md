

## Papers for 2025-08-05

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | Qwen-Image Technical Report (Read more on [arXiv](https://arxiv.org/abs/2508.02324) or [HuggingFace](https://huggingface.co/papers/2508.02324))| Kaiyuan Gao, Junyang Lin, Jingren Zhou, Jiahao Li, Chenfei Wu | The paper introduces Qwen-Image, an image generation foundation model excelling in complex text rendering and precise image editing. It addresses challenges in text rendering and image editing consistency through a comprehensive data pipeline and improved multi-task training. Key to the methodology is dual encoding for balanced semantic consistency and visual fidelity. Qwen-Image achieves state-of-the-art performance on benchmarks like GenEval, LongText-Bench, and CVTG-2K, demonstrating enhanced image generation and text rendering. The model's capabilities offer AI practitioners a robust solution for applications requiring detailed visual synthesis and precise text integration. |
| Natural Language Processing | SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic
  Association and Long Story Comprehension (Read more on [arXiv](https://arxiv.org/abs/2508.01959) or [HuggingFace](https://huggingface.co/papers/2508.01959))| Liyan Xu, Lemao Liu, Yuqing Li, Jiangnan Li, Junjie Wu | The paper introduces SitEmb, a novel approach to context-aware dense retrieval, enhancing performance in semantic association and long story comprehension. It addresses the challenge of limited capacity in embedding models for long documents by situating short chunks within broader context windows. The key methodology involves a new training paradigm with context-dependent instances from book notes and residual learning. Evaluation on a curated book-plot retrieval dataset demonstrates that SitEmb-v1 outperforms state-of-the-art embedding models, achieving over 10% performance improvement with the 8B SitEmb-v1.5 model. SitEmb enables AI practitioners to improve retrieval and downstream tasks by more effectively incorporating contextual information into text embeddings. |
| Machine Learning | CellForge: Agentic Design of Virtual Cell Models (Read more on [arXiv](https://arxiv.org/abs/2508.02276) or [HuggingFace](https://huggingface.co/papers/2508.02276))| Daniel Shao, Yan Cui, Jiapeng Chen, Zhuoyun Yu, Xiangru Tang | The paper introduces CELLFORGE, an agentic system for autonomously designing virtual cell models from single-cell multi-omics data and task descriptions. CELLFORGE aims to predict quantitative responses to diverse perturbations by leveraging a multi-agent framework. The system uses task analysis, method design by expert agents, and experiment execution to output optimized model architectures and executable code. Experiments on six datasets showed that CELLFORGE consistently outperforms task-specific state-of-the-art methods, achieving up to 40% reduction in prediction error and 20% improvement in correlation metrics, demonstrating the benefits of iterative interaction between LLM agents with differing perspectives for better modeling solutions. This indicates potential application for diverse virtual cell modeling challenges beyond perturbation analysis. |
| Reinforcement Learning | Beyond the Trade-off: Self-Supervised Reinforcement Learning for
  Reasoning Models' Instruction Following (Read more on [arXiv](https://arxiv.org/abs/2508.02150) or [HuggingFace](https://huggingface.co/papers/2508.02150))| Jiaqing Liang, Jie Zeng, Bowei Zhang, Qianyu He, Qingyu Ren | The paper introduces a self-supervised reinforcement learning (RL) framework to improve instruction following in reasoning models without external supervision. It addresses the trade-off between reasoning and instruction following by leveraging the model's internal signals. The methodology involves curriculum decomposition for constraint-wise binary classification to construct reward models. Experiments show that the framework significantly improves instruction following capabilities while maintaining reasoning performance, achieving up to 87.1% on IFEval compared to the baseline. This provides a scalable and cost-effective approach for enhancing instruction following in reasoning models, reducing reliance on stronger external models. |
| Natural Language Processing | Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report (Read more on [arXiv](https://arxiv.org/abs/2508.01059) or [HuggingFace](https://huggingface.co/papers/2508.01059))| Anu Vellore, Baturay Saglam, Blaine Nelson, Paul Kassianik, Sajana Weerawardhena | The paper introduces Foundation-Sec-8B-Instruct, a cybersecurity-focused LLM built on Llama 3.1-8B, designed for general-purpose cybersecurity dialogue. It aims to enhance cybersecurity workflows by combining domain-specific knowledge with instruction-following and conversational capabilities. The key methodology involves instruction tuning and preference alignment using a cybersecurity corpus. Results demonstrate that Foundation-Sec-8B-Instruct outperforms Llama 3.1-8B-Instruct on cybersecurity tasks like CTI with state-of-the-art performance on CTIBench-RCM, while matching its instruction-following performance. The model offers AI practitioners a practical tool for cybersecurity tasks and a stepping stone towards broader LLM integration in the field. |
| Multi-Modal | InstructVLA: Vision-Language-Action Instruction Tuning from
  Understanding to Manipulation (Read more on [arXiv](https://arxiv.org/abs/2507.17520) or [HuggingFace](https://huggingface.co/papers/2507.17520))| Yang Tian, Bin Wang, Yilun Chen, Hao Li, Shuai Yang | The paper introduces InstructVLA, a vision-language-action (VLA) model designed to bridge the gap between multimodal reasoning and precise action generation in robotics. It addresses the limitations of existing VLA models that often sacrifice one capability for the other, leading to catastrophic forgetting of pre-trained vision-language knowledge. InstructVLA employs a novel Vision-Language-Action Instruction Tuning (VLA-IT) paradigm, leveraging a curated 650K-sample dataset and a mixture-of-experts adaptation framework to jointly optimize textual reasoning and action generation. On SimplerEnv-Instruct, InstructVLA outperforms fine-tuned OpenVLA by 92% demonstrating improved instruction following and task decomposition. The model's ability to preserve VLM reasoning while achieving strong manipulation performance offers AI practitioners a pathway for more intuitive and steerable human-robot interaction. |
| Multi-Modal | VeOmni: Scaling Any Modality Model Training with Model-Centric
  Distributed Recipe Zoo (Read more on [arXiv](https://arxiv.org/abs/2508.02317) or [HuggingFace](https://huggingface.co/papers/2508.02317))| Bin Jia, Zhongkai Zhao, Zhelun Shi, Yaowei Zheng, Qianli Ma | The paper introduces VeOmni, a modular and efficient training framework for scaling omni-modal large language models. It addresses the challenge of training heterogeneous omni-modal architectures by decoupling communication from computation via model-centric distributed recipes, enabling efficient 3D parallelism. VeOmni supports seamless integration of new modalities with minimal code change and facilitates flexible configuration. Experiments demonstrate that a 30B parameter omni-modal MoE model can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths on 128 GPUs. VeOmni offers AI practitioners an improved approach to efficiently scale and customize omni-modal LLMs by easing the burden on model developers to account for parallelization logic. |
| Multi-Modal | A Glimpse to Compress: Dynamic Visual Token Pruning for Large
  Vision-Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.01548) or [HuggingFace](https://huggingface.co/papers/2508.01548))| Zuxuan Wu, Peng-Tao Jiang, Qilong Wang, Yunheng Li, Quan-Sheng Zeng | The paper introduces GlimpsePrune, a dynamic visual token pruning framework for Large Vision-Language Models (LVLMs). It addresses the issue of inefficient fixed compression ratios by using a data-driven “glimpse” to prune irrelevant visual tokens in a single forward pass. GlimpsePrune achieves a 92.6% pruning rate of visual tokens while retaining baseline performance on free-form VQA tasks. The method also enables more effective fine-tuning, with GlimpsePrune+ achieving 110% of baseline performance while maintaining a similar pruning rate. This work provides a more powerful and efficient means for processing high-resolution inputs in LVLMs. |
| Computer Vision | Personalized Safety Alignment for Text-to-Image Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2508.01151) or [HuggingFace](https://huggingface.co/papers/2508.01151))| Kaidong Yu, Aosong Feng, Qingyu Shi, Jinbin Bai, Yu Lei | This paper introduces Personalized Safety Alignment (PSA), a framework for user-specific safety control in text-to-image diffusion models. The research aims to address the limitation of uniform safety standards by incorporating personalized user profiles into the generation process. PSA leverages a new dataset, Sage, which captures user-specific safety preferences and employs a cross-attention mechanism. Experiments demonstrate that PSA outperforms existing methods in harmful content suppression, achieving higher Win Rate and Pass Rate scores. The implication for AI practitioners is a fine-grained, user-aware safety control mechanism for generative models, moving beyond one-size-fits-all approaches. |
| Machine Learning | Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and
  Regional Languages Around the Globe (Read more on [arXiv](https://arxiv.org/abs/2508.01691) or [HuggingFace](https://huggingface.co/papers/2508.01691))| Thanathai Lertpetchpun, Xuan Shi, Anfeng Xu, Kevin Huang, Tiantian Feng | Voxlect is a new benchmark for evaluating speech foundation models in dialect and regional language classification. The research aims to address the disparities in speech technology performance across diverse linguistic contexts by creating a comprehensive benchmark. Using over 2 million utterances from 30 publicly available speech corpora across 11 languages, the study benchmarks the performance of models like Whisper and MMS in classifying speech dialects. Results show Whisper-Large achieves a Macro-F1 score of 0.923 on Arabic dialect classification, highlighting the effectiveness of multilingual models. The benchmark enables AI practitioners to evaluate and improve the robustness and fairness of speech technologies across dialectal variations, allowing more reliable ASR systems. |
| Multi-Modal | RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong
  Learning in Physical Embodied Systems (Read more on [arXiv](https://arxiv.org/abs/2508.01415) or [HuggingFace](https://huggingface.co/papers/2508.01415))| Junkun Hong, Liangchen Tan, Zezhou Cui, Honghao Cai, Mingcong Lei | The paper introduces RoboMemory, a brain-inspired multi-memory framework for lifelong learning in physical embodied systems. The research aims to address challenges like continuous learning and latency in real-world robotic tasks. RoboMemory integrates an Information Preprocessor, a Lifelong Embodied Memory System, a Closed-Loop Planning Module, and a Low-Level Executer. Evaluations on EmbodiedBench show RoboMemory outperforms the baseline by 25% in average success rate and surpasses the SOTA (Claude3.5-Sonnet) by 5%. This framework offers a scalable solution for integrating multi-modal memory systems, advancing autonomous learning in physical robots. |
| Natural Language Processing | AgentTTS: Large Language Model Agent for Test-time Compute-optimal
  Scaling Strategy in Complex Tasks (Read more on [arXiv](https://arxiv.org/abs/2508.00890) or [HuggingFace](https://huggingface.co/papers/2508.00890))| Zhiwei Zhang, Jingying Zeng, Zhenwei Dai, Hui Liu, Fali Wang | The paper introduces AgentTTS, an LLM agent-based framework for test-time compute-optimal scaling in multi-stage complex tasks. It addresses the problem of selecting suitable models and allocating budgets per subtask to maximize overall performance in complex tasks. The methodology involves an LLM-agent-based framework informed by empirical insights from extensive pilot experiments. Experimental results demonstrate that AgentTTS outperforms traditional baselines in search efficiency, with improved robustness to varying training set sizes. The framework also shows enhanced interpretability, providing insights into budget allocation strategies. |
| Reinforcement Learning | Exploitation Is All You Need... for Exploration (Read more on [arXiv](https://arxiv.org/abs/2508.01287) or [HuggingFace](https://huggingface.co/papers/2508.01287))| Jesse Roberts, Micah Rentschler | This paper challenges the necessity of explicit exploration incentives in meta-reinforcement learning, positing that exploitation alone can drive exploration under specific conditions. The research investigates whether a purely greedy objective can lead to emergent exploratory behavior. Utilizing controlled ablations in multi-armed bandits and gridworlds, the study assesses the impact of recurring environmental structure, agent memory, and long-horizon credit assignment. The results demonstrate that with sufficient structure and memory, a greedy agent can achieve performance comparable to or exceeding Thompson Sampling (e.g., meta-RL agent achieves 0.704±0.055 cumulative reward vs. Thompson Sampling at 0.614±0.017 in a 3-armed bandit environment). The implication is that focusing on memory-rich architectures that leverage repeated patterns may simplify RL design and offer a biologically plausible explanation for exploration. |
| Machine Learning | Cyber-Zero: Training Cybersecurity Agents without Runtime (Read more on [arXiv](https://arxiv.org/abs/2508.00910) or [HuggingFace](https://huggingface.co/papers/2508.00910))| Zijian Wang, Varun Kumar, Hantian Ding, Dingmin Wang, Terry Yue Zhuo | The paper introduces Cyber-Zero, a runtime-free framework for training cybersecurity agents using Large Language Models (LLMs). It addresses the challenge of training such agents without access to runtime environments by synthesizing agent trajectories from CTF writeups using persona-driven LLM simulation. The key methodology involves reverse-engineering runtime behaviors and generating realistic interaction sequences. Results show up to 13.1% absolute performance gains on CTF benchmarks, with the best model matching state-of-the-art proprietary systems while offering superior cost-effectiveness. Cyber-Zero effectively democratizes the development of cybersecurity agents, making state-of-the-art capabilities more accessible. |
| Computer Vision | ReMoMask: Retrieval-Augmented Masked Motion Generation (Read more on [arXiv](https://arxiv.org/abs/2508.02605) or [HuggingFace](https://huggingface.co/papers/2508.02605))| Hao Tang, Zeyu Zhang, Siheng Wang, Zhengdao Li | The paper introduces ReMoMask, a novel framework for text-to-motion generation that integrates retrieval-augmented generation with masked modeling. The research aims to enhance the realism and semantic alignment of generated human motions from text descriptions. ReMoMask employs a bidirectional momentum text-motion model, a semantic spatiotemporal attention mechanism, and retrieval-augmented classifier-free guidance. Experiments demonstrate state-of-the-art performance, achieving a 3.88% improvement in FID scores on HumanML3D and 10.97% on KIT-ML compared to previous methods, enabling better motion synthesis for AI practitioners. |
| Computer Vision | Artificial Intelligence and Misinformation in Art: Can Vision Language
  Models Judge the Hand or the Machine Behind the Canvas? (Read more on [arXiv](https://arxiv.org/abs/2508.01408) or [HuggingFace](https://huggingface.co/papers/2508.01408))| Elena Merino-Gómez, Pedro Reviriego, Gonzalo Martínez, Javier Conde, Tarian Fu | This paper investigates the ability of Vision Language Models (VLMs) to attribute artworks and detect AI-generated art to prevent misinformation. The study examines VLMs' performance on a dataset of nearly 40,000 paintings, using prompts to assess whether VLMs can correctly identify artists and distinguish between real and AI-generated art. The results reveal that the best performing VLMs, Gemma3-12B and LLaMa3.2-11B, achieve only modest normalized accuracy in artist attribution, and models vary considerably in detecting AI-generated art. The study implies that VLMs have limitations in artist attribution and AI-generated image detection, necessitating caution in their deployment to prevent the spread of incorrect information. |
| Machine Learning | Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine
  Learning (Read more on [arXiv](https://arxiv.org/abs/2508.00024) or [HuggingFace](https://huggingface.co/papers/2508.00024))| Cristian Bosch, Carlos Andrés Durán, Mario Bifulco, Luis Fernando Torres Torres, Sebastián Andrés Cajas Ordóñez | This paper introduces an embedding-aware quantum-classical pipeline for scalable quantum machine learning using Support Vector Machines (SVMs). It aims to address scalability challenges in quantum SVMs by integrating class-balanced k-means distillation with pretrained Vision Transformer (ViT) embeddings. The methodology involves extracting ViT embeddings, reducing dimensionality with PCA, and performing quantum kernel classification via tensor network simulation. Results demonstrate up to 8.02% accuracy improvement over classical SVMs on Fashion-MNIST and 4.42% on MNIST when using ViT embeddings. This work suggests that ViT embeddings uniquely enable quantum advantage and provides a practical pathway for leveraging modern neural architectures in scalable quantum machine learning. |
