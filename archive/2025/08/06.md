

## Papers for 2025-08-06

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed
  Inference (Read more on [arXiv](https://arxiv.org/abs/2508.02193) or [HuggingFace](https://huggingface.co/papers/2508.02193))| Fan Xia, Pengyang Gao, Cheng Luo, Zheng Zhang, Yuxuan Song | The paper introduces Seed Diffusion Preview, a large-scale discrete-state diffusion language model designed for high-speed inference in code generation. It aims to address the latency issues inherent in token-by-token decoding by leveraging non-sequential, parallel generation. The key methodology involves a two-stage curriculum with mask-based and edit-based forward processes, combined with on-policy diffusion learning and block-level parallel sampling. Seed Diffusion Preview achieves an inference speed of 2,146 tokens/second on H20 GPUs while maintaining competitive code evaluation benchmark performance. The model enables faster code generation, making diffusion models more practical for real-world applications, especially where low latency is critical. |
| Multi-Modal | Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding
  and Generation (Read more on [arXiv](https://arxiv.org/abs/2508.03320) or [HuggingFace](https://huggingface.co/papers/2508.03320))| Tianyidan Xie, Liang Hu, Yimeng Gan, Yi Peng, Peiyu Wang | Skywork UniPic is a unified autoregressive model for visual understanding, text-to-image generation, and image editing. The paper investigates whether a single, parameter-efficient architecture can excel at these tasks simultaneously. It uses a decoupled encoding strategy with a MAR encoder for synthesis and a SigLIP2 encoder for understanding, feeding a shared autoregressive decoder. The model achieves a GenEval score of 0.86 and generates 1024x1024 images with under 15 GB GPU memory.  This work implies that high-fidelity multimodal integration can be achieved without prohibitive resource demands, establishing a practical paradigm for deployable multimodal AI. |
| Computer Vision | LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation (Read more on [arXiv](https://arxiv.org/abs/2508.03694) or [HuggingFace](https://huggingface.co/papers/2508.03694))| Chenyang Si, Jianfeng Feng, Xian Liu, Zhaoxi Chen, Jianxiong Gao | LongVie is a novel framework for controllable ultra-long video generation (up to one minute) guided by multimodal control signals. The paper addresses challenges of temporal inconsistency and visual degradation in existing methods by proposing a degradation-aware training strategy and unified noise initialization. The methodology involves an autoregressive approach incorporating both dense (depth) and sparse (keypoints) control signals with global normalization. Evaluated on a newly introduced benchmark, LongVGenBench, LongVie achieves state-of-the-art performance in controllable long video generation according to several benchmark measures. By addressing key limitations in long video generation, this research enables the creation of visually coherent and controllable long videos, useful for video editing and content creation. |
| Natural Language Processing | CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and
  Outcome Reward (Read more on [arXiv](https://arxiv.org/abs/2508.03686) or [HuggingFace](https://huggingface.co/papers/2508.03686))| Songyang Gao, Linchen Xiao, Junnan Liu, Hongwei Liu, Shudong Liu | The paper introduces CompassVerifier, a unified verifier model for evaluating and rewarding large language models (LLMs). It addresses the limitations of current answer verification methods, which lack robustness and generalizability across diverse domains. CompassVerifier is trained using data from VerifierBench, a new benchmark with over one million LLM responses across math, knowledge, and reasoning tasks, and achieves state-of-the-art performance on VerifierBench, improving F1 score by 41.3% over similarly sized general LLMs. The authors demonstrate that CompassVerifier can effectively serve as a reward model in reinforcement learning, providing more precise feedback signals and improving the efficiency of policy optimization. The main implication is an accurate and robust means of evaluation and outcome reward, improving efficiency and precision in LLM training. |
| Reinforcement Learning | CRINN: Contrastive Reinforcement Learning for Approximate Nearest
  Neighbor Search (Read more on [arXiv](https://arxiv.org/abs/2508.02091) or [HuggingFace](https://huggingface.co/papers/2508.02091))| Jiwei Li, Chris Shum, Albert Wang, Xiaofei Sun, Xiaoya Li | The paper introduces CRINN, a contrastive reinforcement learning framework for optimizing approximate nearest neighbor search (ANNS) algorithms. The research explores automated ANNS optimization by leveraging reinforcement learning to progressively generate faster ANNS implementations while maintaining accuracy. CRINN uses execution speed as the reward signal in the RL process, guiding an LLM to generate and evaluate code variants. Experimental results demonstrate CRINN achieves best-in-class performance on three benchmarks (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and ties for first on two others, improving QPS by up to 85.25% on MNIST-784 at 0.999 recall. CRINN validates that RL-augmented LLMs can automate complex algorithmic optimizations, reducing the need for manual tuning. |
| Machine Learning | Tool-integrated Reinforcement Learning for Repo Deep Search (Read more on [arXiv](https://arxiv.org/abs/2508.03012) or [HuggingFace](https://huggingface.co/papers/2508.03012))| Yanzhen Zou, Pengfei Gao, Qunhong Zeng, Chao Peng, Zexiong Ma | This paper introduces ToolTrain, a tool-integrated training framework designed to enhance Large Language Models (LLMs) in issue localization by improving their ability to effectively utilize repository retrieval tools. The primary objective is to tackle the challenge of enabling LLMs to perform more effective multi-hop reasoning with tool calls during issue localization. ToolTrain uses a two-stage approach, combining rejection-sampled supervised fine-tuning and tool-integrated reinforcement learning, to train LLMs to strategically use retrieval tools. Experiments show ToolTrain-trained models achieve state-of-the-art performance, with a 32B model surpassing Claude-3.7 on function-level localization, reaching 68.55 recall@5. The enhanced localization improves end-to-end issue resolution, demonstrating that training for issue localization is a viable strategy for improving automated software development. |
| Machine Learning | Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data
  Synthesis and Self-Correction (Read more on [arXiv](https://arxiv.org/abs/2508.03613) or [HuggingFace](https://huggingface.co/papers/2508.03613))| Jui-Hui Chung, Ziran Yang, Bohan Lyu, Shange Tang, Yong Lin | Goedel-Prover-V2 introduces a series of open-source language models for automated theorem proving. The research aims to scale formal theorem proving by incorporating scaffolded data synthesis and verifier-guided self-correction. The methodology involves generating synthetic tasks, leveraging the Lean compiler for feedback, and model averaging to enhance diversity. Results show the Goedel-Prover-V2-8B model achieves 84.6% pass@32 on MiniF2F, outperforming larger models. This work demonstrates advancements in formal theorem proving without relying on extremely large models or proprietary technology, potentially enabling the community to build more capable AI reasoning systems. |
| Computer Vision | Multi-human Interactive Talking Dataset (Read more on [arXiv](https://arxiv.org/abs/2508.03050) or [HuggingFace](https://huggingface.co/papers/2508.03050))| Mike Zheng Shou, Weijia Wu, Zeyu Zhu | The paper introduces Multi-human Interactive Talking (MIT), a novel dataset for multi-human talking video generation. It addresses the gap in existing datasets that primarily focus on single-person scenarios by providing a large-scale resource with fine-grained annotations of body poses and speech interactions in multi-speaker conversations. The methodology involves an automatic pipeline for data collection and annotation, and a baseline model called CovOG, which integrates a Multi-Human Pose Encoder (MPE) and Interactive Audio Driver (IAD). The CovOG achieves a SSIM of 0.64 on a multi-human test set, demonstrating its ability to generate natural interactive behaviors. MIT provides AI practitioners with a benchmark for developing more realistic and expressive multi-human video generation models. |
| Natural Language Processing | LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools? (Read more on [arXiv](https://arxiv.org/abs/2508.01780) or [HuggingFace](https://huggingface.co/papers/2508.01780))| Yaojie Lu, Xuanang Chen, Jiawei Chen, Wenliang Zhong, Guozhao Mo | LiveMCPBench introduces a benchmark to evaluate language model (LLM) agents in real-world Model Context Protocol (MCP) environments. The research questions center on how agents can effectively plan and retrieve tools from large-scale MCP toolsets and whether agents possess meta-tool-learning capabilities. It proposes LiveMCPTool (70 MCP servers, 527 tools) and LiveMCPEval, an LLM-as-a-Judge framework that attains 81% agreement with human reviewers. The best model, Claude-Sonnet-4, reached a 78.95% success rate, indicating potential for robust meta-tool learning. The study aims to provide a foundation for research on agent capabilities in dynamic and tool-rich MCP settings. |
| Computer Vision | LAMIC: Layout-Aware Multi-Image Composition via Scalability of
  Multimodal Diffusion Transformer (Read more on [arXiv](https://arxiv.org/abs/2508.00477) or [HuggingFace](https://huggingface.co/papers/2508.00477))| Shunyu Yao, Kai Kang, Jianhua Wang, Zehua Ma, Yuzhuo Chen | The paper introduces LAMIC, a training-free framework for layout-aware multi-image composition using diffusion transformers. It aims to extend single-reference diffusion models to multi-reference scenarios with layout control. LAMIC employs Group Isolation Attention (GIA) and Region-Modulated Attention (RMA) to enhance entity disentanglement and layout awareness. Experiments show LAMIC achieves state-of-the-art performance, with an Inclusion Ratio (IN-R) of approximately 90 across various settings, outperforming existing methods. LAMIC provides a new paradigm for controllable multi-image composition by leveraging pre-trained models without task-specific training. |
| Computer Vision | ChartCap: Mitigating Hallucination of Dense Chart Captioning (Read more on [arXiv](https://arxiv.org/abs/2508.03164) or [HuggingFace](https://huggingface.co/papers/2508.03164))| Gunhee Kim, Jaewoo Ahn, Junyoung Lim | The paper introduces ChartCap, a large-scale dataset of 565K real-world chart images with type-specific, dense captions to mitigate hallucination in chart captioning. It addresses the lack of high-quality datasets by generating captions that exclude extraneous information and highlight structural elements and key insights. The methodology involves a four-stage pipeline for caption generation and cycle consistency-based human verification. Experiments show that models fine-tuned on ChartCap achieve improved performance, as evidenced by the fact that the model with ChartCap demonstrates a Visual Consistency Score of 0.8983, surpassing both open-source and proprietary models. ChartCap offers AI practitioners a valuable resource for training VLMs to generate more accurate and informative chart captions with reduced hallucinations. |
| Natural Language Processing | AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided
  Decomposition and Riemannian-Geodesic Collision Regularization (Read more on [arXiv](https://arxiv.org/abs/2508.02079) or [HuggingFace](https://huggingface.co/papers/2508.02079))| Aman Chadha, Vinija Jain, Abhilekh Borah, Amitava Das | The paper introduces AlignGuard-LoRA, a framework for preserving alignment during fine-tuning of LLMs by disentangling parameter updates. It addresses the issue of alignment drift by restricting updates in alignment-sensitive subspaces using Fisher Information Matrix-based regularization and collision-aware constraints. The methodology also includes a task-specific regularization and the DriftCheck benchmark for evaluating alignment drift. Empirical results show that AlignGuard-LoRA mitigates alignment drift by up to 50% on safety-critical benchmarks without significantly degrading downstream task performance. AlignGuard-LoRA offers a structured approach to refining LoRA, ensuring alignment preservation with minimal trade-offs. |
| Natural Language Processing | TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to
  Training-Time Belief Sources in LLMs (Read more on [arXiv](https://arxiv.org/abs/2508.02063) or [HuggingFace](https://huggingface.co/papers/2508.02063))| Aman Chadha, Vinija Jain, Amitava Das | TRACEALIGN is a framework for tracing and mitigating alignment drift in LLMs by attributing failures to training-time belief sources. It aims to understand the underlying causes of unsafe completions, addressing the lack of insight into training-time belief sources. The framework introduces the Belief Conflict Index (BCI) to quantify semantic inconsistency and proposes interventions like TRACESHIELD, Contrastive Belief Deconfliction Loss, and Prov-Decode. Results show up to 85% reduction in alignment drift on a curated benchmark, while preserving utility on standard tasks. TRACEALIGN offers AI practitioners a scalable toolkit for understanding and mitigating alignment failures at the source, fostering more robust and accountable LLMs. |
