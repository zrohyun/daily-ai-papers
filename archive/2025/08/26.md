

## Papers for 2025-08-26

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,
  Reasoning, and Efficiency (Read more on [arXiv](https://arxiv.org/abs/2508.18265) or [HuggingFace](https://huggingface.co/papers/2508.18265))| jinglinglin, WesKwong, MIASANMIA, gulixin0922, Weiyun1025 | InternVL3.5 introduces a family of open-source multimodal models enhancing versatility, reasoning, and efficiency. The research aims to improve MLLM reasoning through a novel Cascade Reinforcement Learning (RL) framework and inference efficiency with Visual Resolution Router (ViR) and Decoupled Vision-Language Deployment (DvD). The Cascade RL, ViR, and DvD techniques achieve up to a +16.0% gain in reasoning performance and 4.05x inference speedup compared to InternVL3. The release provides AI practitioners with efficient and high-performing open-source MLLMs, narrowing the gap with leading commercial models. |
| Computer Vision | Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance
  for Text-to-Image Generation (Read more on [arXiv](https://arxiv.org/abs/2508.18032) or [HuggingFace](https://huggingface.co/papers/2508.18032))| Haoxiang Shi, Bu Pi, Mingyang Han, Peng Chen, Yaqi Li | The paper introduces Visual-CoG, a novel stage-aware reinforcement learning framework for text-to-image generation. It addresses the limitation of monolithic final-only guidance by providing immediate rewards at each stage: semantic reasoning, process refining, and outcome evaluation. The methodology involves designing stage-aware rewards to guide the image generation pipeline. Experimental results on GenEval demonstrate a 15% improvement, indicating superior performance. This stage-aware approach provides practitioners with a more effective method for optimizing text-to-image generation models. |
| Computer Vision | MV-RAG: Retrieval Augmented Multiview Diffusion (Read more on [arXiv](https://arxiv.org/abs/2508.16577) or [HuggingFace](https://huggingface.co/papers/2508.16577))| sagiebenaim, omerbenishu, yosepyossi | The paper introduces MV-RAG, a retrieval-augmented diffusion framework for generating consistent multiview images from text prompts, particularly for out-of-distribution concepts. It aims to address the limitations of existing text-to-3D methods in handling rare or OOD prompts by incorporating relevant 2D images retrieved from a large database. MV-RAG employs a hybrid training strategy using structured multiview data and diverse 2D image collections with a novel held-out view prediction objective to improve 3D consistency. Experiments on a new OOD prompt collection demonstrate significant improvements, achieving a CLIP score of 74.28 compared to baseline models. The key implication is that retrieval-augmented diffusion models can effectively enhance the generation of 3D-consistent content for rare concepts by leveraging external visual information. |
| Computer Vision | T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image
  Generation (Read more on [arXiv](https://arxiv.org/abs/2508.17472) or [HuggingFace](https://huggingface.co/papers/2508.17472))| Xihui Liu, Xian Liu, Chengqi Duan, Rongyao Fang, Kaiyue | The paper introduces T2I-ReasonBench, a benchmark to evaluate reasoning capabilities in text-to-image (T2I) generation models. It aims to assess models' ability to infer implicit meaning, integrate domain knowledge, and resolve contextual ambiguities. The benchmark comprises 800 prompts across four dimensions: Idiom Interpretation, Textual Image Design, Entity-Reasoning, and Scientific-Reasoning. Experiments across 14 T2I models reveal limitations in open-source models, while GPT-Image-1 demonstrates stronger reasoning; HiDream achieves over 50% accuracy using a combined LLM approach. T2I-ReasonBench provides a means for AI practitioners to evaluate and improve reasoning in T2I models, pushing the boundaries beyond literal prompt following. |
| Machine Learning | Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory
  and Test-Time Compute Scaling (Read more on [arXiv](https://arxiv.org/abs/2508.16745) or [HuggingFace](https://huggingface.co/papers/2508.16745))| Daniil Orel, mbur, yurakuratov, b1l4lx1, irodkin | This paper explores how architectural choices impact multi-step reasoning in neural models. The study investigates rule abstraction capabilities by training on one-dimensional Cellular Automata, excluding memorization.  It assesses Transformers, LSTMs, and State Space Models, finding increased model depth crucial for sequential computations. The results show that Adaptive Computation Time (ACT) extends the effective model depth and enhances reasoning. The findings indicate that depth-extension methods, such as recurrence and memory, significantly improve reasoning capabilities in neural networks. |
| Reinforcement Learning | Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement
  Learning for General LLM Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.16949) or [HuggingFace](https://huggingface.co/papers/2508.16949))| Jiale Zhao, Wenkai Fang, Shunyu Liu, Sunzhu Li, BAOLONGZHANSHEN | This paper introduces RuscaRL, a novel reinforcement learning framework to improve general LLM reasoning by addressing the exploration bottleneck. It investigates how rubric-scaffolded reinforcement learning can enhance exploration and exploitation in LLMs. The key methodology involves using checklist-style rubrics as explicit scaffolding for exploration and verifiable rewards for model training. RuscaRL achieves a significant boost, increasing Qwen-2.5-7B-Instruct's performance on HealthBench-500 from 23.6 to 50.3, surpassing GPT-4.1. The implication is that instructional scaffolding can enable more efficient exploration during reinforcement learning, improving LLM performance on complex reasoning tasks. |
| Natural Language Processing | PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent
  LLMs (Read more on [arXiv](https://arxiv.org/abs/2508.17188) or [HuggingFace](https://huggingface.co/papers/2508.17188))| Chenyu You, Yiwei Xu, Xiang Zhang, VitaCoco, HadlayZ | The paper introduces PosterGen, a novel multi-agent framework to automate the creation of aesthetic academic posters from research papers. PosterGen utilizes four specialized LLM agents to extract content, organize layout, apply visual design elements, and render the final poster. The framework incorporates design principles and aesthetic considerations often neglected in automated poster generation. Experimental results show PosterGen consistently matches content fidelity and significantly outperforms existing methods in visual design, with scores improved by 0.17-0.18, producing presentation-ready posters with minimal human refinement. This offers AI practitioners a tool that automatically generates visually appealing and semantically grounded posters, reducing manual effort and promoting effective communication of research findings. |
| Natural Language Processing | UQ: Assessing Language Models on Unsolved Questions (Read more on [arXiv](https://arxiv.org/abs/2508.17580) or [HuggingFace](https://huggingface.co/papers/2508.17580))| Wei Liu, Rui Sun, Zihao Wang, Fan Nie, kzliu | The paper introduces UQ, a novel benchmark for assessing language models on unsolved, real-world questions to address the difficulty-realism tension in AI evaluation. It aims to evaluate language models' capabilities on challenging, diverse questions sourced from Stack Exchange by using validator-assisted screening and community verification. The key methodology involves a pipeline of rule-based filters, LLM judges, human review, and LLM-based validation strategies. The top-performing model passed UQ-validation on only 15% of questions, and human verification has identified correct answers among those. The UQ benchmark provides a path for evaluating frontier models on open-ended challenges that have direct real-world value and push the frontier of human knowledge. |
| Multi-Modal | MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for
  N-level Assessment (Read more on [arXiv](https://arxiv.org/abs/2508.17290) or [HuggingFace](https://huggingface.co/papers/2508.17290))| Doratossadat Dastgheib, Seyed Mohammad Hadi Hosseini, Marzia Nouri, Arshia Hemmat, omidgh | The paper introduces MEENA, a multimodal and multilingual educational exam dataset for evaluating Persian VLMs. It addresses the limited availability of non-English VLM benchmarks by providing approximately 7,500 Persian and 3,000 English questions across diverse educational levels and topics. The key methodology involves curating and translating questions, enriching metadata, and conducting zero-shot, few-shot, and other experiments; Gemini 2.0 Flash surpassed GPT-4 and GPT-40-Mini in image mismatch detection, which demonstrates greater reliability in mitigating hallucinations, especially in Persian. MEENA aims to enhance VLM capabilities beyond English, facilitating more comprehensive and culturally relevant assessments for AI practitioners. |
| Computer Vision | Explain Before You Answer: A Survey on Compositional Visual Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.17298) or [HuggingFace](https://huggingface.co/papers/2508.17298))| Xin Zheng, Zixian Ma, Joy Hsu, Fucai Ke, ControlNet | This survey paper provides a comprehensive review of compositional visual reasoning (CVR) from 2023 to 2025, addressing a gap in dedicated syntheses of this rapidly evolving field. The research objective is to formalize core definitions, describe the advantages of compositional approaches, and trace the paradigm shift in CVR, highlighting the architectural designs, strengths, and limitations of existing methods. The methodology involves a systematic review of 260+ papers and catalogs 60+ benchmarks. The primary result is a structured taxonomy and historical roadmap of CVR, identifying key challenges (e.g., LLM hallucination) and outlining future directions. The main implication for AI practitioners is a foundational reference to inspire the next generation of CVR research, offering a unified taxonomy, historical roadmap, and critical outlook. |
| Natural Language Processing | ST-Raptor: LLM-Powered Semi-Structured Table Question Answering (Read more on [arXiv](https://arxiv.org/abs/2508.18190) or [HuggingFace](https://huggingface.co/papers/2508.18190))| Wei Zhou, Boxiu Li, Xuanhe Zhou, Boyu Niu, Zirui Tang | The paper introduces ST-Raptor, a framework for question answering over semi-structured tables using large language models. It addresses the challenge of understanding complex table layouts by proposing a Hierarchical Orthogonal Tree (HO-Tree) representation and a set of basic tree operations. ST-Raptor decomposes user questions into simpler sub-questions and aligns them with the table structure, incorporating a two-stage verification mechanism for execution correctness and answer reliability. Experiments on a newly created SSTQA dataset of real-world semi-structured tables show that ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. This suggests a more effective approach to automating table analysis, which has wide use in real-world data. |
| Computer Vision | SpotEdit: Evaluating Visually-Guided Image Editing Methods (Read more on [arXiv](https://arxiv.org/abs/2508.18159) or [HuggingFace](https://huggingface.co/papers/2508.18159))| Ersin Yumer, Haitong Tian, Wei-An Lin, Sara Ghazanfari | The paper introduces SpotEdit, a benchmark for evaluating visually-guided image editing methods. It addresses the lack of comprehensive evaluations in this area by providing a diverse dataset of real and synthetic video frames with controlled object variations. The research aims to systematically assess different generative models, including diffusion, autoregressive, and hybrid models, identifying performance disparities.  The benchmark includes a hallucination subset to test model robustness, revealing that even GPT-4o can hallucinate object presence and perform incorrect edits. The strongest open-source model achieves only 0.685 similarity score to ground truth, suggesting that visually guided editing remains fundamentally challenging. |
| Natural Language Processing | German4All - A Dataset and Model for Readability-Controlled Paraphrasing
  in German (Read more on [arXiv](https://arxiv.org/abs/2508.17973) or [HuggingFace](https://huggingface.co/papers/2508.17973))| Cristian-George Craciun, Maximilian Müller, Eslam Nasrallah, Thanh Mai Pham, Miriam Anschütz | The paper introduces German4All, the first large-scale German dataset for readability-controlled paragraph-level paraphrasing. It addresses the lack of resources for multi-level paraphrasing in German by creating a dataset of over 25,000 samples spanning five readability levels using GPT-4 and rigorous evaluation. The primary objective is to enable more nuanced and reader-specific text adaptations. The dataset was used to train a readability-controlled paraphrasing model, achieving state-of-the-art performance in German text simplification, as evaluated by both human and LLM-based judgments. German4All provides AI practitioners with a valuable resource for developing more accessible and tailored text simplification systems in German. |
| Machine Learning | Limitations of Normalization in Attention Mechanism (Read more on [arXiv](https://arxiv.org/abs/2508.17821) or [HuggingFace](https://huggingface.co/papers/2508.17821))| Radu State, Tatiana Petrova, mbur, opensapce | This paper investigates the limitations of normalization, especially softmax, in attention mechanisms. It aims to theoretically and empirically analyze the impact of normalization on token selection, separation, and gradient behavior. The paper derives bounds on token distances and geometric separability under softmax scaling and empirically validates these findings on a pre-trained GPT-2 model. The results demonstrate that as the number of selected tokens increases, the model's ability to distinguish informative tokens declines, and gradient sensitivity presents challenges during training. These findings suggest a need for more robust normalization strategies in attention architectures to avoid capacity limitations. |
| Natural Language Processing | TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language
  Modeling (Read more on [arXiv](https://arxiv.org/abs/2508.16790) or [HuggingFace](https://huggingface.co/papers/2508.16790))| Jiaqi Li, Junan Zhang, Xueyao Zhang, Dekun Chen, Yuancheng Wang | The paper introduces TaDiCodec, a novel text-aware diffusion speech tokenizer for speech language modeling. It aims to overcome limitations in current speech tokenizers, such as dependence on complex architectures and pre-trained models. TaDiCodec employs an end-to-end diffusion autoencoder with text guidance to achieve quantization and reconstruction. The model achieves a 6.25 Hz frame rate (0.0875 kbps) while maintaining performance, reflected in a word error rate (WER) of 2.28 on SeedTTS test-en in zero-shot TTS. TaDiCodec's efficient design allows for simplified pipelines and reduces the reconstruction-generation gap, offering improved compression and downstream task performance for AI practitioners. |
| Natural Language Processing | Neither Valid nor Reliable? Investigating the Use of LLMs as Judges (Read more on [arXiv](https://arxiv.org/abs/2508.18076) or [HuggingFace](https://huggingface.co/papers/2508.18076))| Golnoosh Farnadi, Jackie Chi Kit Cheung, Mohammed Haddou, Khaoula Chehbouni | This paper critically examines the validity of using Large Language Models (LLMs) as judges in Natural Language Generation (NLG) evaluation by assessing underlying assumptions like human judgment proxy, evaluation capability, scalability, and cost-effectiveness. The study leverages measurement theory from social sciences to identify limitations of LLMs in these roles, exploring applications in text summarization, data annotation, and safety alignment. The paper argues that enthusiasm for LLMs as judges is premature, as evidenced by underexplored validity. The authors highlight a need for robust standards in evaluation practices to ensure LLMs support rather than undermine progress in NLG. The quantitative metrics mentioned are left vague, focusing on the need for more rigorous assessment of reliability and validity. |
