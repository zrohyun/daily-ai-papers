

## Papers for 2025-08-21

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating
  Financial Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.13491) or [HuggingFace](https://huggingface.co/papers/2508.13491))| Ziyan Kuang, Effoula, QianqianXie1994, hugai101, 2083L | The paper introduces FinCDM, a cognitive diagnosis framework for evaluating financial Large Language Models (LLMs). It aims to address the inadequacies of existing benchmarks by assessing LLMs at the knowledge-skill level rather than relying on score-level evaluation. The methodology involves constructing CPA-QKA, a cognitively informed financial evaluation dataset derived from the Certified Public Accountant (CPA) examination, and applying a non-negative matrix co-factorization based CDM model. Experiments on 30 LLMs reveal significant differences in models' mastery of financial knowledge, with a Krippendorff's alpha of 0.937 for annotation quality on the CPA-KQA dataset. FinCDM enables interpretable, skill-aware diagnosis to support more trustworthy and targeted model development. |
| Machine Learning | FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction (Read more on [arXiv](https://arxiv.org/abs/2508.11987) or [HuggingFace](https://huggingface.co/papers/2508.11987))| tianlecai, Nuori, YinLingyue, Tianci-He, liujiashuo77 | The paper introduces FutureX, a dynamic and live benchmark for evaluating LLM agents in future prediction tasks. The research aims to address the lack of large-scale benchmarks for assessing agent performance in synthesizing real-time data and making predictions under uncertainty. FutureX supports real-time daily updates and eliminates data contamination using an automated pipeline for question gathering and answer collection. Evaluations of 25 LLM/agent models revealed that models with reasoning and search capabilities, such as Grok-4, achieved the highest overall performance. The introduction of FutureX will enable the development of LLM agents capable of performing at the level of professional human analysts in complex reasoning and predictive thinking. |
| Machine Learning | DuPO: Enabling Reliable LLM Self-Verification via Dual Preference
  Optimization (Read more on [arXiv](https://arxiv.org/abs/2508.14460) or [HuggingFace](https://huggingface.co/papers/2508.14460))| Yu Lu, Yu Bao, Shanbo, ShujianHuang, kevinpro | The paper introduces DuPO, a dual learning-based preference optimization framework for enhancing LLM performance via self-verification. It addresses limitations of RLVR and traditional dual learning by decomposing a primal task's input into known and unknown components to construct a dual task for self-supervised reward generation. The framework enhances average translation quality by 2.13 COMET over 756 directions and boosts mathematical reasoning accuracy by an average of 6.4 points across three benchmarks. DuPO's annotation-free paradigm offers a scalable approach to LLM optimization, improving both training and inference. This provides a more generalizable method for LLM enhancement. |
| Computer Vision | MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds (Read more on [arXiv](https://arxiv.org/abs/2508.14879) or [HuggingFace](https://huggingface.co/papers/2508.14879))| Jiangmiao, ZhaoyangLyu, asrnline, Qmh, tangqh | The paper introduces MeshCoder, a framework for reconstructing complex 3D objects from point clouds into editable Blender Python scripts. It addresses the challenge of generating intricate 3D geometries programmatically by using a comprehensive set of Blender Python APIs and a large-scale paired object-code dataset. The key methodology involves training a multimodal large language model (LLM) to translate 3D point clouds into executable Blender Python scripts, achieving part-segmented meshes. Experimentally, MeshCoder demonstrated superior shape-to-code reconstruction, obtaining 86.75% IoU overall. This approach facilitates intuitive geometric and topological editing, improving the reasoning capabilities of LLMs in 3D shape understanding. |
| Computer Vision | Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From
  Sparse Inputs without Per-Scene Optimization (Read more on [arXiv](https://arxiv.org/abs/2508.14811) or [HuggingFace](https://huggingface.co/papers/2508.14811))| Hao Chen, Zhiyue Zhao, Tianjian Feng, Xiaoman Li, Canyu | The paper introduces TINKER, a novel framework for high-fidelity 3D editing from sparse inputs without per-scene optimization. It addresses the challenge of multi-view consistent 3D editing by repurposing pre-trained diffusion models. TINKER employs a referring multi-view editor and an any-view-to-video synthesizer leveraging spatial-temporal priors. Experiments show state-of-the-art performance in editing, novel-view synthesis, and rendering enhancement, achieving aesthetic scores of 6.338 in few-shot settings. TINKER lowers the barrier to generalizable 3D content creation, providing a scalable solution for 3D editing. |
| Machine Learning | From AI for Science to Agentic Science: A Survey on Autonomous
  Scientific Discovery (Read more on [arXiv](https://arxiv.org/abs/2508.14111) or [HuggingFace](https://huggingface.co/papers/2508.14111))| zijieqiu, Wanggsh, schrodingers-tiger, ZhangyangGao, VitaCoco | This survey paper provides a domain-oriented review of autonomous scientific discovery, positioning Agentic Science as a structured paradigm for advancing AI-driven research. It aims to unify perspectives on process, autonomy, and mechanisms in AI for Science. The methodology includes a framework connecting foundational capabilities, core processes, and domain-specific realizations across life sciences, chemistry, materials, and physics. It identifies five core capabilities and models discovery as a four-stage workflow, although quantitative metrics of performance are not specified. The survey establishes a structured paradigm for advancing AI-driven research, providing a synthesis of autonomous scientific discovery for AI practitioners. |
| Natural Language Processing | Quantization Meets dLLMs: A Systematic Study of Post-training
  Quantization for Diffusion LLMs (Read more on [arXiv](https://arxiv.org/abs/2508.14896) or [HuggingFace](https://huggingface.co/papers/2508.14896))| Haobo Xu, cityug7353, ZiyuG, chriswyc, Felix1023 | This paper presents a systematic study on post-training quantization (PTQ) for diffusion large language models (dLLMs). The research investigates the applicability of PTQ techniques for compressing dLLMs, addressing challenges related to massive parameter scales and high resource demands. The authors implement state-of-the-art PTQ methods and evaluate them across various tasks and model variants, finding that 4-bit weight-only quantization is effective. Through empirical evaluation, the study identifies effective quantization strategies and shows that instruction-tuned models exhibit greater robustness to quantization compared to base models, providing guidance for efficient dLLM deployment. |
| Computer Vision | RynnEC: Bringing MLLMs into Embodied World (Read more on [arXiv](https://arxiv.org/abs/2508.14160) or [HuggingFace](https://huggingface.co/papers/2508.14160))| jiangpinliu, CausalLi, maoyunxuan, CircleRadon, RH-Dang | The paper introduces RynnEC, a video multimodal large language model for embodied cognition, enhancing robotic understanding of physical environments. It addresses the scarcity of annotated 3D datasets by proposing an egocentric video-based pipeline for generating embodied cognition data and introduces RynnEC-Bench for evaluating embodied cognitive capabilities. RynnEC incorporates a region encoder and mask decoder, achieving state-of-the-art performance in object property understanding, segmentation, and spatial reasoning. Experiments on RynnEC-Bench demonstrate a significant performance improvement over existing MLLMs, with overall results of 56.2 for RynnEC-7B versus 45.5 for Gemini-2.5 Pro. RynnEC facilitates the development of cognitive cores for embodied agents, improving generalization across diverse embodied tasks. |
| Natural Language Processing | NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid
  Mamba-Transformer Reasoning Model (Read more on [arXiv](https://arxiv.org/abs/2508.14444) or [HuggingFace](https://huggingface.co/papers/2508.14444))| abercovich, aditya-malte, adirendu, aklife97, apaithan | The paper introduces Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed for efficient reasoning. The research aims to enhance throughput for reasoning tasks while maintaining high accuracy compared to similarly sized models. It employs a Mamba-Transformer architecture, FP8 training, Minitron compression, and distillation, enabling 128k context inference on a single NVIDIA A10G GPU. Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks with up to 6x higher inference throughput (e.g., 8k input/16k output tokens). This work provides AI practitioners with a more efficient language model for reasoning workloads in resource-constrained environments. |
| Multi-Modal | ViExam: Are Vision Language Models Better than Humans on Vietnamese
  Multimodal Exam Questions? (Read more on [arXiv](https://arxiv.org/abs/2508.13680) or [HuggingFace](https://huggingface.co/papers/2508.13680))| Daeyoung Kim, Duc Dm, Quang Tau, anvo25, tuongvy2603 | The paper introduces ViExam, a new benchmark for evaluating vision-language models (VLMs) on Vietnamese multimodal educational assessments. The research investigates the ability of VLMs, primarily trained on English data, to handle cross-lingual multimodal reasoning in real-world scenarios. ViExam contains 2,548 multimodal questions across seven academic domains, including Mathematics, Physics, and IQ Tests. Results show that state-of-the-art VLMs achieve only 57.74% mean accuracy, significantly underperforming average human test-takers (66.54%), suggesting a substantial gap in cross-lingual multimodal understanding. The findings highlight the need for VLMs with improved capabilities in handling low-resource languages and complex visual reasoning for genuinely multimodal educational content. |
| Reinforcement Learning | On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised
  Fine-Tuning and Reinforcement Learning via Dynamic Weighting (Read more on [arXiv](https://arxiv.org/abs/2508.11408) or [HuggingFace](https://huggingface.co/papers/2508.11408))| Guoyin Wang, Yanxi Chen, Yuchang Sun, Yuexiang Xie, xiaoniqiu | The paper introduces CHORD, a framework harmonizing Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) by dynamically weighting off-policy expert data within on-policy RL. It investigates the dynamics of integrating SFT and RL, identifying a "shift-readapt-overfit" phenomenon. CHORD employs a global coefficient and token-wise weighting to control the influence of expert data and maintain stable training. Experiments demonstrate that CHORD outperforms existing methods, achieving better performance by flexibly integrating expert data and maintaining model exploration, improving accuracy on MATH benchmarks. CHORD provides practitioners with a more controlled way to leverage expert demonstrations in RL without disrupting existing reasoning capabilities. |
| Computer Vision | Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer (Read more on [arXiv](https://arxiv.org/abs/2508.14187) or [HuggingFace](https://huggingface.co/papers/2508.14187))| Jeremiah Jiang, Lim Jun Hao, Michael N. Cheng, Chiao-An Yang, ashiq24 | This paper introduces a deep equilibrium canonicalizer (DEC) to improve local scale equivariance in deep learning models. The research aims to address the challenge of handling local scale variations within images, where different objects change size independently. The methodology involves incorporating DEC into the latent space of existing deep-net architectures to achieve monotone scale equivariance. Experimental results on ImageNet demonstrate that DEC improves model performance and local scale consistency, for example, achieving a higher top-1 accuracy compared to baseline models. This implies that AI practitioners can enhance the robustness of vision models to local scale changes by integrating DEC into the feature processing pipeline. |
| Natural Language Processing | mSCoRe: a Multilingual and Scalable Benchmark for Skill-based
  Commonsense Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.10137) or [HuggingFace](https://huggingface.co/papers/2508.10137))| anoperson, Franck-Dernoncourt, ntnghia1811 | This paper introduces mSCoRe, a novel multilingual benchmark for evaluating skill-based commonsense reasoning in LLMs. The research aims to comprehensively assess LLMs' reasoning capabilities across diverse languages and cultural contexts with a focus on analyzing reasoning skills. The methodology involves a novel taxonomy of reasoning skills, a robust data synthesis pipeline, and a complexity scaling framework. Experiments on eight LLMs reveal mSCoRe remains challenging, particularly at higher complexity, with GPT-40 achieving 80.5% accuracy at level 0. mSCoRe offers AI practitioners a platform for evaluating and improving multilingual commonsense reasoning in LLMs by assessing their reasoning skills in a granular manner. |
| Multi-Modal | Refining Contrastive Learning and Homography Relations for Multi-Modal
  Recommendation (Read more on [arXiv](https://arxiv.org/abs/2508.13745) or [HuggingFace](https://huggingface.co/papers/2508.13745))| Shiqing Wu, Yawen Zeng, guandongxu, MrShouxingMa | The paper introduces REARM, a novel framework for multi-modal recommendation addressing data sparsity challenges. It aims to improve recommendation performance by refining multi-modal contrastive learning and exploring homography relations between user interests and item co-occurrence. The methodology involves meta-network and orthogonal constraint strategies to filter noise, integrated with user interest and item co-occurrence graphs for enhanced graph learning. Experiments on three datasets demonstrate REARM's superiority, achieving, for example, a Recall@20 of 0.1105 on the Baby dataset, indicating it effectively distinguishes modal-shared and modal-unique features. REARM provides AI practitioners with an enhanced approach for leveraging multi-modal data in recommender systems, particularly in scenarios with sparse user-item interactions. |
