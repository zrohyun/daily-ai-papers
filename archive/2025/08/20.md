

## Papers for 2025-08-20

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Machine Learning | Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent
  Distillation and Agentic RL (Read more on [arXiv](https://arxiv.org/abs/2508.13167) or [HuggingFace](https://huggingface.co/papers/2508.13167))| Liam-Liu, hugteste, kangz, wanwan1212, tianyue818 | The paper introduces Chain-of-Agents (CoA), a novel paradigm for LLM reasoning that enables end-to-end complex problem-solving akin to multi-agent systems within a single model. It addresses the limitations of existing multi-agent systems, such as computational inefficiency and lack of data-centric learning, by proposing a method that dynamically activates different tool agents and role-playing agents to simulate multi-agent collaboration. The key methodology involves a multi-agent distillation framework to distill state-of-the-art multi-agent systems into chain-of-agents trajectories for agentic supervised fine-tuning, followed by agentic reinforcement learning. The resulting Agent Foundation Models (AFMs) achieve new state-of-the-art performance on diverse benchmarks, including 55.3% Pass@1 on GAIA. The CoA paradigm and associated training framework offer a more efficient and potentially more capable approach to complex problem-solving, with potential for improved data-centric learning and wider applicability in AI research and practice. |
| Computer Vision | LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos (Read more on [arXiv](https://arxiv.org/abs/2508.14041) or [HuggingFace](https://huggingface.co/papers/2508.14041))| Yen-Yu Lin, Fu-En Yang, Cheng Sun, cmhungsteve, linjohnss | LongSplat introduces a novel approach for 3D Gaussian Splatting (3DGS) from casually captured long videos without known camera poses. The research addresses the problem of pose drift and memory limitations in long video sequences by jointly optimizing camera poses and 3D Gaussians. LongSplat employs incremental joint optimization, a learned 3D prior for robust pose estimation, and an octree anchor formation mechanism for efficient memory usage. Experiments on the Free dataset demonstrate a PSNR of 27.88 dB, outperforming existing methods in rendering quality and pose accuracy. This enables more robust and efficient novel view synthesis from unconstrained video data for AR/VR and digital mapping applications. |
| Natural Language Processing | Prompt Orchestration Markup Language (Read more on [arXiv](https://arxiv.org/abs/2508.13948) or [HuggingFace](https://huggingface.co/papers/2508.13948))| Yuqing Yang, Nan Chen, Yuge Zhang, Jiahang | The paper introduces Prompt Orchestration Markup Language (POML), a novel markup language designed to bring structure, maintainability, and versatility to advanced prompt engineering. It addresses challenges in structure, data integration, format sensitivity, and tooling by employing component-based markup, specialized tags, and a CSS-like styling system. POML is validated through two case studies, demonstrating its impact on complex application integration (PomLink) and accuracy performance (TableQA). The studies show POML improved performance by 9x in certain circumstances, and that it can enhance real-world development scenarios with up to 2 days saved in initial project work. The main implication for AI practitioners is that POML provides a potentially more efficient and effective method for organizing and managing complex prompts, leading to improved application integration and model performance. |
| Computer Vision | MultiRef: Controllable Image Generation with Multiple Visual References (Read more on [arXiv](https://arxiv.org/abs/2508.06905) or [HuggingFace](https://huggingface.co/papers/2508.06905))| Shiyun Lang, Siyuan Wu, Dongping Chen, Ruoxi Chen, wsnHowest | The paper introduces MultiRef-Bench, a new benchmark for controllable image generation using multiple visual references. It addresses the limitations of existing frameworks that rely on single-source inputs by rigorously evaluating models on synthetic and real-world tasks requiring the integration of multiple visual cues. The key methodology involves a data engine called REFBLEND, which generates diverse training samples by combining various reference modalities and automatically obtaining ground truth pairings. Experiments across interleaved image-text models and agentic frameworks show that the best model (OmniGen) achieves only 66.6% on synthetic and 79.0% on real samples compared to the golden answer. This highlights a weakness in current systems and provides direction for future research toward more flexible and human-like creative tools. |
| Natural Language Processing | Mind the Generation Process: Fine-Grained Confidence Estimation During
  LLM Generation (Read more on [arXiv](https://arxiv.org/abs/2508.12040) or [HuggingFace](https://huggingface.co/papers/2508.12040))| Xinyi Wang, Jie Shi, Shisong Chen, Tingyun Li, JinyiHan | This paper introduces FineCE, a novel method for fine-grained confidence estimation during large language model (LLM) generation. The research aims to provide accurate and continuous confidence scores throughout text generation, addressing the limitations of coarse-grained existing methods. The methodology involves constructing a training dataset capturing LLM response distributions using Monte Carlo sampling and proposing a Backward Confidence Integration (BCI) strategy for refining confidence estimates. Experiments demonstrate that FineCE achieves AUROC scores exceeding 70% and ECE of 5.1% on GSM8K, outperforming existing methods. The primary implication is that FineCE enables more reliable and trustworthy LLM-generated outputs by providing fine-grained confidence signals for output evaluation and self-correction. |
| Computer Vision | Training-Free Text-Guided Color Editing with Multi-Modal Diffusion
  Transformer (Read more on [arXiv](https://arxiv.org/abs/2508.09131) or [HuggingFace](https://huggingface.co/papers/2508.09131))| Deyu Zhou, Xili Dai, dorni, EvanTHU, zachary-yin | The paper introduces ColorCtrl, a training-free method for text-guided color editing of images and videos. It addresses the challenge of fine-grained color manipulation while preserving physical consistency, including geometry, material properties, and light-matter interactions. ColorCtrl leverages the attention mechanisms of Multi-Modal Diffusion Transformers (MM-DiT) to disentangle structure and color, enabling accurate and consistent color editing with word-level control of attribute intensity. Experiments on SD3 and FLUX.1-dev demonstrate that ColorCtrl outperforms existing training-free approaches, achieving state-of-the-art performance in edit quality and consistency, with PSNR increasing by 6-7dB. The method offers AI practitioners a versatile approach to high-fidelity, controllable color editing without the need for training or manual parameter tuning. |
| Machine Learning | Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge (Read more on [arXiv](https://arxiv.org/abs/2508.08777) or [HuggingFace](https://huggingface.co/papers/2508.08777))| Alice Wang, Edoardo D'Amico, Gustavo Penha, marcodena, frafabbri | This paper introduces a profile-aware LLM-as-a-Judge framework for evaluating personalized podcast recommendations. The research aims to provide a scalable and interpretable method for assessing recommendation quality by using LLMs to judge the alignment between user profiles and recommended episodes. The approach involves generating natural-language user profiles from listening histories and prompting the LLM with these profiles and episode metadata for pointwise and pairwise judgments. Experiments with 47 participants demonstrate that the profile-aware judge achieves a ROC-AUC of 0.6442 and outperforms a variant using raw listening histories. The framework enables efficient and profile-aware evaluation for iterative testing and model selection in recommender systems, particularly beneficial for pre-deployment settings. |
| Computer Vision | OmniTry: Virtual Try-On Anything without Masks (Read more on [arXiv](https://arxiv.org/abs/2508.13632) or [HuggingFace](https://huggingface.co/papers/2508.13632))| Xiaoduan Feng, Yiming Chen, Hengyuan Cao, Linlin Zhang, fengyutong | OmniTry is presented as a unified mask-free virtual try-on framework extending beyond garments to encompass any wearable object. The paper addresses the challenge of paired image data scarcity by proposing a two-stage pipeline involving unpaired image pre-training and paired image fine-tuning.  Initially, an inpainting model is repurposed for mask-free object localization using unpaired portraits and MLLM object description. The model is then fine-tuned with paired images to ensure consistency of object appearance; achieving, for example, 0.8327 M-CLIP-I on the whole test dataset.  This approach offers AI practitioners a data-efficient method for virtual try-on across diverse object types. |
| Natural Language Processing | A Stitch in Time Saves Nine: Proactive Self-Refinement for Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2508.12903) or [HuggingFace](https://huggingface.co/papers/2508.12903))| Zishang Jiang, Tingyun li, Haiquan Zhao, Xinyi Wang, JinyiHan | This paper introduces ProActive Self-Refinement (PASR), a novel method for enabling language models to proactively refine their outputs during the generation process. The research objective is to enhance LLMs' ability to dynamically determine when and how to refine based on the evolving generation context. PASR leverages reinforcement learning to train LLMs, using a proxy evaluation strategy comparing refinements relative to standard outputs. Experiments on ten tasks show PASR reduces token consumption by 41.6% on Qwen3-8B while improving accuracy by 8.2%. The main implication is a more efficient and accurate approach to self-refinement in LLMs, potentially reducing computational costs and improving performance across various applications. |
| Machine Learning | Advances in Speech Separation: Techniques, Challenges, and Future Trends (Read more on [arXiv](https://arxiv.org/abs/2508.10830) or [HuggingFace](https://huggingface.co/papers/2508.10830))| Zhuo Chen, Yi Luo, Wendi Sang, Guo Chen, JusperLee | This survey paper comprehensively examines deep neural network-based speech separation techniques, addressing the "cocktail party problem." The main objective is to provide a systematic overview of different learning paradigms and architectures, including supervised, self-supervised, and unsupervised methods. The methodology involves a systematic investigation of learning paradigms, architectural components, and rigorous, reproducible benchmarking of representative models. Quantitative evaluations on standard datasets, such as WSJ0-2mix, demonstrate the performance improvements of recent models. The survey provides a unified reference for both experienced researchers and newcomers navigating the complex landscape of speech separation, clarifying current capabilities and limitations. |
| Multi-Modal | Embodied-R1: Reinforced Embodied Reasoning for General Robotic
  Manipulation (Read more on [arXiv](https://arxiv.org/abs/2508.13998) or [HuggingFace](https://huggingface.co/papers/2508.13998))| Fei Ni, Yibin Chen, Yaoting Huang, Haiqin Cui, Yifu Yuan | The paper introduces Embodied-R1, a vision-language model (VLM) for robotic manipulation that bridges the perception-action gap via reinforced embodied reasoning. Its primary objective is to address generalization challenges stemming from data scarcity and embodiment heterogeneity by using "pointing" as a unified, embodiment-agnostic intermediate representation. The key methodology involves a two-stage Reinforced Fine-tuning (RFT) curriculum using a large-scale dataset, Embodied-Points-200K, and specialized multi-task reward design. Embodied-R1 achieves a 56.2% success rate in the SIMPLEREnv and 87.5% success across 8 real-world XArm tasks, demonstrating robust zero-shot generalization. These results indicate that a pointing-centric representation and RFT training offers an effective pathway for closing the perception-action gap, offering a potentially valuable approach for robotic control and embodied AI systems. |
| Natural Language Processing | Copyright Protection for Large Language Models: A Survey of Methods,
  Challenges, and Trends (Read more on [arXiv](https://arxiv.org/abs/2508.11548) or [HuggingFace](https://huggingface.co/papers/2508.11548))| Xixiang Zhao, Qichen Liu, Xubin Yue, Zhenhua Xu, BreynaldDva | This survey comprehensively reviews copyright protection techniques for large language models (LLMs). It focuses on model fingerprinting, clarifying concepts and categorizing techniques, including text watermarking as fingerprinting. The survey analyzes fingerprint transferability and removal, proposing evaluation metrics like effectiveness, harmlessness, and robustness. It consolidates knowledge, identifies open challenges, and offers a structured research agenda, however, it provides no quantitative results on model performance. The work enables AI practitioners to find methods for robustly ensuring intellectual property protection of LLMs. |
| Computer Vision | TempFlow-GRPO: When Timing Matters for GRPO in Flow Models (Read more on [arXiv](https://arxiv.org/abs/2508.04324) or [HuggingFace](https://huggingface.co/papers/2508.04324))| Jian Yang, Wanli Li, Yuke Zhao, Siming Fu, shreddedpork | The paper introduces TempFlow-GRPO, a novel reinforcement learning framework for text-to-image generation using flow matching models. It addresses the temporal uniformity assumption in existing GRPO methods, which hinders fine-grained reward-based optimization. TempFlow-GRPO incorporates trajectory branching for precise credit assignment and noise-aware weighting to modulate policy optimization based on timestep noise levels. The framework demonstrates state-of-the-art performance on text-to-image benchmarks, achieving an overall Geneval score of 0.97 compared to Flow-GRPO's 0.90. This method enables more effective temporal control and optimization, leading to improved sample quality and preference alignment for AI practitioners using flow-based generative models. |
| Natural Language Processing | Leveraging Large Language Models for Predictive Analysis of Human Misery (Read more on [arXiv](https://arxiv.org/abs/2508.12669) or [HuggingFace](https://huggingface.co/papers/2508.12669))| Abhilash Nandy, Aman Bansal, Rahul Seetharaman, Bishanka Seal | The paper explores the use of Large Language Models (LLMs) for predicting human-perceived misery scores from text. It investigates various prompting strategies to improve the accuracy of misery score predictions. The study benchmarks LLM performance under different prompting regimes, including a novel gamified interactive environment for feedback-driven reasoning. Few-shot prompting with semantically coherent context substantially improves prediction accuracy compared to zero-shot baselines. The study indicates potential for LLMs in dynamic emotional reasoning tasks, achieving a Mean Absolute Error (MAE) of 12.30 with embedding-based prompting. |
| Computer Vision | Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence (Read more on [arXiv](https://arxiv.org/abs/2508.13139) or [HuggingFace](https://huggingface.co/papers/2508.13139))| Xin Chen, Zhiyang Dou, Zixin Yin, Yuhong Zhang, Ling-Hao Chen | The paper introduces Motion2Motion, a novel training-free framework for motion transfer between characters with substantially different skeletal topologies. It addresses the challenge of topological inconsistency by accessing a sparse set of bone correspondences and few target motion examples to transfer animation. Motion2Motion formulates the transfer as conditional patch-based motion matching, synthesizing target motions by blending matched motion patches from example animations. Experiments demonstrate effective and reliable performance, achieving superior diversity and comparable FPS to existing methods without training. Results show improved FID scores and temporal coherence in both similar-skeleton and cross-species transfer scenarios, facilitating industrial applications by offering a practical tool for motion adaptation. |
| Natural Language Processing | CorrSteer: Steering Improves Task Performance and Safety in LLMs through
  Correlation-based Sparse Autoencoder Feature Selection (Read more on [arXiv](https://arxiv.org/abs/2508.12535) or [HuggingFace](https://huggingface.co/papers/2508.12535))| Adriano Koshiyama, Zekun Wu, seonglae | The paper introduces CorrSteer, a method for improving task performance and safety in LLMs by selecting sparse autoencoder features based on their correlation with task outcomes. The research aims to address limitations of existing SAE-based steering approaches, such as the need for contrastive datasets or large activation storage. CorrSteer selects features and computes steering coefficients using only inference-time activations, thus avoiding spurious correlations. Experiments on Gemma 2 2B and LLaMA 3.1 8B show improved performance on QA, bias mitigation, jailbreaking prevention, and reasoning benchmarks, with a +4.1% improvement in MMLU. CorrSteer provides an effective and scalable approach for automated SAE steering across various LLM applications. |
| Computer Vision | MedSAMix: A Training-Free Model Merging Approach for Medical Image
  Segmentation (Read more on [arXiv](https://arxiv.org/abs/2508.11032) or [HuggingFace](https://huggingface.co/papers/2508.11032))| Jonas Geiping, Francesco Sammarco, Jiesi Hu, guinansu, podismine | The paper introduces MedSAMix, a training-free model merging approach for medical image segmentation, aiming to enhance domain-specific capabilities while retaining generalization. It employs a zero-order optimization to automatically discover optimal layer-wise merging solutions between generalist (SAM) and specialist (MedSAM) models. Evaluations on 25 medical segmentation tasks demonstrate improvements in both domain-specific accuracy and generalization, achieving a 6.67% improvement on specialized tasks. MedSAMix mitigates model bias and enhances performance without retraining, providing a practical strategy for leveraging diverse models in scenarios with data privacy constraints. |
| Machine Learning | Semantic IDs for Joint Generative Search and Recommendation (Read more on [arXiv](https://arxiv.org/abs/2508.10478) or [HuggingFace](https://huggingface.co/papers/2508.10478))| Enrico Palumbo, Edoardo D'Amico, Gustavo Penha, frafabbri, marcodena | This paper explores Semantic IDs for joint generative search and recommendation tasks using Large Language Models. The research investigates how to construct Semantic IDs that perform well in both search and recommendation within a unified model, comparing task-specific and cross-task embedding approaches. Results indicate that a bi-encoder model fine-tuned on both search and recommendation offers a strong trade-off, generalizing across tasks with minimal performance loss; specifically, Multi-task approach achieves R@30 of 0.046 and 0.135 for Search and Recommendation respectively. This implies that shared representation spaces via Semantic IDs can streamline generative model design without sacrificing quality in multi-task recommender systems. |
| Multi-Modal | Describe What You See with Multimodal Large Language Models to Enhance
  Video Recommendations (Read more on [arXiv](https://arxiv.org/abs/2508.09789) or [HuggingFace](https://huggingface.co/papers/2508.09789))| Mounia Lalmas, Andreas Damianou, marcodena | This paper introduces a framework for enhancing video recommendations by using Multimodal Large Language Models (MLLMs) to generate semantically rich captions. The research aims to determine if MLLM-derived captions outperform conventional content features in standard ranking tasks. The methodology involves prompting open-weight MLLMs to summarize each clip, concatenating video and audio captions, and feeding the resulting text to standard recommenders. Experiments on the MicroLens-100K dataset demonstrate that MLLM-derived features significantly outperform traditional baselines, with a ~60% relative gain in HR@10 for the two-towers model when replacing audio features with MLLM-generated text. The results suggest that MLLMs can serve as on-the-fly knowledge extractors to build more intent-aware video recommenders. |
| Computer Vision | Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned
  and Addressed for XR Research (Read more on [arXiv](https://arxiv.org/abs/2508.04326) or [HuggingFace](https://huggingface.co/papers/2508.04326))| Susanne Schmidt, Mana Masuda, Mugichoko445, cocolinux | This survey paper examines the use of radiance fields (RF) in XR research, focusing on how they are envisioned versus how they are actually implemented. The study performs a systematic review of 365 RF-related papers, categorizing them based on their mention or detailed addressing of XR applications to analyze the gap between anticipated interests and practical implementations. The analysis identifies nine key research themes, noting that the XR community focuses on XR-Addressed papers whereas other communities mention XR as motivation; only 66 papers addressed a detailed aspect of RF research for XR. This study provides guidance for XR researchers seeking to navigate the rapid developments in RF and apply them effectively in XR settings, though the specifics of the quantitative metrics are uncertain. |
| Multi-Modal | MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic
  Evaluation of Audio General Intelligence (Read more on [arXiv](https://arxiv.org/abs/2508.13992) or [HuggingFace](https://huggingface.co/papers/2508.13992))| Fernando López, Vaibhavi Lokegaonkar, Šimon Sedláček, Sonal Kumar, Sreyan88 | The paper introduces MMAU-Pro, a comprehensive benchmark designed to evaluate audio general intelligence in AI systems. It aims to address the limitations of existing benchmarks in assessing holistic audio understanding by incorporating diverse and complex audio tasks. The benchmark consists of 5,305 expert-annotated instances spanning 49 unique skills across speech, sounds, music, and their mixtures. Evaluation of 22 leading multimodal AI models on MMAU-Pro reveals significant limitations, with Gemini 2.5 Flash achieving only 59.2% accuracy, highlighting the challenges in achieving human-level audio intelligence. The benchmark and analysis provide actionable insights for enhancing future AI systems' progression toward audio general intelligence. |
| Multi-Modal | MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents (Read more on [arXiv](https://arxiv.org/abs/2508.13186) or [HuggingFace](https://huggingface.co/papers/2508.13186))| Jun Dong, Jiaheng Liu, Wenjie Wang, Xingyuan Bu, Shilong Li | MM-BrowseComp is a novel benchmark designed to evaluate the multimodal retrieval and reasoning capabilities of AI browsing agents. The benchmark aims to address the limitations of existing benchmarks by incorporating images in prompts and embedding crucial information within images or videos on webpages. The methodology involves 224 hand-crafted questions with verified checklists, enabling fine-grained analysis of multimodal dependencies and reasoning paths. Evaluation of state-of-the-art models reveals that even top models like OpenAI 03 achieve only 29.02% accuracy, highlighting suboptimal multimodal capabilities. The main implication is the need for developing more robust AI agents with native multimodal reasoning abilities. |
| Natural Language Processing | ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval
  Driven LLM Agents (Read more on [arXiv](https://arxiv.org/abs/2508.04038) or [HuggingFace](https://huggingface.co/papers/2508.04038))| Flora D. Salim, Hao Xue, Breezelled, zechenli03 | The paper presents ZARA, a novel agent-based framework for zero-shot human activity recognition (HAR) directly from raw motion time-series. ZARA aims to address the limitations of existing HAR methods, such as poor generalization, limited zero-shot capability, and lack of interpretability, by integrating a feature knowledge base, a multi-sensor retrieval module, and a hierarchical agent pipeline. The methodology guides a large language model (LLM) to iteratively select features, draw on evidence, and produce both activity predictions and natural-language explanations. Experiments on 8 HAR benchmarks demonstrate that ZARA achieves state-of-the-art zero-shot performance, exceeding the strongest baselines by 2.53x in macro F1. This approach offers AI practitioners a more flexible and interpretable solution for motion time-series analysis without task-specific classifiers or fine-tuning. |
| Natural Language Processing | Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values
  Understanding (Read more on [arXiv](https://arxiv.org/abs/2508.13804) or [HuggingFace](https://huggingface.co/papers/2508.13804))| Alina Landowska, maciejskorski | The paper evaluates how well large language models (LLMs) understand moral values compared to humans. It investigates this by modeling annotator disagreements using a Bayesian framework to capture aleatoric and epistemic uncertainty. The study evaluates several LLMs across 250K+ annotations and 100K+ texts. Results show that AI models typically rank among the top 25% of human annotators and demonstrate a superior ability to detect moral signals (reduced false negatives). The research implies that LLMs can be effectively deployed for moral foundation analysis, especially in detecting moral content that humans might overlook. |
