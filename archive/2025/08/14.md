

## Papers for 2025-08-14

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Computer Vision | Stand-In: A Lightweight and Plug-and-Play Identity Control for Video
  Generation (Read more on [arXiv](https://arxiv.org/abs/2508.07901) or [HuggingFace](https://huggingface.co/papers/2508.07901))| Chen Li, Hao Liu, Wenjing Wang, Qixin Yan, Bowen Xue | The paper introduces Stand-In, a lightweight and plug-and-play framework for identity-preserving video generation. The research aims to achieve robust identity control in generated videos using minimal additional parameters and maintain compatibility with other AIGC tools. This is achieved by introducing a conditional image branch and restricted self-attentions with conditional position mapping. The framework achieves a face similarity score of 0.724, outperforming existing methods while training only ~1% additional parameters. The plug-and-play design enables seamless integration into diverse applications, simplifying the development of identity-consistent video generation pipelines. |
| Natural Language Processing | Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery (Read more on [arXiv](https://arxiv.org/abs/2508.08401) or [HuggingFace](https://huggingface.co/papers/2508.08401))| Di Zhang, Junxian Li, Qinggang Zhang, Weida Wang, Jiatong Li | This paper introduces Mol-R1, a framework to improve explainability and reasoning of LLMs in text-based molecule generation. It addresses the limitation of current Long-CoT models in knowledge-intensive domains by improving understanding of molecular structures and chemical principles. Mol-R1 employs Prior Regulation via In-context Distillation (PRID) to curate a high-quality reasoning dataset and Molecular Iterative Adaptation (MoIA) to combine supervised fine-tuning with reinforcement learning. Experimental results demonstrate superior performance against existing baselines, including a 354% increase in BLEU score compared to QWQ-32B. This provides AI practitioners with a method for enhancing LLMs' reasoning and transparency in molecule discovery. |
| Multi-Agent | AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust
  GAIA Problem Solving (Read more on [arXiv](https://arxiv.org/abs/2508.09889) or [HuggingFace](https://huggingface.co/papers/2508.09889))| Jinjie Gu, Chenyi Zhuang, Chengyue Yu, Qintong Wu, Zhitian Xie | The paper introduces a dynamic multi-agent system (MAS) architecture, AWorld, with stable maneuvering mechanisms for improved problem-solving robustness. It addresses the challenge of maintaining stability in complex, multi-tool agent systems. The key methodology involves dynamic supervision and maneuvering by incorporating a Guard Agent to verify the Execution Agent's reasoning at critical steps. Results on the GAIA benchmark show the MAS achieves a pass@1 accuracy of 67.89%, outperforming single-agent systems and other tool-augmented systems. The implication is that dynamic supervision and collaborative agent roles can lead to more reliable and trustworthy intelligent systems. |
| Natural Language Processing | Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion
  Forcing (Read more on [arXiv](https://arxiv.org/abs/2508.09192) or [HuggingFace](https://huggingface.co/papers/2508.09192))| Hao Zhang, Jiachun Jin, Yijie Jin, Chenkai Xu, Xu Wang | The paper introduces Discrete Diffusion Forcing (D2F), a novel training paradigm for diffusion language models (dLLMs) that achieves faster-than-autoregressive (AR) inference. The primary research question is how to accelerate dLLM inference to surpass AR models while maintaining comparable output quality. D2F employs a generation scheme that conditions on partially predicted tokens from previous blocks, enabling KV cache utilization and parallel generation across blocks. Experiments demonstrate that D2F achieves up to 2.5x faster inference than LLaMA3 on GSM8K. This allows AI practitioners to leverage the parallelization benefits of dLLMs without sacrificing inference speed. |
| Computer Vision | Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved
  Image Generation (Read more on [arXiv](https://arxiv.org/abs/2508.09987) or [HuggingFace](https://huggingface.co/papers/2508.09987))| Zhenghao Hu, Leqi Zhu, Zihao Wang, Dongzhi Jiang, Junyan Ye | The paper introduces Echo-4o, a method to improve image generation by leveraging synthetic images generated by GPT-4o. It addresses the question of why synthetic data is useful when real-world image datasets exist. The approach involves fine-tuning a multimodal generation baseline, Bagel, using a 180K-scale synthetic dataset from GPT-4o and introduces GenEval++ and Imagine-Bench for evaluation. Echo-4o demonstrates strong performance, achieving a score of 0.89 on GenEval, and its strong transferability is further shown with performance gains on standard benchmarks when the Echo-4o-Image dataset is applied to other foundation models. This work implies that AI practitioners can benefit from synthetic data to address coverage gaps and improve text-to-image alignment. |
| Multi-Modal | Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with
  Long-Term Memory (Read more on [arXiv](https://arxiv.org/abs/2508.09736) or [HuggingFace](https://huggingface.co/papers/2508.09736))| Yuan Lin, Yiyuan Pan, Wentao Ye, Yichen He, Lin Long | The paper introduces M3-Agent, a novel multimodal agent framework with long-term memory capabilities for processing real-time visual and auditory inputs. It aims to address the limitations of existing multimodal agents by enabling continuous learning and reasoning over extended periods. The methodology involves building entity-centric, multimodal memories, iteratively reasoning, and employing reinforcement learning for task completion. Experiments on M3-Bench, a new long-video QA benchmark, show that M3-Agent outperforms strong baselines like Gemini-1.5-pro and GPT-4o, achieving 6.7% higher accuracy on M3-Bench-robot and 7.7% on M3-Bench-web. This work advances the development of more human-like, long-term memory capabilities in multimodal agents, providing insights into practical design considerations. |
| Machine Learning | Learning to Align, Aligning to Learn: A Unified Approach for
  Self-Optimized Alignment (Read more on [arXiv](https://arxiv.org/abs/2508.07750) or [HuggingFace](https://huggingface.co/papers/2508.07750))| Lei Fan, Shuowen Zhang, Zhiling Ye, Yun Yue, Haowen Wang | The paper introduces GRAO, a unified framework that combines SFT and RL for language model alignment. It addresses limitations of SFT and RL by using a multi-sample generation strategy and a group direct alignment loss. GRAO outperforms SFT, DPO, PPO, and GRPO, achieving a relative improvement of 57.70%, 17.65%, 7.95%, and 5.18% respectively, on human alignment tasks. The framework dynamically adjusts imitation learning and self-driven exploratory learning. GRAO enables models to obtain more in-depth and universal reasoning behavior. |
| Computer Vision | Story2Board: A Training-Free Approach for Expressive Storyboard
  Generation (Read more on [arXiv](https://arxiv.org/abs/2508.09983) or [HuggingFace](https://huggingface.co/papers/2508.09983))| Dani Lischinski, Dvir Samuel, Omri Avrahami, Matan Levy, David Dinkevich | Story2Board is a training-free framework for generating expressive and coherent storyboards from natural language prompts. The paper addresses the lack of visual storytelling aspects like spatial composition and narrative pacing in existing storyboard generation methods. It introduces Latent Panel Anchoring and Reciprocal Attention Value Mixing to enhance coherence without fine-tuning, achieving better dynamic and narratively engaging storyboards. Evaluated on a new Rich Storyboard Benchmark, Story2Board demonstrates improved performance with higher scene diversity scores, indicating a more dynamic approach to composition. The framework allows AI practitioners to generate consistent yet visually diverse storyboards using state-of-the-art diffusion models without architectural changes or retraining. |
| Multi-Modal | MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math
  Reasoning in Multimodal Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.06009) or [HuggingFace](https://huggingface.co/papers/2508.06009))| Zhihan Zhou, Yue Guo, Zhentao Zhang, Zixin Wang, junfeng0288 | This paper introduces MATHREAL, a new benchmark for evaluating multimodal large language models (MLLMs) in real-world math reasoning scenarios using images captured by mobile devices. It aims to address the gap in existing benchmarks that predominantly use clean or processed images, which do not reflect the challenges faced by real-world users. The benchmark encompasses 2,000 mathematical questions across five core knowledge categories, three question types, and three difficulty levels, further characterized by image quality degradation, perspective variation, and irrelevant content interference. Experiments on various MLLMs reveal a significant performance gap compared to clean images, with the best-performing model achieving only 53.9% accuracy, highlighting the need for more robust visual perception and reasoning abilities in MLLMs for realistic educational contexts. MATHREAL benefits AI practitioners by offering a targeted tool for refining model robustness in vision-language alignment and error tolerance within authentic educational settings. |
| Reinforcement Learning | Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning
  for Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.05613) or [HuggingFace](https://huggingface.co/papers/2508.05613))| Guiyang Hou, Xingyu Wu, Haitao Hong, tricktreat, yanyc | The paper introduces Cooper, a novel reinforcement learning framework for large language models that co-optimizes the policy and reward models to enhance reasoning capabilities. It addresses the limitations of rule-based and model-based rewards by leveraging rule-based rewards for identifying correct responses and dynamically constructing positive-negative sample pairs to train the reward model, enhancing robustness and mitigating reward hacking. The methodology involves a two-stage pipeline consisting of policy model optimization and reward model optimization, supported by a hybrid annotation strategy for generating training data. Experiments demonstrate that Cooper improves end-to-end RL performance, achieving a 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct and mitigating reward hacking. This co-optimization approach provides a more effective way to combat reward hacking, offering valuable insights for integrating reward models into RL. |
| Computer Vision | IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding (Read more on [arXiv](https://arxiv.org/abs/2508.09456) or [HuggingFace](https://huggingface.co/papers/2508.09456))| Di Zhang, Beining Xu, Junxian Li | The paper introduces IAG, a novel input-aware backdoor attack on Vision-Language Models (VLMs) for visual grounding tasks. It investigates the manipulation of VLMs' grounding behavior by injecting adaptive triggers based on semantic information of target objects. The key methodology involves a text-conditional U-Net to embed semantic clues into images, coupled with a reconstruction loss to maintain stealthiness. Experimental results on InternVL-2.5-8B show an attack success rate (ASR@0.5) reaching over 65% on various testing sets. This highlights a significant vulnerability in VLMs deployed in safety-critical applications where visual inputs can be maliciously manipulated. |
| Computer Vision | Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2508.09968) or [HuggingFace](https://huggingface.co/papers/2508.09968))| Zeynep Akata, Nataniel Ruiz, Alexey Dosovitskiy, Shyamgopal Karthik, Luca Eyring | The paper introduces Noise Hypernetworks for efficient test-time compute amortization in diffusion models. It aims to integrate test-time scaling knowledge into a model during post-training by replacing reward-guided noise optimization with a Noise Hypernetwork that modulates initial input noise. The method learns a reward-tilted distribution for distilled generators through a tractable noise-space objective, maintaining fidelity to the base model. Experiments on SD-Turbo, SANA-Sprint, and FLUX-Schnell demonstrate that the approach recovers a substantial portion of the quality gains from explicit test-time optimization at a fraction of the computational cost, achieving a GenEval performance of 0.57 for SD-Turbo which even surpasses SDXL. This approach enables high-quality, reward-aligned generation practically for fast generators by shifting computational costs to a post-training stage. |
| Multi-Modal | VisCodex: Unified Multimodal Code Generation via Merging Vision and
  Coding Models (Read more on [arXiv](https://arxiv.org/abs/2508.09945) or [HuggingFace](https://huggingface.co/papers/2508.09945))| Dongdong Zhang, Yixia Li, Xun Wu, Shaohan Huang, Lingjie Jiang | The paper introduces VisCodex, a unified framework for multimodal code generation by merging vision and coding language models. It aims to enhance MLLMs' ability to generate code from visual inputs by integrating visual and textual understanding. VisCodex uses a task vector-based model merging technique to integrate a coding LLM into a vision-language backbone and introduces the Multimodal Coding Dataset (MCD) and InfiBench-V benchmark for training and evaluation. Experiments show VisCodex achieves state-of-the-art performance among open-source MLLMs, achieving results competitive with proprietary models like GPT-40 with an average score of 72.3 on the evaluated benchmarks. This suggests VisCodex provides a cost-effective method for improving multimodal code generation capabilities in open-source models. |
| Natural Language Processing | Can LLM-Generated Textual Explanations Enhance Model Classification
  Performance? An Empirical Study (Read more on [arXiv](https://arxiv.org/abs/2508.09776) or [HuggingFace](https://huggingface.co/papers/2508.09776))| Gjergji Kasneci, Zineb Attaoui, Ege Erdogan, Juraj Vladika, Mahdi Dhaini | This paper investigates whether LLM-generated textual explanations can enhance model classification performance in Natural Language Inference (NLI) tasks. The study uses multiple LLMs to generate explanations for e-SNLI and HealthFC datasets and evaluates their impact on PLMs and LLMs, employing metrics like BLEU, ROUGE, BERTScore, MAUVE, and G-Eval to measure explanation quality. Results indicate that while both human and LLM explanations improve PLMs' performance, LLM-generated explanations can lead to better performance than human explanations in certain cases (e.g., on HealthFC dataset). The implication is that automated LLM-based explanation generation offers a scalable avenue for extending NLP datasets and enhancing model performance in NLI. |
| Machine Learning | AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal
  Imitation-Exploration Balance (Read more on [arXiv](https://arxiv.org/abs/2508.06944) or [HuggingFace](https://huggingface.co/papers/2508.06944))| Yong Li, Jie Feng, Lixuan He | The paper introduces Adaptive Meta Fine-Tuning (AMFT), a single-stage algorithm that meta-learns the optimal balance between Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for aligning Large Language Model (LLM) reasoners. It aims to dynamically optimize the trade-off between imitation and exploration. AMFT uses a meta-gradient adaptive weight controller treating the SFT-RL balance as a learnable parameter, maximizing long-term task performance. Evaluations on mathematical reasoning, abstract visual reasoning, and vision-language navigation show AMFT consistently establishes state-of-the-art performance with an average accuracy of 61.3% on in-distribution math benchmarks and 63.3% on out-of-distribution benchmarks. The AMFT provides a principled and effective paradigm for LLM alignment, offering superior generalization capabilities. |
