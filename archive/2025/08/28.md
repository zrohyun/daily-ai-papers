

## Papers for 2025-08-28

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Self-Rewarding Vision-Language Model via Reasoning Decomposition (Read more on [arXiv](https://arxiv.org/abs/2508.19652) or [HuggingFace](https://huggingface.co/papers/2508.19652))| Zhenwen Liang, Rui Liu, Wenhao Yu, Zongxia Li, ChengsongHuang | The paper introduces Vision-SR1, a self-rewarding reinforcement learning framework for vision-language models (VLMs) that decomposes reasoning into visual perception and language reasoning. The research aims to improve visual reasoning, mitigate hallucinations, and reduce language shortcuts in VLMs without relying on external visual supervision. Vision-SR1 prompts the VLM to generate self-contained visual perceptions, validates them by re-prompting for language reasoning, and combines this self-reward with output supervision, resulting in a balanced training signal. Experiments demonstrate that Vision-SR1 improves visual reasoning, achieving, for example, 49.1 on MMMU-Pro with Qwen2.5-VL-7B. This methodology provides a way to strengthen both visual perception and language reasoning within VLMs, leading to better task performance and reduced reliance on language priors. |
| Natural Language Processing | Beyond Transcription: Mechanistic Interpretability in ASR (Read more on [arXiv](https://arxiv.org/abs/2508.15882) or [HuggingFace](https://huggingface.co/papers/2508.15882))| Aviv Shamsian, Hilit Segev, Yael Segal-Feldman, AvivNavon, netag | This paper explores mechanistic interpretability techniques in Automatic Speech Recognition (ASR) systems. It investigates how acoustic and semantic information evolves across layers in ASR models, aiming to understand error phenomena like hallucinations and repetitions. The methodology adapts established interpretability methods, including logit lens, linear probing, and activation patching, to analyze internal model dynamics. The experiments reveal internal dynamics, including specific encoder-decoder interactions responsible for repetition hallucinations, achieving up to 93.4% accuracy in hallucination prediction using linear probing of the decoder's residual stream. The findings demonstrate the benefits of applying interpretability techniques to speech recognition, opening avenues for improving model transparency and robustness in ASR systems. |
| Multi-Modal | Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding
  in Vision-Language-Action Policies (Read more on [arXiv](https://arxiv.org/abs/2508.20072) or [HuggingFace](https://huggingface.co/papers/2508.20072))| Sitong Mao, Chengyue Wu, Tianshuo Yang, Yizhuo Li, Zhixuan Liang | The paper introduces Discrete Diffusion VLA, a novel single-transformer policy for vision-language-action tasks using discrete diffusion for action decoding. It addresses the challenge of unified and scalable VLA architectures by modeling discretized action chunks with discrete diffusion, trained with a cross-entropy objective. The methodology involves an adaptive decoding order that resolves easy action elements before harder ones, using secondary re-masking to improve consistency and enable error correction. Discrete Diffusion VLA achieves 96.3% average success rate on LIBERO, outperforming autoregressive and continuous diffusion baselines. The results suggest that discrete diffusion offers a consistent training and precise action modeling approach for VLA, paving the way for scaling VLA to larger models and datasets. |
| Reinforcement Learning | CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer
  Use Agent with Decoupled Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2508.20096) or [HuggingFace](https://huggingface.co/papers/2508.20096))| Jianze Liang, Yuhang Cao, yuhangzang, rookiexiong, Zery | This paper introduces CODA, a novel and trainable compositional framework for autonomous GUI agents, inspired by the human brain's cerebrum-cerebellum coordination. The research aims to address challenges in specialized domains like scientific computing by decoupling planning and execution. CODA employs a two-stage training pipeline involving specialization via decoupled GRPO and generalization via supervised fine-tuning. Evaluated on four ScienceBoard applications, CODA significantly outperforms baselines, establishing a new SOTA; for example, there is some performance gain on average@8 for the applications of algebra, biochem, astron, and overall when compared to previously SOTA model UI-TARS. The proposed method provides a way for AI researchers and practitioners to improve reasoning ability and learning software domain knowledge in autonomous GUI agents. |
| Computer Vision | MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time
  Autoregressive Video Generation (Read more on [arXiv](https://arxiv.org/abs/2508.19320) or [HuggingFace](https://huggingface.co/papers/2508.19320))| Yan Zhou, Haoxian Zhang, Wenyuan Zhang, Liyuan Cui, ChenMing-thu14 | The paper introduces MIDAS, a framework for real-time multimodal interactive digital human synthesis using autoregressive video generation. It addresses the challenge of creating practical interactive systems by enabling multimodal control and low-latency extrapolation. The method employs a multimodal condition projector and a deep compression autoencoder to guide a diffusion head, achieving up to 64x compression. Experiments on duplex conversation and multilingual synthesis validate the approach, achieving improved efficiency and controllability. The proposed method offers AI practitioners a low-latency and efficient approach for building interactive digital human avatars. |
| Natural Language Processing | Predicting the Order of Upcoming Tokens Improves Language Modeling (Read more on [arXiv](https://arxiv.org/abs/2508.19228) or [HuggingFace](https://huggingface.co/papers/2508.19228))| Alham Fikri Aji, Erland, zaydzuhri | The paper introduces Token Order Prediction (TOP) as a novel auxiliary training loss to enhance language modeling. It explores whether predicting the order of upcoming tokens, rather than the exact tokens themselves, can improve next-token prediction. TOP trains models to rank upcoming tokens by proximity using a learning-to-rank loss, requiring only a single additional unembedding layer compared to Multi-Token Prediction (MTP). Experiments on eight NLP benchmarks show TOP outperforms both Next-Token Prediction (NTP) and MTP, even at scale, demonstrating its effectiveness as a regularizer. The results indicate that TOP offers a promising approach for improving language model training through effective auxiliary objectives. |
| Computer Vision | Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health
  Biomarkers Estimation (Read more on [arXiv](https://arxiv.org/abs/2508.17924) or [HuggingFace](https://huggingface.co/papers/2508.17924))| Anton Ivaschenko, Galina Zubkova, Stepan Botman, Konstantin Egorov, blinoff | The paper introduces a novel multi-view video dataset (MCD-rPPG) for remote photoplethysmography (rPPG) and health biomarker estimation. The research aims to address limitations in existing datasets, such as small size and privacy concerns, by providing a large-scale, diverse resource. The methodology involves capturing synchronized video recordings from multiple angles, paired with PPG signals and extended health metrics from 600 subjects. The authors train an efficient rPPG model on this dataset, achieving competitive accuracy, with a MAE of 4.86 bpm for heart rate estimation on the MCD-rPPG test set. This dataset and model facilitate advancements in AI medical assistants by providing a robust benchmark for training and evaluating rPPG algorithms. |
| Natural Language Processing | Diffusion Language Models Know the Answer Before Decoding (Read more on [arXiv](https://arxiv.org/abs/2508.19982) or [HuggingFace](https://huggingface.co/papers/2508.19982))| Shilin Yan, Lu Yin, Dilxat Muhtar, Yefan Zhou, Pengxiang Li | This paper introduces a method for accelerating diffusion language model (DLM) inference by leveraging early answer convergence. The research investigates whether DLMs internally identify correct answers before completing all decoding steps. The authors propose Prophet, a training-free decoding paradigm that dynamically determines when to commit to decoding all remaining tokens based on the confidence gap between the top-2 prediction candidates. Experiments using LLaDA-8B and Dream-7B demonstrate up to 3.4x reduction in decoding steps while preserving generation quality (e.g., achieving 54.0% accuracy on MMLU). Prophet offers AI practitioners a simple, model-agnostic mechanism to improve DLM inference efficiency without significant accuracy degradation. |
| Multi-Modal | Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered
  Smartphone Agents (Read more on [arXiv](https://arxiv.org/abs/2508.19493) or [HuggingFace](https://huggingface.co/papers/2508.19493))| Yue Yao, Yibo Shi, Shidong Pan, Zhixin Lin, Jungang | This paper introduces SAPA-Bench, a novel benchmark for evaluating privacy awareness in smartphone agents powered by multi-modal large language models (MLLMs). The research investigates the extent to which these agents expose users' private information during task automation. They benchmark seven available smartphone agents across 7,138 real-world scenarios, annotated with privacy types, sensitivity levels, and locations. Results show that most agents exhibit unsatisfying privacy awareness, with Gemini 2.0-flash achieving the highest Risk Awareness (RA) of 67%. The study implies the need for rethinking the utility-privacy tradeoff in designing smartphone agents, emphasizing the importance of specialized privacy training and prompt engineering. |
| Multi-Modal | AudioStory: Generating Long-Form Narrative Audio with Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2508.20088) or [HuggingFace](https://huggingface.co/papers/2508.20088))| Yixiao Ge, Shijie Ma, Yuying Ge, Yuxin Guo, wybertwang | The paper introduces AudioStory, a unified framework for generating long-form narrative audio by integrating large language models (LLMs) with text-to-audio (TTA) systems. It aims to address the challenge of maintaining temporal coherence and compositional reasoning in long-form audio generation. The methodology involves a decoupled bridging mechanism with semantic and residual tokens, and an end-to-end training approach to enhance instruction following. Experiments on the AudioStory-10K benchmark show AudioStory surpasses prior TTA baselines, achieving a CLAP score of 0.392 and FAD of 3.00.  AudioStory enables AI practitioners to create structured, coherent audio narratives from complex instructions, improving the capabilities of generative audio models. |
| Natural Language Processing | StepWiser: Stepwise Generative Judges for Wiser Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.19229) or [HuggingFace](https://huggingface.co/papers/2508.19229))| Olga Golovneva, Weizhe Yuan, Wenting Zhao, Wei Xiong, sainbar | The paper introduces STEPWISER, a novel approach to improve multi-step reasoning in language models by using a stepwise generative judge. It aims to address limitations of existing process reward models (PRMs) by reframing stepwise reward modeling as a reasoning task. STEPWISER employs a self-segmentation technique for coherent reasoning chunks, assigns rewards via relative outcomes of rollouts, and trains a generative judge with reinforcement learning. Experiments on ProcessBench demonstrate that STEPWISER achieves better judgment accuracy, improving policy model training and inference search with a 61.9 average score on the 7B model using Relative Effective Reward Thresholding. The implication is that RL-trained meta-reasoning can significantly enhance the performance of language models in complex reasoning tasks, enabling more effective supervision and iterative refinement. |
| Computer Vision | MotionFlux: Efficient Text-Guided Motion Generation through Rectified
  Flow Matching and Preference Alignment (Read more on [arXiv](https://arxiv.org/abs/2508.19527) or [HuggingFace](https://huggingface.co/papers/2508.19527))| An-An Liu, Chao Xue, Diqiong Jiang, Dan Song, Zhiting Gao | MotionFlux introduces a novel framework for efficient text-guided motion generation using rectified flow matching and preference alignment. The paper addresses the challenge of precisely aligning linguistic descriptions with motion semantics and the inefficiency of multi-step inference. The proposed method, integrating TMR++ Aligned Preference Optimization (TAPO) with MotionFLUX, achieves real-time synthesis by constructing optimal transport paths between noise and motion spaces. MotionFlux demonstrates superior performance, achieving a Frechet Inception Distance (FID) of 0.078, balancing semantic consistency and motion quality while accelerating generation speed. This approach offers AI practitioners a faster and more controllable method for generating realistic human motions from text. |
| Machine Learning | Taming the Chaos: Coordinated Autoscaling for Heterogeneous and
  Disaggregated LLM Inference (Read more on [arXiv](https://arxiv.org/abs/2508.19559) or [HuggingFace](https://huggingface.co/papers/2508.19559))| Chunlei Han, Sida Zhao, Zefang Chu, Ruogu Du, Rongzhi Li | The paper introduces HeteroScale, a coordinated autoscaling framework for heterogeneous and disaggregated Large Language Model (LLM) inference. It addresses challenges like hardware inefficiency and architectural imbalance in Prefill-Decode architectures. HeteroScale uses a topology-aware scheduler with a metric-driven policy derived from production autoscaling data, leveraging decode Tokens-Per-Second (TPS) for joint scaling. Deployed on tens of thousands of GPUs, HeteroScale increased GPU utilization by 26.6 percentage points while saving GPU-hours. The implication is a more efficient and balanced resource management solution for large-scale LLM serving in production environments. |
| Natural Language Processing | DeepScholar-Bench: A Live Benchmark and Automated Evaluation for
  Generative Research Synthesis (Read more on [arXiv](https://arxiv.org/abs/2508.20033) or [HuggingFace](https://huggingface.co/papers/2508.20033))| Ion Stoica, Ankita Sundar, Harshit Gupta, Negar Arabzadeh, Liana Patel | This paper introduces DeepScholar-bench, a live benchmark and automated evaluation framework for generative research synthesis systems. It addresses the challenge of evaluating systems that synthesize knowledge by generating related work sections for academic papers. The framework holistically assesses knowledge synthesis, retrieval quality, and verifiability, drawing queries from recent ArXiv papers. Experiments show that no existing system exceeds a score of 19% across all metrics, indicating significant room for improvement. The benchmark provides a foundation for advancing AI systems capable of generative research synthesis. |
