

## Papers for 2025-08-07

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | VeriGUI: Verifiable Long-Chain GUI Dataset (Read more on [arXiv](https://arxiv.org/abs/2508.04026) or [HuggingFace](https://huggingface.co/papers/2508.04026))| Zhenyu Cui, Huichi Zhou, Shunyu Liu, weihao1115, Liam-Liu | The paper introduces VeriGUI, a new verifiable long-chain GUI dataset designed to facilitate the development and evaluation of generalist GUI agents. The main research objective is to address the limitations of existing GUI datasets by emphasizing long-chain complexity and subtask-level verifiability. The dataset consists of GUI task trajectories across desktop and web annotated by human experts. Experiments using various agents on VeriGUI reveal significant performance gaps in handling long-horizon tasks. This dataset highlights the need for more robust planning and decision-making capabilities in GUI agents, providing a new benchmark for future research. |
| Machine Learning | Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens (Read more on [arXiv](https://arxiv.org/abs/2508.01191) or [HuggingFace](https://huggingface.co/papers/2508.01191))| Zhen Tan, Bohan, wjldw, ympc08, chengshuaizhao | This paper investigates the chain-of-thought (CoT) reasoning capabilities of large language models (LLMs) through a data distribution lens, questioning whether it reflects genuine inference or pattern matching. The study introduces DATAALCHEMY, a controlled environment to train LLMs and systematically probe them under various distribution shifts across task, length, and format dimensions. Results demonstrate that CoT reasoning exhibits sharp performance degradation under distribution shifts, suggesting it is more akin to shallow pattern replication than robust reasoning.  The work shows that SFT can achieve high performance. The study implies that CoT should be treated cautiously as a universal problem-solving paradigm due to its potential for fluent but logically inconsistent reasoning. |
| Natural Language Processing | Efficient Agents: Building Effective Agents While Reducing Cost (Read more on [arXiv](https://arxiv.org/abs/2508.02694) or [HuggingFace](https://huggingface.co/papers/2508.02694))| Yue Hou, He Zhu, Pai Liu, Xavier Hu, Ningning Wang | The paper explores the efficiency-effectiveness trade-off in LLM-driven agent systems by addressing cost-effective designs without sacrificing performance. It investigates the inherent complexity of agentic tasks, diminishing returns of additional modules, and the design of efficient agent frameworks. The methodology includes empirical analysis on the GAIA benchmark, evaluating the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies using the cost-of-pass metric. Results show that EFFICIENT AGENTS retains 96.7% of OWL's performance while reducing operational costs from $0.398 to $0.228, improving cost-of-pass by 28.4%.  This work offers insights for designing efficient, high-performing agent systems to advance the accessibility and sustainability of AI-driven solutions. |
| Reinforcement Learning | SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from
  Experience (Read more on [arXiv](https://arxiv.org/abs/2508.04700) or [HuggingFace](https://huggingface.co/papers/2508.04700))| Xiaoyi Dong, Yuhang Cao, Ziyu Liu, yuhangzang, Zery | This paper presents SEAgent, a self-evolving framework for computer use agents (CUAs) that learn from experience without human annotations. The research aims to address the challenge of CUAs struggling with novel software, especially in the absence of human-labeled data. SEAgent utilizes a World State Model for trajectory assessment and a Curriculum Generator to create increasingly diverse tasks, updating its policy through adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO). Validated across five software environments within OS-World, SEAgent achieves a 23.2% improvement in success rate, from 11.3% to 34.5% over UI-TARS. The proposed approach enables more powerful and versatile CUAs by autonomously learning software functionalities and improving agent performance without relying on costly human supervision. |
| Reinforcement Learning | Agent Lightning: Train ANY AI Agents with Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2508.03680) or [HuggingFace](https://huggingface.co/papers/2508.03680))| Zilong Wang, Xufang Luo, SiyunZhao, hzy46, ultmaster | The paper introduces Agent Lightning, a flexible framework for Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. It addresses the challenge of decoupling agent execution and training by formulating agent execution as a Markov decision process and proposing a hierarchical RL algorithm. The methodology includes a Training-Agent Disaggregation architecture with a standardized agent finetuning interface, incorporating agent observability frameworks. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable and continuous improvements. Agent Lightning enables seamless integration with existing agents and provides a pathway for real-world agent training and deployment. |
| Machine Learning | CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and
  Prediction (Read more on [arXiv](https://arxiv.org/abs/2508.03159) or [HuggingFace](https://huggingface.co/papers/2508.03159))| Donghyeon Lee, Soyon Park, Minju Song, Jueon Park, P-YI | The paper introduces CoTox, a novel framework that integrates large language models (LLMs) with chain-of-thought (CoT) reasoning to improve multi-organ toxicity prediction. The research aims to enhance toxicity prediction by incorporating chemical structure in IUPAC format and biologically relevant features, such as pathways and GO terms, enabling interpretable step-by-step reasoning. CoTox, using GPT-4o, outperforms traditional machine learning and deep learning models, achieving an average F1-score of 0.663. Representing chemical structures with IUPAC names enhances the model's reasoning ability and predictive performance, indicating the importance of structural format. The framework offers AI practitioners an interpretable and practical tool for early-stage drug development, potentially capturing latent toxicity signals and improving drug safety assessment. |
| Reinforcement Learning | Training Long-Context, Multi-Turn Software Engineering Agents with
  Reinforcement Learning (Read more on [arXiv](https://arxiv.org/abs/2508.03501) or [HuggingFace](https://huggingface.co/papers/2508.03501))| Maksim Nekrashevich, Ibragim Badertdinov, Sergei Polezhaev, Maria Trofimova, Alexander Golubev | This paper explores the application of reinforcement learning (RL) to train large language models (LLMs) for software engineering (SWE) tasks. The research aims to improve the performance of open-weight LLM agents in complex, multi-turn SWE scenarios. They employ a modified Decoupled Advantage Policy Optimization (DAPO) algorithm to train a Qwen2.5-72B-Instruct agent. The agent achieves a success rate of 39% on the SWE-bench Verified benchmark, a significant improvement over a rejection fine-tuned baseline. The results demonstrate the potential of RL to create more capable autonomous agents for real-world problems using open-weight models. |
| Reinforcement Learning | Sotopia-RL: Reward Design for Social Intelligence (Read more on [arXiv](https://arxiv.org/abs/2508.03905) or [HuggingFace](https://huggingface.co/papers/2508.03905))| Keyang Xuan, Kolby Nottingham, Yining Zhao, Zhengyang Qi, Haofei Yu | This paper introduces SOTOPIA-RL, a reward design framework for training socially intelligent agents via reinforcement learning. It addresses the challenges of partial observability and multi-dimensionality in social interactions by refining episode-level feedback into utterance-level, multi-dimensional rewards. The key methodology involves utterance-level credit assignment and multi-dimensional reward design to mitigate partial observability and reward hacking, respectively. Experiments in the SOTOPIA environment demonstrate that SOTOPIA-RL achieves state-of-the-art social goal completion scores, reaching 7.17 on SOTOPIA-hard and 8.31 on SOTOPIA-full, significantly outperforming existing approaches. SOTOPIA-RL provides AI practitioners with a framework for designing more effective and stable RL training strategies for socially intelligent agents. |
| Multi-Modal | LaTCoder: Converting Webpage Design to Code with Layout-as-Thought (Read more on [arXiv](https://arxiv.org/abs/2508.03560) or [HuggingFace](https://huggingface.co/papers/2508.03560))| Tianpeng Lv, Guohao Wang, Zhongyi Zhang, Zhen Li, starmage520 | The paper introduces LaTCoder, a novel approach for converting webpage designs into code with enhanced layout preservation using a Layout-as-Thought (LAT) mechanism. It aims to address the problem of layout inaccuracies when using Multimodal Large Language Models (MLLMs) for design-to-code tasks. LaTCoder divides webpage designs into image blocks, generates code for each block using a CoT-based approach, and applies assembly strategies, either absolute positioning or MLLM-based, followed by dynamic selection. Experiments on Design2Code-HARD show a 66.67% increase in TreeBLEU score, indicating improved structural similarity; annotators favored webpages generated by LaTCoder in over 60% of cases, suggesting improved overall quality. |
| Natural Language Processing | Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web
  Agents (Read more on [arXiv](https://arxiv.org/abs/2508.01858) or [HuggingFace](https://huggingface.co/papers/2508.01858))| Xinyu Yang, Hongliang He, Aiwen Sun, Cong Guo, Gnonymous | The paper introduces Web-CogReasoner, a framework that integrates knowledge content learning and cognitive processes for web agents. The research aims to improve cognitive reasoning in web agents by decomposing their capabilities into knowledge content learning and cognitive processes. The methodology involves a Web-CogKnowledge framework, the Web-CogDataset, and a knowledge-driven Chain-of-Thought reasoning framework. Experimentation reveals significant superiority over existing models, particularly in generalization to unseen tasks. For example, the model achieves 84.4% overall accuracy on the Web-CogBench benchmark, suggesting a structured acquisition of knowledge improves performance in complex scenarios. |
| Computer Vision | HPSv3: Towards Wide-Spectrum Human Preference Score (Read more on [arXiv](https://arxiv.org/abs/2508.03789) or [HuggingFace](https://huggingface.co/papers/2508.03789))| Hongsheng Li, Keqiang Sun, Xiaoshi Wu, Yuhang Ma | The paper introduces Human Preference Score v3 (HPSv3) for evaluating text-to-image generation models. The research aims to address limitations in existing human-centric metrics, including data coverage and model designs, by creating a more comprehensive preference dataset. The authors release HPDv3, a dataset of 1.08M text-image pairs and 1.17M pairwise comparisons, and train a VLM-based preference model using an uncertainty-aware ranking loss. HPSv3 demonstrates a Spearman correlation of 0.94 with human annotations, indicating superior performance. HPSv3 offers a more robust and reliable evaluation framework for text-to-image models and facilitates human-aligned image generation. |
| Computer Vision | Gaussian Variation Field Diffusion for High-fidelity Video-to-4D
  Synthesis (Read more on [arXiv](https://arxiv.org/abs/2507.23785) or [HuggingFace](https://huggingface.co/papers/2507.23785))| Feng Zhao, Jiaolong Yang, Chuxin Wang, Sicheng Xu, BwZhang | The paper introduces a novel framework for high-fidelity video-to-4D synthesis using a Gaussian Variation Field diffusion model. It addresses the challenge of direct 4D diffusion modeling by introducing a Direct 4DMesh-to-GS Variation Field VAE that encodes canonical Gaussian Splats and their temporal variations. The method trains a diffusion model conditioned on input videos and canonical GS, achieving superior generation quality with an FVD of 476.83 compared to existing methods. This work provides AI practitioners with a novel approach for generating high-quality animated 3D content from video inputs, demonstrating strong generalization to real-world scenarios despite training on synthetic data. |
| Machine Learning | LeanK: Learnable K Cache Channel Pruning for Efficient Decoding (Read more on [arXiv](https://arxiv.org/abs/2508.02215) or [HuggingFace](https://huggingface.co/papers/2508.02215))| Yuqing Yang, Chengruidong Zhang, Huiqiang Jiang, hzy46, zhangyik21 | The paper introduces LeanK, a learning-based method to prune K cache channels for efficient LLM decoding. LeanK aims to reduce GPU memory usage and improve decoding speed by exploiting static channel sparsity using a two-stage training process for channel mask learning. The key methodology involves learning channel-wise static masks based on channel importance while satisfying hardware alignment requirements. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction, with a custom decoding kernel enabling a 1.3x speedup for attention computation while maintaining model accuracy. LeanK offers AI practitioners a method to optimize LLM inference by statically pruning unimportant K cache channels, thereby accelerating decoding and reducing GPU memory demands. |
| Natural Language Processing | Sculptor: Empowering LLMs with Cognitive Agency via Active Context
  Management (Read more on [arXiv](https://arxiv.org/abs/2508.04664) or [HuggingFace](https://huggingface.co/papers/2508.04664))| Yunxin Liu, Ting Cao, Qitai Tan, L. H. Xu, Mor-Li | The paper introduces Sculptor, a framework for empowering LLMs with active context management (ACM) to mitigate performance degradation in long contexts. It addresses the research question of how to improve LLM performance in long contexts by actively sculpting their internal working memory. Sculptor equips LLMs with tools for context fragmentation, summarization/hiding, and intelligent search. Evaluation on PI-LLM and NeedleBench shows significant performance improvements, with Claude-4-Sonnet achieving 90% accuracy on a 5-needle task in NeedleBench using Sculptor tools. This suggests that explicit context-control strategies, rather than simply larger token windows, are key to robustness at scale, providing a new approach for AI practitioners to manage and enhance LLM performance. |
| Other | Position: The Current AI Conference Model is Unsustainable! Diagnosing
  the Crisis of Centralized AI Conference (Read more on [arXiv](https://arxiv.org/abs/2508.04586) or [HuggingFace](https://huggingface.co/papers/2508.04586))| Jiaying Wu, Qian Wang, Andre Huikai Lin, Moming Duan, nuojohnchen | This paper argues that the centralized AI conference model is unsustainable due to strains on scientific dissemination, equity, and community well-being. It aims to diagnose the structural crisis of AI conferences using data-driven analysis. The methodology involves analyzing publication trends, environmental impact, sentiment on social media, and conference statistics. The analysis reveals that the average per-author publication rate has more than doubled in the past decade, and 71% of online community discourse reflects negative sentiment; the carbon footprint of NeurIPS 2024 exceeds the daily emissions of its host city. The paper proposes a Community-Federated Conference (CFC) model to decentralize peer review, presentation, and networking, aiming for a more sustainable and inclusive AI research ecosystem. |
| Other | EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust
  Translation (Read more on [arXiv](https://arxiv.org/abs/2508.04295) or [HuggingFace](https://huggingface.co/papers/2508.04295))| Dong Chen, Jie Wang, Tingrui Yu, Chaofan Wang, YerbaPage | The paper introduces EvOC2RUST, an automated framework for translating entire C projects into equivalent Rust code to improve memory safety. The research aims to address the challenges in C-to-Rust translation, specifically related to linguistic discrepancies and project-level code dependencies. The key methodology involves a skeleton-guided translation strategy, enhancing LLMs with safety-preserving mappings, incremental translation, and iterative repair. The evaluation on open-source benchmarks and industrial projects shows that EvOC2RUST achieves a 17.24% improvement in syntax accuracy and a 14.32% improvement in semantic accuracy over LLM-based approaches. EvOC2RUST provides AI practitioners with a new approach to automate C-to-Rust code translation, ensuring syntax correctness, semantic equivalence, and memory safety. |
| Computer Vision | DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a
  Stage-Wise Diffusion Transformer Framework (Read more on [arXiv](https://arxiv.org/abs/2508.02807) or [HuggingFace](https://huggingface.co/papers/2508.02807))| Chao Liang, Ente Lin, Shuliang Ning, Zaiyu Huang, Tongchun Zuo | DreamVVT is introduced as a novel framework for realistic video virtual try-on in unconstrained environments. The research aims to address the challenges of preserving garment details and maintaining temporal consistency in VVT by decoupling the process into two stages leveraging Diffusion Transformers. The method uses a multi-frame try-on model integrated with a vision-language model for keyframe generation, followed by a video generation model enhanced with LoRA adapters; keyframes, pose features, and textual descriptions are used as conditional inputs for the video generation. Experimental results show that DreamVVT achieves state-of-the-art performance with competitive VFID metrics in the ViViD-S dataset. This framework enables AI practitioners to develop more robust and realistic VVT systems adaptable to diverse real-world scenarios. |
| Reinforcement Learning | Enhancing Vision-Language Model Training with Reinforcement Learning in
  Synthetic Worlds for Real-World Success (Read more on [arXiv](https://arxiv.org/abs/2508.04280) or [HuggingFace](https://huggingface.co/papers/2508.04280))| Ruslan Rakhimov, Viacheslav Sinii, Stanislav Dereka, kefirski, GeorgeBredis | This paper explores enhancing vision-language models (VLMs) with reinforcement learning (RL) in synthetic environments to improve real-world performance. It addresses the limitations of current VLMs in interactive multimodal tasks by proposing Vision-Language Decoupled Actor-Critic (VL-DAC), a hyperparameter-free RL algorithm. The key methodology involves applying PPO updates to action tokens and step-level value learning, decoupling action and environment-level signals. Experiments demonstrate that VL-DAC trained in inexpensive simulators yields policies that generalize well, achieving up to +50% relative improvement on the BALROG benchmark. This suggests that simple RL algorithms can train VLMs in synthetic worlds while providing measurable gains on real-image agentic and reasoning benchmarks. |
| Computer Vision | A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding (Read more on [arXiv](https://arxiv.org/abs/2508.01197) or [HuggingFace](https://huggingface.co/papers/2508.01197))| Jianke Zhu, Junbo Chen, Zhan Shi, songw-zju | This paper introduces a novel approach to 3D occupancy grounding, enabling the localization of objects in a 3D scene based on natural language descriptions with voxel-level precision. The primary objective is to integrate occupancy prediction into 3D visual grounding, enhancing fine-grained spatial reasoning for autonomous driving. The proposed GroundingOcc model leverages a multi-branch architecture with 2D grounding and depth estimation modules, fusing visual, textual, and point cloud features. Experiments on the newly introduced Talk2Occ benchmark demonstrate a significant improvement in localization accuracy, outperforming baselines by 18.13%. The results suggest that the incorporation of occupancy prediction into 3D visual grounding provides AI practitioners with a more precise method for spatially aware perception. |
| Reinforcement Learning | RL-PLUS: Countering Capability Boundary Collapse of LLMs in
  Reinforcement Learning with Hybrid-policy Optimization (Read more on [arXiv](https://arxiv.org/abs/2508.00222) or [HuggingFace](https://huggingface.co/papers/2508.00222))| Kechi Zhang, Huanyu Liu, Yongding Tao, Xue Jiang, Yihong Dong | The paper introduces RL-PLUS, a hybrid-policy optimization approach designed to counter capability boundary collapse in LLMs trained with Reinforcement Learning with Verifiable Reward (RLVR). It addresses the limited exploration and knowledge acquisition by synergizing internal exploitation with external data through Multiple Importance Sampling and Exploration-Based Advantage Function. RL-PLUS achieves state-of-the-art performance on math reasoning benchmarks, improving upon SFT+GRPO by 5.2 average points, and demonstrates superior generalization to out-of-distribution tasks. The results show consistent improvements across diverse model families, with average relative gains up to 69.2%, indicating an effective resolution of the capability boundary collapse problem. AI practitioners can leverage RL-PLUS to enhance LLMs' reasoning abilities, particularly in complex problem-solving domains where exploration and external knowledge integration are crucial. |
| Natural Language Processing | Reasoning Language Models for Root Cause Analysis in 5G Wireless
  Networks (Read more on [arXiv](https://arxiv.org/abs/2507.21974) or [HuggingFace](https://huggingface.co/papers/2507.21974))| Haozhe Zhang, Yibin Kang, Antonio De Domenico, Mohamed Sana, nicopi | This paper introduces a framework leveraging Large Language Models (LLMs) for Root Cause Analysis (RCA) in 5G mobile networks. The research aims to improve the accuracy and reasoning quality of LLMs for network troubleshooting by integrating domain knowledge. A two-stage training methodology combines supervised fine-tuning with reinforcement learning using a new dataset, TeleLogs, for RCA task-specific adaptation. The results demonstrate significant performance gains, achieving 95.86% accuracy pass@1 on TeleLogs with the Qwen2.5-RCA-32B model. The work suggests domain-adapted, reasoning-enhanced LLMs offer practical and explainable solutions for network operation and management, though generalizability to real-world operational data needs further investigation. |
| Reinforcement Learning | IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with
  Verifiable Rewards (Read more on [arXiv](https://arxiv.org/abs/2508.04632) or [HuggingFace](https://huggingface.co/papers/2508.04632))| Ling-I Wu, Xiaogui Yang, Tong Jian, Tianyi Liang, Xu Guo | The paper introduces Instruction Following Decorator (IFDecorator), a framework for improving the sample efficiency and robustness of Reinforcement Learning with Verifiable Rewards (RLVR) in instruction following. IFDecorator consists of a cooperative-adversarial data flywheel for instruction-verification pair co-evolution, an IntentCheck module for enforcing intent alignment, and diagnostic trip wires for detecting reward hacking. The methodology utilizes a cooperative-adversarial data flywheel and a bypass verification module, IntentCheck, to mitigate over-optimization. Experiments show that the IFDecorator framework improves instruction following, achieving 87.43% accuracy on IFEval with the Qwen2.5-32B-Instruct-IFDecorator. The main implication is a more robust and efficient RLVR training pipeline for instruction following tasks. |
| Natural Language Processing | OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers
  for Biomedical NER Across 12 Public Datasets (Read more on [arXiv](https://arxiv.org/abs/2508.01630) or [HuggingFace](https://huggingface.co/papers/2508.01630))| MaziyarPanahi | The paper introduces OpenMed NER, a suite of open-source, domain-adapted transformer models for biomedical named-entity recognition (NER). It aims to achieve state-of-the-art performance across diverse entity types while maintaining computational efficiency. The approach combines domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA) on a 350k-passage corpus. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of 12 biomedical NER benchmarks, including a +2.70 pp improvement on BC5CDR-Disease. This demonstrates that strategically adapted open-source models can surpass closed-source solutions with remarkable efficiency, completing training in under 12 hours on a single GPU. |
| Machine Learning | SonicMaster: Towards Controllable All-in-One Music Restoration and
  Mastering (Read more on [arXiv](https://arxiv.org/abs/2508.03448) or [HuggingFace](https://huggingface.co/papers/2508.03448))| Ambuj Mehrish, Jan Melechovsky, dorienh | SonicMaster is a unified generative model for music restoration and mastering controllable via text prompts. The paper addresses the challenge of correcting various audio artifacts by training a flow-matching model on a newly constructed dataset of degraded and high-quality music pairs. The model is conditioned on natural language instructions for targeted enhancements or operates in an automatic mode for general restoration. Results demonstrate that SonicMaster significantly improves sound quality across artifact categories, with objective audio quality metrics indicating substantial improvement and subjective listening tests confirming listener preference for the enhanced outputs. This work provides AI practitioners with a novel approach for unified music restoration and mastering, offering fine-grained creative control via text prompts. |
| Computer Vision | IAUNet: Instance-Aware U-Net (Read more on [arXiv](https://arxiv.org/abs/2508.01928) or [HuggingFace](https://huggingface.co/papers/2508.01928))| Dmytro Fishman, Ali Zeynalli, Illia Tsiporenko, YaroslavPrytula | The paper introduces IAUNet, a novel query-based U-Net architecture for instance segmentation in biomedical imaging, particularly for cells. The research aims to improve the accuracy of cell instance segmentation, especially in cases of overlapping cells and brightfield microscopy. IAUNet enhances U-Net with a lightweight Pixel decoder and a Transformer decoder for multi-scale feature refinement and query processing; it was evaluated on several datasets, including a newly introduced Revvity Full Cell Segmentation Dataset. IAUNet achieves superior performance compared to state-of-the-art models with fewer parameters, reporting, for instance, a 45.3 AP score on the LIVECell dataset using a ResNet-50 backbone. The approach offers AI practitioners an efficient and accurate method for cell instance segmentation, potentially improving biomedical image analysis workflows. |
| Computer Vision | Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D
  Generation (Read more on [arXiv](https://arxiv.org/abs/2508.00428) or [HuggingFace](https://huggingface.co/papers/2508.00428))| Hao Huang, Shiqi Jiang, Haiwen Huang, Nan Xiang, tianyilt | The paper presents Sel3DCraft, an interactive visual prompt engineering system for user-friendly text-to-3D generation. The research addresses the bottleneck of blind trial-and-error prompting in T23D by introducing a guided visual process. Sel3DCraft combines retrieval and generation, multi-view hybrid scoring using MLLMs, and a prompt-driven visual analytics suite. User studies show that Sel3DCraft reduces model creation time by 70.5% (118.83s vs 402.17s) and prompt iterations by 66.2% compared to baselines. This system enables designers to more efficiently explore and refine 3D models from text, fostering greater creative control. |
| Computer Vision | The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in
  Text-to-Image Models (Read more on [arXiv](https://arxiv.org/abs/2507.23313) or [HuggingFace](https://huggingface.co/papers/2507.23313))| Elisabetta Rocchetti, Alfio Ferrara, sergiopicascia | This research analyzes how text-to-image diffusion models interpret artistic prompts, specifically regarding content and style. The study employs cross-attention heatmaps to attribute image pixels to specific prompt tokens, isolating regions influenced by content versus style descriptions. Results indicate varying degrees of content-style separation, with content tokens primarily affecting object regions and style tokens influencing background/texture (IoUcs is lower than mIoUB by 0.64 standard deviations). The findings suggest an emergent understanding of content-style distinction in these models, informing their application in nuanced image generation. |
| Machine Learning | MiDashengLM: Efficient Audio Understanding with General Audio Captions (Read more on [arXiv](https://arxiv.org/abs/2508.03983) or [HuggingFace](https://huggingface.co/papers/2508.03983))| Yadong Niu, Jian Luan, Jizhong Liu, Gang Li, Heinrich Dinkel | MiDashengLM is a novel open audio-language model (LALM) for efficient and comprehensive audio understanding. The paper addresses the limitation of closed data sources in current LALMs by proposing an open audio-language model with general audio captions utilizing a new dataset called ACAVCaps. MiDashengLM integrates an open-source audio encoder (Dasheng) and utilizes a general captioning strategy, achieving up to 4x speedup in time-to-first-token (TTFT) and up to 20x higher throughput compared to other comparable models. These results suggest that general captioning can effectively bridge the audio and text modalities, providing a robust foundation for general audio understanding tasks in AI applications. |
| Natural Language Processing | Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and
  Self-Checking for Complex Instruction Following (Read more on [arXiv](https://arxiv.org/abs/2508.03178) or [HuggingFace](https://huggingface.co/papers/2508.03178))| Liang Xu, Xiangzheng Zhang, Shousheng Jia, Liang Wen, Chenyang Wang | The paper introduces Light-IF, a framework to improve LLMs' ability to follow complex instructions by addressing lazy reasoning. It aims to mitigate poor instruction adherence in LLMs through a preview and self-checking mechanism during reasoning. The method employs hardness-aware prompt synthesis, Zero-RL training, entropy-preserving SFT, and TEA-RL with dense rewards. Experiments show that Light-IF-32B outperforms larger models like DeepSeek-R1 on instruction-following benchmarks, achieving a score of 0.575 on SuperCLUE. The framework provides AI practitioners with a method to enhance LLMs' generalizable reasoning and adherence to instructions without extensive supervised data. |
