

## Papers for 2025-08-13

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent (Read more on [arXiv](https://arxiv.org/abs/2508.05748) or [HuggingFace](https://huggingface.co/papers/2508.05748))| zhaoyd, callanwu, zhzhen23, richardxp888, Ornamentt | The paper introduces WebWatcher, a multimodal agent for deep research that enhances visual-language reasoning. It aims to address the limitations of text-centric deep research by incorporating visual information. The agent utilizes synthetic multimodal trajectories for training, various tools for reasoning, and reinforcement learning for generalization. WebWatcher significantly outperforms proprietary baselines and open-source agents in VQA benchmarks, achieving 58.7% Pass@1 on BrowseComp-VL. The agent's enhanced visual-language reasoning capabilities pave the way for solving complex multimodal information-seeking tasks. |
| Computer Vision | Matrix-3D: Omnidirectional Explorable 3D World Generation (Read more on [arXiv](https://arxiv.org/abs/2508.08086) or [HuggingFace](https://huggingface.co/papers/2508.08086))| Yuqi Li, Wenhang Ge, Zhongqi Yang, kangfei, dearamy | The paper introduces Matrix-3D, a framework for generating omnidirectional explorable 3D worlds from a single image or text input. It addresses the problem of limited scope in existing 3D world generation methods by utilizing panoramic representations and combining conditional video generation with panoramic 3D reconstruction. The method involves training a trajectory-guided panoramic video diffusion model with scene mesh renders as condition, and lifting 2D panorama videos to 3D worlds using reconstruction methods. The Matrix-Pano dataset, a large-scale synthetic collection, is introduced to facilitate training. The framework achieves state-of-the-art performance in panoramic video generation with a Fr√©chet Video Distance (FVD) score of 140, offering improved visual quality and geometric consistency for AI practitioners. |
| Reinforcement Learning | Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale
  Asynchronous RL (Read more on [arXiv](https://arxiv.org/abs/2508.07976) or [HuggingFace](https://huggingface.co/papers/2508.07976))| Chuyi He, Shusheng Xu, Minyang Xie, Wei Fu, Jiaxuan Gao | The paper introduces ASearcher, an open-source project for large-scale reinforcement learning of search agents, enhancing Search Intelligence. It addresses the challenge of scalability and data quality in training search agents for complex tasks requiring external knowledge. ASearcher utilizes fully asynchronous RL training to enable long-horizon search and introduces a prompt-based LLM agent for autonomously synthesizing high-quality question-answer pairs. The ASearcher-Web-QwQ agent achieves substantial improvements, with Avg@4 gains of 46.7% and 20.8% on xBench and GAIA, respectively. This facilitates expert-level search strategies without reliance on external LLMs, beneficial for practitioners aiming to implement complex search behaviors in LLM agents. |
| Computer Vision | CharacterShot: Controllable and Consistent 4D Character Animation (Read more on [arXiv](https://arxiv.org/abs/2508.07409) or [HuggingFace](https://huggingface.co/papers/2508.07409))| Fei Shen, Yanhong Zeng, Wenran Liu, LiJiaxing, Gaojunyao | CharacterShot introduces a novel framework for controllable 4D character animation from a single reference image and a 2D pose sequence. The research aims to synthesize dynamic 3D characters with precise motion control and view consistency across time and viewpoints. The methodology involves a DiT-based image-to-video model enhanced with dual-attention modules and a neighbor-constrained 4D Gaussian Splatting optimization. Experiments on the CharacterBench dataset demonstrate superior performance, achieving a SSIM of 0.967, significantly outperforming existing methods. This allows AI practitioners to create high-quality 4D character animations with custom motions and views from minimal inputs. |
| Natural Language Processing | Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2508.09138) or [HuggingFace](https://huggingface.co/papers/2508.09138))| Chenchen Jing, Bozhen Fang, Wen Wang, qiuyuu, tricktreat | The paper explores temporal dynamics in diffusion language models (dLLMs) to improve text generation. It addresses the issue of temporal oscillation, where correct answers are overwritten during the denoising process, by investigating methods for leveraging intermediate predictions. The methodology involves two complementary approaches: Temporal Self-Consistency Voting, a training-free decoding strategy, and Temporal Consistency Reinforcement, a post-training reinforcement learning method using Temporal Semantic Entropy (TSE) as a reward. Experiments demonstrate that combining TSE with accuracy rewards achieves absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and 25.3% on Countdown. This implies that incorporating temporal consistency significantly enhances dLLM performance without additional data, offering a new path for dLLM improvement. |
| Natural Language Processing | HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating
  Local and Web Searches (Read more on [arXiv](https://arxiv.org/abs/2508.08088) or [HuggingFace](https://huggingface.co/papers/2508.08088))| Qiang Ju, Jiehan Cheng, Yan Yu, Zhicheng Dou, zstanjj | The paper introduces HierSearch, a hierarchical agentic deep search framework designed for enterprise information retrieval integrating local and Web knowledge sources. The research aims to improve deep search performance by coordinating local and web searches effectively. HierSearch employs hierarchical reinforcement learning to train local, Web search agents, and a planner agent while incorporating a knowledge refiner to filter hallucinations and irrelevant evidence. Experiments across six benchmarks show HierSearch outperforms flat RL and other baselines achieving better performance, for example, in OmniEval with an EM score of 53.00. This approach offers a practical framework for building private RAG systems leveraging diverse knowledge sources. |
| Computer Vision | Test-Time Reinforcement Learning for GUI Grounding via Region
  Consistency (Read more on [arXiv](https://arxiv.org/abs/2508.05615) or [HuggingFace](https://huggingface.co/papers/2508.05615))| Zhengxi Lu, Fei Tang, tricktreat, yanyc, DIONG1024 | The paper introduces GUI grounding by mapping natural language instructions to screen coordinates using test-time reinforcement learning. It aims to enhance GUI grounding without relying on additional labeled data by leveraging spatial consistency in model predictions. The proposed GUI-RC and GUI-RCPO methods aggregate spatial information across multiple predictions and transforms consistency patterns into rewards for test-time reinforcement learning, respectively. Experiments show GUI-RC boosts Qwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, with GUI-RCPO further improving it to 85.14%. The approach offers a data-efficient path toward robust GUI agents through self-supervised optimization during inference. |
| Computer Vision | VertexRegen: Mesh Generation with Continuous Level of Detail (Read more on [arXiv](https://arxiv.org/abs/2508.09062) or [HuggingFace](https://huggingface.co/papers/2508.09062))| Jakob Engel, Chris Xie, Armen Avetisyan, Yawar Siddiqui, zx1239856 | The paper introduces VertexRegen, a novel framework for generating 3D meshes with continuous levels of detail. It addresses the limitation of existing autoregressive mesh generation methods that produce incomplete structures at intermediate steps. VertexRegen reframes mesh generation as the reversal of edge collapse, learned through a generative model, enabling anytime generation with varying levels of detail. Experiments demonstrate comparable mesh quality to state-of-the-art methods, achieving a JSD of 2.89 on unconditional mesh generation. This approach offers AI practitioners the flexibility to generate meshes with varying levels of detail, suitable for applications requiring adaptable geometric complexity. |
| Computer Vision | UNCAGE: Contrastive Attention Guidance for Masked Generative
  Transformers in Text-to-Image Generation (Read more on [arXiv](https://arxiv.org/abs/2508.05399) or [HuggingFace](https://huggingface.co/papers/2508.05399))| Kevin Galim, Minjae Lee, Byeongkeun Ahn, Wonjun Kang, JakeOh | The paper introduces UNCAGE, a training-free method to improve compositional text-to-image generation with Masked Generative Transformers (MGTs). UNCAGE addresses misaligned attribute binding by prioritizing the unmasking of tokens that clearly represent individual objects using contrastive attention guidance. Quantitatively, UNCAGE achieves improved CLIP text-image similarity and GPT-based evaluation scores across multiple benchmarks. This approach provides a computationally efficient way to enhance MGTs for improved compositional fidelity without requiring retraining. |
| Natural Language Processing | Aryabhata: An exam-focused language model for JEE Math (Read more on [arXiv](https://arxiv.org/abs/2508.08665) or [HuggingFace](https://huggingface.co/papers/2508.08665))| Sandeep Varma, Sachin Dharashivkar, RitvikPW | The paper introduces Aryabhata 1.0, a 7B parameter language model tailored for mathematical reasoning in the context of the Indian Joint Entrance Examination (JEE). It aims to improve accuracy, transparency, and efficiency in educational AI applications. The methodology involves model merging of open-weight reasoning models, supervised fine-tuning with curriculum learning on chain-of-thought traces, and reinforcement learning with verifiable rewards. Evaluated on JEE Main 2025, Aryabhata achieves 86.0% accuracy in January and 90.2% in April. This work provides AI practitioners with a compact, exam-centric, open-source LLM for educational applications. |
| Natural Language Processing | Train Long, Think Short: Curriculum Learning for Efficient Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.08940) or [HuggingFace](https://huggingface.co/papers/2508.08940))| Marzyeh Ghassemi, Elie Bou-Zeid, Abed Hammoud, Kumail Alhamoud, Hasan Abed Al Kader Hammoud | This paper introduces a curriculum learning strategy for length-controlled reasoning in large language models to improve efficiency. The research aims to enhance reasoning accuracy and token efficiency by gradually tightening the token budget during training using Group Relative Policy Optimization (GRPO). The methodology involves augmenting GRPO with a reward function that balances task correctness, length efficiency, and formatting adherence. Experiments on GSM8K demonstrate that curriculum-based training outperforms fixed-budget baselines, achieving higher accuracy (86.20% vs 82.71%) and improved token efficiency. The implication is that progressive constraint serves as a powerful inductive bias for training more efficient reasoning models. |
| Reinforcement Learning | Towards Affordance-Aware Robotic Dexterous Grasping with Human-like
  Priors (Read more on [arXiv](https://arxiv.org/abs/2508.08896) or [HuggingFace](https://huggingface.co/papers/2508.08896))| Haoran Xu, Cheng Zeng, Xingyue Zhao, Linghao Zhuang, Haoyu Zhao | This paper presents AffordDex, a novel framework for affordance-aware robotic dexterous grasping that incorporates human-like priors. The research aims to improve grasping success rates and human-likeness by understanding motion priors and object affordances. AffordDex uses a two-stage training approach: pre-training on human hand motions and refining with a residual module guided by a Negative Affordance-aware Segmentation (NAA) module. Experiments show AffordDex outperforms baselines, achieving higher success rates (89.2% on seen objects) and improved human-likeness and affordance scores. The framework offers a pathway towards more natural and functionally appropriate robotic manipulation. |
| Natural Language Processing | DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech
  Recognition (Read more on [arXiv](https://arxiv.org/abs/2508.08938) or [HuggingFace](https://huggingface.co/papers/2508.08938))| Luk√°≈° Burget, Bolaji Yusuf, Karel Bene≈°, Santosh Kesiraju, Alexander Polok | The paper introduces Decoder-Centric Regularization in Encoder-Decoder (DeCRED) for improved ASR model generalization. It aims to enhance the internal language model within the decoder to boost robustness, especially in out-of-domain scenarios. DeCRED employs auxiliary classifiers at intermediate decoder layers, trained with the ASR prediction objective, for additional supervision. Experiments show a 2.0% macro WER reduction on out-of-domain datasets and a 36.6% perplexity reduction of internal language model. DeCRED provides a computationally efficient method to regularize decoder models, leading to better generalization capabilities for ASR practitioners. |
| Computer Vision | Cut2Next: Generating Next Shot via In-Context Tuning (Read more on [arXiv](https://arxiv.org/abs/2508.08244) or [HuggingFace](https://huggingface.co/papers/2508.08244))| Yu Qiao, Ziqi Huang, Jiajun Li, Hongbo Liu, Jingwen He | The paper introduces Cut2Next for generating subsequent video shots that adhere to cinematic editing patterns while maintaining visual consistency. It addresses the problem of neglecting editing patterns in narrative video generation. The approach uses a Diffusion Transformer with hierarchical multi-prompting and architectural innovations like Context-Aware Condition Injection (CACI) and Hierarchical Attention Mask (HAM). Experiments on a new CutBench benchmark show improved visual consistency and text fidelity, achieving a Fr√©chet Inception Distance (FID) of 59.37. The method provides AI practitioners with a new framework to generate high-quality, narratively expressive, and cinematically coherent subsequent shots. |
| Computer Vision | Adversarial Video Promotion Against Text-to-Video Retrieval (Read more on [arXiv](https://arxiv.org/abs/2508.06964) or [HuggingFace](https://huggingface.co/papers/2508.06964))| Shuai Liu, Qian Li, Zhengyu Zhao, Chenhao Lin, michaeltqw108 | The paper addresses the overlooked vulnerability of video promotion in text-to-video retrieval (T2VR) systems via adversarial attacks. It introduces the Video Promotion attack (ViPro) to promote videos towards selected queries, contrasting existing suppression attacks. A modality refinement (MoRe) technique is proposed to enhance black-box transferability by modeling finer-grained interactions between modalities. Experiments on three T2VR models and datasets show ViPro outperforms baselines by 30/10/4% on average in white/grey/black-box settings. This highlights a security gap in T2VR systems where adversarial promotion can be exploited for malicious purposes. |
| Multi-Modal | OpenCUA: Open Foundations for Computer-Use Agents (Read more on [arXiv](https://arxiv.org/abs/2508.09123) or [HuggingFace](https://huggingface.co/papers/2508.09123))| Tianbao Xie, Junlin Yang, Dunjie Lu, Bowen Wang, xywang626 | The paper introduces OPENCUA, an open-source framework for scaling computer-use agent (CUA) data and foundation models. It addresses the lack of open resources for studying CUA capabilities and limitations by providing an annotation infrastructure, a large-scale computer-use task dataset (AGENTNET), and a scalable data transformation pipeline. OPENCUA-32B achieves a 34.8% success rate on OSWorld-Verified, surpassing existing open-source models and being comparable to OpenAI CUA (GPT-4o). The framework facilitates research into CUA capabilities, limitations, and risks by offering annotation tools, datasets, code, and models. |
| Natural Language Processing | AutoCodeBench: Large Language Models are Automatic Code Benchmark
  Generators (Read more on [arXiv](https://arxiv.org/abs/2508.09101) or [HuggingFace](https://huggingface.co/papers/2508.09101))| Tao Zhang, Zhiying Zeng, Yuchi Deng, Ao Liu, Jason Chou | This paper introduces AutoCodeBench, an automated benchmark for evaluating code generation capabilities of Large Language Models (LLMs). It aims to address limitations of existing benchmarks by creating a high-difficulty, multilingual dataset without manual annotations. The method uses LLMs and a sandbox environment to generate test cases in reverse order, filtering for quality and diversity. The authors introduce AutoCodeBench comprising 3,920 problems across 20 programming languages, and evaluate over 30 LLMs, finding even advanced models struggle with complexity, diversity, and multilingual nature. The benchmark offers a valuable resource for focusing community efforts on more challenging and practical code generation scenarios. |
| Natural Language Processing | Feedback-Driven Tool-Use Improvements in Large Language Models via
  Automated Build Environments (Read more on [arXiv](https://arxiv.org/abs/2508.08791) or [HuggingFace](https://huggingface.co/papers/2508.08791))| Xuesong Yao, Yufei Xu, Zhengyin Du, Changhao Jiang, Junjie-Ye | The paper introduces a framework to improve tool use in large language models (LLMs) by automating the construction of diverse and verifiable training environments. The research aims to address the limitations of existing reinforcement learning (RL) frameworks for tool use, such as unstable training environments and unreliable reward signals. Their methodology involves a five-stage pipeline for environment construction, coupled with a verifiable reward mechanism that evaluates tool use precision and task execution completeness. Experiments on LLMs of varying scales show significant improvements in tool-use performance across benchmarks, with a parameter-level analysis indicating that the gains are largely driven by updates in lower-layer MLP parameters. This approach provides AI practitioners with a scalable and reliable framework for enhancing LLM tool-use capabilities. |
| Other | Bridging Theory and Practice in Quantum Game Theory: Optimized
  Implementation of the Battle of the Sexes with Error Mitigation on NISQ
  Hardware (Read more on [arXiv](https://arxiv.org/abs/2508.09050) or [HuggingFace](https://huggingface.co/papers/2508.09050))| Jhon Alejandro Andrade, Mateo Buenaventura Samboni, Carlos Andres Duran Paredes, Germ√°n D√≠az Agreda, sebasmos | This paper presents an experimental validation of the quantum "Battle of the Sexes" game on NISQ hardware using the Eisert-Wilkens-Lewenstein framework. The research objective is to assess the viability of quantum strategic advantages under realistic noise conditions by implementing and comparing different quantum strategies. The study introduces a Guided Circuit Mapping (GCM) strategy to mitigate noise, dynamically selecting qubit pairs based on real-time calibration data. Experimental results with GCM preserve payoff trends within 3.5%-12% relative error of theoretical predictions. This demonstrates that quantum advantages in strategic coordination can persist under realistic NISQ conditions, providing a pathway for practical applications of quantum game theory. |
| Computer Vision | WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface
  Temperature Estimation via Spatio-Temporal Fusion (Read more on [arXiv](https://arxiv.org/abs/2508.06485) or [HuggingFace](https://huggingface.co/papers/2508.06485))| Rachid Nedjai, Raphael Canals, Adel Hafiane, sofianebouaziz | The paper introduces WGAST, a weakly-supervised generative network for daily 10 m Land Surface Temperature (LST) estimation via spatio-temporal fusion. It aims to estimate high-resolution LST by fusing data from Terra MODIS, Landsat 8, and Sentinel-2 satellites. WGAST employs a conditional GAN architecture with a generator consisting of feature extraction, fusion, LST reconstruction, and noise suppression stages, trained with a weak supervision strategy. Experiments show WGAST outperforms existing methods, reducing RMSE by 17.18% and improving SSIM by 11.00% compared to a baseline. This demonstrates the potential of generative models for high-resolution LST estimation, benefiting applications requiring fine-scale thermal data. |
| Natural Language Processing | GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via
  General Samples Replay (Read more on [arXiv](https://arxiv.org/abs/2508.04676) or [HuggingFace](https://huggingface.co/papers/2508.04676))| Yang Fan, Yuefeng Li, Mengchen Zhao, Shuoran Jiang, Yunan Zhang | The paper introduces General Sample Replay (GeRe), an efficient anti-forgetting framework for continually learning Large Language Models (LLMs). It addresses catastrophic forgetting by using pretraining texts and neural states. GeRe leverages threshold-based margin (TM) loss to maintain neural activation state consistency during replay learning. Experiments show TM improves performance and robustness, with a small, fixed set of general replay samples retaining general capabilities while promoting overall sequential task performance. For example, the BaselineR+TM method achieves a F1 Avg of 61.9756. The implication is that efficient LLM replay strategies can be realized using fixed, general replay samples, enhancing the applicability of CL to resource-constrained scenarios. |
| Natural Language Processing | BiasGym: Fantastic Biases and How to Find (and Remove) Them (Read more on [arXiv](https://arxiv.org/abs/2508.08855) or [HuggingFace](https://huggingface.co/papers/2508.08855))| Arnav Arora, Haeun Yu, Siddhesh Milind Pawar, Nadav Borenstein, sekhcopenlu | The paper introduces BiasGym, a framework for injecting, analyzing, and mitigating biases in LLMs. It aims to provide a simple and generalizable approach for understanding and debiasing conceptual associations within models. The methodology involves injecting specific biases via token-based fine-tuning and using injected signals to identify and steer components responsible for biased behavior. Experiments demonstrate the framework's effectiveness in reducing real-world stereotypes with stereotype strength metrics decreasing for a range of biases. BiasGym allows for consistent bias elicitation and targeted mitigation in LLMs. |
| Natural Language Processing | NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech
  Modeling with Paralinguistic Vocalizations (Read more on [arXiv](https://arxiv.org/abs/2508.04195) or [HuggingFace](https://huggingface.co/papers/2508.04195))| Haoyue Zhan, Yiheng Lu, Yuancheng Wang, Qinke Ni, Huan Liao | This paper presents NVSpeech, an integrated pipeline for modeling human-like speech by incorporating paralinguistic vocalizations in Mandarin. The research aims to improve ASR and TTS systems by creating a word-level annotated corpus of 18 paralinguistic categories and developing paralinguistic-aware models. The methodology includes manual annotation of a subset, scalable labeling via a paralinguistic-aware ASR, and expressive TTS modeling with zero-shot fine-tuning. Results demonstrate a paralinguistic tag recognition F1 score of up to 0.84, suggesting improved performance. NVSpeech enables controllable paralinguistic cue insertion and improves the naturalness of synthesized speech, offering a scalable foundation for expressive speech modeling. |
