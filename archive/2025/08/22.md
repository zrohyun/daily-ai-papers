

## Papers for 2025-08-22

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Multi-Modal | Intern-S1: A Scientific Multimodal Foundation Model (Read more on [arXiv](https://arxiv.org/abs/2508.15763) or [HuggingFace](https://huggingface.co/papers/2508.15763))| xuhuang87, ZhouqiHUA, Jerry-hyl, guox18, gaoyang07 | The paper introduces Intern-S1, a scientific multimodal foundation model designed to advance AGI by analyzing diverse scientific data. Its primary objective is to bridge the performance gap between open-source and closed-source models in scientific domains. The model employs a Mixture-of-Experts architecture with 28B activated parameters, pre-trained on 5T tokens, and refined using reinforcement learning with a Mixture-of-Rewards approach. Intern-S1 achieves state-of-the-art performance among open-source models in scientific reasoning tasks, outperforming closed-source models in professional tasks like molecular synthesis, reaction condition prediction, and predicting thermodynamic stabilities for crystals. The model provides AI practitioners with a powerful tool for accelerating scientific discovery across various modalities, potentially transforming research workflows. |
| Multi-Modal | Mobile-Agent-v3: Foundamental Agents for GUI Automation (Read more on [arXiv](https://arxiv.org/abs/2508.15144) or [HuggingFace](https://huggingface.co/papers/2508.15144))| Haowei Liu, Haiyang Xu, Xi Zhang, Jiabo Ye, LZXzju | The paper introduces GUI-Owl and Mobile-Agent-v3, a framework for GUI automation. It aims to create foundational GUI agents capable of strong performance across various environments. The methodology includes large-scale environment infrastructure, diverse agent capability construction via pretraining on datasets for UI grounding, planning, and action semantics, and scalable environment RL. GUI-Owl-7B achieves 73.3 on AndroidWorld and 37.7 on OSWorld, outperforming existing open-source models. Mobile-Agent-v3 enables versatile decision-making and collaborative multi-agent coordination, and both GUI-Owl and Mobile-Agent-v3 are open-sourced, supporting AI practitioners to build powerful, generalizable GUI automation systems. |
| Machine Learning | Deep Think with Confidence (Read more on [arXiv](https://arxiv.org/abs/2508.15260) or [HuggingFace](https://huggingface.co/papers/2508.15260))| Xuewei Wang, jiaweizhao, tydsh, Viol2000 | The paper introduces Deep Think with Confidence (DeepConf), a method to enhance reasoning efficiency and performance in Large Language Models (LLMs) by leveraging model-internal confidence signals. DeepConf dynamically filters low-quality reasoning traces during or after generation, requiring no additional model training or hyperparameter tuning. The key methodology involves utilizing model-internal confidence to identify and discard less reliable reasoning paths. On the AIME 2025 benchmark, DeepConf achieved up to 99.9% accuracy while reducing generated tokens by up to 84.7% compared to full parallel thinking. This approach offers AI practitioners a computationally efficient way to improve the accuracy of LLMs in reasoning tasks by adaptive trace filtering based on confidence measures. |
| Computer Vision | SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass (Read more on [arXiv](https://arxiv.org/abs/2508.15769) or [HuggingFace](https://huggingface.co/papers/2508.15769))| Ya Zhang, Yanxu Meng, Weidi, haoningwu | The paper introduces SceneGen, a novel feedforward framework for generating multiple 3D assets within a single scene image. It addresses the challenge of synthesizing 3D scenes by simultaneously producing asset geometry, texture, and spatial arrangement from a single image and object masks. SceneGen employs a feature aggregation module to integrate local and global scene information and predicts asset positions using a position head coupled with structure decoders. Quantitative evaluations on the 3D-FUTURE dataset demonstrate superior performance, achieving a scene-level Chamfer Distance of 0.0118 and F-Score of 90.60, surpassing existing methods. SceneGen offers AI practitioners a computationally efficient approach for high-quality 3D content generation without optimization or asset retrieval. |
| Computer Vision | Waver: Wave Your Way to Lifelike Video Generation (Read more on [arXiv](https://arxiv.org/abs/2508.15761) or [HuggingFace](https://huggingface.co/papers/2508.15761))| Yifu Zhang, sweetrabor, xiaofengmei, clin1223, yifeihu | Waver is a high-performance foundation model for unified image and video generation. The research aims to create lifelike videos by addressing challenges in motion capture and temporal consistency. The methodology employs a Hybrid Stream DiT architecture, data curation with MLLM-based quality filtering, and detailed training recipes. Waver achieves Top 3 ranking on Artificial Analysis leaderboards, outperforming open-source models. This offers AI practitioners a robust approach and technical details for efficiently training high-quality video generation models. |
| Natural Language Processing | LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on
  Challenging Queries (Read more on [arXiv](https://arxiv.org/abs/2508.15760) or [HuggingFace](https://huggingface.co/papers/2508.15760))| huuuyeah, Ironieser, sileixu, dinghanshen, Kevin355 | This paper introduces LiveMCP-101, a benchmark for evaluating AI agents in realistic, dynamic environments using the Model Context Protocol (MCP). The benchmark consists of 101 carefully curated real-world queries requiring coordinated use of diverse MCP tools. A novel evaluation approach leverages ground-truth execution plans, rather than raw API outputs, to better reflect real-world conditions. Experiments show even frontier LLMs achieve a task success rate below 60%, indicating challenges in tool orchestration. The benchmark and associated analysis highlight failure modes and token inefficiencies, offering concrete directions for advancing autonomous AI systems. |
| Computer Vision | ATLAS: Decoupling Skeletal and Shape Parameters for Expressive
  Parametric Human Modeling (Read more on [arXiv](https://arxiv.org/abs/2508.15767) or [HuggingFace](https://huggingface.co/papers/2508.15767))| Shunsuke Saito, Javier Romero, Jinhyung Park, rawalkhirodkar, TakaakiWB | The paper introduces ATLAS, a high-fidelity parametric human body model that decouples skeletal and shape parameters for improved expressiveness and control. It aims to address limitations of existing models by explicitly separating shape and skeleton bases, grounding the mesh in the human skeleton. The methodology involves learning separate linear bases for external shape and internal skeleton from a large dataset of 600k high-resolution scans. Evaluations demonstrate ATLAS outperforms existing methods, achieving lower vertex error in fitting unseen subjects (e.g., 2.34mm on Goliath-Test). This decoupling offers AI practitioners more precise customization of body attributes and improved realism in human modeling applications. |
| Multi-Modal | "Does the cafe entrance look accessible? Where is the door?" Towards
  Geospatial AI Agents for Visual Inquiries (Read more on [arXiv](https://arxiv.org/abs/2508.15752) or [HuggingFace](https://huggingface.co/papers/2508.15752))| Xia Su, John S. O'Meara, Zeyu Wang, Jared Hwang, Jon E. Froehlich | This paper introduces Geo-Visual Agents, a novel multimodal AI system for understanding and responding to visual-spatial inquiries about the world. The primary objective is to leverage large-scale geospatial image repositories, combined with GIS data, to address geo-visual questions. The methodology involves analyzing streetscapes, place-based photos, and aerial imagery using multimodal AI techniques. As a proof of concept, StreetViewAI was developed combining geographic context, user information, and street view images into an MLLM, accessible via an AI chat interface achieving effective results in user navigation studies. Geo-Visual Agents can potentially transform navigation and place understanding, impacting applications for accessibility, landmark-based navigation, and personal safety. |
| Natural Language Processing | A Survey on Large Language Model Benchmarks (Read more on [arXiv](https://arxiv.org/abs/2508.15361) or [HuggingFace](https://huggingface.co/papers/2508.15361))| Siyi Li, Xuanang Chen, Shuaimin Li, Guhong Chen, Shiwen Ni | This paper presents a survey of 283 benchmarks used for evaluating Large Language Models (LLMs). It investigates the design motivations and limitations of existing benchmarks by categorizing them into general capabilities, domain-specific, and target-specific categories. The methodology includes analyzing data sources, formats, and evaluation methods of the surveyed benchmarks. The study identifies issues such as inflated scores due to data contamination and biases, emphasizing the need for improved benchmark design. This informs AI practitioners about the current state and future directions for evaluating LLMs, offering a referable design paradigm for benchmark innovation, though quantitative results directly from benchmark evaluations of specific models are not explicitly discussed. |
| Machine Learning | aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery
  Generated by AI Scientists (Read more on [arXiv](https://arxiv.org/abs/2508.15126) or [HuggingFace](https://huggingface.co/papers/2508.15126))| Heng Zhang, Yang Qi, Guowei Huang, Xiang Hu, Pengsong Zhang | The paper introduces aiXiv, a next-generation open-access platform for scientific discovery facilitated by AI agents. The main objective is to create a scalable ecosystem where AI agents can autonomously generate, review, refine, and publish scientific content. The platform integrates multi-agent workflows, a structured review system, and iterative refinement pipelines, implementing a multi-agent system and a closed-loop review process including prompt injection safeguards. Experiments show that aiXiv significantly enhances the quality of AI-generated research, with pairwise assessment accuracy of 77% on proposal-level benchmarks and 81% on paper-level benchmarks.  This work has implications for accelerating the publication and dissemination of high-quality AI-generated research content by offering a novel platform for autonomous scientific discovery. |
| Natural Language Processing | Fin-PRM: A Domain-Specialized Process Reward Model for Financial
  Reasoning in Large Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.15202) or [HuggingFace](https://huggingface.co/papers/2508.15202))| Lifan Guo, Junhui Li, Shuo Jiang, Yuanchen Zhou, amazingj | The paper introduces Fin-PRM, a domain-specialized process reward model for financial reasoning in large language models. The main objective is to improve intermediate reasoning steps in financial tasks, where structured, symbolic, and factually correct reasoning is essential. Fin-PRM integrates step-level and trajectory-level reward supervision and knowledge verification signals, and a dual-level training paradigm. Experiments on financial reasoning benchmarks demonstrate that Fin-PRM outperforms general-purpose PRMs, achieving a 12.9% gain in supervised learning. This highlights the importance of domain-specialized reward modeling for aligning LLMs with expert-level financial reasoning. |
| Computer Vision | Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in
  Milliseconds (Read more on [arXiv](https://arxiv.org/abs/2508.14892) or [HuggingFace](https://huggingface.co/papers/2508.14892))| Chuiyun Wu, Chen Yang, Jiemin Fang, Jia Lu, thewhole | Snap-Snap introduces a novel feed-forward framework for reconstructing 3D human bodies from only two images in milliseconds. The research aims to address the challenge of building 3D consistency and recovering missing information from sparse views. The method redesigns a geometry reconstruction model based on a foundation model and applies an enhancement algorithm for missing color information. Experiments demonstrate state-of-the-art performance, achieving human reconstruction in 190 ms on a single NVIDIA RTX 4090 with 1024x1024 images, as well as a LPIPS score of 13.24 on the THuman2.0 dataset. The method enables fast and accessible human reconstruction even with low-cost mobile devices, reducing data collection requirements. |
| Computer Vision | When and What: Diffusion-Grounded VideoLLM with Entity Aware
  Segmentation for Long Video Understanding (Read more on [arXiv](https://arxiv.org/abs/2508.15641) or [HuggingFace](https://huggingface.co/papers/2508.15641))| Rui Guo, Yuxia Chen, Pengcheng Fang | The paper introduces Grounded-VideoDiT, a Video-LLM designed for fine-grained temporal grounding and entity-level alignment in long videos. It aims to improve temporal perception in Video-LLMs by enhancing boundary sensitivity and maintaining temporal consistency. The key methodology involves a Diffusion Temporal Latent (DTL) encoder, object-grounded representations via semantic segmentation, and a mixed token scheme. The model achieves state-of-the-art performance on temporal video grounding, with 39.5 mIoU on Charades-STA. This work enables AI practitioners to build video understanding systems with improved temporal reasoning and object-aware understanding capabilities. |
| Natural Language Processing | INTIMA: A Benchmark for Human-AI Companionship Behavior (Read more on [arXiv](https://arxiv.org/abs/2508.09998) or [HuggingFace](https://huggingface.co/papers/2508.09998))| Yacine Jernite, Giada Pistilli, frimelle | The paper introduces INTIMA, a benchmark for evaluating companionship behaviors in language models. The research investigates how effectively AI systems manage emotionally charged interactions with users, drawing from psychological theories. INTIMA comprises 368 prompts and 31 behaviors, evaluating responses as companionship-reinforcing, boundary-maintaining, or neutral. Applied to models like Gemma-3, Phi-4, and Claude-4, results reveal that companionship-reinforcing behaviors are generally more prevalent, although differences across models exist. The study implies the need for consistent approaches in AI to handle emotionally charged interactions to ensure appropriate boundary setting and emotional support for users. |
