

## Papers for 2025-08-19

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long
  Narrative Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.10419) or [HuggingFace](https://huggingface.co/papers/2508.10419))| Yufeng Wang, Wei Wei, Rongchen Zhao, Juyuan Wang, lxucs | The paper introduces ComoRAG, a cognitive-inspired retrieval-augmented generation (RAG) framework for stateful long narrative reasoning. It addresses the limitation of stateless, single-step retrieval in traditional RAG methods by incorporating a dynamic memory workspace and iterative reasoning cycles, inspired by human cognitive processes. ComoRAG undergoes iterative reasoning cycles, generates probing queries, integrates retrieved evidence into a global memory pool, and shows consistent relative gains up to 11% compared to strong RAG baselines across four long-context narrative benchmarks. The primary result is a significant improvement in complex queries requiring global comprehension. The approach provides AI practitioners with a cognitively motivated paradigm for stateful reasoning in retrieval-based long-context comprehension. |
| Computer Vision | 4DNeX: Feed-Forward 4D Generative Modeling Made Easy (Read more on [arXiv](https://arxiv.org/abs/2508.13154) or [HuggingFace](https://huggingface.co/papers/2508.13154))| Zeng Tao, Jiawei Ren, Long Zhuo, Tianqi Liu, Zhaoxi Chen | The paper introduces 4DNeX, a feed-forward framework for generating dynamic 3D scene representations from a single image. It addresses the challenge of image-to-4D generation by fine-tuning a pretrained video diffusion model. The key methodology involves constructing a large-scale 4D dataset (4DNeX-10M) and introducing a unified 6D video representation with simple yet effective fine-tuning strategies. Experiments demonstrate that 4DNeX achieves high-quality dynamic point cloud generation enabling novel-view video synthesis.  The method achieves superior results with 97.2% consistency on the generated results, which can be leveraged for scalable image-to-4D modeling and simulating dynamic scene evolution. |
| Computer Vision | Next Visual Granularity Generation (Read more on [arXiv](https://arxiv.org/abs/2508.12811) or [HuggingFace](https://huggingface.co/papers/2508.12811))| Kang Liao, Qingyi Tao, Zhonghua Wu, Zhouxia Wang, yikaiwang | The paper introduces Next Visual Granularity (NVG) generation, a novel image generation framework that decomposes images into structured sequences representing varying levels of visual granularity. The primary objective is to achieve fine-grained control over the image generation process. NVG iteratively generates structure maps and corresponding tokens to refine the image, starting from coarse structures and progressing to fine details. The model demonstrates scalability, achieving an improved FID score of 3.03 compared to VAR's 3.30 on ImageNet. This structured approach allows for more intuitive image manipulation and improved generation quality compared to methods lacking explicit structural modeling. |
| Natural Language Processing | Speed Always Wins: A Survey on Efficient Architectures for Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.09834) or [HuggingFace](https://huggingface.co/papers/2508.09834))| Jusen Du, Yucheng Zhou, Jiaxi Hu, Weigao Sun, landisen | This survey paper provides a systematic examination of efficient architectures for large language models (LLMs). It addresses the computational limitations of traditional transformer architectures, aiming to boost efficiency in both training and deployment. The survey categorizes innovative architectures like linear attention, sparse sequence modeling, mixture-of-experts, and diffusion LLMs, discussing their technical details and applications across modalities. By grouping recent studies, the paper presents a blueprint for modern efficient LLM architectures. This benefits AI practitioners by offering resource-aware foundation model strategies for scalable and versatile AI systems. |
| Computer Vision | Has GPT-5 Achieved Spatial Intelligence? An Empirical Study (Read more on [arXiv](https://arxiv.org/abs/2508.13142) or [HuggingFace](https://huggingface.co/papers/2508.13142))| Ruisi Wang, Qingping Sun, Yubo Wang, yl-1993, caizhongang | This paper empirically investigates the spatial intelligence capabilities of multi-modal models, focusing on GPT-5's performance. It aims to evaluate whether GPT-5 has achieved spatial intelligence across eight benchmarks and six fundamental capabilities. The study evaluates state-of-the-art proprietary and open-source models on benchmarks, standardizing prompts and evaluation strategies. The empirical results reveal that GPT-5 demonstrates strength in spatial intelligence but falls short of human performance, achieving a chance-adjusted accuracy of 64.18% on SITE compared to 67.5% by humans. The study highlights the remaining challenges and provides insights for advancing future research on spatial intelligence in MLLMs. |
| Machine Learning | HeroBench: A Benchmark for Long-Horizon Planning and Structured
  Reasoning in Virtual Worlds (Read more on [arXiv](https://arxiv.org/abs/2508.12782) or [HuggingFace](https://huggingface.co/papers/2508.12782))| Artyom Sorokin, Viktor Volkov, Stefan Rebrikov, Petr Anokhin, roxal | HeroBench is a novel benchmark for evaluating long-horizon planning and structured reasoning in virtual worlds. The research aims to address the lack of benchmarks capturing complexities of realistic planning environments for large language models (LLMs). The paper introduces a rigorously constructed dataset of RPG-inspired tasks in a simulated environment to evaluate LLM performance. The experimental evaluation of 25 state-of-the-art LLMs revealed substantial performance disparities with Grok-4 achieving the highest success rate (91.7%). The benchmark advances the evaluation of LLM reasoning and provides a foundation for autonomous planning research. |
| Natural Language Processing | When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness
  Methods for LLMs (Read more on [arXiv](https://arxiv.org/abs/2508.11383) or [HuggingFace](https://huggingface.co/papers/2508.11383))| Elena Tutubalina, Gleb Ershov, Mikhail Chaichuk, apanc, myyycroft | This paper presents a large-scale evaluation of prompt robustness methods for LLMs, addressing the sensitivity of LLMs to variations in prompt formatting. The study benchmarks five methods across eight models from Llama, Qwen, and Gemma families using 52 tasks from Natural Instructions. The key methodology involves a unified experimental framework testing generalization against distribution shifts. Results show that calibration-based approaches improve robustness in the absence of distribution shifts, and frontier models exhibit greater inherent robustness. Practitioners can use these findings to make informed decisions when aiming for stable LLM performance, especially by leveraging calibration techniques where applicable. |
| Computer Vision | Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive
  World Model (Read more on [arXiv](https://arxiv.org/abs/2508.13009) or [HuggingFace](https://huggingface.co/papers/2508.13009))| Yifan Zhang, Boyang Wang, Zexiang Liu, Chunli Peng, Xianglong He | Matrix-Game 2.0 is an interactive world model for real-time, streaming video generation. It addresses the challenge of generating long, interactive videos at real-time speeds. The model employs a few-step auto-regressive diffusion process, action injection modules for interactive conditions, and distillation based on causal architecture. The framework achieves 25 FPS generation on a single H100 GPU while maintaining minute-long temporal consistency. The release of the model weights and codebase can advance research in interactive world modeling. |
| Computer Vision | Lumen: Consistent Video Relighting and Harmonious Background Replacement
  with Video Generative Models (Read more on [arXiv](https://arxiv.org/abs/2508.12945) or [HuggingFace](https://huggingface.co/papers/2508.12945))| Zixiang Gao, Chenxuan Miao, Yutong Feng, Yuxuan Liu, Jianshu Zeng | This paper presents Lumen, a framework for consistent video relighting and background replacement using video generative models. It aims to achieve harmonious blending between foreground and background while preserving foreground properties and temporal consistency. The methodology involves constructing a large-scale dataset with both synthetic and realistic videos and employing a multi-domain joint training curriculum with a style adapter. Experimental results demonstrate improved performance, with Lumen achieving a PSNR of 23.06 on realistic paired videos. This work enables AI practitioners to create cinematic-quality relighted videos with better control over lighting and background. |
| Computer Vision | S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of
  Diffusion Models (Read more on [arXiv](https://arxiv.org/abs/2508.12880) or [HuggingFace](https://huggingface.co/papers/2508.12880))| Meiqi Wu, Nisha Huang, Xiaokun Feng, Jiashu Zhu, Chubin Chen | The paper introduces S^2-Guidance, a training-free approach to enhance diffusion models by improving classifier-free guidance (CFG). It addresses semantic incoherence and low-quality outputs of CFG by leveraging stochastic block-dropping to construct sub-networks that guide the model away from suboptimal predictions.  S^2-Guidance constructs weak models to address the suboptimal predictions of CFG using stochastic block-dropping.  Evaluations on text-to-image and text-to-video generation tasks show S^2-Guidance surpasses CFG and other guidance strategies, achieving state-of-the-art performance (e.g., consistently outperforming on HPSv2.1 across all dimensions). The method offers AI practitioners a simple and effective way to improve the quality and coherence of diffusion model outputs without additional training. |
| Natural Language Processing | Representing Speech Through Autoregressive Prediction of Cochlear Tokens (Read more on [arXiv](https://arxiv.org/abs/2508.11598) or [HuggingFace](https://huggingface.co/papers/2508.11598))| Daniel L. K. Yamins, Evelina Fedorenko, Greta Tuckute, klemenk | The paper introduces AuriStream, a biologically-inspired speech representation model leveraging autoregressive prediction of cochlear tokens for versatile speech encoding. It aims to learn robust speech representations by mimicking human auditory processing through a two-stage framework: WavCoch (waveform-to-cochleagram tokenizer) and AuriStream (autoregressive sequence model). The methodology involves training an autoregressive transformer on discrete cochlear tokens derived from a time-frequency representation inspired by the human cochlea. AuriStream achieves competitive performance on downstream SUPERB speech tasks and state-of-the-art results on lexical semantics (sSIMI), demonstrating strong phoneme and word representation capabilities. AuriStream provides a framework for developing more human-like speech models by focusing on biologically-inspired representations. |
| Multi-Modal | Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision
  Mapping (Read more on [arXiv](https://arxiv.org/abs/2508.12466) or [HuggingFace](https://huggingface.co/papers/2508.12466))| Tyler Derr, xuhuizhan5 | Inverse-LLaVA introduces a novel text-to-vision mapping approach that eliminates the need for alignment pre-training in multimodal learning. The paper investigates whether projecting text embeddings into continuous visual space can preserve visual information richness and improve multimodal understanding.  The method fuses text and vision embeddings within transformer layers using selective additive components in attention mechanisms. Experiments across nine multimodal benchmarks demonstrate notable improvements on reasoning-intensive tasks (e.g., cognitive reasoning: +27.2%) but show expected decreases in perception tasks requiring visual-text associations.  This suggests that architectural innovation can potentially substitute for data-intensive alignment procedures, and can create more efficient multimodal architectures. |
| Computer Vision | Precise Action-to-Video Generation Through Visual Action Prompts (Read more on [arXiv](https://arxiv.org/abs/2508.13104) or [HuggingFace](https://huggingface.co/papers/2508.13104))| Minghan Qin, Sida Peng, Haoyu Guo, walsvid, angshineee | The paper presents a novel approach to action-driven video generation using visual action prompts for improved precision and cross-domain adaptability. The research aims to address the precision-generality tradeoff in action representation by using visual skeletons as domain-agnostic prompts. They construct skeletons from human-object interaction (HOI) and robotic manipulation datasets, fine-tuning a pre-trained video generation model with these prompts. Experiments on EgoVid, RT-1, and DROID demonstrate the effectiveness, achieving an ST-IoU of 0.604 on RT-1 dataset using visual action prompts compared to 0.507 with raw state actions. This allows for more precise action control and enables joint training on heterogeneous datasets for cross-domain knowledge transfer. |
| Computer Vision | G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior
  Integration (Read more on [arXiv](https://arxiv.org/abs/2508.11379) or [HuggingFace](https://huggingface.co/papers/2508.11379))| Evgeny Burnaev, Peter Wonka, Artem Komarichev, rusrakhimov, smileyenot983 | The paper introduces G-CUT3R, a novel feed-forward approach for guided 3D scene reconstruction that integrates prior information like depth maps and camera parameters. The research aims to enhance the CUT3R model by leveraging commonly available auxiliary data. G-CUT3R incorporates dedicated encoders for each modality, fusing features with RGB image tokens via zero convolution. Experiments demonstrate significant performance improvements across multiple benchmarks, achieving state-of-the-art results and enabling more reliable 3D reconstructions (e.g., 61% reduction in ATE on Sintel). The flexible design enables AI practitioners to utilize various prior information sources to improve 3D scene reconstruction accuracy. |
| Machine Learning | Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning
  Models to Ask for Information (Read more on [arXiv](https://arxiv.org/abs/2508.11252) or [HuggingFace](https://huggingface.co/papers/2508.11252))| Xi Yang, Duanyu Feng, Chen Huang, Bowen Qin, YouchengHuang | This paper introduces CRITIC-math, a new dataset designed to evaluate large reasoning models (LRMs) on their ability to proactively ask for information when solving incomplete mathematical problems. The research investigates the extent to which LRMs can identify incomplete problems, the reasons for their failure to ask questions, and the potential for supervised fine-tuning (SFT) to improve this ability. The study reveals that LRMs exhibit an inability to proactively ask for information, often overthinking or hallucinating solutions. SFT demonstrates the potential for improving this ability, although a dilemma exists between deep-thinking and proactively asking questions, achieving an accuracy of 87.86% on well-defined problems after SFT. The work highlights the need for a shift in how LRMs are developed, focusing on proactive information-seeking rather than solely on solving well-defined quizzes. |
