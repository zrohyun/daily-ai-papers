

## Papers for 2025-08-11

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models (Read more on [arXiv](https://arxiv.org/abs/2508.06471) or [HuggingFace](https://huggingface.co/papers/2508.06471))| GLM-4. 5 Team, zixuanlimit, ZAHNGYUXUAN, LiquidAmmonia, Stanislas | GLM-4.5 is an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters designed to achieve strong performance across agentic, reasoning, and coding tasks. The paper aims to create a generalist model that excels across agentic abilities, complex reasoning, and advanced coding skills. The model employs multi-stage training on 23T tokens, expert model iteration, and reinforcement learning using a hybrid reasoning method supporting thinking and direct response modes. GLM-4.5 achieves 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified, ranking 3rd overall among evaluated models. The release of GLM-4.5 and GLM-4.5-Air (106B parameters) advances research in reasoning and agentic AI systems by providing open-source models with strong ARC task performance. |
| Computer Vision | Voost: A Unified and Scalable Diffusion Transformer for Bidirectional
  Virtual Try-On and Try-Off (Read more on [arXiv](https://arxiv.org/abs/2508.04825) or [HuggingFace](https://huggingface.co/papers/2508.04825))| jgkwak, RyanL22 | The paper introduces Voost, a unified diffusion transformer for bidirectional virtual try-on and try-off. It addresses the challenge of accurately modeling garment-body correspondence, especially under pose and appearance variation, by jointly training try-on and try-off tasks. The methodology employs token-level concatenation and flexible conditioning over generation direction and garment category. Voost achieves state-of-the-art results on both try-on and try-off benchmarks, outperforming baselines in alignment accuracy, visual fidelity, and generalization; for instance, it attains an FID of 5.269 on the VITON-HD dataset. This work enables more robust and generalizable virtual try-on/try-off systems without task-specific networks or additional labels, paving the way for enhanced human-garment interaction in AI applications. |
| Multi-Modal | InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy
  Optimization (Read more on [arXiv](https://arxiv.org/abs/2508.05731) or [HuggingFace](https://huggingface.co/papers/2508.05731))| Pengxiang Li, Shuanghe Zhu, Zeyu Liu, xiaotianhan, SiriusL | The paper introduces InfiGUI-G1, a new framework for improving GUI grounding in MLLMs using adaptive exploration. It addresses the challenge of robustly grounding natural language instructions to GUI elements, focusing on semantic alignment. The core methodology is Adaptive Exploration Policy Optimization (AEPO), integrating multi-answer generation and a theoretically grounded Adaptive Exploration Reward (AER). The InfiGUI-G1 models achieve state-of-the-art results on GUI grounding benchmarks, with relative improvements up to 9.0% against a naive RLVR baseline. The framework presents a more efficient exploration strategy, enabling AI practitioners to create better-performing GUI agents with enhanced semantic understanding. |
| Natural Language Processing | Memp: Exploring Agent Procedural Memory (Read more on [arXiv](https://arxiv.org/abs/2508.06433) or [HuggingFace](https://huggingface.co/papers/2508.06433))| Shuofei Qiao, Jialong Wu, Xiaobin Wang, Yuan Liang, Runnan Fang | The paper introduces Memp, a framework to endow LLM agents with learnable, updatable procedural memory to address brittle memory issues. It investigates strategies for building, retrieving, and updating procedural memory by distilling agent trajectories into step-by-step instructions and script-like abstractions. The key methodology involves a dynamic regimen that continuously updates, corrects, and deprecates memory content based on new experiences. Empirical evaluation on TravelPlanner and ALFWorld demonstrates that agents using refined procedural memory achieve higher success rates and greater efficiency, including reducing steps. Transfer learning of procedural memory from a stronger model to a weaker model yields substantial performance gains, implying improved adaptability. |
| Machine Learning | Pruning the Unsurprising: Efficient Code Reasoning via First-Token
  Surprisal (Read more on [arXiv](https://arxiv.org/abs/2508.05988) or [HuggingFace](https://huggingface.co/papers/2508.05988))| Chengcheng Wan, Chao Hu, Yaoning Wang, Wenhao Zeng, YerbaPage | The paper introduces ASAP, a coarse-to-fine framework to compress Chain-of-Thought (CoT) reasoning traces for efficient code reasoning. It addresses the challenge of excessively long reasoning traces in Large Reasoning Models (LRMs) by pruning redundant information. ASAP leverages anchor-guided pruning to preserve the core reasoning structure and first-token surprisal to select logically essential steps. Experiments on LiveCodeBench v4_v5 show ASAP reduces token generation by 23.5% and inference latency by 43.5% while maintaining a competitive 36.19% Pass@1 accuracy. This enables efficient reasoning in coding tasks by distilling concise CoTs into LRMs. |
| Computer Vision | GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing (Read more on [arXiv](https://arxiv.org/abs/2508.02831) or [HuggingFace](https://huggingface.co/papers/2508.02831))| Przemys≈Çaw Spurek, Tomasz Szczepanik, Krzysztof Byrski, MikolajZ | The paper introduces GENIE, a hybrid model combining Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) for interactive 3D scene editing. It addresses the challenge of editing implicit NeRF representations by encoding Gaussian primitives with trainable feature embeddings to condition a NeRF network. The key methodology involves Ray-Traced Gaussian Proximity Search (RT-GPS) for efficient nearest Gaussian retrieval and Splash Grid Encoding for multi-resolution feature updates. GENIE achieves comparable reconstruction quality to SOTA methods on the NeRF-Synthetic dataset, outperforming RIP-NeRF in most scenes. GENIE provides AI practitioners with a framework enabling intuitive scene manipulation and dynamic interaction by bridging geometry-based editing with neural rendering. |
| Multi-Modal | Adapting Vision-Language Models Without Labels: A Comprehensive Survey (Read more on [arXiv](https://arxiv.org/abs/2508.05547) or [HuggingFace](https://huggingface.co/papers/2508.05547))| Eleni Chatzi, Ran He, Jian Liang, Lijun Sheng, Hao Dong | This survey comprehensively reviews unsupervised adaptation techniques for vision-language models (VLMs). It addresses the lack of task-oriented surveys by categorizing methods based on unlabeled visual data availability. The paper proposes a taxonomy of data-free transfer, unsupervised domain transfer, episodic test-time adaptation, and online test-time adaptation, analyzing methodologies within each paradigm to establish a systematic understanding. It reviews representative benchmarks across applications and identifies open challenges. An actively maintained repository is available at the URL provided. This survey assists practitioners in selecting appropriate techniques and facilitates comparisons across future work. |
| Multi-Modal | MELLA: Bridging Linguistic Capability and Cultural Groundedness for
  Low-Resource Language MLLMs (Read more on [arXiv](https://arxiv.org/abs/2508.05502) or [HuggingFace](https://huggingface.co/papers/2508.05502))| Guohang Yan, Ruirui Chen, Nuo Chen, Jiaying Fei, Yufei Gao | The paper introduces MELLA, a new approach to enhance low-resource language MLLMs by bridging linguistic capability and cultural groundedness. It aims to address the limitations of existing multilingual methods that often neglect multimodal informativeness and cultural context. The methodology involves a dual-source strategy: native web alt-text for culture and MLLM-generated captions for linguistics, resulting in the MELLA dataset. Experimentally, fine-tuning MLLMs on MELLA demonstrates general performance improvement across eight languages, resulting in a higher keyword accuracy. This suggests improved generation of "thick descriptions" leading to enhanced linguistic capability and cultural groundedness in MLLMs for low-resource languages. |
| Computer Vision | MeshLLM: Empowering Large Language Models to Progressively Understand
  and Generate 3D Mesh (Read more on [arXiv](https://arxiv.org/abs/2508.01242) or [HuggingFace](https://huggingface.co/papers/2508.01242))| Yi Yang, Yi-Hsuan Tsai, Yufeng Wang, I-Chao Shen, Shuangkang Fang | The paper introduces MeshLLM, a framework for enabling large language models (LLMs) to understand and generate 3D meshes represented as text-serialized data. It addresses limitations in existing methods by introducing a Primitive-Mesh decomposition strategy, creating a dataset of 1500k+ samples, and employing vertex-face prediction and local mesh assembly training. MeshLLM outperforms the state-of-the-art LLaMA-Mesh in mesh generation quality and shape understanding, achieving superior results in COV metric (47.33 compared to 19.53) for chairs. The framework has the potential to enhance 3D data processing and reasoning capabilities within LLMs, opening new avenues for multimodal AI applications. |
| Reinforcement Learning | UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and
  Precise Inference-Time Grounding (Read more on [arXiv](https://arxiv.org/abs/2507.22025) or [HuggingFace](https://huggingface.co/papers/2507.22025))| Bingqi Chen, Zihan Song, Jia Ma, Yuhang Wu, LianShuQuan | The paper introduces UI-AGILE, a framework for enhancing GUI agents via reinforcement learning and precise grounding. It addresses the limitations of current methods, which struggle with reasoning design, ineffective reward, and visual noise. The key methodology involves a continuous reward function, a "Simple Thinking" reward, cropping-based resampling during training, and decomposed grounding with selection during inference. Experiments on ScreenSpot-Pro demonstrate a 23% grounding accuracy improvement over the best baseline by using both training and inference enhancement methods. UI-AGILE provides AI practitioners with a means to improve GUI agent performance and robustness on high-resolution displays. |
| Computer Vision | LightSwitch: Multi-view Relighting with Material-guided Diffusion (Read more on [arXiv](https://arxiv.org/abs/2508.06494) or [HuggingFace](https://huggingface.co/papers/2508.06494))| Shubham Tulsiani, Fernando De la Torre, thebluser | LightSwitch is a novel framework for multi-view relighting of 3D scenes. The paper aims to improve relighting consistency and quality by incorporating multi-view information and material properties into a diffusion model. LightSwitch leverages a finetuned material-relighting diffusion framework, employing multi-view attention and inferred intrinsic properties. The method demonstrates superior 2D relighting prediction quality compared to existing methods, with 2D relighting achieving a PSNR of 26.01 and also matches or outperforms state-of-the-art diffusion inverse rendering methods in relighting synthetic and real objects. This enables AI practitioners to generate consistent and realistic relighting of 3D scenes more efficiently. |
