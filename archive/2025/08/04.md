

## Papers for 2025-08-04

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Natural Language Processing | Beyond Fixed: Variable-Length Denoising for Diffusion Large Language
  Models (Read more on [arXiv](https://arxiv.org/abs/2508.00819) or [HuggingFace](https://huggingface.co/papers/2508.00819))| Jiaqi Wang, Yuhang Cao, Yuhang Zang, Xiaoyi Dong, Jinsong Li | This paper introduces DAEDAL, a novel training-free denoising strategy for Diffusion Large Language Models (DLLMs) that enables dynamic, variable-length generation. The core objective is to overcome the limitation of DLLMs requiring a static, predefined generation length, thereby improving efficiency and capability. DAEDAL employs a two-phase approach: an initial length adjustment based on sequence completion metrics, followed by iterative mask insertion to expand insufficient generation regions. Experiments demonstrate that DAEDAL achieves performance comparable (and sometimes superior) to tuned fixed-length baselines, reaching 85.8 accuracy on GSM8K, while also enhancing computational efficiency by improving effective token ratios. The method allows for more flexible and efficient DLLM usage by eliminating the rigid length constraint, making DLLMs more competitive with autoregressive models. |
| Computer Vision | PixNerd: Pixel Neural Field Diffusion (Read more on [arXiv](https://arxiv.org/abs/2507.23268) or [HuggingFace](https://huggingface.co/papers/2507.23268))| Limin Wang, Weilin Huang, Chenhui Zhu, Ziteng Gao, Shuai Wang | The paper introduces PixNerd, a single-stage pixel space diffusion model enhanced by neural fields for image generation. It addresses the limitations of latent diffusion models, which suffer from accumulated errors and decoding artifacts, by directly operating in pixel space. PixNerd uses a diffusion transformer to predict the weights of neural field MLPs for patch-wise decoding, modeling fine details under large patch configurations. The model achieves 2.15 FID on ImageNet 256x256 without a VAE or cascade pipeline, demonstrating competitive performance. PixNerd offers a more efficient and end-to-end alternative to existing diffusion models, enabling high-quality image generation with reduced computational demands. |
| Machine Learning | SWE-Exp: Experience-Driven Software Issue Resolution (Read more on [arXiv](https://arxiv.org/abs/2507.23361) or [HuggingFace](https://huggingface.co/papers/2507.23361))| Heng Lian, Yuling Shi, Xiaodong Gu, Shaoxin Lin, Silin Chen | This paper introduces SWE-Exp, an experience-enhanced approach for software issue resolution using large language model agents. The research focuses on improving agent performance by incorporating reusable knowledge from past repair attempts to overcome the limitations of memoryless exploration. SWE-Exp distills experiences from prior agent trajectories into a multi-faceted experience bank and employs a dual-agent architecture for strategic planning and tactical implementation. Experiments on SWE-bench-Verified achieve state-of-the-art results, with a 41.6% Pass@1 resolution rate. SWE-Exp demonstrates that accumulating and leveraging repair expertise significantly improves automated software engineering agents' performance, shifting from trial-and-error exploration to strategic, experience-driven issue resolution. |
| Computer Vision | Multimodal Referring Segmentation: A Survey (Read more on [arXiv](https://arxiv.org/abs/2508.00265) or [HuggingFace](https://huggingface.co/papers/2508.00265))| Zuxuan Wu, Chang Liu, Shuting He, Song Tang, Henghui Ding | This survey paper provides a comprehensive review of multimodal referring segmentation, which segments objects in visual scenes based on text or audio referring expressions. The research aims to categorize task formulations, input modalities, and challenges within referring segmentation. The authors summarize a unified meta-architecture and review representative methods across images, videos, and 3D scenes. Extensive performance comparisons on standard benchmarks are provided; although specific quantitative results are not highlighted in the abstract, the authors continually track related work. The survey fosters a deeper understanding of the field and aids in advancing multimodal solutions. |
| Computer Vision | 3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding (Read more on [arXiv](https://arxiv.org/abs/2507.23478) or [HuggingFace](https://huggingface.co/papers/2507.23478))| Hao Tang, Zeyu Zhang, Ting Huang | This paper introduces 3D-R1, a foundation model for enhancing reasoning in 3D Vision-Language Models (VLMs) for unified scene understanding. The research aims to improve robust reasoning and generalization in 3D VLMs by addressing limitations in spatial data and viewpoint assumptions. The key methodology involves constructing a high-quality synthetic dataset (Scene-30K) with CoT and leveraging RLHF with reward functions for perception, semantic similarity, and format. Experiments show that 3D-R1 achieves an average improvement of 10% across various 3D scene benchmarks. 3D-R1 provides AI practitioners with enhanced reasoning and generalization capabilities for applications in embodied AI, robotics, and mixed reality. |
| Machine Learning | SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution (Read more on [arXiv](https://arxiv.org/abs/2507.23348) or [HuggingFace](https://huggingface.co/papers/2507.23348))| Heng Lian, Xiaodong Gu, Shaoxin Lin, Yuling Shi, Han Li | The paper introduces SWE-Debate, a competitive multi-agent debate framework for software issue resolution. It aims to improve fault localization by encouraging diverse reasoning paths and consolidated fix plans. The methodology involves creating fault propagation traces, organizing a three-round debate among specialized agents with distinct reasoning perspectives, and integrating the consolidated plan into an MCTS-based code modification agent. Experiments on the SWE-bench benchmark demonstrate that SWE-Debate achieves a new state-of-the-art result, with a 41.4% success rate on issue resolution, outperforming baselines by a large margin. The framework's emphasis on competitive reasoning and structured argumentation provides AI practitioners with a method to enhance automated software repair systems by enabling more accurate fault localization and consolidated fix generation. |
| Natural Language Processing | Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges (Read more on [arXiv](https://arxiv.org/abs/2508.00454) or [HuggingFace](https://huggingface.co/papers/2508.00454))| Chengfei Lv, Zhiwen Chen, Yunfeng Wang, Kehua Feng, Yuqi Tang | This paper introduces an efficient multi-turn dialogue evaluator (MTDEval) that aggregates preference knowledge from multiple LLM judges. The research aims to reduce the computational overhead of traditional multi-judge approaches for dialogue quality assessment. MTDEval employs a text-embedding model with specialized scoring heads, trained using a maximum likelihood estimation approach to capture the collective wisdom of multiple LLM judges. Experimental results on seven dialogue evaluation benchmarks demonstrate that MTDEval outperforms existing baselines, achieving improvements of over 10% compared to ArmoRM on xDial-IEval. The findings provide a computationally efficient and robust alternative for evaluating dialogue quality in large-scale applications, enabling practitioners to quickly assess LLM performance. |
| Natural Language Processing | Investigating Hallucination in Conversations for Low Resource Languages (Read more on [arXiv](https://arxiv.org/abs/2507.22720) or [HuggingFace](https://huggingface.co/papers/2507.22720))| Fatemeh Jamshidi, Zheng Zhang, Souvika Sarkar, Md. Najib Hasan, Amit Das | This paper investigates hallucination in conversational LLMs across low-resource languages. The research focuses on assessing the factual accuracy and reliability of LLMs when generating text in Hindi, Farsi, and Mandarin. The methodology involves analyzing outputs from GPT-3.5, GPT-40, Llama-3.1, Gemma-2.0, DeepSeek-R1 and Qwen-3 using ROUGE scores to quantify hallucination levels. Results indicate minimal hallucination in Mandarin, with ROUGE scores barely exceeding 1.0, but significantly higher hallucination in Hindi and Farsi. The study implies that LLM performance is strongly influenced by language-resource availability, highlighting the need for targeted mitigation strategies in low-resource languages to ensure equitable and reliable conversational AI systems. |
| Computer Vision | IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation (Read more on [arXiv](https://arxiv.org/abs/2508.00823) or [HuggingFace](https://huggingface.co/papers/2508.00823))| Jianjiang Feng, Ziwei Wang, Hang Yin, Xiuwei Xu, Wenxuan Guo | This paper introduces IGL-Nav, a novel framework for efficient image-goal navigation using incremental 3D Gaussian localization. The research aims to address the limitations of existing methods in accurately localizing the goal image within the explored 3D environment. IGL-Nav incrementally updates a 3D Gaussian scene representation with feed-forward monocular prediction and employs a coarse-to-fine localization strategy leveraging geometric and photometric information. The method outperforms state-of-the-art methods by a large margin across diverse experimental configurations, achieving a success rate of 82.8% on the Habitat simulator. The proposed IGL-Nav allows for enhanced and more robust visual navigation in complex environments with implications for practical applications in robotics. |
| Computer Vision | SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware
  Video Generation (Read more on [arXiv](https://arxiv.org/abs/2508.00782) or [HuggingFace](https://huggingface.co/papers/2508.00782))| Long Chen, Qifeng Chen, Yazhou Xing, Yingqing He, Kien T. Pham | The paper introduces SpA2V, a framework for generating spatially-aware videos from audio inputs by harnessing spatial auditory cues. It aims to improve audio-visual correspondence in generated videos by explicitly incorporating spatial information derived from the audio. The method decomposes the generation into audio-guided video planning and layout-grounded video generation, using a MLLM to construct Video Scene Layouts (VSLs) and a diffusion model to generate videos from these layouts. Evaluated on AVLBench, SpA2V demonstrates improved semantic and spatial correspondence with input audios, achieving a high MaxIoU score compared to baselines. This approach offers AI practitioners a novel way to synthesize videos with improved spatial coherence, addressing a key limitation of existing audio-to-video generation techniques. |
