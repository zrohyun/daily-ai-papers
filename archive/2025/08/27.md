

## Papers for 2025-08-27

| Category | Title | Authors | Summary |
|----------|-------|---------|---------|
| Machine Learning | CMPhysBench: A Benchmark for Evaluating Large Language Models in
  Condensed Matter Physics (Read more on [arXiv](https://arxiv.org/abs/2508.18124) or [HuggingFace](https://huggingface.co/papers/2508.18124))| Dongchen Huang, komusama0930, BoringMarsh, di-zhang-fdu, weidawang | The paper introduces CMPhysBench, a novel benchmark for evaluating large language models (LLMs) in the domain of condensed matter physics. It aims to assess the proficiency of LLMs in solving calculation problems requiring a deep understanding of physics. The benchmark comprises over 520 graduate-level questions, and a new Scalable Expression Edit Distance (SEED) score is proposed for fine-grained evaluation. Experiments reveal a performance gap, with the best models achieving only a 36 average SEED score and 28% accuracy. This highlights the need for domain-specific training and evaluation methods to improve LLMs' capabilities in complex scientific fields. |
| Reinforcement Learning | TreePO: Bridging the Gap of Policy Optimization and Efficacy and
  Inference Efficiency with Heuristic Tree-based Modeling (Read more on [arXiv](https://arxiv.org/abs/2508.17445) or [HuggingFace](https://huggingface.co/papers/2508.17445))| Zhoufutu Wen, Qingshui Gu, zhangysk, aaabiao, yizhilll | The paper introduces TreePO, a reinforcement learning framework designed to improve both policy optimization and inference efficiency for large language models. It addresses the challenge of balancing exploration and exploitation by employing a self-guided rollout algorithm based on tree-structured search and fixed-length segment decoding. The methodology includes a segment-wise sampling algorithm and a tree-based segment-level advantage estimation, leveraging local uncertainty to warrant additional branches. Experiments on reasoning benchmarks show TreePO achieves up to 43% savings in GPU hours for sampling, while improving or maintaining performance compared to existing methods (e.g., 54.61% overall accuracy on test sets). TreePO offers a practical approach to scaling RL-based post-training with fewer samples and reduced computational cost, though the specific implications for general AI practitioners depend on their computational resources and desired level of performance. |
| Natural Language Processing | VibeVoice Technical Report (Read more on [arXiv](https://arxiv.org/abs/2508.19205) or [HuggingFace](https://huggingface.co/papers/2508.19205))| Yaoyao Chang, Wenhui Wang, Jianwei Yu, Zhiliang Peng, unilm | The paper introduces VibeVoice, a novel model for synthesizing long-form, multi-speaker speech. It addresses the challenge of generating extended conversational audio by employing next-token diffusion with a custom speech tokenizer. The model achieves a 3200x data compression rate while maintaining audio fidelity and computational efficiency. Subjective evaluations demonstrate that VibeVoice outperforms existing open-source and proprietary models in preference, realism, and richness; for example, it can synthesize over 5000 seconds of audio. This advancement enables AI practitioners to create more realistic and extended conversational AI applications, although the results are limited to English and Chinese. |
| Computer Vision | VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D
  Space (Read more on [arXiv](https://arxiv.org/abs/2508.19247) or [HuggingFace](https://huggingface.co/papers/2508.19247))| Rui Chen, Gengxiong Zhuang, Zehuan Huang, fenghora, Nelipot | VoxHammer is a novel training-free framework for precise and coherent 3D local editing in native 3D space. The research addresses the challenge of maintaining consistency and coherence when editing 3D models. It leverages a pretrained 3D latent diffusion model, introducing precise 3D inversion and denoising-based editing using inverted latents and key-value tokens to preserve unedited regions. Experiments on the Edit3D-Bench demonstrate that VoxHammer outperforms existing methods, achieving a DINO-I score of 0.947. This method provides AI practitioners with a way to synthesize high-quality edited paired 3D data. |
| Machine Learning | Spacer: Towards Engineered Scientific Inspiration (Read more on [arXiv](https://arxiv.org/abs/2508.17661) or [HuggingFace](https://huggingface.co/papers/2508.17661))| zerojun48, kohandy, rallyduck1005, MoonRainy21, mhlee1022 | The paper introduces Spacer, a novel scientific discovery system aimed at overcoming creative limitations of LLMs by decoupling ideation from contextual constraints. Spacer operates through (i) NURI, an inspiration engine that builds keyword sets, and (ii) the Manifesting Pipeline that refines these sets into scientific statements. NURI extracts novel keyword sets from a biological keyword graph containing 180,000 academic publications and then the Manifesting Pipeline links the keywords. Experiments show that the evaluation metric of NURI accurately classifies high-impact publications with an AUROC score of 0.737 and also successfully reconstructs core concepts from the latest top-journal articles. The architecture facilitates novel knowledge generation by mitigating contextual biases inherent in standard LLMs, potentially enabling more creative AI-driven scientific research. |
| Computer Vision | OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive
  Simulation (Read more on [arXiv](https://arxiv.org/abs/2508.19209) or [HuggingFace](https://huggingface.co/papers/2508.19209))| Jiaqi Yang, Zerong Zheng, Weihong Zeng, Jianwen Jiang, chao0412 | The paper introduces OmniHuman-1.5, a framework for generating semantically coherent and expressive avatar animations by instilling an active mind via cognitive simulation. The primary objective is to generate character animations that are not only physically plausible but also contextually aware and emotionally resonant, moving beyond simple audio-driven synchronization. This is achieved through a Multimodal Large Language Model (MLLM) to synthesize structured textual representations for high-level semantic guidance and a specialized Multimodal DiT architecture with a Pseudo Last Frame design to fuse multimodal inputs. Experiments demonstrate that OmniHuman-1.5 achieves leading performance in semantic consistency with textual prompts, achieving a Fr√©chet Inception Distance (FID) of 31.160 on the CyberHost test set. The framework provides AI practitioners with a new perspective on avatar modeling, combining reactive and deliberative processes for enhanced realism and control. |
| Machine Learning | UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior
  Long-Context Learning (Read more on [arXiv](https://arxiv.org/abs/2508.18756) or [HuggingFace](https://huggingface.co/papers/2508.18756))| Ran Guo, Siyan Chen, Qiyang Min, Yu Bao, FetchFortune | UltraMemV2 introduces a memory-layer architecture to achieve performance parity with 8-expert MoE models while reducing memory access costs. The paper aims to bridge the performance gap between embedding-based and expert-based sparse models through architectural innovations and optimized training. UltraMemV2 integrates memory layers into every transformer block and adopts FFN-based value processing, along with optimized initialization and computational rebalancing. Results show UltraMemV2 achieves performance parity with 8-expert MoE models under the same computation and parameters, with +1.6 points improvement on long-context memorization. This work provides a compelling alternative for efficient sparse computation in large language models, balancing memory access and model performance. |
| Computer Vision | Pixie: Fast and Generalizable Supervised Learning of 3D Physics from
  Pixels (Read more on [arXiv](https://arxiv.org/abs/2508.17437) or [HuggingFace](https://huggingface.co/papers/2508.17437))| Dinesh Jayaraman, Chuhao Chen, Chen Wang, Ryan Lucas, vlongle | The paper introduces PIXIE, a method for learning simulatable physics of 3D scenes from visual features. PIXIE trains a neural network to predict physical properties like Young's modulus, Poisson's ratio, and density, addressing limitations in existing methods that rely on slow, per-scene optimization. The method leverages 3D visual features and supervised losses for generalizable physics prediction. PIXIE achieves 1.46-4.39x better realism scores than test-time optimization methods, while significantly reducing inference time. This enables faster and more generalizable material field inference for realistic 3D simulations. |
| Computer Vision | Autoregressive Universal Video Segmentation Model (Read more on [arXiv](https://arxiv.org/abs/2508.19242) or [HuggingFace](https://huggingface.co/papers/2508.19242))| Albert Gu, Yu-Chiang Frank Wang, Sukjun Hwang, Miran Heo, cmhungsteve | The paper introduces an autoregressive universal video segmentation model (AUSM) that unifies prompted and unprompted video segmentation tasks. It aims to address the fragmentation of task-specific models by recasting video segmentation as sequential mask prediction. AUSM employs state-space models to maintain a fixed-size spatial state and scales to video streams of arbitrary length with components designed for parallel training. On standard benchmarks, AUSM outperforms prior universal streaming video segmentation methods and achieves up to 2.5x faster training. This unified approach and parallel training scheme provide a more scalable and efficient solution for video segmentation tasks. |
| Computer Vision | Wan-S2V: Audio-Driven Cinematic Video Generation (Read more on [arXiv](https://arxiv.org/abs/2508.18621) or [HuggingFace](https://huggingface.co/papers/2508.18621))| Chaonan Ji, Mingyang Huang, Siqi Hu, Li Hu, Xin Gao | The paper introduces Wan-S2V, an audio-driven model for cinematic video generation capable of producing film-quality human animations. It aims to enhance the expressiveness and fidelity of audio-driven video generation in complex scenarios beyond simple talking heads. The methodology involves leveraging a DiT-based foundation model with specialized audio processing modules, a hybrid training strategy (FSDP with Context Parallelism), and optimized motion frame token reduction. Experimental results show that Wan-S2V outperforms existing methods like Hunyuan-Avatar and Omnihuman, achieving improved image quality (e.g., lower FID score of 15.66) and enhanced hand motion richness (higher HKV value). This advancement offers AI practitioners a robust tool for generating realistic and expressive human videos driven by audio, enabling diverse applications in film, television, and content creation. |
| Computer Vision | CineScale: Free Lunch in High-Resolution Cinematic Visual Generation (Read more on [arXiv](https://arxiv.org/abs/2508.15774) or [HuggingFace](https://huggingface.co/papers/2508.15774))| Ziwei Liu, Paul Debevec, Ziqi Huang, Ning Yu, Haonan Qiu | CineScale presents a tuning-free inference paradigm for high-resolution visual generation using pre-trained diffusion models. It addresses the challenge of generating high-fidelity visuals at resolutions exceeding training data limitations. CineScale employs tailored self-cascade upscaling, restrained dilated convolution, and scale fusion to maintain visual structure and eliminate repetitive patterns. Experiments show that CineScale enables 8k image generation and achieves 4k video generation with minimal LoRA fine-tuning, achieving FID scores as low as 44.723 for image generation. The approach empowers AI practitioners to leverage existing diffusion models for high-resolution content creation without extensive retraining or high-resolution datasets. |
| Computer Vision | FastMesh:Efficient Artistic Mesh Generation via Component Decoupling (Read more on [arXiv](https://arxiv.org/abs/2508.19188) or [HuggingFace](https://huggingface.co/papers/2508.19188))| Xingang Pan, Yongwei Chen, Armando Fortes, Yushi Lan, Jeonghwan Kim | The paper introduces FASTMESH, an efficient framework for artistic mesh generation through component decoupling. It addresses the redundancy in existing mesh generation approaches by separating vertex and face generation. The method uses an autoregressive model for vertex generation and a bidirectional transformer for face construction, significantly reducing token count. Experiments on the Toys4K dataset show FASTMESH achieves over 8x faster mesh generation compared to state-of-the-art approaches, while maintaining high mesh quality, evidenced by a Chamfer Distance of 4.05. FASTMESH offers AI practitioners a faster and more memory-efficient solution for generating high-quality 3D meshes. |
| Natural Language Processing | ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks (Read more on [arXiv](https://arxiv.org/abs/2508.15804) or [HuggingFace](https://huggingface.co/papers/2508.15804))| Kai Jia, Cong Ma, Zhihao Cheng, Ying Zeng, Minghao Li | ReportBench is a benchmark designed to evaluate the content quality of research reports generated by large language models (LLMs). The study addresses the need for rigorous evaluation of deep research agents by focusing on the quality of cited literature and the faithfulness of statements.  ReportBench leverages high-quality arXiv survey papers to derive domain-specific prompts and systematically analyzes generated reports by extracting citations and statements.  Empirical results show that commercial deep research agents generate more comprehensive reports than standalone LLMs augmented with search tools. However, there remains room for improvement in breadth, depth, and factual consistency, indicating ongoing challenges for LLM-based knowledge synthesis and AI-generated academic surveys. |
| Natural Language Processing | ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large
  Language Models (Read more on [arXiv](https://arxiv.org/abs/2508.18773) or [HuggingFace](https://huggingface.co/papers/2508.18773))| Jiangjie Chen, Mingxuan Wang, Xuefeng Li, Siyu Yuan, Qianyu He | The paper introduces ThinkDial, an open-recipe framework for controlling reasoning effort in large language models via discrete operational modes. It aims to address the challenge of controlling computational effort for practical deployment of LLMs by implementing gpt-oss-style controllable reasoning. The framework uses a novel end-to-end training paradigm integrating budget-mode supervised fine-tuning and two-phase budget-aware reinforcement learning with adaptive reward shaping. Experiments demonstrate successful target compression-performance trade-offs with clear response length reductions while maintaining performance thresholds, achieving gpt-oss-style reasoning. ThinkDial enables AI practitioners to implement controllable reasoning in LLMs without relying on proprietary systems, offering increased flexibility and efficiency. |
| Multi-Modal | MovieCORE: COgnitive REasoning in Movies (Read more on [arXiv](https://arxiv.org/abs/2508.19026) or [HuggingFace](https://huggingface.co/papers/2508.19026))| Hung-Ting Su, Ying Cheng, Jia-Fong Yeh, Gueter Josmy Faure, cmhungsteve | The paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content.  The primary objective is to challenge vision-language models (VLMs) with questions that require System-2 thinking about cinematic content.  The authors use an agentic brainstorming approach with multiple large language models (LLMs) to generate and refine high-quality question-answer pairs, and also introduce Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training. Experiments show that ACE improves model reasoning capabilities post-training by 25%. This implies that lightweight post-generation refinement strategies can unlock significant untapped potential in existing VLMs for deeper cognitive tasks. |
| Machine Learning | Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning
  Tasks (Read more on [arXiv](https://arxiv.org/abs/2508.18672) or [HuggingFace](https://huggingface.co/papers/2508.18672))| Daisuke Nohara, Takumi Okamoto, Masaki Kawamura, Satoki Ishikawa, Taishi-N324 | This paper investigates the optimal sparsity of Mixture-of-Experts (MoE) language models for memorization and reasoning tasks. The study trains families of MoE Transformers, varying total parameters, active parameters, and top-k routing while holding compute constant. Results show memorization benchmarks improve monotonically with total parameters, while reasoning performance saturates and can even regress despite gains in total parameters and training loss. The findings suggest that over-sparse models exhibit a reasoning deficit unrecoverable by GRPO or increased test-time compute, implying the need for careful sparsity tuning in MoE architectures to maximize reasoning ability. |
| Machine Learning | Training Language Model Agents to Find Vulnerabilities with CTF-Dojo (Read more on [arXiv](https://arxiv.org/abs/2508.18370) or [HuggingFace](https://huggingface.co/papers/2508.18370))| Zijian Wang, Varun Kumar, Hantian Ding, Dingmin Wang, terryyz | This paper introduces CTF-DOJO, a large-scale executable runtime environment tailored for training language models (LLMs) in cybersecurity. The research aims to develop scalable and generalizable execution-grounded environments for training ML agents to find vulnerabilities. They propose CTF-FORGE, an automated pipeline, to transform publicly available CTF artifacts into Docker containers. LLM agents trained on CTF-DOJO achieve up to 11.6% absolute gains over baselines and reach 31.9% Pass@1 on CTF benchmarks. This implies that execution-grounded training signals are pivotal for advancing high-performance ML agents in cybersecurity without relying on proprietary systems. |
| Natural Language Processing | QueryBandits for Hallucination Mitigation: Exploiting Semantic Features
  for No-Regret Rewriting (Read more on [arXiv](https://arxiv.org/abs/2508.16697) or [HuggingFace](https://huggingface.co/papers/2508.16697))| Manuela Veloso, Sumitra Ganesh, Alec Koppel, William Watson, Nicole Cho | This paper introduces QueryBandits, a bandit framework for mitigating hallucinations in Large Language Models (LLMs) by proactively rewriting queries. It aims to maximize a reward model, based on the sensitivities of 17 linguistic features, to steer LLMs away from generating hallucinations. The methodology employs a bandit framework that designs rewrite strategies based on the linguistic features of the input query to maximize a reward model. Empirically, the top contextual QueryBandit achieves an 87.5% win rate over a no-rewrite baseline and outperforms static prompting strategies. The main implication is that leveraging semantic features with QueryBandits can induce shifts in output behavior through forward-pass mechanisms, offering an efficient strategy for trustworthy interfacing with LLMs. |
| Computer Vision | ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion
  Models (Read more on [arXiv](https://arxiv.org/abs/2508.18271) or [HuggingFace](https://huggingface.co/papers/2508.18271))| Beiqi Chen, Gangshan Wu, Jie Tang, Jie Liu, Haitang Feng | The paper introduces ObjFiller-3D, a novel framework for consistent multi-view 3D inpainting using video diffusion models. It addresses the challenge of inconsistencies in multi-view 2D image inpainting, which often leads to artifacts in 3D object completion. The method leverages a video editing model adapted for 3D scene inpainting, incorporating a reference-based approach to enhance reconstruction quality. Experiments demonstrate superior performance compared to previous methods, achieving a PSNR of 26.6 versus NeRFiller's 15.9. This approach offers AI practitioners a faster, high-fidelity method for completing and editing 3D objects, surpassing previous approaches that were limited by view constraints and computational cost. |
| Natural Language Processing | Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and
  Reasoning (Read more on [arXiv](https://arxiv.org/abs/2508.19202) or [HuggingFace](https://huggingface.co/papers/2508.19202))| Arman Cohan, Doug Downey, Arpan Sarkar, Yixin Liu, Alan Li | The paper introduces SCIREAS and KRUX to demystify scientific problem-solving in Large Language Models (LLMs). It investigates the distinct roles of knowledge and reasoning in scientific tasks using a novel probing framework. Results show that retrieving task-relevant knowledge from model parameters is a critical bottleneck, and reasoning models consistently benefit from in-context knowledge. Furthermore, enhancing verbalized reasoning improves LLMs' ability to surface task-relevant knowledge.  The findings imply that improvements in knowledge retrieval and utilization, in conjunction with reasoning, can significantly enhance LLMs' scientific problem-solving capabilities. |
| Natural Language Processing | Unraveling the cognitive patterns of Large Language Models through
  module communities (Read more on [arXiv](https://arxiv.org/abs/2508.18192) or [HuggingFace](https://huggingface.co/papers/2508.18192))| Jianxi Gao, Pin-Yu Chen, KBhandari11 | This paper introduces a network-based framework to analyze cognitive skills and modularity in Large Language Models (LLMs). The research aims to understand the inner mechanisms of LLMs by linking cognitive skills, LLM architectures, and datasets. The methodology involves adopting network community structure analysis to uncover interdependencies among cognitive skills and LLM modules, evaluating their contribution to LLM performance. The results show that while LLMs exhibit unique module communities with emergent skill patterns, they do not strictly parallel biological systems; moreover, fine-tuning specific modules did not yield significant accuracy improvements. The study suggests that effective fine-tuning strategies should leverage distributed learning dynamics rather than rigid modular interventions, providing implications for improving LLM interpretability and optimization. |
